<!DOCTYPE html>
<html lang="en">

<head>
    <title>YWL's Arxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                YWL's Arxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-27T00:00:00Z">2024-03-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">51</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3P-LLM: Probabilistic Path Planning using Large Language Model for
  Autonomous Robot Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Latif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Much worldly semantic knowledge can be encoded in large language models
(LLMs). Such information could be of great use to robots that want to carry out
high-level, temporally extended commands stated in natural language. However,
the lack of real-world experience that language models have is a key limitation
that makes it challenging to use them for decision-making inside a particular
embodiment. This research assesses the feasibility of using LLM (GPT-3.5-turbo
chatbot by OpenAI) for robotic path planning. The shortcomings of conventional
approaches to managing complex environments and developing trustworthy plans
for shifting environmental conditions serve as the driving force behind the
research. Due to the sophisticated natural language processing abilities of
LLM, the capacity to provide effective and adaptive path-planning algorithms in
real-time, great accuracy, and few-shot learning capabilities, GPT-3.5-turbo is
well suited for path planning in robotics. In numerous simulated scenarios, the
research compares the performance of GPT-3.5-turbo with that of
state-of-the-art path planners like Rapidly Exploring Random Tree (RRT) and A*.
We observed that GPT-3.5-turbo is able to provide real-time path planning
feedback to the robot and outperforms its counterparts. This paper establishes
the foundation for LLM-powered path planning for robotic systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Exploratory Study</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CaT: Constraints as Terminations for Legged Locomotion Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elliot Chane-Sane, Pierre-Alexandre Leziart, Thomas Flayols, Olivier Stasse, Philippe Souères, Nicolas Mansard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (RL) has demonstrated impressive results in
solving complex robotic tasks such as quadruped locomotion. Yet, current
solvers fail to produce efficient policies respecting hard constraints. In this
work, we advocate for integrating constraints into robot learning and present
Constraints as Terminations (CaT), a novel constrained RL algorithm. Departing
from classical constrained RL formulations, we reformulate constraints through
stochastic terminations during policy learning: any violation of a constraint
triggers a probability of terminating potential future rewards the RL agent
could attain. We propose an algorithmic approach to this formulation, by
minimally modifying widely used off-the-shelf RL algorithms in robot learning
(such as Proximal Policy Optimization). Our approach leads to excellent
constraint adherence without introducing undue complexity and computational
overhead, thus mitigating barriers to broader adoption. Through empirical
evaluation on the real quadruped robot Solo crossing challenging obstacles, we
demonstrate that CaT provides a compelling solution for incorporating
constraints into RL frameworks. Videos and code are available at
https://constraints-as-terminations.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://constraints-as-terminations.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Logic Formalisation of ISO 34502 Critical Scenarios: Modular
  Construction with the RSS Safety Distance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesse Reimann, Nico Mansion, James Haydon, Benjamin Bray, Agnishom Chattopadhyay, Sota Sato, Masaki Waga, Étienne André, Ichiro Hasuo, Naoki Ueda, Yosuke Yokoyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the development of autonomous vehicles progresses, efficient safety
assurance methods become increasingly necessary. Safety assurance methods such
as monitoring and scenario-based testing call for formalisation of driving
scenarios. In this paper, we develop a temporal-logic formalisation of an
important class of critical scenarios in the ISO standard 34502. We use signal
temporal logic (STL) as a logical formalism. Our formalisation has two main
features: 1) modular composition of logical formulas for systematic and
comprehensive formalisation (following the compositional methodology of ISO
34502); 2) use of the RSS distance for defining danger. We find our
formalisation comes with few parameters to tune thanks to the RSS distance. We
experimentally evaluated our formalisation; using its results, we discuss the
validity of our formalisation and its stability with respect to the choice of
some parameter values.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures, 5 tables. Accepted to SAC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ModaLink: Unifying Modalities for Efficient Image-to-PointCloud Place
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weidong Xie, Lun Luo, Nanfei Ye, Yi Ren, Shaoyi Du, Minhang Wang, Jintao Xu, Rui Ai, Weihao Gu, Xieyuanli Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Place recognition is an important task for robots and autonomous cars to
localize themselves and close loops in pre-built maps. While single-modal
sensor-based methods have shown satisfactory performance, cross-modal place
recognition that retrieving images from a point-cloud database remains a
challenging problem. Current cross-modal methods transform images into 3D
points using depth estimation for modality conversion, which are usually
computationally intensive and need expensive labeled data for depth
supervision. In this work, we introduce a fast and lightweight framework to
encode images and point clouds into place-distinctive descriptors. We propose
an effective Field of View (FoV) transformation module to convert point clouds
into an analogous modality as images. This module eliminates the necessity for
depth estimation and helps subsequent modules achieve real-time performance. We
further design a non-negative factorization-based encoder to extract mutually
consistent semantic features between point clouds and images. This encoder
yields more distinctive global descriptors for retrieval. Experimental results
on the KITTI dataset show that our proposed methods achieve state-of-the-art
performance while running in real time. Additional evaluation on the HAOMO
dataset covering a 17 km trajectory further shows the practical generalization
capabilities. We have released the implementation of our methods as open source
at: https://github.com/haomo-ai/ModaLink.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 11 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MLDT: Multi-Level Decomposition for Complex Long-Horizon Robotic Task
  Planning with Open-Source Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yike Wu, Jiatao Zhang, Nan Hu, LanLing Tang, Guilin Qi, Jun Shao, Jie Ren, Wei Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of data-driven AI technology, the application of open-source
large language models (LLMs) in robotic task planning represents a significant
milestone. Recent robotic task planning methods based on open-source LLMs
typically leverage vast task planning datasets to enhance models' planning
abilities. While these methods show promise, they struggle with complex
long-horizon tasks, which require comprehending more context and generating
longer action sequences. This paper addresses this limitation by proposing
MLDT, theMulti-Level Decomposition Task planning method. This method
innovatively decomposes tasks at the goal-level, task-level, and action-level
to mitigate the challenge of complex long-horizon tasks. In order to enhance
open-source LLMs' planning abilities, we introduce a goal-sensitive corpus
generation method to create high-quality training data and conduct instruction
tuning on the generated corpus. Since the complexity of the existing datasets
is not high enough, we construct a more challenging dataset, LongTasks, to
specifically evaluate planning ability on complex long-horizon tasks. We
evaluate our method using various LLMs on four datasets in VirtualHome. Our
results demonstrate a significant performance enhancement in robotic task
planning, showcasing MLDT's effectiveness in overcoming the limitations of
existing methods based on open-source LLMs as well as its practicality in
complex, real-world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PhysicsAssistant: An LLM-Powered Interactive Learning Robot for Physics
  Lab Investigations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Latif, Ramviyas Parasuraman, Xiaoming Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot systems in education can leverage Large language models' (LLMs) natural
language understanding capabilities to provide assistance and facilitate
learning. This paper proposes a multimodal interactive robot (PhysicsAssistant)
built on YOLOv8 object detection, cameras, speech recognition, and chatbot
using LLM to provide assistance to students' physics labs. We conduct a user
study on ten 8th-grade students to empirically evaluate the performance of
PhysicsAssistant with a human expert. The Expert rates the assistants'
responses to student queries on a 0-4 scale based on Bloom's taxonomy to
provide educational support. We have compared the performance of
PhysicsAssistant (YOLOv8+GPT-3.5-turbo) with GPT-4 and found that the human
expert rating of both systems for factual understanding is the same. However,
the rating of GPT-4 for conceptual and procedural knowledge (3 and 3.2 vs 2.2
and 2.6, respectively) is significantly higher than PhysicsAssistant (p <
0.05). However, the response time of GPT-4 is significantly higher than
PhysicsAssistant (3.54 vs 1.64 sec, p < 0.05). Hence, despite the relatively
lower response quality of PhysicsAssistant than GPT-4, it has shown potential
for being used as a real-time lab assistant to provide timely responses and can
offload teachers' labor to assist with repetitive tasks. To the best of our
knowledge, this is the first attempt to build such an interactive multimodal
robotic assistant for K-12 science (physics) education.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE RO-MAN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Efficient Risk-aware Branch MPC for Automated Driving that is Robust
  to Uncertain Vehicle Behaviors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luyao Zhang, George Pantazis, Shaohang Han, Sergio Grammatico
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the critical challenges in automated driving is ensuring safety of
automated vehicles despite the unknown behavior of the other vehicles. Although
motion prediction modules are able to generate a probability distribution
associated with various behavior modes, their probabilistic estimates are often
inaccurate, thus leading to a possibly unsafe trajectory. To overcome this
challenge, we propose a risk-aware motion planning framework that appropriately
accounts for the ambiguity in the estimated probability distribution. We
formulate the risk-aware motion planning problem as a min-max optimization
problem and develop an efficient iterative method by incorporating a
regularization term in the probability update step. Via extensive numerical
studies, we validate the convergence of our method and demonstrate its
advantages compared to the state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teaching Introductory HRI: UChicago Course "Human-Robot Interaction:
  Research and Practice" 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarah Sebo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In 2020, I designed the course CMSC 20630/30630 Human-Robot Interaction:
Research and Practice as a hands-on introduction to human-robot interaction
(HRI) research for both undergraduate and graduate students at the University
of Chicago. Since 2020, I have taught and refined this course each academic
year. Human-Robot Interaction: Research and Practice focuses on the core
concepts and cutting-edge research in the field of human-robot interaction
(HRI), covering topics that include: nonverbal robot behavior, verbal robot
behavior, social dynamics, norms & ethics, collaboration & learning, group
interactions, applications, and future challenges of HRI. Course meetings
involve students in the class leading discussions about cutting-edge
peer-reviewed research HRI publications. Students also participate in a
quarter-long collaborative research project, where they pursue an HRI research
question that often involves conducing their own human-subjects research study
where they recruit human subjects to interact with a robot. In this paper, I
detail the structure of the course and its learning goals as well as my
reflections and student feedback on the course.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 2 tables, Presented at the Designing an Intro to HRI Course
  Workshop at HRI 2024 (arXiv:2403.05588)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sampling-Based Motion Planning with Online Racing Line Generation for
  Autonomous Driving on Three-Dimensional Race Tracks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Levent Ögretmen, Matthias Rowold, Boris Lohmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing approaches to trajectory planning for autonomous racing employ
sampling-based methods, generating numerous jerk-optimal trajectories and
selecting the most favorable feasible trajectory based on a cost function
penalizing deviations from an offline-calculated racing line. While successful
on oval tracks, these methods face limitations on complex circuits due to the
simplistic geometry of jerk-optimal edges failing to capture the complexity of
the racing line. Additionally, they only consider two-dimensional tracks,
potentially neglecting or surpassing the actual dynamic potential. In this
paper, we present a sampling-based local trajectory planning approach for
autonomous racing that can maintain the lap time of the racing line even on
complex race tracks and consider the race track's three-dimensional effects. In
simulative experiments, we demonstrate that our approach achieves lower lap
times and improved utilization of dynamic limits compared to existing
approaches. We also investigate the impact of online racing line generation, in
which the time-optimal solution is planned from the current vehicle state for a
limited spatial horizon, in contrast to a closed racing line calculated
offline. We show that combining the sampling-based planner with the online
racing line generation can significantly reduce lap times in multi-vehicle
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, submitted to be published at the 35th IEEE Intelligent
  Vehicles Symposium, June 2 - 5, 2024, Jeju Shinhwa World, Jeju Island, Korea</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Will You Participate? Exploring the Potential of Robotics Competitions
  on Human-centric Topics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchong Zhang, Miguel Vasco, Mårten Björkman, Danica Kragic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents findings from an exploratory needfinding study
investigating the research current status and potential participation of the
competitions on the robotics community towards four human-centric topics:
safety, privacy, explainability, and federated learning. We conducted a survey
with 34 participants across three distinguished European robotics consortia,
nearly 60% of whom possessed over five years of research experience in
robotics. Our qualitative and quantitative analysis revealed that current
mainstream robotic researchers prioritize safety and explainability, expressing
a greater willingness to invest in further research in these areas. Conversely,
our results indicate that privacy and federated learning garner less attention
and are perceived to have lower potential. Additionally, the study suggests a
lack of enthusiasm within the robotics community for participating in
competitions related to these topics. Based on these findings, we recommend
targeting other communities, such as the machine learning community, for future
competitions related to these four human-centric topics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in
  Instructional Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Zare, Yulei Niu, Hammad Ayyubi, Shih-fu Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedure Planning in instructional videos entails generating a sequence of
action steps based on visual observations of the initial and target states.
Despite the rapid progress in this task, there remain several critical
challenges to be solved: (1) Adaptive procedures: Prior works hold an
unrealistic assumption that the number of action steps is known and fixed,
leading to non-generalizable models in real-world scenarios where the sequence
length varies. (2) Temporal relation: Understanding the step temporal relation
knowledge is essential in producing reasonable and executable plans. (3)
Annotation cost: Annotating instructional videos with step-level labels (i.e.,
timestamp) or sequence-level labels (i.e., action category) is demanding and
labor-intensive, limiting its generalizability to large-scale datasets.In this
work, we propose a new and practical setting, called adaptive procedure
planning in instructional videos, where the procedure length is not fixed or
pre-determined. To address these challenges we introduce Retrieval-Augmented
Planner (RAP) model. Specifically, for adaptive procedures, RAP adaptively
determines the conclusion of actions using an auto-regressive model
architecture. For temporal relation, RAP establishes an external memory module
to explicitly retrieve the most relevant state-action pairs from the training
videos and revises the generated procedures. To tackle high annotation cost,
RAP utilizes a weakly-supervised learning manner to expand the training dataset
to other task-relevant, unannotated videos by generating pseudo labels for
action steps. Experiments on CrossTask and COIN benchmarks show the superiority
of RAP over traditional fixed-length models, establishing it as a strong
baseline solution for adaptive procedure planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 6 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Heatmap-Guided 6-Dof Grasp Detection in Cluttered Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siang Chen, Wei Tang, Pengwei Xie, Wenming Yang, Guijin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fast and robust object grasping in clutter is a crucial component of
robotics. Most current works resort to the whole observed point cloud for 6-Dof
grasp generation, ignoring the guidance information excavated from global
semantics, thus limiting high-quality grasp generation and real-time
performance. In this work, we show that the widely used heatmaps are
underestimated in the efficiency of 6-Dof grasp generation. Therefore, we
propose an effective local grasp generator combined with grasp heatmaps as
guidance, which infers in a global-to-local semantic-to-point way.
Specifically, Gaussian encoding and the grid-based strategy are applied to
predict grasp heatmaps as guidance to aggregate local points into graspable
regions and provide global semantic information. Further, a novel non-uniform
anchor sampling mechanism is designed to improve grasp accuracy and diversity.
Benefiting from the high-efficiency encoding in the image space and focusing on
points in local graspable regions, our framework can perform high-quality grasp
detection in real-time and achieve state-of-the-art results. In addition, real
robot experiments demonstrate the effectiveness of our method with a success
rate of 94% and a clutter completion rate of 100%. Our code is available at
https://github.com/THU-VCLab/HGGD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extensive results on GraspNet-1B dataset</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the Gap: Regularized Reinforcement Learning for Improved
  Classical Motion Planning with Safety Modules 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elias Goldsztejn, Ronen I. Brafman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classical navigation planners can provide safe navigation, albeit often
suboptimally and with hindered human norm compliance. ML-based, contemporary
autonomous navigation algorithms can imitate more natural and humancompliant
navigation, but usually require large and realistic datasets and do not always
provide safety guarantees. We present an approach that leverages a classical
algorithm to guide reinforcement learning. This greatly improves the results
and convergence rate of the underlying RL algorithm and requires no
human-expert demonstrations to jump-start the process. Additionally, we
incorporate a practical fallback system that can switch back to a classical
planner to ensure safety. The outcome is a sample efficient ML approach for
mobile navigation that builds on classical algorithms, improves them to ensure
human compliance, and guarantees safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoBOS: Constraint-Based Online Scheduler for Human-Robot Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marina Ionova, Jan Kristof Behrens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assembly processes involving humans and robots are challenging scenarios
because the individual activities and access to shared workspace have to be
coordinated. Fixed robot programs leave no room to diverge from a fixed
protocol. Working on such a process can be stressful for the user and lead to
ineffective behavior or failure. We propose a novel approach of online
constraint-based scheduling in a reactive execution control framework
facilitating behavior trees called CoBOS. This allows the robot to adapt to
uncertain events such as delayed activity completions and activity selection
(by the human). The user will experience less stress as the robotic coworkers
adapt their behavior to best complement the human-selected activities to
complete the common task. In addition to the improved working conditions, our
algorithm leads to increased efficiency, even in highly uncertain scenarios. We
evaluate our algorithm using a probabilistic simulation study with 56000
experiments. We outperform all baselines by a margin of 4-10%. Initial real
robot experiments using a Franka Emika Panda robot and human tracking based on
HTC Vive VR gloves look promising.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inverse kinematics learning of a continuum manipulator using limited
  real time data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alok Ranjan Sahoo, Pavan Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data driven control of a continuum manipulator requires a lot of data for
training but generating sufficient amount of real time data is not cost
efficient. Random actuation of the manipulator can also be unsafe sometimes.
Meta learning has been used successfully to adapt to a new environment. Hence,
this paper tries to solve the above mentioned problem using meta learning. We
consider two cases for that. First, this paper proposes a method to use
simulation data for training the model using MAML(Model-Agnostic
Meta-Learning). Then, it adapts to the real world using gradient steps.
Secondly,if the simulation model is not available or difficult to formulate,
then we propose a CGAN(Conditional Generative adversial network)-MAML based
method for it. The model is trained using a small amount of real time data and
augmented data for different loading conditions. Then, adaptation is done in
the real environment. It has been found out from the experiments that the
relative positioning error for both the cases are below 3%. The proposed models
are experimentally verified on a real continuum manipulator.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inhwan Bae, Young-Jae Park, Hae-Gon Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are five types of trajectory prediction tasks: deterministic,
stochastic, domain adaptation, momentary observation, and few-shot. These
associated tasks are defined by various factors, such as the length of input
paths, data split and pre-processing methods. Interestingly, even though they
commonly take sequential coordinates of observations as input and infer future
paths in the same coordinates as output, designing specialized architectures
for each task is still necessary. For the other task, generality issues can
lead to sub-optimal performances. In this paper, we propose SingularTrajectory,
a diffusion-based universal trajectory prediction framework to reduce the
performance gap across the five tasks. The core of SingularTrajectory is to
unify a variety of human dynamics representations on the associated tasks. To
do this, we first build a Singular space to project all types of motion
patterns from each task into one embedding space. We next propose an adaptive
anchor working in the Singular space. Unlike traditional fixed anchor methods
that sometimes yield unacceptable paths, our adaptive anchor enables correct
anchors, which are put into a wrong location, based on a traversability map.
Finally, we adopt a diffusion-based predictor to further enhance the prototype
paths using a cascaded denoising process. Our unified framework ensures the
generality across various benchmark settings such as input modality, and
trajectory lengths. Extensive experiments on five public benchmarks demonstrate
that SingularTrajectory substantially outperforms existing models, highlighting
its effectiveness in estimating general dynamics of human movements. Code is
publicly available at https://github.com/inhwanbae/SingularTrajectory .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Language Beat Numerical Regression? Language-Based Multimodal
  Trajectory Prediction <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inhwan Bae, Junoh Lee, Hae-Gon Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have demonstrated impressive ability in context understanding
and generative performance. Inspired by the recent success of language
foundation models, in this paper, we propose LMTraj (Language-based Multimodal
Trajectory predictor), which recasts the trajectory prediction task into a sort
of question-answering problem. Departing from traditional numerical regression
models, which treat the trajectory coordinate sequence as continuous signals,
we consider them as discrete signals like text prompts. Specially, we first
transform an input space for the trajectory coordinate into the natural
language space. Here, the entire time-series trajectories of pedestrians are
converted into a text prompt, and scene images are described as text
information through image captioning. The transformed numerical and image data
are then wrapped into the question-answering template for use in a language
model. Next, to guide the language model in understanding and reasoning
high-level knowledge, such as scene context and social relationships between
pedestrians, we introduce an auxiliary multi-task question and answering. We
then train a numerical tokenizer with the prompt data. We encourage the
tokenizer to separate the integer and decimal parts well, and leverage it to
capture correlations between the consecutive numbers in the language model.
Lastly, we train the language model using the numerical tokenizer and all of
the question-answer prompts. Here, we propose a beam-search-based most-likely
prediction and a temperature-based multimodal prediction to implement both
deterministic and stochastic inferences. Applying our LMTraj, we show that the
language-based model can be a powerful pedestrian trajectory predictor, and
outperforms existing numerical-based predictor methods. Code is publicly
available at https://github.com/inhwanbae/LMTrajectory .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyRRT-Connect: A Bidirectional Rapidly-Exploring Random Trees Motion
  Planning Algorithm for Hybrid Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Wang, Ricardo G. Sanfelice
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a bidirectional rapidly-exploring random trees (RRT)
algorithm to solve the motion planning problem for hybrid systems. The proposed
algorithm, called HyRRT-Connect, propagates in both forward and backward
directions in hybrid time until an overlap between the forward and backward
propagation results is detected. Then, HyRRT-Connect constructs a motion plan
through the reversal and concatenation of functions defined on hybrid time
domains, ensuring the motion plan thoroughly satisfies the given hybrid
dynamics. To address the potential discontinuity along the flow caused by
tolerating some distance between the forward and backward partial motion plans,
we reconstruct the backward partial motion plan by a forward-in-hybrid-time
simulation from the final state of the forward partial motion plan. By applying
the reversed input of the backward partial motion plan, the reconstruction
process effectively eliminates the discontinuity and ensures that as the
tolerance distance decreases to zero, the distance between the endpoint of the
reconstructed motion plan and the final state set approaches zero. The proposed
algorithm is applied to an actuated bouncing ball example and a walking robot
example so as to highlight its generality and computational improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 8th IFAC International Conference on Analysis and
  Design of Hybrid Systems (ADHS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extensible Hook System for Rendesvouz and Docking of a Cubesat Swarm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos J. Pérez-del-Pulgar, Antonio López-Palomeque, Jesús Juli, Matteo Madi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of cubesat swarms is being proposed for different missions where
cooperation between satellites is required. Commonly, the cube swarm requires
formation flight and even rendezvous and docking, which are very challenging
tasks since they required more energy and the use of advanced guidance,
navigation and control techniques. In this paper, we propose the use of an
extensible hook system to mitigate these drawbacks,i.e. it allows to save fuel
and reduce the system complexity by including techniques that have been
previously demonstrated on Earth. This system is based on a scissor boom
structure, which could reach up to five meters for a 4U dimension, including
three degrees of freedom to place the end effector at any pose within the
system workspace. We simulated the dynamic behaviour of a cubesat with the
proposed system, demonstrating the required power for a 16U cubesat equipped
with one extensible hook system is considered acceptable according to the
current state of the art actuators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Imaging radar and <span class="highlight-title">LiDAR</span> image translation for 3-DOF extrinsic
  calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangwoo Jung, Hyesu Jang, Minwoo Jung, Ayoung Kim, Myung-Hwan Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of sensor data is crucial in the field of robotics to take
full advantage of the various sensors employed. One critical aspect of this
integration is determining the extrinsic calibration parameters, such as the
relative transformation, between each sensor. The use of data fusion between
complementary sensors, such as radar and LiDAR, can provide significant
benefits, particularly in harsh environments where accurate depth data is
required. However, noise included in radar sensor data can make the estimation
of extrinsic calibration challenging. To address this issue, we present a novel
framework for the extrinsic calibration of radar and LiDAR sensors, utilizing
CycleGAN as amethod of image-to-image translation. Our proposed method employs
translating radar bird-eye-view images into LiDAR-style images to estimate the
3-DOF extrinsic parameters. The use of image registration techniques, as well
as deskewing based on sensor odometry and B-spline interpolation, is employed
to address the rolling shutter effect commonly present in spinning sensors. Our
method demonstrates a notable improvement in extrinsic calibration compared to
filter-based methods using the MulRan dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RoboKeyGen: Robot Pose and Joint Angles Estimation via Diffusion-based
  3D Keypoint Generation <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Tian, Jiyao Zhang, Guowei Huang, Bin Wang, Ping Wang, Jiangmiao Pang, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating robot pose and joint angles is significant in advanced robotics,
enabling applications like robot collaboration and online hand-eye
calibration.However, the introduction of unknown joint angles makes prediction
more complex than simple robot pose estimation, due to its higher
dimensionality.Previous methods either regress 3D keypoints directly or utilise
a render&compare strategy. These approaches often falter in terms of
performance or efficiency and grapple with the cross-camera gap problem.This
paper presents a novel framework that bifurcates the high-dimensional
prediction task into two manageable subtasks: 2D keypoints detection and
lifting 2D keypoints to 3D. This separation promises enhanced performance
without sacrificing the efficiency innate to keypoint-based techniques.A vital
component of our method is the lifting of 2D keypoints to 3D keypoints. Common
deterministic regression methods may falter when faced with uncertainties from
2D detection errors or self-occlusions.Leveraging the robust modeling potential
of diffusion models, we reframe this issue as a conditional 3D keypoints
generation task. To bolster cross-camera adaptability, we introduce
theNormalised Camera Coordinate Space (NCCS), ensuring alignment of estimated
2D keypoints across varying camera intrinsics.Experimental results demonstrate
that the proposed method outperforms the state-of-the-art render\&compare
method and achieves higher inference speed.Furthermore, the tests accentuate
our method's robust cross-camera generalisation capabilities.We intend to
release both the dataset and code in https://nimolty.github.io/Robokeygen/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Manipulating Neural Path Planners via Slight Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikang Xiong, Suresh Jagannathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven neural path planners are attracting increasing interest in the
robotics community. However, their neural network components typically come as
black boxes, obscuring their underlying decision-making processes. Their
black-box nature exposes them to the risk of being compromised via the
insertion of hidden malicious behaviors. For example, an attacker may hide
behaviors that, when triggered, hijack a delivery robot by guiding it to a
specific (albeit wrong) destination, trapping it in a predefined region, or
inducing unnecessary energy expenditure by causing the robot to repeatedly
circle a region. In this paper, we propose a novel approach to specify and
inject a range of hidden malicious behaviors, known as backdoors, into neural
path planners. Our approach provides a concise but flexible way to define these
behaviors, and we show that hidden behaviors can be triggered by slight
perturbations (e.g., inserting a tiny unnoticeable object), that can
nonetheless significantly compromise their integrity. We also discuss potential
techniques to identify these backdoors aimed at alleviating such risks. We
demonstrate our approach on both sampling-based and search-based neural path
planners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-AGV Path Planning Method via Reinforcement Learning and Particle
  Filters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shao Shuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Reinforcement Learning (RL) algorithm, renowned for its robust learning
capability and search stability, has garnered significant attention and found
extensive application in Automated Guided Vehicle (AGV) path planning. However,
RL planning algorithms encounter challenges stemming from the substantial
variance of neural networks caused by environmental instability and significant
fluctuations in system structure. These challenges manifest in slow convergence
speed and low learning efficiency. To tackle this issue, this paper presents
the Particle Filter-Double Deep Q-Network (PF-DDQN) approach, which
incorporates the Particle Filter (PF) into multi-AGV reinforcement learning
path planning. The PF-DDQN method leverages the imprecise weight values of the
network as state values to formulate the state space equation. Through the
iterative fusion process of neural networks and particle filters, the DDQN
model is optimized to acquire the optimal true weight values, thus enhancing
the algorithm's efficiency. The proposed method's effectiveness and superiority
are validated through numerical simulations. Overall, the simulation results
demonstrate that the proposed algorithm surpasses the traditional DDQN
algorithm in terms of path planning superiority and training time indicators by
92.62% and 76.88%, respectively. In conclusion, the PF-DDQN method addresses
the challenges encountered by RL planning algorithms in AGV path planning. By
integrating the Particle Filter and optimizing the DDQN model, the proposed
method achieves enhanced efficiency and outperforms the traditional DDQN
algorithm in terms of path planning superiority and training time indicators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-Aware Deployment of Pre-trained Language-Conditioned
  Imitation Learning Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Wu, Bruce D. Lee, Kostas Daniilidis, Bernadette Bucher, Nikolai Matni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale robotic policies trained on data from diverse tasks and robotic
platforms hold great promise for enabling general-purpose robots; however,
reliable generalization to new environment conditions remains a major
challenge. Toward addressing this challenge, we propose a novel approach for
uncertainty-aware deployment of pre-trained language-conditioned imitation
learning agents. Specifically, we use temperature scaling to calibrate these
models and exploit the calibrated model to make uncertainty-aware decisions by
aggregating the local information of candidate actions. We implement our
approach in simulation using three such pre-trained models, and showcase its
potential to significantly enhance task completion rates. The accompanying code
is accessible at the link:
https://github.com/BobWu1998/uncertainty_quant_all.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preference-Based Planning in Stochastic Environments: From
  Partially-Ordered Temporal Goals to Most Preferred Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hazhar Rahmani, Abhishek N. Kulkarni, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human preferences are not always represented via complete linear orders: It
is natural to employ partially-ordered preferences for expressing incomparable
outcomes. In this work, we consider decision-making and probabilistic planning
in stochastic systems modeled as Markov decision processes (MDPs), given a
partially ordered preference over a set of temporally extended goals.
Specifically, each temporally extended goal is expressed using a formula in
Linear Temporal Logic on Finite Traces (LTL$_f$). To plan with the partially
ordered preference, we introduce order theory to map a preference over temporal
goals to a preference over policies for the MDP. Accordingly, a most preferred
policy under a stochastic ordering induces a stochastic nondominated
probability distribution over the finite paths in the MDP. To synthesize a most
preferred policy, our technical approach includes two key steps. In the first
step, we develop a procedure to transform a partially ordered preference over
temporal goals into a computational model, called preference automaton, which
is a semi-automaton with a partial order over acceptance conditions. In the
second step, we prove that finding a most preferred policy is equivalent to
computing a Pareto-optimal policy in a multi-objective MDP that is constructed
from the original MDP, the preference automaton, and the chosen stochastic
ordering relation. Throughout the paper, we employ running examples to
illustrate the proposed preference specification and solution approaches. We
demonstrate the efficacy of our algorithm using these examples, providing
detailed analysis, and then discuss several potential future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2209.12267</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long and Short-Term Constraints Driven Safe Reinforcement Learning for
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuemin Hu, Pan Chen, Yijun Wen, Bo Tang, Long Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has been widely used in decision-making tasks,
but it cannot guarantee the agent's safety in the training process due to the
requirements of interaction with the environment, which seriously limits its
industrial applications such as autonomous driving. Safe RL methods are
developed to handle this issue by constraining the expected safety violation
costs as a training objective, but they still permit unsafe state occurrence,
which is unacceptable in autonomous driving tasks. Moreover, these methods are
difficult to achieve a balance between the cost and return expectations, which
leads to learning performance degradation for the algorithms. In this paper, we
propose a novel algorithm based on the long and short-term constraints (LSTC)
for safe RL. The short-term constraint aims to guarantee the short-term state
safety that the vehicle explores, while the long-term constraint ensures the
overall safety of the vehicle throughout the decision-making process. In
addition, we develop a safe RL method with dual-constraint optimization based
on the Lagrange multiplier to optimize the training process for end-to-end
autonomous driving. Comprehensive experiments were conducted on the MetaDrive
simulator. Experimental results demonstrate that the proposed method achieves
higher safety in continuous state and action tasks, and exhibits higher
exploration performance in long-distance decision-making tasks compared with
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Road Obstacle Detection based on Unknown Objectness Scores <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chihiro Noguchi, Toshiaki Ohgushi, Masao Yamanaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection of unknown traffic obstacles is vital to ensure safe autonomous
driving. The standard object-detection methods cannot identify unknown objects
that are not included under predefined categories. This is because
object-detection methods are trained to assign a background label to pixels
corresponding to the presence of unknown objects. To address this problem, the
pixel-wise anomaly-detection approach has attracted increased research
attention. Anomaly-detection techniques, such as uncertainty estimation and
perceptual difference from reconstructed images, make it possible to identify
pixels of unknown objects as out-of-distribution (OoD) samples. However, when
applied to images with many unknowns and complex components, such as driving
scenes, these methods often exhibit unstable performance. The purpose of this
study is to achieve stable performance for detecting unknown objects by
incorporating the object-detection fashions into the pixel-wise anomaly
detection methods. To achieve this goal, we adopt a semantic-segmentation
network with a sigmoid head that simultaneously provides pixel-wise anomaly
scores and objectness scores. Our experimental results show that the objectness
scores play an important role in improving the detection performance. Based on
these results, we propose a novel anomaly score by integrating these two
scores, which we term as unknown objectness score. Quantitative evaluations
show that the proposed method outperforms state-of-the-art methods when applied
to the publicly available datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sailing Through Point Clouds: Safe Navigation Using Point Cloud Based
  Control Barrier Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bolun Dai, Rooholla Khorrambakht, Prashanth Krishnamurthy, Farshad Khorrami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capability to navigate safely in an unstructured environment is crucial
when deploying robotic systems in real-world scenarios. Recently, control
barrier function (CBF) based approaches have been highly effective in
synthesizing safety-critical controllers. In this work, we propose a novel
CBF-based local planner comprised of two components: Vessel and Mariner. The
Vessel is a novel scaling factor based CBF formulation that synthesizes CBFs
using only point cloud data. The Mariner is a CBF-based preview control
framework that is used to mitigate getting stuck in spurious equilibria during
navigation. To demonstrate the efficacy of our proposed approach, we first
compare the proposed point cloud based CBF formulation with other point cloud
based CBF formulations. Then, we demonstrate the performance of our proposed
approach and its integration with global planners using experimental studies on
the Unitree B1 and Unitree Go2 quadruped robots in various environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LocoMan: Advancing Versatile Quadrupedal Dexterity with Lightweight
  Loco-Manipulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changyi Lin, Xingyu Liu, Yuxiang Yang, Yaru Niu, Wenhao Yu, Tingnan Zhang, Jie Tan, Byron Boots, Ding Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quadrupedal robots have emerged as versatile agents capable of locomoting and
manipulating in complex environments. Traditional designs typically rely on the
robot's inherent body parts or incorporate top-mounted arms for manipulation
tasks. However, these configurations may limit the robot's operational
dexterity, efficiency and adaptability, particularly in cluttered or
constrained spaces. In this work, we present LocoMan, a dexterous quadrupedal
robot with a novel morphology to perform versatile manipulation in diverse
constrained environments. By equipping a Unitree Go1 robot with two low-cost
and lightweight modular 3-DoF loco-manipulators on its front calves, LocoMan
leverages the combined mobility and functionality of the legs and grippers for
complex manipulation tasks that require precise 6D positioning of the end
effector in a wide workspace. To harness the loco-manipulation capabilities of
LocoMan, we introduce a unified control framework that extends the whole-body
controller (WBC) to integrate the dynamics of loco-manipulators. Through
experiments, we validate that the proposed whole-body controller can accurately
and stably follow desired 6D trajectories of the end effector and torso, which,
when combined with the large workspace from our design, facilitates a diverse
set of challenging dexterous loco-manipulation tasks in confined spaces, such
as opening doors, plugging into sockets, picking objects in narrow and
low-lying spaces, and bimanual manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://linchangyi1.github.io/LocoMan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCANet: Correcting LEGO Assembly Errors with Self-Correct Assembly
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Wan, Kaichen Zhou, jinhong Chen, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous assembly in robotics and 3D vision presents significant
challenges, particularly in ensuring assembly correctness. Presently,
predominant methods such as MEPNet focus on assembling components based on
manually provided images. However, these approaches often fall short in
achieving satisfactory results for tasks requiring long-term planning.
Concurrently, we observe that integrating a self-correction module can
partially alleviate such issues. Motivated by this concern, we introduce the
single-step assembly error correction task, which involves identifying and
rectifying misassembled components. To support research in this area, we
present the LEGO Error Correction Assembly Dataset (LEGO-ECA), comprising
manual images for assembly steps and instances of assembly failures.
Additionally, we propose the Self-Correct Assembly Network (SCANet), a novel
method to address this task. SCANet treats assembled components as queries,
determining their correctness in manual images and providing corrections when
necessary. Finally, we utilize SCANet to correct the assembly results of
MEPNet. Experimental results demonstrate that SCANet can identify and correct
MEPNet's misassembled results, significantly improving the correctness of
assembly. Our code and dataset are available at
https://github.com/Yaser-wyx/SCANet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Embedding Multi-Scale CLIP Features into 3D Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shun Taguchi, Hideki Deguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces a novel approach to online embedding of multi-scale
CLIP (Contrastive Language-Image Pre-Training) features into 3D maps. By
harnessing CLIP, this methodology surpasses the constraints of conventional
vocabulary-limited methods and enables the incorporation of semantic
information into the resultant maps. While recent approaches have explored the
embedding of multi-modal features in maps, they often impose significant
computational costs, lacking practicality for exploring unfamiliar environments
in real time. Our approach tackles these challenges by efficiently computing
and embedding multi-scale CLIP features, thereby facilitating the exploration
of unfamiliar environments through real-time map generation. Moreover, the
embedding CLIP features into the resultant maps makes offline retrieval via
linguistic queries feasible. In essence, our approach simultaneously achieves
real-time object search and mapping of unfamiliar environments. Additionally,
we propose a zero-shot object-goal navigation system based on our mapping
approach, and we validate its efficacy through object-goal navigation, offline
object retrieval, and multi-object-goal navigation in both simulated
environments and real robot experiments. The findings demonstrate that our
method not only exhibits swifter performance than state-of-the-art mapping
methods but also surpasses them in terms of the success rate of object-goal
navigation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision-Based Force Estimation for Minimally Invasive Telesurgery Through
  Contact Detection and Local Stiffness Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyuan Yang, My H. Le, Kyle R. Golobish, Juan C. Beaver, Zonghe Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In minimally invasive telesurgery, obtaining accurate force information is
difficult due to the complexities of in-vivo end effector force sensing. This
constrains development and implementation of haptic feedback and force-based
automated performance metrics, respectively. Vision-based force sensing
approaches using deep learning are a promising alternative to intrinsic end
effector force sensing. However, they have limited ability to generalize to
novel scenarios, and require learning on high-quality force sensor training
data that can be difficult to obtain. To address these challenges, this paper
presents a novel vision-based contact-conditional approach for force estimation
in telesurgical environments. Our method leverages supervised learning with
human labels and end effector position data to train deep neural networks.
Predictions from these trained models are optionally combined with robot joint
torque information to estimate forces indirectly from visual data. We benchmark
our method against ground truth force sensor data and demonstrate generality by
fine-tuning to novel surgical scenarios in a data-efficient manner. Our methods
demonstrated greater than 90% accuracy on contact detection and less than 10%
force prediction error. These results suggest potential usefulness of
contact-conditional force estimation for sensory substitution haptic feedback
and tissue handling skill evaluation in clinical settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint of an article accepted in Journal of Medical Robotics
  Research \copyright 2024 copyright World Scientific Publishing Company</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SOAC: Spatio-Temporal Overlap-Aware Multi-Sensor Calibration using
  Neural Radiance Fields <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15803v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15803v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quentin Herau, Nathan Piasco, Moussab Bennehar, Luis Roldão, Dzmitry Tsishkou, Cyrille Migniot, Pascal Vasseur, Cédric Demonceaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In rapidly-evolving domains such as autonomous driving, the use of multiple
sensors with different modalities is crucial to ensure high operational
precision and stability. To correctly exploit the provided information by each
sensor in a single common frame, it is essential for these sensors to be
accurately calibrated. In this paper, we leverage the ability of Neural
Radiance Fields (NeRF) to represent different sensors modalities in a common
volumetric representation to achieve robust and accurate spatio-temporal sensor
calibration. By designing a partitioning approach based on the visible part of
the scene for each sensor, we formulate the calibration problem using only the
overlapping areas. This strategy results in a more robust and accurate
calibration that is less prone to failure. We demonstrate that our approach
works on outdoor urban scenes by validating it on multiple established driving
datasets. Results show that our method is able to get better accuracy and
robustness compared to existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024. Project page: https://qherau.github.io/SOAC/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sim-to-Real gap in RL: Use Case with TIAGo and Isaac Sim/Gym 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07091v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07091v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaume Albardaner, Alberto San Miguel, Néstor García, Magí Dalmau-Moreno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores policy-learning approaches in the context of sim-to-real
transfer for robotic manipulation using a TIAGo mobile manipulator, focusing on
two state-of-art simulators, Isaac Gym and Isaac Sim, both developed by Nvidia.
Control architectures are discussed, with a particular emphasis on achieving
collision-less movement in both simulation and the real environment. Presented
results demonstrate successful sim-to-real transfer, showcasing similar
movements executed by an RL-trained model in both simulated and real setups.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ERF24 workshop "Towards Efficient and Portable Robot
  Learning for Real-World Settings". To be published in Springer Proceedings in
  Advanced Robotics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling and Control of Intrinsically Elasticity Coupled Soft-Rigid
  Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.05362v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.05362v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zach J. Patterson, Cosimo Della Santina, Daniela Rus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While much work has been done recently in the realm of model-based control of
soft robots and soft-rigid hybrids, most works examine robots that have an
inherently serial structure. While these systems have been prevalent in the
literature, there is an increasing trend toward designing soft-rigid hybrids
with intrinsically coupled elasticity between various degrees of freedom. In
this work, we seek to address the issues of modeling and controlling such
structures, particularly when underactuated. We introduce several simple models
for elastic coupling, typical of those seen in these systems. We then propose a
controller that compensates for the elasticity, and we prove its stability with
Lyapunov methods without relying on the elastic dominance assumption. This
controller is applicable to the general class of underactuated soft robots.
After evaluating the controller in simulated cases, we then develop a simple
hardware platform to evaluate both the models and the controller. Finally,
using the hardware, we demonstrate a novel use case for underactuated,
elastically coupled systems in "sensorless" force control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe Control for Soft-Rigid Robots with Self-Contact using Control
  Barrier Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03189v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03189v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zach J. Patterson, Wei Xiao, Emily Sologuren, Daniela Rus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating both flexible and rigid components in robot designs offers a
unique solution to the limitations of traditional rigid robotics by enabling
both compliance and strength. This paper explores the challenges and solutions
for controlling soft-rigid hybrid robots, particularly addressing the issue of
self-contact. Conventional control methods prioritize precise state tracking,
inadvertently increasing the system's overall stiffness, which is not always
desirable in interactions with the environment or within the robot itself. To
address this, we investigate the application of Control Barrier Functions
(CBFs) and High Order CBFs to manage self-contact scenarios in serially
connected soft-rigid hybrid robots. Through an analysis based on Piecewise
Constant Curvature (PCC) kinematics, we establish CBFs within a classical
control framework for self-contact dynamics. Our methodology is rigorously
evaluated in both simulation environments and physical hardware systems. The
findings demonstrate that our proposed control strategy effectively regulates
self-contact in soft-rigid hybrid robotic systems, marking a significant
advancement in the field of robotics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures, submitted to IEEE Robosoft 2024 Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DRIVE: Data-driven Robot Input Vector Exploration <span class="chip">ICRA2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10718v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10718v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominic Baril, Simon-Pierre Deschênes, Luc Coupal, Cyril Goffin, Julien Lépine, Philippe Giguère, François Pomerleau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An accurate motion model is a fundamental component of most autonomous
navigation systems. While much work has been done on improving model
formulation, no standard protocol exists for gathering empirical data required
to train models. In this work, we address this issue by proposing Data-driven
Robot Input Vector Exploration (DRIVE), a protocol that enables characterizing
uncrewed ground vehicles (UGVs) input limits and gathering empirical model
training data. We also propose a novel learned slip approach outperforming
similar acceleration learning approaches. Our contributions are validated
through an extensive experimental evaluation, cumulating over 7 km and 1.8 h of
driving data over three distinct UGVs and four terrain types. We show that our
protocol offers increased predictive performance over common human-driven
data-gathering protocols. Furthermore, our protocol converges with 46 s of
training data, almost four times less than the shortest human dataset gathering
protocol. We show that the operational limit for our model is reached in
extreme slip conditions encountered on surfaced ice. DRIVE is an efficient way
of characterizing UGV motion in its operational conditions. Our code and
dataset are both available online at this link:
https://github.com/norlab-ulaval/DRIVE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, 1 table, accepted for publication at the 2024
  IEEE International Conference on Robotics and Automation (ICRA2024),
  Yokohama, Japan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Frontier-Based Exploration for Multi-Robot Rendezvous in
  Communication-Restricted Unknown Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11617v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11617v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mauro Tellaroli, Matteo Luperto, Michele Antonazzi, Nicola Basilico
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-robot rendezvous and exploration are fundamental challenges in the
domain of mobile robotic systems. This paper addresses multi-robot rendezvous
within an initially unknown environment where communication is only possible
after the rendezvous. Traditionally, exploration has been focused on rapidly
mapping the environment, often leading to suboptimal rendezvous performance in
later stages. We adapt a standard frontier-based exploration technique to
integrate exploration and rendezvous into a unified strategy, with a mechanism
that allows robots to re-visit previously explored regions thus enhancing
rendezvous opportunities. We validate our approach in 3D realistic simulations
using ROS, showcasing its effectiveness in achieving faster rendezvous times
compared to exploration strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Natural-artificial hybrid swarm: Cyborg-insect group navigation in
  unknown obstructed soft terrain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17392v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17392v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Bai, Phuoc Thanh Tran Ngoc, Huu Duoc Nguyen, Duc Long Le, Quang Huy Ha, Kazuki Kai, Yu Xiang See To, Yaosheng Deng, Jie Song, Naoki Wakamiya, Hirotaka Sato, Masaki Ogura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigating multi-robot systems in complex terrains has always been a
challenging task. This is due to the inherent limitations of traditional robots
in collision avoidance, adaptation to unknown environments, and sustained
energy efficiency. In order to overcome these limitations, this research
proposes a solution by integrating living insects with miniature electronic
controllers to enable robotic-like programmable control, and proposing a novel
control algorithm for swarming. Although these creatures, called cyborg
insects, have the ability to instinctively avoid collisions with neighbors and
obstacles while adapting to complex terrains, there is a lack of literature on
the control of multi-cyborg systems. This research gap is due to the difficulty
in coordinating the movements of a cyborg system under the presence of insects'
inherent individual variability in their reactions to control input. In
response to this issue, we propose a novel swarm navigation algorithm
addressing these challenges. The effectiveness of the algorithm is demonstrated
through an experimental validation in which a cyborg swarm was successfully
navigated through an unknown sandy field with obstacles and hills. This
research contributes to the domain of swarm robotics and showcases the
potential of integrating biological organisms with robotics and control theory
to create more intelligent autonomous systems with real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Polygonal Cone Control Barrier Functions (PolyC2BF) for safe navigation
  in cluttered environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08787v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08787v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manan Tayal, Shishir Kolathaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In fields such as mining, search and rescue, and archaeological exploration,
ensuring real-time, collision-free navigation of robots in confined, cluttered
environments is imperative. Despite the value of established path planning
algorithms, they often face challenges in convergence rates and handling
dynamic infeasibilities. Alternative techniques like collision cones struggle
to accurately represent complex obstacle geometries. This paper introduces a
novel category of control barrier functions, known as Polygonal Cone Control
Barrier Function (PolyC2BF), which addresses overestimation and computational
complexity issues. The proposed PolyC2BF, formulated as a Quadratic Programming
(QP) problem, proves effective in facilitating collision-free movement of
multiple robots in complex environments. The efficacy of this approach is
further demonstrated through PyBullet simulations on quadruped (unicycle
model), and crazyflie 2.1 (quadrotor model) in cluttered environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 Pages, 6 Figures. Accepted at European Control Conference (ECC)
  2024. arXiv admin note: text overlap with arXiv:2303.15871</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Risk-aware Control for Robots with Non-Gaussian Belief Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12857v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12857v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matti Vahs, Jana Tumova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of safety-critical control of autonomous
robots, considering the ubiquitous uncertainties arising from unmodeled
dynamics and noisy sensors. To take into account these uncertainties,
probabilistic state estimators are often deployed to obtain a belief over
possible states. Namely, Particle Filters (PFs) can handle arbitrary
non-Gaussian distributions in the robot's state. In this work, we define the
belief state and belief dynamics for continuous-discrete PFs and construct safe
sets in the underlying belief space. We design a controller that provably keeps
the robot's belief state within this safe set. As a result, we ensure that the
risk of the unknown robot's state violating a safety specification, such as
avoiding a dangerous area, is bounded. We provide an open-source implementation
as a ROS2 package and evaluate the solution in simulations and hardware
experiments involving high-dimensional belief spaces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Quadruped Locomotion Using Differentiable Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14864v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14864v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunlong Song, Sangbae Kim, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While most recent advancements in legged robot control have been driven by
model-free reinforcement learning, we explore the potential of differentiable
simulation. Differentiable simulation promises faster convergence and more
stable training by computing low-variant first-order gradients using the robot
model, but so far, its use for legged robot control has remained limited to
simulation. The main challenge with differentiable simulation lies in the
complex optimization landscape of robotic tasks due to discontinuities in
contact-rich environments, e.g., quadruped locomotion. This work proposes a
new, differentiable simulation framework to overcome these challenges. The key
idea involves decoupling the complex whole-body simulation, which may exhibit
discontinuities due to contact, into two separate continuous domains.
Subsequently, we align the robot state resulting from the simplified model with
a more precise, non-differentiable simulator to maintain sufficient simulation
accuracy. Our framework enables learning quadruped walking in minutes using a
single simulated robot without any parallelization. When augmented with GPU
parallelization, our approach allows the quadruped robot to master diverse
locomotion skills, including trot, pace, bound, and gallop, on challenging
terrains in minutes. Additionally, our policy achieves robust locomotion
performance in the real world zero-shot. To the best of our knowledge, this
work represents the first demonstration of using differentiable simulation for
controlling a real quadruped robot. This work provides several important
insights into using differentiable simulations for legged locomotion in the
real world.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-smooth Control Barrier Functions for Stochastic Dynamical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06494v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06494v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matti Vahs, Jana Tumova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainties arising in various control systems, such as robots that are
subject to unknown disturbances or environmental variations, pose significant
challenges for ensuring system safety, such as collision avoidance. At the same
time, safety specifications are getting more and more complex, e.g., by
composing multiple safety objectives through Boolean operators resulting in
non-smooth descriptions of safe sets. Control Barrier Functions (CBFs) have
emerged as a control technique to provably guarantee system safety. In most
settings, they rely on an assumption of having deterministic dynamics and
smooth safe sets. This paper relaxes these two assumptions by extending CBFs to
encompass control systems with stochastic dynamics and safe sets defined by
non-smooth functions. By explicitly considering the stochastic nature of system
dynamics and accommodating complex safety specifications, our method enables
the design of safe control strategies in uncertain and complex systems. We
provide formal guarantees on the safety of the system by leveraging the
theoretical foundations of stochastic CBFs and non-smooth safe sets. Numerical
simulations demonstrate the effectiveness of the approach in various scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMP++: Motion Manifold Primitives with Parametric Curve Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17072v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17072v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghyeon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion Manifold Primitives (MMP), a manifold-based approach for encoding
basic motion skills, can produce diverse trajectories, enabling the system to
adapt to unseen constraints. Nonetheless, we argue that current MMP models lack
crucial functionalities of movement primitives, such as temporal and via-points
modulation, found in traditional approaches. This shortfall primarily stems
from MMP's reliance on discrete-time trajectories. To overcome these
limitations, we introduce Motion Manifold Primitives++ (MMP++), a new model
that integrates the strengths of both MMP and traditional methods by
incorporating parametric curve representations into the MMP framework.
Furthermore, we identify a significant challenge with MMP++: performance
degradation due to geometric distortions in the latent space, meaning that
similar motions are not closely positioned. To address this, Isometric Motion
Manifold Primitives++ (IMMP++) is proposed to ensure the latent space
accurately preserves the manifold's geometry. Our experimental results across
various applications, including 2-DoF planar motions, 7-DoF robot arm motions,
and SE(3) trajectory planning, show that MMP++ and IMMP++ outperform existing
methods in trajectory generation tasks, achieving substantial improvements in
some cases. Moreover, they enable the modulation of latent coordinates and
via-points, thereby allowing efficient online adaptation to dynamic
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages. This work has been submitted to the IEEE for possible
  publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoboDuet: A Framework Affording Mobile-Manipulation and Cross-Embodiment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17367v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17367v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoping Pan, Qingwei Ben, Zhecheng Yuan, Guangqi Jiang, Yandong Ji, Jiangmiao Pang, Houde Liu, Huazhe Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining the mobility of legged robots with the manipulation skills of arms
has the potential to significantly expand the operational range and enhance the
capabilities of robotic systems in performing various mobile manipulation
tasks. Existing approaches are confined to imprecise six degrees of freedom
(DoF) manipulation and possess a limited arm workspace. In this paper, we
propose a novel framework, RoboDuet, which employs two collaborative policies
to realize locomotion and manipulation simultaneously, achieving whole-body
control through interactions between each other. Surprisingly, going beyond the
large-range pose tracking, we find that the two-policy framework may enable
cross-embodiment deployment such as using different quadrupedal robots or other
arms. Our experiments demonstrate that the policies trained through RoboDuet
can accomplish stable gaits, agile 6D end-effector pose tracking, and zero-shot
exchange of legged robots, and can be deployed in the real world to perform
various mobile manipulation tasks. Our project page with demo videos is at
https://locomanip-duet.github.io .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nigel -- Mechatronic Design and Robust Sim2Real Control of an
  Over-Actuated Autonomous Vehicle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11542v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11542v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinmay Vilas Samak, Tanmay Vilas Samak, Javad Mohammadpour Velni, Venkat Narayan Krovi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulation to reality (sim2real) transfer from a dynamics and controls
perspective usually involves re-tuning or adapting the designed algorithms to
suit real-world operating conditions, which often violates the performance
guarantees established originally. This work presents a generalizable framework
for achieving reliable sim2real transfer of autonomy-oriented control systems
using multi-model multi-objective robust optimal control synthesis, which lends
well to uncertainty handling and disturbance rejection with theoretical
guarantees. Particularly, this work is centered around a novel
actuation-redundant scaled autonomous vehicle called Nigel, with independent
all-wheel drive and independent all-wheel steering architecture, whose enhanced
configuration space bodes well for robust control applications. To this end, we
present the mechatronic design, dynamics modeling, parameter identification,
and robust stabilizing as well as tracking control of Nigel using the proposed
framework, with exhaustive experimentation and benchmarking in simulation as
well as real-world settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PPAD: Iterative Interactions of Prediction and Planning for End-to-end
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08100v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08100v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhili Chen, Maosheng Ye, Shuangjie Xu, Tongyi Cao, Qifeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new interaction mechanism of prediction and planning for
end-to-end autonomous driving, called PPAD (Iterative Interaction of Prediction
and Planning Autonomous Driving), which considers the timestep-wise interaction
to better integrate prediction and planning. An ego vehicle performs motion
planning at each timestep based on the trajectory prediction of surrounding
agents (e.g., vehicles and pedestrians) and its local road conditions. Unlike
existing end-to-end autonomous driving frameworks, PPAD models the interactions
among ego, agents, and the dynamic environment in an autoregressive manner by
interleaving the Prediction and Planning processes at every timestep, instead
of a single sequential process of prediction followed by planning.
Specifically, we design ego-to-agent, ego-to-map, and ego-to-BEV interaction
mechanisms with hierarchical dynamic key objects attention to better model the
interactions. The experiments on the nuScenes benchmark show that our approach
outperforms state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SchurVINS: Schur Complement-Based Lightweight Visual Inertial Navigation
  System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01616v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01616v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfei Fan, Tianyu Zhao, Guidong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accuracy and computational efficiency are the most important metrics to
Visual Inertial Navigation System (VINS). The existing VINS algorithms with
either high accuracy or low computational complexity, are difficult to provide
the high precision localization in resource-constrained devices. To this end,
we propose a novel filter-based VINS framework named SchurVINS, which could
guarantee both high accuracy by building a complete residual model and low
computational complexity with Schur complement. Technically, we first formulate
the full residual model where Gradient, Hessian and observation covariance are
explicitly modeled. Then Schur complement is employed to decompose the full
model into ego-motion residual model and landmark residual model. Finally,
Extended Kalman Filter (EKF) update is implemented in these two models with
high efficiency. Experiments on EuRoC and TUM-VI datasets show that our method
notably outperforms state-of-the-art (SOTA) methods in both accuracy and
computational complexity. The experimental code of SchurVINS is available at
https://github.com/bytedance/SchurVINS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffPrompter: Differentiable Implicit Visual Prompts for
  Semantic-Segmentation in Adverse Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04181v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04181v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanket Kalwar, Mihir Ungarala, Shruti Jain, Aaron Monis, Krishna Reddy Konda, Sourav Garg, K Madhava Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation in adverse weather scenarios is a critical task for
autonomous driving systems. While foundation models have shown promise, the
need for specialized adaptors becomes evident for handling more challenging
scenarios. We introduce DiffPrompter, a novel differentiable visual and latent
prompting mechanism aimed at expanding the learning capabilities of existing
adaptors in foundation models. Our proposed $\nabla$HFC image processing block
excels particularly in adverse weather conditions, where conventional methods
often fall short. Furthermore, we investigate the advantages of jointly
training visual and latent prompts, demonstrating that this combined approach
significantly enhances performance in out-of-distribution scenarios. Our
differentiable visual prompts leverage parallel and series architectures to
generate prompts, effectively improving object segmentation tasks in adverse
conditions. Through a comprehensive series of experiments and evaluations, we
provide empirical evidence to support the efficacy of our approach. Project
page at https://diffprompter.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Symmetry in RL-based Legged Locomotion Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17320v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17320v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi Su, Xiaoyu Huang, Daniel Ordoñez-Apraez, Yunfei Li, Zhongyu Li, Qiayuan Liao, Giulio Turrisi, Massimiliano Pontil, Claudio Semini, Yi Wu, Koushil Sreenath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model-free reinforcement learning is a promising approach for autonomously
solving challenging robotics control problems, but faces exploration difficulty
without information of the robot's kinematics and dynamics morphology. The
under-exploration of multiple modalities with symmetric states leads to
behaviors that are often unnatural and sub-optimal. This issue becomes
particularly pronounced in the context of robotic systems with morphological
symmetries, such as legged robots for which the resulting asymmetric and
aperiodic behaviors compromise performance, robustness, and transferability to
real hardware. To mitigate this challenge, we can leverage symmetry to guide
and improve the exploration in policy learning via equivariance/invariance
constraints. In this paper, we investigate the efficacy of two approaches to
incorporate symmetry: modifying the network architectures to be strictly
equivariant/invariant, and leveraging data augmentation to approximate
equivariant/invariant actor-critics. We implement the methods on challenging
loco-manipulation and bipedal locomotion tasks and compare with an
unconstrained baseline. We find that the strictly equivariant policy
consistently outperforms other methods in sample efficiency and task
performance in simulation. In addition, symmetry-incorporated approaches
exhibit better gait quality, higher robustness and can be deployed zero-shot in
real-world experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Sensor Deception to Deviate from an Allowed Itinerary 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.00911v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.00911v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hazhar Rahmani, Arash Ahadi, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we study a class of deception planning problems in which an
agent aims to alter a security monitoring system's sensor readings so as to
disguise its adversarial itinerary as an allowed itinerary in the environment.
The adversarial itinerary set and allowed itinerary set are captured by regular
languages. To deviate without being detected, we investigate whether there
exists a strategy for the agent to alter the sensor readings, with a minimal
cost, such that for any of those paths it takes, the system thinks the agent
took a path within the allowed itinerary. Our formulation assumes an offline
sensor alteration where the agent determines the sensor alteration strategy and
implement it, and then carry out any path in its deviation itinerary. We prove
that the problem of solving the optimal sensor alteration is NP-hard, by a
reduction from the directed multi-cut problem. Further, we present an exact
algorithm based on integer linear programming and demonstrate the correctness
and the efficacy of the algorithm in case studies.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real Acoustic Fields: An Audio-Visual Room Acoustics <span class="highlight-title">Dataset</span> and
  Benchmark <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Chen, Israel D. Gebru, Christian Richardt, Anurag Kumar, William Laney, Andrew Owens, Alexander Richard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new dataset called Real Acoustic Fields (RAF) that captures real
acoustic room data from multiple modalities. The dataset includes high-quality
and densely captured room impulse response data paired with multi-view images,
and precise 6DoF pose tracking data for sound emitters and listeners in the
rooms. We used this dataset to evaluate existing methods for novel-view
acoustic synthesis and impulse response generation which previously relied on
synthetic data. In our evaluation, we thoroughly assessed existing audio and
audio-visual models against multiple criteria and proposed settings to enhance
their performance on real-world data. We also conducted experiments to
investigate the impact of incorporating visual data (i.e., images and depth)
into neural acoustic field models. Additionally, we demonstrated the
effectiveness of a simple sim2real approach, where a model is pre-trained with
simulated data and fine-tuned with sparse real-world data, resulting in
significant improvements in the few-shot learning approach. RAF is the first
dataset to provide densely captured room acoustic data, making it an ideal
resource for researchers working on audio and audio-visual neural acoustic
field modeling techniques. Demos and datasets are available on our project
page: https://facebookresearch.github.io/real-acoustic-fields/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024. Project site:
  https://facebookresearch.github.io/real-acoustic-fields/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetaCap: Meta-learning Priors from Multi-View Imagery for Sparse-view
  Human Performance Capture and Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxing Sun, Rishabh Dabral, Pascal Fua, Christian Theobalt, Marc Habermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Faithful human performance capture and free-view rendering from sparse RGB
observations is a long-standing problem in Vision and Graphics. The main
challenges are the lack of observations and the inherent ambiguities of the
setting, e.g. occlusions and depth ambiguity. As a result, radiance fields,
which have shown great promise in capturing high-frequency appearance and
geometry details in dense setups, perform poorly when na\"ively supervising
them on sparse camera views, as the field simply overfits to the sparse-view
inputs. To address this, we propose MetaCap, a method for efficient and
high-quality geometry recovery and novel view synthesis given very sparse or
even a single view of the human. Our key idea is to meta-learn the radiance
field weights solely from potentially sparse multi-view videos, which can serve
as a prior when fine-tuning them on sparse imagery depicting the human. This
prior provides a good network weight initialization, thereby effectively
addressing ambiguities in sparse-view capture. Due to the articulated structure
of the human body and motion-induced surface deformations, learning such a
prior is non-trivial. Therefore, we propose to meta-learn the field weights in
a pose-canonicalized space, which reduces the spatial feature range and makes
feature learning more effective. Consequently, one can fine-tune our field
parameters to quickly generalize to unseen poses, novel illumination conditions
as well as novel and sparse (even monocular) camera views. For evaluating our
method under different scenarios, we collect a new dataset, WildDynaCap, which
contains subjects captured in, both, a dense camera dome and in-the-wild sparse
camera rigs, and demonstrate superior results compared to recent
state-of-the-art methods on both public and WildDynaCap dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://vcai.mpi-inf.mpg.de/projects/MetaCap/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Object Detectors with COCO: A New Path Forward 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shweta Singh, Aayan Yadav, Jitesh Jain, Humphrey Shi, Justin Johnson, Karan Desai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Common Objects in Context (COCO) dataset has been instrumental in
benchmarking object detectors over the past decade. Like every dataset, COCO
contains subtle errors and imperfections stemming from its annotation
procedure. With the advent of high-performing models, we ask whether these
errors of COCO are hindering its utility in reliably benchmarking further
progress. In search for an answer, we inspect thousands of masks from COCO
(2017 version) and uncover different types of errors such as imprecise mask
boundaries, non-exhaustively annotated instances, and mislabeled masks. Due to
the prevalence of COCO, we choose to correct these errors to maintain
continuity with prior research. We develop COCO-ReM (Refined Masks), a cleaner
set of annotations with visibly better mask quality than COCO-2017. We evaluate
fifty object detectors and find that models that predict visually sharper masks
score higher on COCO-ReM, affirming that they were being incorrectly penalized
due to errors in COCO-2017. Moreover, our models trained using COCO-ReM
converge faster and score higher than their larger variants trained using
COCO-2017, highlighting the importance of data quality in improving object
detectors. With these findings, we advocate using COCO-ReM for future object
detection research. Our dataset is available at https://cocorem.xyz
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report. Dataset website: https://cocorem.xyz and code:
  https://github.com/kdexd/coco-rem</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object
  Removal and Insertion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18818v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18818v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Winter, Matan Cohen, Shlomi Fruchter, Yael Pritch, Alex Rav-Acha, Yedid Hoshen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have revolutionized image editing but often generate images
that violate physical laws, particularly the effects of objects on the scene,
e.g., occlusions, shadows, and reflections. By analyzing the limitations of
self-supervised approaches, we propose a practical solution centered on a
\q{counterfactual} dataset. Our method involves capturing a scene before and
after removing a single object, while minimizing other changes. By fine-tuning
a diffusion model on this dataset, we are able to not only remove objects but
also their effects on the scene. However, we find that applying this approach
for photorealistic object insertion requires an impractically large dataset. To
tackle this challenge, we propose bootstrap supervision; leveraging our object
removal model trained on a small counterfactual dataset, we synthetically
expand this dataset considerably. Our approach significantly outperforms prior
methods in photorealistic object removal and insertion, particularly at
modeling the effects of objects on the scene.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Garment3DGen: 3D Garment Stylization and Texture Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Sarafianos, Tuur Stuyck, Xiaoyu Xiang, Yilei Li, Jovan Popovic, Rakesh Ranjan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Garment3DGen a new method to synthesize 3D garment assets from a
base mesh given a single input image as guidance. Our proposed approach allows
users to generate 3D textured clothes based on both real and synthetic images,
such as those generated by text prompts. The generated assets can be directly
draped and simulated on human bodies. First, we leverage the recent progress of
image to 3D diffusion methods to generate 3D garment geometries. However, since
these geometries cannot be utilized directly for downstream tasks, we propose
to use them as pseudo ground-truth and set up a mesh deformation optimization
procedure that deforms a base template mesh to match the generated 3D target.
Second, we introduce carefully designed losses that allow the input base mesh
to freely deform towards the desired target, yet preserve mesh quality and
topology such that they can be simulated. Finally, a texture estimation module
generates high-fidelity texture maps that are globally and locally consistent
and faithfully capture the input guidance, allowing us to render the generated
3D assets. With Garment3DGen users can generate the textured 3D garment of
their choice without the need of artist intervention. One can provide a textual
prompt describing the garment they desire to generate a simulation-ready 3D
asset. We present a plethora of quantitative and qualitative comparisons on
various assets both real and generated and provide use-cases of how one can
generate simulation-ready 3D garments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://nsarafianos.github.io/garment3dgen</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mini-Gemini: Mining the Potential of Multi-modality Vision Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce Mini-Gemini, a simple and effective framework
enhancing multi-modality Vision Language Models (VLMs). Despite the
advancements in VLMs facilitating basic visual dialog and reasoning, a
performance gap persists compared to advanced models like GPT-4 and Gemini. We
try to narrow the gap by mining the potential of VLMs for better performance
and any-to-any workflow from three aspects, i.e., high-resolution visual
tokens, high-quality data, and VLM-guided generation. To enhance visual tokens,
we propose to utilize an additional visual encoder for high-resolution
refinement without increasing the visual token count. We further construct a
high-quality dataset that promotes precise image comprehension and
reasoning-based generation, expanding the operational scope of current VLMs. In
general, Mini-Gemini further mines the potential of VLMs and empowers current
frameworks with image understanding, reasoning, and generation simultaneously.
Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs)
from 2B to 34B. It is demonstrated to achieve leading performance in several
zero-shot benchmarks and even surpasses the developed private models. Code and
models are available at https://github.com/dvlab-research/MiniGemini.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and models are available at
  https://github.com/dvlab-research/MiniGemini</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance
  Accompaniment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Siyao, Tianpei Gu, Zhitao Yang, Zhengyu Lin, Ziwei Liu, Henghui Ding, Lei Yang, Chen Change Loy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel task within the field of 3D dance generation, termed
dance accompaniment, which necessitates the generation of responsive movements
from a dance partner, the "follower", synchronized with the lead dancer's
movements and the underlying musical rhythm. Unlike existing solo or group
dance generation tasks, a duet dance scenario entails a heightened degree of
interaction between the two participants, requiring delicate coordination in
both pose and position. To support this task, we first build a large-scale and
diverse duet interactive dance dataset, DD100, by recording about 117 minutes
of professional dancers' performances. To address the challenges inherent in
this task, we propose a GPT-based model, Duolando, which autoregressively
predicts the subsequent tokenized motion conditioned on the coordinated
information of the music, the leader's and the follower's movements. To further
enhance the GPT's capabilities of generating stable results on unseen
conditions (music and leader motions), we devise an off-policy reinforcement
learning strategy that allows the model to explore viable trajectories from
out-of-distribution samplings, guided by human-defined rewards. Based on the
collected dataset and proposed method, we establish a benchmark with several
carefully designed metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth
  Estimation <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suraj Patni, Aradhye Agarwal, Chetan Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the absence of parallax cues, a learning-based single image depth
estimation (SIDE) model relies heavily on shading and contextual cues in the
image. While this simplicity is attractive, it is necessary to train such
models on large and varied datasets, which are difficult to capture. It has
been shown that using embeddings from pre-trained foundational models, such as
CLIP, improves zero shot transfer in several applications. Taking inspiration
from this, in our paper we explore the use of global image priors generated
from a pre-trained ViT model to provide more detailed contextual information.
We argue that the embedding vector from a ViT model, pre-trained on a large
dataset, captures greater relevant information for SIDE than the usual route of
generating pseudo image captions, followed by CLIP based text embeddings. Based
on this idea, we propose a new SIDE model using a diffusion backbone which is
conditioned on ViT embeddings. Our proposed design establishes a new
state-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of
0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on
KITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to
0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model
trained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)
over NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,
18%, 45%, 9%) by ZoeDepth. The code is available at
https://github.com/Aradhye2002/EcoDepth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gamba: Marry Gaussian Splatting with Mamba for single view 3D
  reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuhong Shen, Xuanyu Yi, Zike Wu, Pan Zhou, Hanwang Zhang, Shuicheng Yan, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the challenge of efficiently reconstructing a 3D asset from a
single image with growing demands for automated 3D content creation pipelines.
Previous methods primarily rely on Score Distillation Sampling (SDS) and Neural
Radiance Fields (NeRF). Despite their significant success, these approaches
encounter practical limitations due to lengthy optimization and considerable
memory usage. In this report, we introduce Gamba, an end-to-end amortized 3D
reconstruction model from single-view images, emphasizing two main insights:
(1) 3D representation: leveraging a large number of 3D Gaussians for an
efficient 3D Gaussian splatting process; (2) Backbone design: introducing a
Mamba-based sequential network that facilitates context-dependent reasoning and
linear scalability with the sequence (token) length, accommodating a
substantial number of Gaussians. Gamba incorporates significant advancements in
data preprocessing, regularization design, and training methodologies. We
assessed Gamba against existing optimization-based and feed-forward 3D
generation approaches using the real-world scanned OmniObject3D dataset. Here,
Gamba demonstrates competitive generation capabilities, both qualitatively and
quantitatively, while achieving remarkable speed, approximately 0.6 second on a
single NVIDIA A100 GPU.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object Pose Estimation via the Aggregation of Diffusion Features <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianfu Wang, Guosheng Hu, Hongguang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the pose of objects from images is a crucial task of 3D scene
understanding, and recent approaches have shown promising results on very large
benchmarks. However, these methods experience a significant performance drop
when dealing with unseen objects. We believe that it results from the limited
generalizability of image features. To address this problem, we have an
in-depth analysis on the features of diffusion models, e.g. Stable Diffusion,
which hold substantial potential for modeling unseen objects. Based on this
analysis, we then innovatively introduce these diffusion features for object
pose estimation. To achieve this, we propose three distinct architectures that
can effectively capture and aggregate diffusion features of different
granularity, greatly improving the generalizability of object pose estimation.
Our approach outperforms the state-of-the-art methods by a considerable margin
on three popular benchmark datasets, LM, O-LM, and T-LESS. In particular, our
method achieves higher accuracy than the previous best arts on unseen objects:
98.2% vs. 93.5% on Unseen LM, 85.9% vs. 76.3% on Unseen O-LM, showing the
strong generalizability of our method. Our code is released at
https://github.com/Tianfu18/diff-feats-pose.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable
  Surface 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Luo, Jing Liu, James Davis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SplatFace, a novel Gaussian splatting framework designed for 3D
human face reconstruction without reliance on accurate pre-determined geometry.
Our method is designed to simultaneously deliver both high-quality novel view
rendering and accurate 3D mesh reconstructions. We incorporate a generic 3D
Morphable Model (3DMM) to provide a surface geometric structure, making it
possible to reconstruct faces with a limited set of input images. We introduce
a joint optimization strategy that refines both the Gaussians and the morphable
surface through a synergistic non-rigid alignment process. A novel distance
metric, splat-to-surface, is proposed to improve alignment by considering both
the Gaussian position and covariance. The surface information is also utilized
to incorporate a world-space densification process, resulting in superior
reconstruction quality. Our experimental analysis demonstrates that the
proposed method is competitive with both other Gaussian splatting techniques in
novel view synthesis and other 3D reconstruction methods in producing 3D face
meshes with high geometric precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImageNet-D: Benchmarking Neural Network Robustness on Diffusion
  Synthetic Object <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenshuang Zhang, Fei Pan, Junmo Kim, In So Kweon, Chengzhi Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish rigorous benchmarks for visual perception robustness. Synthetic
images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific
type of evaluation over synthetic corruptions, backgrounds, and textures, yet
those robustness benchmarks are restricted in specified variations and have low
synthetic quality. In this work, we introduce generative model as a data source
for synthesizing hard images that benchmark deep models' robustness. Leveraging
diffusion models, we are able to generate images with more diversified
backgrounds, textures, and materials than any prior work, where we term this
benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a
significant accuracy drop to a range of vision models, from the standard ResNet
visual classifier to the latest foundation models like CLIP and MiniGPT-4,
significantly reducing their accuracy by up to 60\%. Our work suggests that
diffusion models can be an effective source to test vision models. The code and
dataset are available at https://github.com/chenshuang-zhang/imagenet_d.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ModaLink: Unifying Modalities for Efficient Image-to-PointCloud Place
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weidong Xie, Lun Luo, Nanfei Ye, Yi Ren, Shaoyi Du, Minhang Wang, Jintao Xu, Rui Ai, Weihao Gu, Xieyuanli Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Place recognition is an important task for robots and autonomous cars to
localize themselves and close loops in pre-built maps. While single-modal
sensor-based methods have shown satisfactory performance, cross-modal place
recognition that retrieving images from a point-cloud database remains a
challenging problem. Current cross-modal methods transform images into 3D
points using depth estimation for modality conversion, which are usually
computationally intensive and need expensive labeled data for depth
supervision. In this work, we introduce a fast and lightweight framework to
encode images and point clouds into place-distinctive descriptors. We propose
an effective Field of View (FoV) transformation module to convert point clouds
into an analogous modality as images. This module eliminates the necessity for
depth estimation and helps subsequent modules achieve real-time performance. We
further design a non-negative factorization-based encoder to extract mutually
consistent semantic features between point clouds and images. This encoder
yields more distinctive global descriptors for retrieval. Experimental results
on the KITTI dataset show that our proposed methods achieve state-of-the-art
performance while running in real time. Additional evaluation on the HAOMO
dataset covering a 17 km trajectory further shows the practical generalization
capabilities. We have released the implementation of our methods as open source
at: https://github.com/haomo-ai/ModaLink.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 11 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection of subclinical atherosclerosis by image-based deep learning on
  chest x-ray 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guglielmo Gallone, Francesco Iodice, Alberto Presta, Davide Tore, Ovidio de Filippo, Michele Visciano, Carlo Alberto Barbano, Alessandro Serafini, Paola Gorrini, Alessandro Bruno, Walter Grosso Marra, James Hughes, Mario Iannaccone, Paolo Fonio, Attilio Fiandrotti, Alessandro Depaoli, Marco Grangetto, Gaetano Maria de Ferrari, Fabrizio D'Ascenzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aims. To develop a deep-learning based system for recognition of subclinical
atherosclerosis on a plain frontal chest x-ray. Methods and Results. A
deep-learning algorithm to predict coronary artery calcium (CAC) score (the
AI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20%
internal validation cohort) of primary prevention patients (58.4% male, median
age 63 [51-74] years) with available paired chest x-ray and chest computed
tomography (CT) indicated for any clinical reason and performed within 3
months. The CAC score calculated on chest CT was used as ground truth. The
model was validated on an temporally-independent cohort of 90 patients from the
same institution (external validation). The diagnostic accuracy of the AI-CAC
model assessed by the area under the curve (AUC) was the primary outcome.
Overall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC.
AUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation
cohort and 0.77 in the external validation cohort. Sensitivity was consistently
above 92% in both cohorts. In the overall cohort (n=540), among patients with
AI-CAC=0, a single ASCVD event occurred, after 4.3 years. Patients with
AI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events
(13.5% vs. 3.4%, log-rank=0.013). Conclusion. The AI-CAC model seems to
accurately detect subclinical atherosclerosis on chest x-ray with elevated
sensitivity, and to predict ASCVD events with elevated negative predictive
value. Adoption of the AI-CAC model to refine CV risk stratification or as an
opportunistic screening tool requires prospective evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to European Heart Journal - Cardiovascular Imaging Added
  also the additional material 44 pages (30 main paper, 14 additional
  material), 14 figures (5 main manuscript, 9 additional material)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A vascular synthetic model for improved aneurysm segmentation and
  detection via Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafic Nader, Florent Autrusseau, Vincent L'Allinec, Romain Bourcier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We hereby present a full synthetic model, able to mimic the various
constituents of the cerebral vascular tree: the cerebral arteries, the
bifurcations and the intracranial aneurysms. By building this model, our goal
was to provide a substantial dataset of brain arteries which could be used by a
3D Convolutional Neural Network (CNN) to either segment or detect/recognize
various vascular diseases (such as artery dissection/thrombosis) or even some
portions of the cerebral vasculature, such as the bifurcations or aneurysms. In
this study, we will particularly focus on Intra-Cranial Aneurysm (ICA)
detection and segmentation. The cerebral aneurysms most often occur on a
particular structure of the vascular tree named the Circle of Willis. Various
studies have been conducted to detect and monitor the ICAs and those based on
Deep Learning (DL) achieve the best performances. Specifically, in this work,
we propose a full synthetic 3D model able to mimic the brain vasculature as
acquired by Magnetic Resonance Angiography (MRA), and more particularly the
Time Of Flight (TOF) principle. Among the various MRI modalities, the MRA-TOF
allows to have a relatively good rendering of the blood vessels and is
non-invasive (no contrast liquid injection). Our model has been designed to
simultaneously mimic the arteries geometry, the ICA shape and the background
noise. The geometry of the vascular tree is modeled thanks to an interpolation
with 3D Spline functions, and the statistical properties of the background MRI
noise is collected from MRA acquisitions and reproduced within the model. In
this work, we thoroughly describe the synthetic vasculature model, we build up
a neural network designed for ICA segmentation and detection, and finally, we
carry out an in-depth evaluation of the performance gap gained thanks to the
synthetic model data augmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Manufacturing Quality Prediction Models through the
  Integration of Explainability Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Gross, Helge Spieker, Arnaud Gotlieb, Ricardo Knoblauch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research presents a method that utilizes explainability techniques to
amplify the performance of machine learning (ML) models in forecasting the
quality of milling processes, as demonstrated in this paper through a
manufacturing use case. The methodology entails the initial training of ML
models, followed by a fine-tuning phase where irrelevant features identified
through explainability methods are eliminated. This procedural refinement
results in performance enhancements, paving the way for potential reductions in
manufacturing costs and a better understanding of the trained ML models. This
study highlights the usefulness of explainability techniques in both explaining
and optimizing predictive models in the manufacturing realm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Image Ambient Lighting Normalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florin-Alexandru Vasluianu, Tim Seizinger, Zongwei Wu, Rakesh Ranjan, Radu Timofte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lighting normalization is a crucial but underexplored restoration task with
broad applications. However, existing works often simplify this task within the
context of shadow removal, limiting the light sources to one and
oversimplifying the scene, thus excluding complex self-shadows and restricting
surface classes to smooth ones. Although promising, such simplifications hinder
generalizability to more realistic settings encountered in daily use. In this
paper, we propose a new challenging task termed Ambient Lighting Normalization
(ALN), which enables the study of interactions between shadows, unifying image
restoration and shadow removal in a broader context. To address the lack of
appropriate datasets for ALN, we introduce the large-scale high-resolution
dataset Ambient6K, comprising samples obtained from multiple light sources and
including self-shadows resulting from complex geometries, which is the first of
its kind. For benchmarking, we select various mainstream methods and rigorously
evaluate them on Ambient6K. Additionally, we propose IFBlend, a novel strong
baseline that maximizes Image-Frequency joint entropy to selectively restore
local areas under different lighting conditions, without relying on shadow
localization priors. Experiments show that IFBlend achieves SOTA scores on
Ambient6K and exhibits competitive performance on conventional shadow removal
benchmarks compared to shadow-specific models with mask priors. The dataset,
benchmark, and code are available at https://github.com/fvasluianu97/IFBlend.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Supervised Learning for Deep Causal Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasin Ibrahim, Hermione Warr, Konstantinos Kamnitsas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing models that can answer questions of the form "How would $x$ change
if $y$ had been $z$?" is fundamental for advancing medical image analysis.
Training causal generative models that address such counterfactual questions,
though, currently requires that all relevant variables have been observed and
that corresponding labels are available in training data. However, clinical
data may not have complete records for all patients and state of the art causal
generative models are unable to take full advantage of this. We thus develop,
for the first time, a semi-supervised deep causal generative model that
exploits the causal relationships between variables to maximise the use of all
available data. We explore this in the setting where each sample is either
fully labelled or fully unlabelled, as well as the more clinically realistic
case of having different labels missing for each sample. We leverage techniques
from causal inference to infer missing values and subsequently generate
realistic counterfactuals, even for samples with incomplete labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Hallucinations in Large Vision-Language Models with
  Instruction Contrastive Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xintong Wang, Jingheng Pan, Liang Ding, Chris Biemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) are increasingly adept at generating
contextually detailed and coherent responses from visual inputs. However, their
application in multimodal decision-making and open-ended generation is hindered
by a notable rate of hallucinations, where generated text inaccurately
represents the visual contents. To address this issue, this paper introduces
the Instruction Contrastive Decoding (ICD) method, a novel approach designed to
reduce hallucinations during LVLM inference. Our method is inspired by our
observation that what we call disturbance instructions significantly exacerbate
hallucinations in multimodal fusion modules. ICD contrasts distributions from
standard and instruction disturbance, thereby increasing alignment uncertainty
and effectively subtracting hallucinated concepts from the original
distribution. Through comprehensive experiments on discriminative benchmarks
(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that
ICD significantly mitigates both object-level and attribute-level
hallucinations. Moreover, our method not only addresses hallucinations but also
significantly enhances the general perception and recognition capabilities of
LVLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bringing Textual Prompt to AI-Generated Image Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Qu, Haohui Li, Wei Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-Generated Images (AGIs) have inherent multimodal nature. Unlike
traditional image quality assessment (IQA) on natural scenarios, AGIs quality
assessment (AGIQA) takes the correspondence of image and its textual prompt
into consideration. This is coupled in the ground truth score, which confuses
the unimodal IQA methods. To solve this problem, we introduce IP-IQA (AGIs
Quality Assessment via Image and Prompt), a multimodal framework for AGIQA via
corresponding image and prompt incorporation. Specifically, we propose a novel
incremental pretraining task named Image2Prompt for better understanding of
AGIs and their corresponding textual prompts. An effective and efficient
image-prompt fusion module, along with a novel special [QA] token, are also
applied. Both are plug-and-play and beneficial for the cooperation of image and
its corresponding prompt. Experiments demonstrate that our IP-IQA achieves the
state-of-the-art on AGIQA-1k and AGIQA-3k datasets. Code will be available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, accepted by ICME2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAT-NGP : Unleashing Neural Graphics Primitives for Fast Relightable
  Transient-Free 3D reconstruction from Satellite Imagery <span class="chip">RSS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Camille Billouard, Dawa Derksen, Emmanuelle Sarrazin, Bruno Vallet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current stereo-vision pipelines produce high accuracy 3D reconstruction when
using multiple pairs or triplets of satellite images. However, these pipelines
are sensitive to the changes between images that can occur as a result of
multi-date acquisitions. Such variations are mainly due to variable shadows,
reflexions and transient objects (cars, vegetation). To take such changes into
account, Neural Radiance Fields (NeRF) have recently been applied to multi-date
satellite imagery. However, Neural methods are very compute-intensive, taking
dozens of hours to learn, compared with minutes for standard stereo-vision
pipelines. Following the ideas of Instant Neural Graphics Primitives we propose
to use an efficient sampling strategy and multi-resolution hash encoding to
accelerate the learning. Our model, Satellite Neural Graphics Primitives
(SAT-NGP) decreases the learning time to 15 minutes while maintaining the
quality of the 3D reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 1 table; Accepted to International Geoscience and
  Remote Sensing Symposium (IGARSS) 2024; Code available at
  https://github.com/Ellimac0/SAT-NGP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dense Vision Transformer Compression with Few Samples <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanxiao Zhang, Yifan Zhou, Guo-Hua Wang, Jianxin Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot model compression aims to compress a large model into a more compact
one with only a tiny training set (even without labels). Block-level pruning
has recently emerged as a leading technique in achieving high accuracy and low
latency in few-shot CNN compression. But, few-shot compression for Vision
Transformers (ViT) remains largely unexplored, which presents a new challenge.
In particular, the issue of sparse compression exists in traditional CNN
few-shot methods, which can only produce very few compressed models of
different model sizes. This paper proposes a novel framework for few-shot ViT
compression named DC-ViT. Instead of dropping the entire block, DC-ViT
selectively eliminates the attention module while retaining and reusing
portions of the MLP module. DC-ViT enables dense compression, which outputs
numerous compressed models that densely populate the range of model complexity.
DC-ViT outperforms state-of-the-art few-shot compression methods by a
significant margin of 10 percentage points, along with lower latency in the
compression of ViT and its variants.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024. Note: Jianxin Wu is a contributing author for
  the arXiv version of this paper but is not listed as an author in the CVPR
  version due to his role as Program Chair</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Annolid: Annotate, Segment, and Track Anything You Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Yang, Thomas A. Cleland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Annolid is a deep learning-based software package designed for the
segmentation, labeling, and tracking of research targets within video files,
focusing primarily on animal behavior analysis. Based on state-of-the-art
instance segmentation methods, Annolid now harnesses the Cutie video object
segmentation model to achieve resilient, markerless tracking of multiple
animals from single annotated frames, even in environments in which they may be
partially or entirely concealed by environmental features or by one another.
Our integration of Segment Anything and Grounding-DINO strategies additionally
enables the automatic masking and segmentation of recognizable animals and
objects by text command, removing the need for manual annotation. Annolid's
comprehensive approach to object segmentation flexibly accommodates a broad
spectrum of behavior analysis applications, enabling the classification of
diverse behavioral states such as freezing, digging, pup huddling, and social
interactions in addition to the tracking of animals and their body parts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for Robust and Explainable Models in Computer Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza Amirian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent breakthroughs in machine and deep learning (ML and DL) research have
provided excellent tools for leveraging enormous amounts of data and optimizing
huge models with millions of parameters to obtain accurate networks for image
processing. These developments open up tremendous opportunities for using
artificial intelligence (AI) in the automation and human assisted AI industry.
However, as more and more models are deployed and used in practice, many
challenges have emerged. This thesis presents various approaches that address
robustness and explainability challenges for using ML and DL in practice.
  Robustness and reliability are the critical components of any model before
certification and deployment in practice. Deep convolutional neural networks
(CNNs) exhibit vulnerability to transformations of their inputs, such as
rotation and scaling, or intentional manipulations as described in the
adversarial attack literature. In addition, building trust in AI-based models
requires a better understanding of current models and developing methods that
are more explainable and interpretable a priori.
  This thesis presents developments in computer vision models' robustness and
explainability. Furthermore, this thesis offers an example of using vision
models' feature response visualization (models' interpretations) to improve
robustness despite interpretability and robustness being seemingly unrelated in
the related research. Besides methodological developments for robust and
explainable vision models, a key message of this thesis is introducing model
interpretation techniques as a tool for understanding vision models and
improving their design and robustness. In addition to the theoretical
developments, this thesis demonstrates several applications of ML and DL in
different contexts, such as medical imaging and affective computing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>150 pages, 37 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InstructBrush: Learning Attention-based Instruction Optimization for
  Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruoyu Zhao, Qingnan Fan, Fei Kou, Shuai Qin, Hong Gu, Wei Wu, Pengcheng Xu, Mingrui Zhu, Nannan Wang, Xinbo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, instruction-based image editing methods have garnered
significant attention in image editing. However, despite encompassing a wide
range of editing priors, these methods are helpless when handling editing tasks
that are challenging to accurately describe through language. We propose
InstructBrush, an inversion method for instruction-based image editing methods
to bridge this gap. It extracts editing effects from exemplar image pairs as
editing instructions, which are further applied for image editing. Two key
techniques are introduced into InstructBrush, Attention-based Instruction
Optimization and Transformation-oriented Instruction Initialization, to address
the limitations of the previous method in terms of inversion effects and
instruction generalization. To explore the ability of instruction inversion
methods to guide image editing in open scenarios, we establish a
TransformationOriented Paired Benchmark (TOP-Bench), which contains a rich set
of scenes and editing types. The creation of this benchmark paves the way for
further exploration of instruction inversion. Quantitatively and qualitatively,
our approach achieves superior performance in editing and is more semantically
consistent with the target editing effects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://royzhao926.github.io/InstructBrush/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing Data Annotation Challenges in Multiple Sensors: A Solution
  for Scania Collected <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ajinkya Khoche, Aron Asefaw, Alejandro Gonzalez, Bogdan Timus, Sina Sharif Mansouri, Patric Jensfelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data annotation in autonomous vehicles is a critical step in the development
of Deep Neural Network (DNN) based models or the performance evaluation of the
perception system. This often takes the form of adding 3D bounding boxes on
time-sequential and registered series of point-sets captured from active
sensors like Light Detection and Ranging (LiDAR) and Radio Detection and
Ranging (RADAR). When annotating multiple active sensors, there is a need to
motion compensate and translate the points to a consistent coordinate frame and
timestamp respectively. However, highly dynamic objects pose a unique
challenge, as they can appear at different timestamps in each sensor's data.
Without knowing the speed of the objects, their position appears to be
different in different sensor outputs. Thus, even after motion compensation,
highly dynamic objects are not matched from multiple sensors in the same frame,
and human annotators struggle to add unique bounding boxes that capture all
objects. This article focuses on addressing this challenge, primarily within
the context of Scania collected datasets. The proposed solution takes a track
of an annotated object as input and uses the Moving Horizon Estimation (MHE) to
robustly estimate its speed. The estimated speed profile is utilized to correct
the position of the annotated box and add boxes to object clusters missed by
the original annotation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to European Control Conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transformers-based architectures for stroke segmentation: A <span class="highlight-title">review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yalda Zafari-Ghadim, Essam A. Rashed, Mohamed Mabrok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stroke remains a significant global health concern, necessitating precise and
efficient diagnostic tools for timely intervention and improved patient
outcomes. The emergence of deep learning methodologies has transformed the
landscape of medical image analysis. Recently, Transformers, initially designed
for natural language processing, have exhibited remarkable capabilities in
various computer vision applications, including medical image analysis. This
comprehensive review aims to provide an in-depth exploration of the
cutting-edge Transformer-based architectures applied in the context of stroke
segmentation. It commences with an exploration of stroke pathology, imaging
modalities, and the challenges associated with accurate diagnosis and
segmentation. Subsequently, the review delves into the fundamental ideas of
Transformers, offering detailed insights into their architectural intricacies
and the underlying mechanisms that empower them to effectively capture complex
spatial information within medical images. The existing literature is
systematically categorized and analyzed, discussing various approaches that
leverage Transformers for stroke segmentation. A critical assessment is
provided, highlighting the strengths and limitations of these methods,
including considerations of performance and computational efficiency.
Additionally, this review explores potential avenues for future research and
development
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlexEdit: Flexible and Controllable Diffusion-based Object-centric Image
  Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trong-Tung Nguyen, Duc-Anh Nguyen, Anh Tran, Cuong Pham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our work addresses limitations seen in previous approaches for object-centric
editing problems, such as unrealistic results due to shape discrepancies and
limited control in object replacement or insertion. To this end, we introduce
FlexEdit, a flexible and controllable editing framework for objects where we
iteratively adjust latents at each denoising step using our FlexEdit block.
Initially, we optimize latents at test time to align with specified object
constraints. Then, our framework employs an adaptive mask, automatically
extracted during denoising, to protect the background while seamlessly blending
new content into the target image. We demonstrate the versatility of FlexEdit
in various object editing tasks and curate an evaluation test suite with
samples from both real and synthetic images, along with novel evaluation
metrics designed for object-centric editing. We conduct extensive experiments
on different editing scenarios, demonstrating the superiority of our editing
framework over recent advanced text-guided image editing methods. Our project
page is published at https://flex-edit.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our project page: https://flex-edit.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in
  Instructional Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Zare, Yulei Niu, Hammad Ayyubi, Shih-fu Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedure Planning in instructional videos entails generating a sequence of
action steps based on visual observations of the initial and target states.
Despite the rapid progress in this task, there remain several critical
challenges to be solved: (1) Adaptive procedures: Prior works hold an
unrealistic assumption that the number of action steps is known and fixed,
leading to non-generalizable models in real-world scenarios where the sequence
length varies. (2) Temporal relation: Understanding the step temporal relation
knowledge is essential in producing reasonable and executable plans. (3)
Annotation cost: Annotating instructional videos with step-level labels (i.e.,
timestamp) or sequence-level labels (i.e., action category) is demanding and
labor-intensive, limiting its generalizability to large-scale datasets.In this
work, we propose a new and practical setting, called adaptive procedure
planning in instructional videos, where the procedure length is not fixed or
pre-determined. To address these challenges we introduce Retrieval-Augmented
Planner (RAP) model. Specifically, for adaptive procedures, RAP adaptively
determines the conclusion of actions using an auto-regressive model
architecture. For temporal relation, RAP establishes an external memory module
to explicitly retrieve the most relevant state-action pairs from the training
videos and revises the generated procedures. To tackle high annotation cost,
RAP utilizes a weakly-supervised learning manner to expand the training dataset
to other task-relevant, unannotated videos by generating pseudo labels for
action steps. Experiments on CrossTask and COIN benchmarks show the superiority
of RAP over traditional fixed-length models, establishing it as a strong
baseline solution for adaptive procedure planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 6 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote
  Sensing Image Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Run Shao, Zhaoyang Zhang, Chao Tao, Yunsheng Zhang, Chengli Peng, Haifeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The tokenizer, as one of the fundamental components of large models, has long
been overlooked or even misunderstood in visual tasks. One key factor of the
great comprehension power of the large language model is that natural language
tokenizers utilize meaningful words or subwords as the basic elements of
language. In contrast, mainstream visual tokenizers, represented by patch-based
methods such as Patch Embed, rely on meaningless rectangular patches as basic
elements of vision, which cannot serve as effectively as words or subwords in
language. Starting from the essence of the tokenizer, we defined semantically
independent regions (SIRs) for vision. We designed a simple HOmogeneous visual
tOKenizer: HOOK. HOOK mainly consists of two modules: the Object Perception
Module (OPM) and the Object Vectorization Module (OVM). To achieve homogeneity,
the OPM splits the image into 4*4 pixel seeds and then utilizes the attention
mechanism to perceive SIRs. The OVM employs cross-attention to merge seeds
within the same SIR. To achieve adaptability, the OVM defines a variable number
of learnable vectors as cross-attention queries, allowing for the adjustment of
token quantity. We conducted experiments on the NWPU-RESISC45, WHU-RS19
classification dataset, and GID5 segmentation dataset for sparse and dense
tasks. The results demonstrate that the visual tokens obtained by HOOK
correspond to individual objects, which demonstrates homogeneity. HOOK
outperformed Patch Embed by 6\% and 10\% in the two tasks and achieved
state-of-the-art performance compared to the baselines used for comparison.
Compared to Patch Embed, which requires more than one hundred tokens for one
image, HOOK requires only 6 and 8 tokens for sparse and dense tasks,
respectively, resulting in efficiency improvements of 1.5 to 2.8 times. The
code is available at https://github.com/GeoX-Lab/Hook.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Users prefer Jpegli over same-sized libjpeg-turbo or MozJPEG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Bruse, Luca Versari, Zoltan Szabadka, Jyrki Alakuijala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We performed pairwise comparisons by human raters of JPEG images from
MozJPEG, libjpeg-turbo and our new Jpegli encoder. When compressing images at a
quality similar to libjpeg-turbo quality 95, the Jpegli images were 54% likely
to be preferred over both libjpeg-turbo and MozJPEG images, but used only 2.8
bits per pixel compared to libjpeg-turbo and MozJPEG that used 3.8 and 3.5 bits
per pixel respectively. The raw ratings and source images are publicly
available for further analysis and study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency
  Attacks in Computer Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Müller, Erwin Quiring
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Resource efficiency plays an important role for machine learning nowadays.
The energy and decision latency are two critical aspects to ensure a
sustainable and practical application. Unfortunately, the energy consumption
and decision latency are not robust against adversaries. Researchers have
recently demonstrated that attackers can compute and submit so-called sponge
examples at inference time to increase the energy consumption and decision
latency of neural networks. In computer vision, the proposed strategy crafts
inputs with less activation sparsity which could otherwise be used to
accelerate the computation. In this paper, we analyze the mechanism how these
energy-latency attacks reduce activation sparsity. In particular, we find that
input uniformity is a key enabler. A uniform image, that is, an image with
mostly flat, uniformly colored surfaces, triggers more activations due to a
specific interplay of convolution, batch normalization, and ReLU activation.
Based on these insights, we propose two new simple, yet effective strategies
for crafting sponge examples: sampling images from a probability distribution
and identifying dense, yet inconspicuous inputs in natural datasets. We
empirically examine our findings in a comprehensive evaluation with multiple
image classification models and show that our attack achieves the same sparsity
effect as prior sponge-example methods, but at a fraction of computation
effort. We also show that our sponge examples transfer between different neural
networks. Finally, we discuss applications of our findings for the good by
improving efficiency by increasing sparsity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the DLSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional
  Synthesis and Sampling of Hand-Object Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Xu, Haipeng Li, Yinqiao Wang, Shuaicheng Liu, Chi-Wing Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing 3D hand mesh robustly from a single image is very challenging,
due to the lack of diversity in existing real-world datasets. While data
synthesis helps relieve the issue, the syn-to-real gap still hinders its usage.
In this work, we present HandBooster, a new approach to uplift the data
diversity and boost the 3D hand-mesh reconstruction performance by training a
conditional generative space on hand-object interactions and purposely sampling
the space to synthesize effective data samples. First, we construct versatile
content-aware conditions to guide a diffusion model to produce realistic images
with diverse hand appearances, poses, views, and backgrounds; favorably,
accurate 3D annotations are obtained for free. Then, we design a novel
condition creator based on our similarity-aware distribution sampling
strategies to deliberately find novel and realistic interaction poses that are
distinctive from the training set. Equipped with our method, several baselines
can be significantly improved beyond the SOTA on the HO3D and DexYCB
benchmarks. Our code will be released on
https://github.com/hxwork/HandBooster_Pytorch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artifact Reduction in 3D and 4D Cone-beam Computed Tomography Images
  with Deep Learning -- A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza Amirian, Daniel Barco, Ivo Herzig, Frank-Peter Schilling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning based approaches have been used to improve image quality in
cone-beam computed tomography (CBCT), a medical imaging technique often used in
applications such as image-guided radiation therapy, implant dentistry or
orthopaedics. In particular, while deep learning methods have been applied to
reduce various types of CBCT image artifacts arising from motion, metal
objects, or low-dose acquisition, a comprehensive review summarizing the
successes and shortcomings of these approaches, with a primary focus on the
type of artifacts rather than the architecture of neural networks, is lacking
in the literature. In this review, the data generation and simulation
pipelines, and artifact reduction techniques are specifically investigated for
each type of artifact. We provide an overview of deep learning techniques that
have successfully been shown to reduce artifacts in 3D, as well as in
time-resolved (4D) CBCT through the use of projection- and/or volume-domain
optimizations, or by introducing neural networks directly within the CBCT
reconstruction algorithms. Research gaps are identified to suggest avenues for
future exploration. One of the key findings of this work is an observed trend
towards the use of generative models including GANs and score-based or
diffusion models, accompanied with the need for more diverse and open training
datasets and simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures, 1 Table, published in IEEE Access Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CosalPure: Learning Concept from Group Images for Robust Co-Saliency
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Zhu, Qing Guo, Felix Juefei-Xu, Yihao Huang, Yang Liu, Geguang Pu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Co-salient object detection (CoSOD) aims to identify the common and salient
(usually in the foreground) regions across a given group of images. Although
achieving significant progress, state-of-the-art CoSODs could be easily
affected by some adversarial perturbations, leading to substantial accuracy
reduction. The adversarial perturbations can mislead CoSODs but do not change
the high-level semantic information (e.g., concept) of the co-salient objects.
In this paper, we propose a novel robustness enhancement framework by first
learning the concept of the co-salient objects based on the input group images
and then leveraging this concept to purify adversarial perturbations, which are
subsequently fed to CoSODs for robustness enhancement. Specifically, we propose
CosalPure containing two modules, i.e., group-image concept learning and
concept-guided diffusion purification. For the first module, we adopt a
pre-trained text-to-image diffusion model to learn the concept of co-salient
objects within group images where the learned concept is robust to adversarial
examples. For the second module, we map the adversarial image to the latent
space and then perform diffusion generation by embedding the learned concept
into the noise prediction function as an extra condition. Our method can
effectively alleviate the influence of the SOTA adversarial attack containing
different adversarial patterns, including exposure and noise. The extensive
results demonstrate that our method could enhance the robustness of CoSODs
significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention Calibration for Disentangled Text-to-Image Personalization <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbing Zhang, Mengping Yang, Qin Zhou, Zhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent thrilling progress in large-scale text-to-image (T2I) models has
unlocked unprecedented synthesis quality of AI-generated content (AIGC)
including image generation, 3D and video composition. Further, personalized
techniques enable appealing customized production of a novel concept given only
several images as reference. However, an intriguing problem persists: Is it
possible to capture multiple, novel concepts from one single reference image?
In this paper, we identify that existing approaches fail to preserve visual
consistency with the reference image and eliminate cross-influence from
concepts. To alleviate this, we propose an attention calibration mechanism to
improve the concept-level understanding of the T2I model. Specifically, we
first introduce new learnable modifiers bound with classes to capture
attributes of multiple concepts. Then, the classes are separated and
strengthened following the activation of the cross-attention operation,
ensuring comprehensive and self-contained concepts. Additionally, we suppress
the attention activation of different classes to mitigate mutual influence
among concepts. Together, our proposed method, dubbed DisenDiff, can learn
disentangled multiple concepts from one single image and produce novel
customized images with learned concepts. We demonstrate that our method
outperforms the current state of the art in both qualitative and quantitative
evaluations. More importantly, our proposed techniques are compatible with LoRA
and inpainting pipelines, enabling more interactive experiences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OrCo: Towards Better Generalization via Orthogonality and Contrast for
  Few-Shot Class-Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noor Ahmed, Anna Kukleva, Bernt Schiele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-Shot Class-Incremental Learning (FSCIL) introduces a paradigm in which
the problem space expands with limited data. FSCIL methods inherently face the
challenge of catastrophic forgetting as data arrives incrementally, making
models susceptible to overwriting previously acquired knowledge. Moreover,
given the scarcity of labeled samples available at any given time, models may
be prone to overfitting and find it challenging to strike a balance between
extensive pretraining and the limited incremental data. To address these
challenges, we propose the OrCo framework built on two core principles:
features' orthogonality in the representation space, and contrastive learning.
In particular, we improve the generalization of the embedding space by
employing a combination of supervised and self-supervised contrastive losses
during the pretraining phase. Additionally, we introduce OrCo loss to address
challenges arising from data limitations during incremental sessions. Through
feature space perturbations and orthogonality between classes, the OrCo loss
maximizes margins and reserves space for the following incremental data. This,
in turn, ensures the accommodation of incoming classes in the feature space
without compromising previously acquired knowledge. Our experimental results
showcase state-of-the-art performance across three benchmark datasets,
including mini-ImageNet, CIFAR100, and CUB datasets. Code is available at
https://github.com/noorahmedds/OrCo
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Semi-supervised Nighttime Dehazing Baseline with Spatial-Frequency
  Aware and Realistic Brightness Constraint <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Cong, Jie Gui, Jing Zhang, Junming Hou, Hao Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing research based on deep learning has extensively explored the problem
of daytime image dehazing. However, few studies have considered the
characteristics of nighttime hazy scenes. There are two distinctions between
nighttime and daytime haze. First, there may be multiple active colored light
sources with lower illumination intensity in nighttime scenes, which may cause
haze, glow and noise with localized, coupled and frequency inconsistent
characteristics. Second, due to the domain discrepancy between simulated and
real-world data, unrealistic brightness may occur when applying a dehazing
model trained on simulated data to real-world data. To address the above two
issues, we propose a semi-supervised model for real-world nighttime dehazing.
First, the spatial attention and frequency spectrum filtering are implemented
as a spatial-frequency domain information interaction module to handle the
first issue. Second, a pseudo-label-based retraining strategy and a local
window-based brightness loss for semi-supervised training process is designed
to suppress haze and glow while achieving realistic brightness. Experiments on
public benchmarks validate the effectiveness of the proposed method and its
superiority over state-of-the-art methods. The source code and Supplementary
Materials are placed in the https://github.com/Xiaofeng-life/SFSNiD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Heatmap-Guided 6-Dof Grasp Detection in Cluttered Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siang Chen, Wei Tang, Pengwei Xie, Wenming Yang, Guijin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fast and robust object grasping in clutter is a crucial component of
robotics. Most current works resort to the whole observed point cloud for 6-Dof
grasp generation, ignoring the guidance information excavated from global
semantics, thus limiting high-quality grasp generation and real-time
performance. In this work, we show that the widely used heatmaps are
underestimated in the efficiency of 6-Dof grasp generation. Therefore, we
propose an effective local grasp generator combined with grasp heatmaps as
guidance, which infers in a global-to-local semantic-to-point way.
Specifically, Gaussian encoding and the grid-based strategy are applied to
predict grasp heatmaps as guidance to aggregate local points into graspable
regions and provide global semantic information. Further, a novel non-uniform
anchor sampling mechanism is designed to improve grasp accuracy and diversity.
Benefiting from the high-efficiency encoding in the image space and focusing on
points in local graspable regions, our framework can perform high-quality grasp
detection in real-time and achieve state-of-the-art results. In addition, real
robot experiments demonstrate the effectiveness of our method with a success
rate of 94% and a clutter completion rate of 100%. Our code is available at
https://github.com/THU-VCLab/HGGD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extensive results on GraspNet-1B dataset</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Plays a Pivotal Role in the Object-Attribute Compositional
  Generalization of CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Abbasi, Mohammad Samiei, Mohammad Hossein Rohban, Mahdieh Soleymani Baghshah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models, such as CLIP, have shown promising
Out-of-Distribution (OoD) generalization under various types of distribution
shifts. Recent studies attempted to investigate the leading cause of this
capability. In this work, we follow the same path, but focus on a specific type
of OoD data - images with novel compositions of attribute-object pairs - and
study whether such models can successfully classify those images into
composition classes. We carefully designed an authentic image test dataset
called ImageNet-AO, consisting of attributes for objects that are unlikely
encountered in the CLIP training sets. We found that CLIPs trained with large
datasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude
improvement in effective compositional OoD generalization compared to both
supervised models and CLIPs trained with smaller datasets, such as CC-12M and
YFCC-15M. Our results provide evidence that the scale and diversity of training
data and language supervision play a key role in unlocking the compositional
generalization abilities of vision-language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Oral accepted at OODCV 2023(http://www.ood-cv.org)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CT-3DFlow : Leveraging 3D Normalizing Flows for Unsupervised Detection
  of Pathological Pulmonary CT scans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aissam Djahnine, Alexandre Popoff, Emilien Jupin-Delevaux, Vincent Cottin, Olivier Nempont, Loic Boussel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised pathology detection can be implemented by training a model on
healthy data only and measuring the deviation from the training set upon
inference, for example with CNN-based feature extraction and one-class
classifiers, or reconstruction-score-based methods such as AEs, GANs and
Diffusion models. Normalizing Flows (NF) have the ability to directly learn the
probability distribution of training examples through an invertible
architecture. We leverage this property in a novel 3D NF-based model named
CT-3DFlow, specifically tailored for patient-level pulmonary pathology
detection in chest CT data. Our model is trained unsupervised on healthy 3D
pulmonary CT patches, and detects deviations from its log-likelihood
distribution as anomalies. We aggregate patches-level likelihood values from a
patient's CT scan to provide a patient-level 'normal'/'abnormal' prediction.
Out-of-distribution detection performance is evaluated using expert annotations
on a separate chest CT test dataset, outperforming other state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ParCo: Part-Coordinating Text-to-Motion Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiran Zou, Shangyuan Yuan, Shian Du, Yu Wang, Chang Liu, Yi Xu, Jie Chen, Xiangyang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a challenging task: text-to-motion synthesis, aiming to generate
motions that align with textual descriptions and exhibit coordinated movements.
Currently, the part-based methods introduce part partition into the motion
synthesis process to achieve finer-grained generation. However, these methods
encounter challenges such as the lack of coordination between different part
motions and difficulties for networks to understand part concepts. Moreover,
introducing finer-grained part concepts poses computational complexity
challenges. In this paper, we propose Part-Coordinating Text-to-Motion
Synthesis (ParCo), endowed with enhanced capabilities for understanding part
motions and communication among different part motion generators, ensuring a
coordinated and fined-grained motion synthesis. Specifically, we discretize
whole-body motion into multiple part motions to establish the prior concept of
different parts. Afterward, we employ multiple lightweight generators designed
to synthesize different part motions and coordinate them through our part
coordination module. Our approach demonstrates superior performance on common
benchmarks with economic computations, including HumanML3D and KIT-ML,
providing substantial evidence of its effectiveness. Code is available at
https://github.com/qrzou/ParCo .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HEMIT: H&E to Multiplex-immunohistochemistry Image Translation with
  Dual-Branch Pix2pix Generator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Bian, Beth Philips, Tim Cootes, Martin Fergie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational analysis of multiplexed immunofluorescence histology data is
emerging as an important method for understanding the tumour micro-environment
in cancer. This work presents HEMIT, a dataset designed for translating
Hematoxylin and Eosin (H&E) sections to multiplex-immunohistochemistry (mIHC)
images, featuring DAPI, CD3, and panCK markers. Distinctively, HEMIT's mIHC
images are multi-component and cellular-level aligned with H&E, enriching
supervised stain translation tasks. To our knowledge, HEMIT is the first
publicly available cellular-level aligned dataset that enables H&E to
multi-target mIHC image translation. This dataset provides the computer vision
community with a valuable resource to develop novel computational methods which
have the potential to gain new insights from H&E slide archives.
  We also propose a new dual-branch generator architecture, using residual
Convolutional Neural Networks (CNNs) and Swin Transformers which achieves
better translation outcomes than other popular algorithms. When evaluated on
HEMIT, it outperforms pix2pixHD, pix2pix, U-Net, and ResNet, achieving the
highest overall score on key metrics including the Structural Similarity Index
Measure (SSIM), Pearson correlation score (R), and Peak signal-to-noise Ratio
(PSNR). Additionally, downstream analysis has been used to further validate the
quality of the generated mIHC images. These results set a new benchmark in the
field of stain translation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direct mineral content prediction from drill core images via transfer
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romana Boiger, Sergey V. Churakov, Ignacio Ballester Llagaria, Georg Kosakowski, Raphael Wüst, Nikolaos I. Prasianakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep subsurface exploration is important for mining, oil and gas industries,
as well as in the assessment of geological units for the disposal of chemical
or nuclear waste, or the viability of geothermal energy systems. Typically,
detailed examinations of subsurface formations or units are performed on
cuttings or core materials extracted during drilling campaigns, as well as on
geophysical borehole data, which provide detailed information about the
petrophysical properties of the rocks. Depending on the volume of rock samples
and the analytical program, the laboratory analysis and diagnostics can be very
time-consuming. This study investigates the potential of utilizing machine
learning, specifically convolutional neural networks (CNN), to assess the
lithology and mineral content solely from analysis of drill core images, aiming
to support and expedite the subsurface geological exploration. The paper
outlines a comprehensive methodology, encompassing data preprocessing, machine
learning methods, and transfer learning techniques. The outcome reveals a
remarkable 96.7% accuracy in the classification of drill core segments into
distinct formation classes. Furthermore, a CNN model was trained for the
evaluation of mineral content using a learning data set from multidimensional
log analysis data (silicate, total clay, carbonate). When benchmarked against
laboratory XRD measurements on samples from the cores, both the advanced
multidimensional log analysis model and the neural network approach developed
here provide equally good performance. This work demonstrates that deep
learning and particularly transfer learning can support extracting
petrophysical properties, including mineral content and formation
classification, from drill core images, thus offering a road map for enhancing
model performance and data set quality in image-based analysis of drill cores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VersaT2I: Improving Text-to-Image Models with Versatile Reward 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianshu Guo, Wenhao Chai, Jie Deng, Hsiang-Wei Huang, Tian Ye, Yichen Xu, Jiawei Zhang, Jenq-Neng Hwang, Gaoang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent text-to-image (T2I) models have benefited from large-scale and
high-quality data, demonstrating impressive performance. However, these T2I
models still struggle to produce images that are aesthetically pleasing,
geometrically accurate, faithful to text, and of good low-level quality. We
present VersaT2I, a versatile training framework that can boost the performance
with multiple rewards of any T2I model. We decompose the quality of the image
into several aspects such as aesthetics, text-image alignment, geometry,
low-level quality, etc. Then, for every quality aspect, we select high-quality
images in this aspect generated by the model as the training set to finetune
the T2I model using the Low-Rank Adaptation (LoRA). Furthermore, we introduce a
gating function to combine multiple quality aspects, which can avoid conflicts
between different quality aspects. Our method is easy to extend and does not
require any manual annotation, reinforcement learning, or model architecture
changes. Extensive experiments demonstrate that VersaT2I outperforms the
baseline methods across various quality criteria.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ I2CKD : Intra- and Inter-Class Knowledge Distillation for Semantic
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayoub Karine, Thibault Napoléon, Maher Jridi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a new knowledge distillation method tailored for image
semantic segmentation, termed Intra- and Inter-Class Knowledge Distillation
(I2CKD). The focus of this method is on capturing and transferring knowledge
between the intermediate layers of teacher (cumbersome model) and student
(compact model). For knowledge extraction, we exploit class prototypes derived
from feature maps. To facilitate knowledge transfer, we employ a triplet loss
in order to minimize intra-class variances and maximize inter-class variances
between teacher and student prototypes. Consequently, I2CKD enables the student
to better mimic the feature representation of the teacher for each class,
thereby enhancing the segmentation performance of the compact network.
Extensive experiments on three segmentation datasets, i.e., Cityscapes, Pascal
VOC and CamVid, using various teacher-student network pairs demonstrate the
effectiveness of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling uncertainty for Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Savant, Diego Valsesia, Enrico Magli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Stochastic Gaussian Splatting (SGS): the first framework for
uncertainty estimation using Gaussian Splatting (GS). GS recently advanced the
novel-view synthesis field by achieving impressive reconstruction quality at a
fraction of the computational cost of Neural Radiance Fields (NeRF). However,
contrary to the latter, it still lacks the ability to provide information about
the confidence associated with their outputs. To address this limitation, in
this paper, we introduce a Variational Inference-based approach that seamlessly
integrates uncertainty prediction into the common rendering pipeline of GS.
Additionally, we introduce the Area Under Sparsification Error (AUSE) as a new
term in the loss function, enabling optimization of uncertainty estimation
alongside image reconstruction. Experimental results on the LLFF dataset
demonstrate that our method outperforms existing approaches in terms of both
image rendering quality and uncertainty estimation accuracy. Overall, our
framework equips practitioners with valuable insights into the reliability of
synthesized views, facilitating safer decision-making in real-world
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffusionFace: Towards a Comprehensive <span class="highlight-title">Dataset</span> for Diffusion-Based Face
  Forgery Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongxi Chen, Ke Sun, Ziyin Zhou, Xianming Lin, Xiaoshuai Sun, Liujuan Cao, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid progress in deep learning has given rise to hyper-realistic facial
forgery methods, leading to concerns related to misinformation and security
risks. Existing face forgery datasets have limitations in generating
high-quality facial images and addressing the challenges posed by evolving
generative techniques. To combat this, we present DiffusionFace, the first
diffusion-based face forgery dataset, covering various forgery categories,
including unconditional and Text Guide facial image generation, Img2Img,
Inpaint, and Diffusion-based facial exchange algorithms. Our DiffusionFace
dataset stands out with its extensive collection of 11 diffusion models and the
high-quality of the generated images, providing essential metadata and a
real-world internet-sourced forgery facial image dataset for evaluation.
Additionally, we provide an in-depth analysis of the data and introduce
practical evaluation protocols to rigorously assess discriminative models'
effectiveness in detecting counterfeit facial images, aiming to enhance
security in facial image authentication processes. The dataset is available for
download at \url{https://github.com/Rapisurazurite/DiffFace}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain
  Adaptive Segmentation of 3D Point Clouds <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhimin Yuan, Wankang Zeng, Yanfei Su, Weiquan Liu, Ming Cheng, Yulan Guo, Cheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D synthetic-to-real unsupervised domain adaptive segmentation is crucial to
annotating new domains. Self-training is a competitive approach for this task,
but its performance is limited by different sensor sampling patterns (i.e.,
variations in point density) and incomplete training strategies. In this work,
we propose a density-guided translator (DGT), which translates point density
between domains, and integrates it into a two-stage self-training pipeline
named DGT-ST. First, in contrast to existing works that simultaneously conduct
data generation and feature/output alignment within unstable adversarial
training, we employ the non-learnable DGT to bridge the domain gap at the input
level. Second, to provide a well-initialized model for self-training, we
propose a category-level adversarial network in stage one that utilizes the
prototype to prevent negative transfer. Finally, by leveraging the designs
above, a domain-mixed self-training method with source-aware consistency loss
is proposed in stage two to narrow the domain gap further. Experiments on two
synthetic-to-real segmentation tasks (SynLiDAR $\rightarrow$ semanticKITTI and
SynLiDAR $\rightarrow$ semanticPOSS) demonstrate that DGT-ST outperforms
state-of-the-art methods, achieving 9.4$\%$ and 4.3$\%$ mIoU improvements,
respectively. Code is available at \url{https://github.com/yuan-zm/DGT-ST}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning Segmentation and Classification of Red Blood Cells Using a
  Large Multi-Scanner <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Elmanna, Ahmed Elsafty, Yomna Ahmed, Muhammad Rushdi, Ahmed Morsy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital pathology has recently been revolutionized by advancements in
artificial intelligence, deep learning, and high-performance computing. With
its advanced tools, digital pathology can help improve and speed up the
diagnostic process, reduce human errors, and streamline the reporting step. In
this paper, we report a new large red blood cell (RBC) image dataset and
propose a two-stage deep learning framework for RBC image segmentation and
classification. The dataset is a highly diverse dataset of more than 100K RBCs
containing eight different classes. The dataset, which is considerably larger
than any publicly available hematopathology dataset, was labeled independently
by two hematopathologists who also manually created masks for RBC cell
segmentation. Subsequently, in the proposed framework, first, a U-Net model was
trained to achieve automatic RBC image segmentation. Second, an EfficientNetB0
model was trained to classify RBC images into one of the eight classes using a
transfer learning approach with a 5X2 cross-validation scheme. An IoU of 98.03%
and an average classification accuracy of 96.5% were attained on the test set.
Moreover, we have performed experimental comparisons against several prominent
CNN models. These comparisons show the superiority of the proposed model with a
good balance between performance and computational cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 12 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffStyler: Diffusion-based Localized Image Style Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoxu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image style transfer aims to imbue digital imagery with the distinctive
attributes of style targets, such as colors, brushstrokes, shapes, whilst
concurrently preserving the semantic integrity of the content. Despite the
advancements in arbitrary style transfer methods, a prevalent challenge remains
the delicate equilibrium between content semantics and style attributes. Recent
developments in large-scale text-to-image diffusion models have heralded
unprecedented synthesis capabilities, albeit at the expense of relying on
extensive and often imprecise textual descriptions to delineate artistic
styles. Addressing these limitations, this paper introduces DiffStyler, a novel
approach that facilitates efficient and precise arbitrary image style transfer.
DiffStyler lies the utilization of a text-to-image Stable Diffusion model-based
LoRA to encapsulate the essence of style targets. This approach, coupled with
strategic cross-LoRA feature and attention injection, guides the style transfer
process. The foundation of our methodology is rooted in the observation that
LoRA maintains the spatial feature consistency of UNet, a discovery that
further inspired the development of a mask-wise style transfer technique. This
technique employs masks extracted through a pre-trained FastSAM model,
utilizing mask prompts to facilitate feature fusion during the denoising
process, thereby enabling localized style transfer that preserves the original
image's unaffected regions. Moreover, our approach accommodates multiple style
targets through the use of corresponding masks. Through extensive
experimentation, we demonstrate that DiffStyler surpasses previous methods in
achieving a more harmonious balance between content preservation and style
integration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Vision-and-Language Navigation With Offline RL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valay Bundele, Mahesh Bhupati, Biplab Banerjee, Aditya Grover
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of vision-and-language navigation (VLN) has typically relied on
expert trajectories, which may not always be available in real-world situations
due to the significant effort required to collect them. On the other hand,
existing approaches to training VLN agents that go beyond available expert data
involve data augmentations or online exploration which can be tedious and
risky. In contrast, it is easy to access large repositories of suboptimal
offline trajectories. Inspired by research in offline reinforcement learning
(ORL), we introduce a new problem setup of VLN-ORL which studies VLN using
suboptimal demonstration data. We introduce a simple and effective
reward-conditioned approach that can account for dataset suboptimality for
training VLN agents, as well as benchmarks to evaluate progress and promote
research in this area. We empirically study various noise models for
characterizing dataset suboptimality among other unique challenges in VLN-ORL
and instantiate it for the VLN$\circlearrowright$BERT and MTVM architectures in
the R2R and RxR environments. Our experiments demonstrate that the proposed
reward-conditioned approach leads to significant performance improvements, even
in complex and intricate environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (04/2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inhwan Bae, Young-Jae Park, Hae-Gon Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are five types of trajectory prediction tasks: deterministic,
stochastic, domain adaptation, momentary observation, and few-shot. These
associated tasks are defined by various factors, such as the length of input
paths, data split and pre-processing methods. Interestingly, even though they
commonly take sequential coordinates of observations as input and infer future
paths in the same coordinates as output, designing specialized architectures
for each task is still necessary. For the other task, generality issues can
lead to sub-optimal performances. In this paper, we propose SingularTrajectory,
a diffusion-based universal trajectory prediction framework to reduce the
performance gap across the five tasks. The core of SingularTrajectory is to
unify a variety of human dynamics representations on the associated tasks. To
do this, we first build a Singular space to project all types of motion
patterns from each task into one embedding space. We next propose an adaptive
anchor working in the Singular space. Unlike traditional fixed anchor methods
that sometimes yield unacceptable paths, our adaptive anchor enables correct
anchors, which are put into a wrong location, based on a traversability map.
Finally, we adopt a diffusion-based predictor to further enhance the prototype
paths using a cascaded denoising process. Our unified framework ensures the
generality across various benchmark settings such as input modality, and
trajectory lengths. Extensive experiments on five public benchmarks demonstrate
that SingularTrajectory substantially outperforms existing models, highlighting
its effectiveness in estimating general dynamics of human movements. Code is
publicly available at https://github.com/inhwanbae/SingularTrajectory .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Language Beat Numerical Regression? Language-Based Multimodal
  Trajectory Prediction <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inhwan Bae, Junoh Lee, Hae-Gon Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have demonstrated impressive ability in context understanding
and generative performance. Inspired by the recent success of language
foundation models, in this paper, we propose LMTraj (Language-based Multimodal
Trajectory predictor), which recasts the trajectory prediction task into a sort
of question-answering problem. Departing from traditional numerical regression
models, which treat the trajectory coordinate sequence as continuous signals,
we consider them as discrete signals like text prompts. Specially, we first
transform an input space for the trajectory coordinate into the natural
language space. Here, the entire time-series trajectories of pedestrians are
converted into a text prompt, and scene images are described as text
information through image captioning. The transformed numerical and image data
are then wrapped into the question-answering template for use in a language
model. Next, to guide the language model in understanding and reasoning
high-level knowledge, such as scene context and social relationships between
pedestrians, we introduce an auxiliary multi-task question and answering. We
then train a numerical tokenizer with the prompt data. We encourage the
tokenizer to separate the integer and decimal parts well, and leverage it to
capture correlations between the consecutive numbers in the language model.
Lastly, we train the language model using the numerical tokenizer and all of
the question-answer prompts. Here, we propose a beam-search-based most-likely
prediction and a temperature-based multimodal prediction to implement both
deterministic and stochastic inferences. Applying our LMTraj, we show that the
language-based model can be a powerful pedestrian trajectory predictor, and
outperforms existing numerical-based predictor methods. Code is publicly
available at https://github.com/inhwanbae/LMTrajectory .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $\mathrm{F^2Depth}$: Self-supervised Indoor Monocular Depth Estimation
  via Optical Flow Consistency and Feature Map Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaotong Guo, Huijie Zhao, Shuwei Shao, Xudong Li, Baochang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised monocular depth estimation methods have been increasingly
given much attention due to the benefit of not requiring large, labelled
datasets. Such self-supervised methods require high-quality salient features
and consequently suffer from severe performance drop for indoor scenes, where
low-textured regions dominant in the scenes are almost indiscriminative. To
address the issue, we propose a self-supervised indoor monocular depth
estimation framework called $\mathrm{F^2Depth}$. A self-supervised optical flow
estimation network is introduced to supervise depth learning. To improve
optical flow estimation performance in low-textured areas, only some patches of
points with more discriminative features are adopted for finetuning based on
our well-designed patch-based photometric loss. The finetuned optical flow
estimation network generates high-accuracy optical flow as a supervisory signal
for depth estimation. Correspondingly, an optical flow consistency loss is
designed. Multi-scale feature maps produced by finetuned optical flow
estimation network perform warping to compute feature map synthesis loss as
another supervisory signal for depth learning. Experimental results on the NYU
Depth V2 dataset demonstrate the effectiveness of the framework and our
proposed losses. To evaluate the generalization ability of our
$\mathrm{F^2Depth}$, we collect a Campus Indoor depth dataset composed of
approximately 1500 points selected from 99 images in 18 scenes. Zero-shot
generalization experiments on 7-Scenes dataset and Campus Indoor achieve
$\delta_1$ accuracy of 75.8% and 76.0% respectively. The accuracy results show
that our model can generalize well to monocular images captured in unknown
indoor scenes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Backpropagation-free Network for 3D Test-time Adaptation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanshuo Wang, Ali Cheraghian, Zeeshan Hayder, Jie Hong, Sameera Ramasinghe, Shafin Rahman, David Ahmedt-Aristizabal, Xuesong Li, Lars Petersson, Mehrtash Harandi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world systems often encounter new data over time, which leads to
experiencing target domain shifts. Existing Test-Time Adaptation (TTA) methods
tend to apply computationally heavy and memory-intensive backpropagation-based
approaches to handle this. Here, we propose a novel method that uses a
backpropagation-free approach for TTA for the specific case of 3D data. Our
model uses a two-stream architecture to maintain knowledge about the source
domain as well as complementary target-domain-specific information. The
backpropagation-free property of our model helps address the well-known
forgetting problem and mitigates the error accumulation issue. The proposed
method also eliminates the need for the usually noisy process of
pseudo-labeling and reliance on costly self-supervised training. Moreover, our
method leverages subspace learning, effectively reducing the distribution
variance between the two domains. Furthermore, the source-domain-specific and
the target-domain-specific streams are aligned using a novel entropy-based
adaptive fusion strategy. Extensive experiments on popular benchmarks
demonstrate the effectiveness of our method. The code will be available at
https://github.com/abie-e/BFTT3D.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilias Mitsouras, Eleftherios Tsonis, Paraskevi Tzouveli, Athanasios Voulodimos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated remarkable performance in text-to-image
synthesis, producing realistic and high resolution images that faithfully
adhere to the corresponding text-prompts. Despite their great success, they
still fall behind in sketch-to-image synthesis tasks, where in addition to
text-prompts, the spatial layout of the generated images has to closely follow
the outlines of certain reference sketches. Employing an MLP latent edge
predictor to guide the spatial layout of the synthesized image by predicting
edge maps at each denoising step has been recently proposed. Despite yielding
promising results, the pixel-wise operation of the MLP does not take into
account the spatial layout as a whole, and demands numerous denoising
iterations to produce satisfactory images, leading to time inefficiency. To
this end, we introduce U-Sketch, a framework featuring a U-Net type latent edge
predictor, which is capable of efficiently capturing both local and global
features, as well as spatial correlations between pixels. Moreover, we propose
the addition of a sketch simplification network that offers the user the choice
of preprocessing and simplifying input sketches for enhanced outputs. The
experimental results, corroborated by user feedback, demonstrate that our
proposed U-Net latent edge predictor leads to more realistic results, that are
better aligned with the spatial outlines of the reference sketches, while
drastically reducing the number of required denoising steps and, consequently,
the overall execution time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECNet: Effective Controllable Text-to-Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicheng Li, Keqiang Sun, Zhixin Lai, Xiaoshi Wu, Feng Qiu, Haoran Xie, Kazunori Miyata, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conditional text-to-image diffusion models have garnered significant
attention in recent years. However, the precision of these models is often
compromised mainly for two reasons, ambiguous condition input and inadequate
condition guidance over single denoising loss. To address the challenges, we
introduce two innovative solutions. Firstly, we propose a Spatial Guidance
Injector (SGI) which enhances conditional detail by encoding text inputs with
precise annotation information. This method directly tackles the issue of
ambiguous control inputs by providing clear, annotated guidance to the model.
Secondly, to overcome the issue of limited conditional supervision, we
introduce Diffusion Consistency Loss (DCL), which applies supervision on the
denoised latent code at any given time step. This encourages consistency
between the latent code at each time step and the input signal, thereby
enhancing the robustness and accuracy of the output. The combination of SGI and
DCL results in our Effective Controllable Network (ECNet), which offers a more
accurate controllable end-to-end text-to-image generation framework with a more
precise conditioning input and stronger controllable supervision. We validate
our approach through extensive experiments on generation under various
conditions, such as human body skeletons, facial landmarks, and sketches of
general objects. The results consistently demonstrate that our method
significantly enhances the controllability and robustness of the generated
images, outperforming existing state-of-the-art controllable text-to-image
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Channel-ensemble Approach: Unbiased and Low-variance Pseudo-labels is
  Critical for Semi-supervised Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Wu, Junbiao Pang, Baochang Zhang, Qingming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) is a practical challenge in computer vision.
Pseudo-label (PL) methods, e.g., FixMatch and FreeMatch, obtain the State Of
The Art (SOTA) performances in SSL. These approaches employ a
threshold-to-pseudo-label (T2L) process to generate PLs by truncating the
confidence scores of unlabeled data predicted by the self-training method.
However, self-trained models typically yield biased and high-variance
predictions, especially in the scenarios when a little labeled data are
supplied. To address this issue, we propose a lightweight channel-based
ensemble method to effectively consolidate multiple inferior PLs into the
theoretically guaranteed unbiased and low-variance one. Importantly, our
approach can be readily extended to any SSL framework, such as FixMatch or
FreeMatch. Experimental results demonstrate that our method significantly
outperforms state-of-the-art techniques on CIFAR10/100 in terms of
effectiveness and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering
  Using a VLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonkyun Kim, Changin Choi, Wonseok Lee, Wonjong Rhee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stimulated by the sophisticated reasoning capabilities of recent Large
Language Models (LLMs), a variety of strategies for bridging video modality
have been devised. A prominent strategy involves Video Language Models
(VideoLMs), which train a learnable interface with video data to connect
advanced vision encoders with LLMs. Recently, an alternative strategy has
surfaced, employing readily available foundation models, such as VideoLMs and
LLMs, across multiple stages for modality bridging. In this study, we introduce
a simple yet novel strategy where only a single Vision Language Model (VLM) is
utilized. Our starting point is the plain insight that a video comprises a
series of images, or frames, interwoven with temporal information. The essence
of video comprehension lies in adeptly managing the temporal aspects along with
the spatial details of each frame. Initially, we transform a video into a
single composite image by arranging multiple frames in a grid layout. The
resulting single image is termed as an image grid. This format, while
maintaining the appearance of a solitary image, effectively retains temporal
information within the grid structure. Therefore, the image grid approach
enables direct application of a single high-performance VLM without
necessitating any video-data training. Our extensive experimental analysis
across ten zero-shot video question answering benchmarks, including five
open-ended and five multiple-choice benchmarks, reveals that the proposed Image
Grid Vision Language Model (IG-VLM) surpasses the existing methods in nine out
of ten benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at https://github.com/imagegridworth/IG-VLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Colour and Brush Stroke Pattern Recognition in Abstract Art using
  Modified Deep Convolutional Generative Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srinitish Srinivasan, Varenya Pathak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstract Art is an immensely popular, discussed form of art that often has
the ability to depict the emotions of an artist. Many researchers have made
attempts to study abstract art in the form of edge detection, brush stroke and
emotion recognition algorithms using machine and deep learning. This papers
describes the study of a wide distribution of abstract paintings using
Generative Adversarial Neural Networks(GAN). GANs have the ability to learn and
reproduce a distribution enabling researchers and scientists to effectively
explore and study the generated image space. However, the challenge lies in
developing an efficient GAN architecture that overcomes common training
pitfalls. This paper addresses this challenge by introducing a modified-DCGAN
(mDCGAN) specifically designed for high-quality artwork generation. The
approach involves a thorough exploration of the modifications made, delving
into the intricate workings of DCGANs, optimisation techniques, and
regularisation methods aimed at improving stability and realism in art
generation enabling effective study of generated patterns. The proposed mDCGAN
incorporates meticulous adjustments in layer configurations and architectural
choices, offering tailored solutions to the unique demands of art generation
while effectively combating issues like mode collapse and gradient vanishing.
Further this paper explores the generated latent space by performing random
walks to understand vector relationships between brush strokes and colours in
the abstract art space and a statistical analysis of unstable outputs after a
certain period of GAN training and compare its significant difference. These
findings validate the effectiveness of the proposed approach, emphasising its
potential to revolutionise the field of digital art generation and digital art
ecosystem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 5 tables, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FTBC: Forward Temporal Bias Correction for Optimizing ANN-SNN Conversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Wu, Velibor Bojkovic, Bin Gu, Kun Suo, Kai Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) offer a promising avenue for energy-efficient
computing compared with Artificial Neural Networks (ANNs), closely mirroring
biological neural processes. However, this potential comes with inherent
challenges in directly training SNNs through spatio-temporal backpropagation --
stemming from the temporal dynamics of spiking neurons and their discrete
signal processing -- which necessitates alternative ways of training, most
notably through ANN-SNN conversion. In this work, we introduce a lightweight
Forward Temporal Bias Correction (FTBC) technique, aimed at enhancing
conversion accuracy without the computational overhead. We ground our method on
provided theoretical findings that through proper temporal bias calibration the
expected error of ANN-SNN conversion can be reduced to be zero after each time
step. We further propose a heuristic algorithm for finding the temporal bias
only in the forward pass, thus eliminating the computational burden of
backpropagation and we evaluate our method on CIFAR-10/100 and ImageNet
datasets, achieving a notable increase in accuracy on all datasets. Codes are
released at a GitHub repository.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Multi-modal Models are Good Class-Incremental Learners <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xusheng Cao, Haori Lu, Linlan Huang, Xialei Liu, Ming-Ming Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic
forgetting caused by the classifier's bias towards the current task has long
posed a significant challenge. It is mainly caused by the characteristic of
discriminative models. With the growing popularity of the generative
multi-modal models, we would explore replacing discriminative models with
generative ones for CIL. However, transitioning from discriminative to
generative models requires addressing two key challenges. The primary challenge
lies in transferring the generated textual information into the classification
of distinct categories. Additionally, it requires formulating the task of CIL
within a generative framework. To this end, we propose a novel generative
multi-modal model (GMM) framework for class-incremental learning. Our approach
directly generates labels for images using an adapted generative model. After
obtaining the detailed text, we use a text encoder to extract text features and
employ feature matching to determine the most similar label as the
classification prediction. In the conventional CIL settings, we achieve
significantly better results in long-sequence task scenarios. Under the
Few-shot CIL setting, we have improved by at least 14\% accuracy over all the
current state-of-the-art methods with significantly less forgetting. Our code
is available at \url{https://github.com/DoubleClass/GMM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BAM: Box Abstraction Monitors for Real-time OoD Detection in Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changshun Wu, Weicheng He, Chih-Hong Cheng, Xiaowei Huang, Saddek Bensalem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OoD) detection techniques for deep neural networks
(DNNs) become crucial thanks to their filtering of abnormal inputs, especially
when DNNs are used in safety-critical applications and interact with an open
and dynamic environment. Nevertheless, integrating OoD detection into
state-of-the-art (SOTA) object detection DNNs poses significant challenges,
partly due to the complexity introduced by the SOTA OoD construction methods,
which require the modification of DNN architecture and the introduction of
complex loss functions. This paper proposes a simple, yet surprisingly
effective, method that requires neither retraining nor architectural change in
object detection DNN, called Box Abstraction-based Monitors (BAM). The novelty
of BAM stems from using a finite union of convex box abstractions to capture
the learned features of objects for in-distribution (ID) data, and an important
observation that features from OoD data are more likely to fall outside of
these boxes. The union of convex regions within the feature space allows the
formation of non-convex and interpretable decision boundaries, overcoming the
limitations of VOS-like detectors without sacrificing real-time performance.
Experiments integrating BAM into Faster R-CNN-based object detection DNNs
demonstrate a considerably improved performance against SOTA OoD detection
techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ship in Sight: Diffusion Models for Ship-Image Super Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luigi Sigillo, Riccardo Fosco Gramaccioni, Alessandro Nicolosi, Danilo Comminiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, remarkable advancements have been achieved in the field of
image generation, primarily driven by the escalating demand for high-quality
outcomes across various image generation subtasks, such as inpainting,
denoising, and super resolution. A major effort is devoted to exploring the
application of super-resolution techniques to enhance the quality of
low-resolution images. In this context, our method explores in depth the
problem of ship image super resolution, which is crucial for coastal and port
surveillance. We investigate the opportunity given by the growing interest in
text-to-image diffusion models, taking advantage of the prior knowledge that
such foundation models have already learned. In particular, we present a
diffusion-model-based architecture that leverages text conditioning during
training while being class-aware, to best preserve the crucial details of the
ships during the generation of the super-resoluted image. Since the specificity
of this task and the scarcity availability of off-the-shelf data, we also
introduce a large labeled ship dataset scraped from online ship images, mostly
from ShipSpotting\footnote{\url{www.shipspotting.com}} website. Our method
achieves more robust results than other deep learning models previously
employed for super resolution, as proven by the multiple experiments performed.
Moreover, we investigate how this model can benefit downstream tasks, such as
classification and object detection, thus emphasizing practical implementation
in a real-world scenario. Experimental results show flexibility, reliability,
and impressive performance of the proposed framework over state-of-the-art
methods for different tasks. The code is available at:
https://github.com/LuigiSigillo/ShipinSight .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2024 International Joint Conference on Neural Networks
  (IJCNN)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViTAR: Vision Transformer with Any Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qihang Fan, Quanzeng You, Xiaotian Han, Yongfei Liu, Yunzhe Tao, Huaibo Huang, Ran He, Hongxia Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  his paper tackles a significant challenge faced by Vision Transformers
(ViTs): their constrained scalability across different image resolutions.
Typically, ViTs experience a performance decline when processing resolutions
different from those seen during training. Our work introduces two key
innovations to address this issue. Firstly, we propose a novel module for
dynamic resolution adjustment, designed with a single Transformer block,
specifically to achieve highly efficient incremental token integration.
Secondly, we introduce fuzzy positional encoding in the Vision Transformer to
provide consistent positional awareness across multiple resolutions, thereby
preventing overfitting to any single training resolution. Our resulting model,
ViTAR (Vision Transformer with Any Resolution), demonstrates impressive
adaptability, achieving 83.3\% top-1 accuracy at a 1120x1120 resolution and
80.4\% accuracy at a 4032x4032 resolution, all while reducing computational
costs. ViTAR also shows strong performance in downstream tasks such as instance
and semantic segmentation and can easily combined with self-supervised learning
techniques like Masked AutoEncoder. Our work provides a cost-effective solution
for enhancing the resolution scalability of ViTs, paving the way for more
versatile and efficient high-resolution image processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning CNN on ViT: A Hybrid Model to Explicitly Class-specific
  Boundaries for Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ba Hung Ngo, Nhat-Tuong Do-Tran, Tuan-Ngoc Nguyen, Hae-Gon Jeon, Tae Jong Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most domain adaptation (DA) methods are based on either a convolutional
neural networks (CNNs) or a vision transformers (ViTs). They align the
distribution differences between domains as encoders without considering their
unique characteristics. For instance, ViT excels in accuracy due to its
superior ability to capture global representations, while CNN has an advantage
in capturing local representations. This fact has led us to design a hybrid
method to fully take advantage of both ViT and CNN, called Explicitly
Class-specific Boundaries (ECB). ECB learns CNN on ViT to combine their
distinct strengths. In particular, we leverage ViT's properties to explicitly
find class-specific decision boundaries by maximizing the discrepancy between
the outputs of the two classifiers to detect target samples far from the source
support. In contrast, the CNN encoder clusters target features based on the
previously defined class-specific boundaries by minimizing the discrepancy
between the probabilities of the two classifiers. Finally, ViT and CNN mutually
exchange knowledge to improve the quality of pseudo labels and reduce the
knowledge discrepancies of these models. Compared to conventional DA methods,
our ECB achieves superior performance, which verifies its effectiveness in this
hybrid model. The project website can be found
https://dotrannhattuong.github.io/ECB/website/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MonoHair: High-Fidelity Hair Modeling from a Monocular Video <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keyu Wu, Lingchen Yang, Zhiyi Kuang, Yao Feng, Xutao Han, Yuefan Shen, Hongbo Fu, Kun Zhou, Youyi Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Undoubtedly, high-fidelity 3D hair is crucial for achieving realism, artistic
expression, and immersion in computer graphics. While existing 3D hair modeling
methods have achieved impressive performance, the challenge of achieving
high-quality hair reconstruction persists: they either require strict capture
conditions, making practical applications difficult, or heavily rely on learned
prior data, obscuring fine-grained details in images. To address these
challenges, we propose MonoHair,a generic framework to achieve high-fidelity
hair reconstruction from a monocular video, without specific requirements for
environments. Our approach bifurcates the hair modeling process into two main
stages: precise exterior reconstruction and interior structure inference. The
exterior is meticulously crafted using our Patch-based Multi-View Optimization
(PMVO). This method strategically collects and integrates hair information from
multiple views, independent of prior data, to produce a high-fidelity exterior
3D line map. This map not only captures intricate details but also facilitates
the inference of the hair's inner structure. For the interior, we employ a
data-driven, multi-view 3D hair reconstruction method. This method utilizes 2D
structural renderings derived from the reconstructed exterior, mirroring the
synthetic 2D inputs used during training. This alignment effectively bridges
the domain gap between our training data and real-world data, thereby enhancing
the accuracy and reliability of our interior structure inference. Lastly, we
generate a strand model and resolve the directional ambiguity by our hair
growth algorithm. Our experiments demonstrate that our method exhibits
robustness across diverse hairstyles and achieves state-of-the-art performance.
For more results, please refer to our project page
https://keyuwu-cs.github.io/MonoHair/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Diverse Agricultural Data for Vision-Based Farming
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikolaj Cieslak, Umabharathi Govindarajan, Alejandro Garcia, Anuradha Chandrashekar, Torsten Hädrich, Aleksander Mendoza-Drosik, Dominik L. Michels, Sören Pirk, Chia-Chun Fu, Wojciech Pałubicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a specialized procedural model for generating synthetic
agricultural scenes, focusing on soybean crops, along with various weeds. This
model is capable of simulating distinct growth stages of these plants, diverse
soil conditions, and randomized field arrangements under varying lighting
conditions. The integration of real-world textures and environmental factors
into the procedural generation process enhances the photorealism and
applicability of the synthetic data. Our dataset includes 12,000 images with
semantic labels, offering a comprehensive resource for computer vision tasks in
precision agriculture, such as semantic segmentation for autonomous weed
control. We validate our model's effectiveness by comparing the synthetic data
against real agricultural images, demonstrating its potential to significantly
augment training data for machine learning models in agriculture. This approach
not only provides a cost-effective solution for generating high-quality,
diverse data but also addresses specific needs in agricultural vision tasks
that are not fully covered by general-purpose models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Quantum Fuzzy-based Approach for Real-Time Detection of Solar Coronal
  Holes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanmoy Bandyopadhyay, Suman Kundu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection and analysis of the solar coronal holes (CHs) is an important
field of study in the domain of solar physics. Mainly, it is required for the
proper prediction of the geomagnetic storms which directly or indirectly affect
various space and ground-based systems. For the detection of CHs till date, the
solar scientist depends on manual hand-drawn approaches. However, with the
advancement of image processing technologies, some automated image segmentation
methods have been used for the detection of CHs. In-spite of this, fast and
accurate detection of CHs are till a major issues. Here in this work, a novel
quantum computing-based fast fuzzy c-mean technique has been developed for fast
detection of the CHs region. The task has been carried out in two stages, in
first stage the solar image has been segmented using a quantum computing based
fast fuzzy c-mean (QCFFCM) and in the later stage the CHs has been extracted
out from the segmented image based on image morphological operation. In the
work, quantum computing has been used to optimize the cost function of the fast
fuzzy c-mean (FFCM) algorithm, where quantum approximate optimization algorithm
(QAOA) has been used to optimize the quadratic part of the cost function. The
proposed method has been tested for 193 \AA{} SDO/AIA full-disk solar image
datasets and has been compared with the existing techniques. The outcome shows
the comparable performance of the proposed method with the existing one within
a very lesser time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying and Mitigating Unimodal Biases in Multimodal Large Language
  Models: A Causal Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meiqi Chen, Yixin Cao, Yan Zhang, Chaochao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have facilitated the
development of Multimodal LLMs (MLLMs). Despite their impressive capabilities,
MLLMs often suffer from an over-reliance on unimodal biases (e.g., language
bias and vision bias), leading to incorrect answers in complex multimodal
tasks. To investigate this issue, we propose a causal framework to interpret
the biases in Visual Question Answering (VQA) problems. Within our framework,
we devise a causal graph to elucidate the predictions of MLLMs on VQA problems,
and assess the causal effect of biases through an in-depth causal analysis.
Motivated by the causal graph, we introduce a novel MORE dataset, consisting of
12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities,
necessitating multi-hop reasoning and the surmounting of unimodal biases.
Furthermore, we propose two strategies to mitigate unimodal biases and enhance
MLLMs' reasoning capabilities, including a Decompose-Verify-Answer (DeVA)
framework for limited-access MLLMs and the refinement of open-source MLLMs
through fine-tuning. Extensive quantitative and qualitative experiments offer
valuable insights for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Inclusion Matching for Animation Paint Bucket Colorization <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuekun Dai, Shangchen Zhou, Qinyue Li, Chongyi Li, Chen Change Loy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Colorizing line art is a pivotal task in the production of hand-drawn cel
animation. This typically involves digital painters using a paint bucket tool
to manually color each segment enclosed by lines, based on RGB values
predetermined by a color designer. This frame-by-frame process is both arduous
and time-intensive. Current automated methods mainly focus on segment matching.
This technique migrates colors from a reference to the target frame by aligning
features within line-enclosed segments across frames. However, issues like
occlusion and wrinkles in animations often disrupt these direct
correspondences, leading to mismatches. In this work, we introduce a new
learning-based inclusion matching pipeline, which directs the network to
comprehend the inclusion relationships between segments rather than relying
solely on direct visual correspondences. Our method features a two-stage
pipeline that integrates a coarse color warping module with an inclusion
matching module, enabling more nuanced and accurate colorization. To facilitate
the training of our network, we also develope a unique dataset, referred to as
PaintBucket-Character. This dataset includes rendered line arts alongside their
colorized counterparts, featuring various 3D characters. Extensive experiments
demonstrate the effectiveness and superiority of our method over existing
techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to CVPR 2024. Project Page:
  https://ykdai.github.io/projects/InclusionMatching</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ H2ASeg: Hierarchical Adaptive Interaction and Weighting Network for
  Tumor Segmentation in PET/CT Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinpeng Lu, Jingyun Chen, Linghan Cai, Songhan Jiang, Yongbing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Positron emission tomography (PET) combined with computed tomography (CT)
imaging is routinely used in cancer diagnosis and prognosis by providing
complementary information. Automatically segmenting tumors in PET/CT images can
significantly improve examination efficiency. Traditional multi-modal
segmentation solutions mainly rely on concatenation operations for modality
fusion, which fail to effectively model the non-linear dependencies between PET
and CT modalities. Recent studies have investigated various approaches to
optimize the fusion of modality-specific features for enhancing joint
representations. However, modality-specific encoders used in these methods
operate independently, inadequately leveraging the synergistic relationships
inherent in PET and CT modalities, for example, the complementarity between
semantics and structure. To address these issues, we propose a Hierarchical
Adaptive Interaction and Weighting Network termed H2ASeg to explore the
intrinsic cross-modal correlations and transfer potential complementary
information. Specifically, we design a Modality-Cooperative Spatial Attention
(MCSA) module that performs intra- and inter-modal interactions globally and
locally. Additionally, a Target-Aware Modality Weighting (TAMW) module is
developed to highlight tumor-related features within multi-modal features,
thereby refining tumor segmentation. By embedding these modules across
different layers, H2ASeg can hierarchically model cross-modal correlations,
enabling a nuanced understanding of both semantic and structural tumor
features. Extensive experiments demonstrate the superiority of H2ASeg,
outperforming state-of-the-art methods on AutoPet-II and Hecktor2022
benchmarks. The code is released at https://github.com/G14nTDo4/H2ASeg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages,4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DODA: Diffusion for Object-detection Domain Adaptation in Agriculture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Xiang, Pieter M. Blok, James Burridge, Haozhou Wang, Wei Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The diverse and high-quality content generated by recent generative models
demonstrates the great potential of using synthetic data to train downstream
models. However, in vision, especially in objection detection, related areas
are not fully explored, the synthetic images are merely used to balance the
long tails of existing datasets, and the accuracy of the generated labels is
low, the full potential of generative models has not been exploited. In this
paper, we propose DODA, a data synthesizer that can generate high-quality
object detection data for new domains in agriculture. Specifically, we improve
the controllability of layout-to-image through encoding layout as an image,
thereby improving the quality of labels, and use a visual encoder to provide
visual clues for the diffusion model to decouple visual features from the
diffusion model, and empowering the model the ability to generate data in new
domains. On the Global Wheat Head Detection (GWHD) Dataset, which is the
largest dataset in agriculture and contains diverse domains, using the data
synthesized by DODA improves the performance of the object detector by
12.74-17.76 AP$_{50}$ in the domain that was significantly shifted from the
training data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tracking-Assisted Object Detection with Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting-Kang Yen, Igor Morawski, Shusil Dangi, Kai He, Chung-Yi Lin, Jia-Fong Yeh, Hung-Ting Su, Winston Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event-based object detection has recently garnered attention in the computer
vision community due to the exceptional properties of event cameras, such as
high dynamic range and no motion blur. However, feature asynchronism and
sparsity cause invisible objects due to no relative motion to the camera,
posing a significant challenge in the task. Prior works have studied various
memory mechanisms to preserve as many features as possible at the current time,
guided by temporal clues. While these implicit-learned memories retain some
short-term information, they still struggle to preserve long-term features
effectively. In this paper, we consider those invisible objects as
pseudo-occluded objects and aim to reveal their features. Firstly, we introduce
visibility attribute of objects and contribute an auto-labeling algorithm to
append additional visibility labels on an existing event camera dataset.
Secondly, we exploit tracking strategies for pseudo-occluded objects to
maintain their permanence and retain their bounding boxes, even when features
have not been available for a very long time. These strategies can be treated
as an explicit-learned memory guided by the tracking objective to record the
displacements of objects across frames. Lastly, we propose a spatio-temporal
feature aggregation module to enrich the latent features and a consistency loss
to increase the robustness of the overall pipeline. We conduct comprehensive
experiments to verify our method's effectiveness where still objects are
retained but real occluded objects are discarded. The results demonstrate that
(1) the additional visibility labels can assist in supervised training, and (2)
our method outperforms state-of-the-art approaches with a significant
improvement of 7.9% absolute mAP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PIPNet3D: Interpretable Detection of Alzheimer in MRI Scans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisa Anita De Santi, Jörg Schlötterer, Michael Scheschenja, Joel Wessendorf, Meike Nauta, Vincenzo Positano, Christin Seifert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information from neuroimaging examinations (CT, MRI) is increasingly used to
support diagnoses of dementia, e.g., Alzheimer's disease. While current
clinical practice is mainly based on visual inspection and feature engineering,
Deep Learning approaches can be used to automate the analysis and to discover
new image-biomarkers. Part-prototype neural networks (PP-NN) are an alternative
to standard blackbox models, and have shown promising results in general
computer vision. PP-NN's base their reasoning on prototypical image regions
that are learned fully unsupervised, and combined with a simple-to-understand
decision layer. We present PIPNet3D, a PP-NN for volumetric images. We apply
PIPNet3D to the clinical case study of Alzheimer's Disease diagnosis from
structural Magnetic Resonance Imaging (sMRI). We assess the quality of
prototypes under a systematic evaluation framework, propose new metrics to
evaluate brain prototypes and perform an evaluation with domain experts. Our
results show that PIPNet3D is an interpretable, compact model for Alzheimer's
diagnosis with its reasoning well aligned to medical domain knowledge. Notably,
PIPNet3D achieves the same accuracy as its blackbox counterpart; and removing
the remaining clinically irrelevant prototypes from its decision process does
not decrease predictive performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implementation of the Principal Component Analysis onto High-Performance
  Computer Facilities for Hyperspectral Dimensionality Reduction: Results and
  Comparisons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        E. Martel, R. Lazcano, J. Lopez, D. Madroñal, R. Salvador, S. Lopez, E. Juarez, R. Guerra, C. Sanz, R. Sarmiento
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dimensionality reduction represents a critical preprocessing step in order to
increase the efficiency and the performance of many hyperspectral imaging
algorithms. However, dimensionality reduction algorithms, such as the Principal
Component Analysis (PCA), suffer from their computationally demanding nature,
becoming advisable for their implementation onto high-performance computer
architectures for applications under strict latency constraints. This work
presents the implementation of the PCA algorithm onto two different
high-performance devices, namely, an NVIDIA Graphics Processing Unit (GPU) and
a Kalray manycore, uncovering a highly valuable set of tips and tricks in order
to take full advantage of the inherent parallelism of these high-performance
computing platforms, and hence, reducing the time that is required to process a
given hyperspectral image. Moreover, the achieved results obtained with
different hyperspectral images have been compared with the ones that were
obtained with a field programmable gate array (FPGA)-based implementation of
the PCA algorithm that has been recently published, providing, for the first
time in the literature, a comprehensive analysis in order to highlight the pros
and cons of each option.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-Aware SAR ATR: Defending Against Adversarial Attacks via
  Bayesian Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Ye, Rajgopal Kannan, Viktor Prasanna, Carl Busart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks have demonstrated the vulnerability of Machine Learning
(ML) image classifiers in Synthetic Aperture Radar (SAR) Automatic Target
Recognition (ATR) systems. An adversarial attack can deceive the classifier
into making incorrect predictions by perturbing the input SAR images, for
example, with a few scatterers attached to the on-ground objects. Therefore, it
is critical to develop robust SAR ATR systems that can detect potential
adversarial attacks by leveraging the inherent uncertainty in ML classifiers,
thereby effectively alerting human decision-makers. In this paper, we propose a
novel uncertainty-aware SAR ATR for detecting adversarial attacks.
Specifically, we leverage the capability of Bayesian Neural Networks (BNNs) in
performing image classification with quantified epistemic uncertainty to
measure the confidence for each input SAR image. By evaluating the uncertainty,
our method alerts when the input SAR image is likely to be adversarially
generated. Simultaneously, we also generate visual explanations that reveal the
specific regions in the SAR image where the adversarial scatterers are likely
to to be present, thus aiding human decision-making with hints of evidence of
adversarial attacks. Experiments on the MSTAR dataset demonstrate that our
approach can identify over 80% adversarial SAR images with fewer than 20% false
alarms, and our visual explanations can identify up to over 90% of scatterers
in an adversarial SAR image.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrinivas Ramasubramanian, Harsh Rangwani, Sho Takemori, Kunal Samanta, Yuhei Umeda, Venkatesh Babu Radhakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise in internet usage has led to the generation of massive amounts of
data, resulting in the adoption of various supervised and semi-supervised
machine learning algorithms, which can effectively utilize the colossal amount
of data to train models. However, before deploying these models in the real
world, these must be strictly evaluated on performance measures like worst-case
recall and satisfy constraints such as fairness. We find that current
state-of-the-art empirical techniques offer sub-optimal performance on these
practical, non-decomposable performance objectives. On the other hand, the
theoretical techniques necessitate training a new model from scratch for each
performance objective. To bridge the gap, we propose SelMix, a selective
mixup-based inexpensive fine-tuning technique for pre-trained models, to
optimize for the desired objective. The core idea of our framework is to
determine a sampling distribution to perform a mixup of features between
samples from particular classes such that it optimizes the given objective. We
comprehensively evaluate our technique against the existing empirical and
theoretically principled methods on standard benchmark datasets for imbalanced
classification. We find that proposed SelMix fine-tuning significantly improves
the performance for various practical non-decomposable objectives across
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 SpotLight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-scale Unified Network for Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenzhuo Liu, Fei Zhu, Cheng-Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional Neural Networks (CNNs) have advanced significantly in visual
representation learning and recognition. However, they face notable challenges
in performance and computational efficiency when dealing with real-world,
multi-scale image inputs. Conventional methods rescale all input images into a
fixed size, wherein a larger fixed size favors performance but rescaling small
size images to a larger size incurs digitization noise and increased
computation cost. In this work, we carry out a comprehensive, layer-wise
investigation of CNN models in response to scale variation, based on Centered
Kernel Alignment (CKA) analysis. The observations reveal lower layers are more
sensitive to input image scale variations than high-level layers. Inspired by
this insight, we propose Multi-scale Unified Network (MUSN) consisting of
multi-scale subnets, a unified network, and scale-invariant constraint. Our
method divides the shallow layers into multi-scale subnets to enable feature
extraction from multi-scale inputs, and the low-level features are unified in
deep layers for extracting high-level semantic features. A scale-invariant
constraint is posed to maintain feature consistency across different scales.
Extensive experiments on ImageNet and other scale-diverse datasets, demonstrate
that MSUN achieves significant improvements in both model performance and
computational efficiency. Particularly, MSUN yields an accuracy increase up to
44.53% and diminishes FLOPs by 7.01-16.13% in multi-scale scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Test-Time Adaptation of Vision-Language Models <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adilbek Karmanov, Dayan Guan, Shijian Lu, Abdulmotaleb El Saddik, Eric Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time adaptation with pre-trained vision-language models has attracted
increasing attention for tackling distribution shifts during the test time.
Though prior studies have achieved very promising performance, they involve
intensive computation which is severely unaligned with test-time adaptation. We
design TDA, a training-free dynamic adapter that enables effective and
efficient test-time adaptation with vision-language models. TDA works with a
lightweight key-value cache that maintains a dynamic queue with few-shot pseudo
labels as values and the corresponding test-sample features as keys. Leveraging
the key-value cache, TDA allows adapting to test data gradually via progressive
pseudo label refinement which is super-efficient without incurring any
backpropagation. In addition, we introduce negative pseudo labeling that
alleviates the adverse impact of pseudo label noises by assigning pseudo labels
to certain negative classes when the model is uncertain about its pseudo label
predictions. Extensive experiments over two benchmarks demonstrate TDA's
superior effectiveness and efficiency as compared with the state-of-the-art.
The code has been released in \url{https://kdiaaa.github.io/tda/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024. The code has been released in
  \url{https://kdiaaa.github.io/tda/}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Non-Exemplar Semi-Supervised Class-Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenzhuo Liu, Fei Zhu, Cheng-Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks perform remarkably well in close-world scenarios.
However, novel classes emerged continually in real applications, making it
necessary to learn incrementally. Class-incremental learning (CIL) aims to
gradually recognize new classes while maintaining the discriminability of old
ones. Existing CIL methods have two limitations: a heavy reliance on preserving
old data for forgetting mitigation and the need for vast labeled data for
knowledge adaptation. To overcome these issues, we propose a non-exemplar
semi-supervised CIL framework with contrastive learning and semi-supervised
incremental prototype classifier (Semi-IPC). On the one hand, contrastive
learning helps the model learn rich representations, easing the trade-off
between learning representations of new classes and forgetting that of old
classes. On the other hand, Semi-IPC learns a prototype for each class with
unsupervised regularization, enabling the model to incrementally learn from
partially labeled new data while maintaining the knowledge of old classes.
Experiments on benchmark datasets demonstrate the strong performance of our
method: without storing any old samples and only using less than 1% of labels,
Semi-IPC outperforms advanced exemplar-based methods. We hope our work offers
new insights for future CIL research. The code will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SGDM: Static-Guided Dynamic Module Make Stronger Visual Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjie Xing, Zhenchao Cui, Jing Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The spatial attention mechanism has been widely used to improve object
detection performance. However, its operation is currently limited to static
convolutions lacking content-adaptive features. This paper innovatively
approaches from the perspective of dynamic convolution. We propose Razor
Dynamic Convolution (RDConv) to address thetwo flaws in dynamic weight
convolution, making it hard to implement in spatial mechanism: 1) it is
computation-heavy; 2) when generating weights, spatial information is
disregarded. Firstly, by using Razor Operation to generate certain features, we
vastly reduce the parameters of the entire dynamic convolution operation.
Secondly, we added a spatial branch inside RDConv to generate convolutional
kernel parameters with richer spatial information. Embedding dynamic
convolution will also bring the problem of sensitivity to high-frequency noise.
We propose the Static-Guided Dynamic Module (SGDM) to address this limitation.
By using SGDM, we utilize a set of asymmetric static convolution kernel
parameters to guide the construction of dynamic convolution. We introduce the
mechanism of shared weights in static convolution to solve the problem of
dynamic convolution being sensitive to high-frequency noise. Extensive
experiments illustrate that multiple different object detection backbones
equipped with SGDM achieve a highly competitive boost in performance(e.g., +4%
mAP with YOLOv5n on VOC and +1.7% mAP with YOLOv8n on COCO) with negligible
parameter increase(i.e., +0.33M on YOLOv5n and +0.19M on YOLOv8n).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIR-HLoc: Adaptive Image Retrieval for Efficient Visual Localisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changkun Liu, Huajian Huang, Zhengyang Ma, Tristan Braud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art (SOTA) hierarchical localisation pipelines (HLoc) rely on
image retrieval (IR) techniques to establish 2D-3D correspondences by selecting
the $k$ most similar images from a reference image database for a given query
image. Although higher values of $k$ enhance localisation robustness, the
computational cost for feature matching increases linearly with $k$. In this
paper, we observe that queries that are the most similar to images in the
database result in a higher proportion of feature matches and, thus, more
accurate positioning. Thus, a small number of images is sufficient for queries
very similar to images in the reference database. We then propose a novel
approach, AIR-HLoc, which divides query images into different localisation
difficulty levels based on their similarity to the reference image database. We
consider an image with high similarity to the reference image as an easy query
and an image with low similarity as a hard query. Easy queries show a limited
improvement in accuracy when increasing $k$. Conversely, higher values of $k$
significantly improve accuracy for hard queries. Given the limited improvement
in accuracy when increasing $k$ for easy queries and the significant
improvement for hard queries, we adapt the value of $k$ to the query's
difficulty level. Therefore, AIR-HLoc optimizes processing time by adaptively
assigning different values of $k$ based on the similarity between the query and
reference images without losing accuracy. Our extensive experiments on the
Cambridge Landmarks, 7Scenes, and Aachen Day-Night-v1.1 datasets demonstrate
our algorithm's efficacy, reducing 30\%, 26\%, and 11\% in computational
overhead while maintaining SOTA accuracy compared to HLoc with fixed image
retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DVLO: Deep Visual-<span class="highlight-title">LiDAR</span> <span class="highlight-title">Odometry</span> with Local-to-Global Feature Fusion and
  Bi-Directional Structure Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiuming Liu, Dong Zhuo, Zhiheng Feng, Siting Zhu, Chensheng Peng, Zhe Liu, Hesheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information inside visual and LiDAR data is well complementary derived from
the fine-grained texture of images and massive geometric information in point
clouds. However, it remains challenging to explore effective visual-LiDAR
fusion, mainly due to the intrinsic data structure inconsistency between two
modalities: Images are regular and dense, but LiDAR points are unordered and
sparse. To address the problem, we propose a local-to-global fusion network
with bi-directional structure alignment. To obtain locally fused features, we
project points onto image plane as cluster centers and cluster image pixels
around each center. Image pixels are pre-organized as pseudo points for
image-to-point structure alignment. Then, we convert points to pseudo images by
cylindrical projection (point-to-image structure alignment) and perform
adaptive global feature fusion between point features with local fused
features. Our method achieves state-of-the-art performance on KITTI odometry
and FlyingThings3D scene flow datasets compared to both single-modal and
multi-modal methods. Codes will be released later.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unleashing the Potential of SAM for Medical Adaptation via Hierarchical
  Decoding <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiheng Cheng, Qingyue Wei, Hongru Zhu, Yan Wang, Liangqiong Qu, Wei Shao, Yuyin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Segment Anything Model (SAM) has garnered significant attention for its
versatile segmentation abilities and intuitive prompt-based interface. However,
its application in medical imaging presents challenges, requiring either
substantial training costs and extensive medical datasets for full model
fine-tuning or high-quality prompts for optimal performance. This paper
introduces H-SAM: a prompt-free adaptation of SAM tailored for efficient
fine-tuning of medical images via a two-stage hierarchical decoding procedure.
In the initial stage, H-SAM employs SAM's original decoder to generate a prior
probabilistic mask, guiding a more intricate decoding process in the second
stage. Specifically, we propose two key designs: 1) A class-balanced,
mask-guided self-attention mechanism addressing the unbalanced label
distribution, enhancing image embedding; 2) A learnable mask cross-attention
mechanism spatially modulating the interplay among different image regions
based on the prior mask. Moreover, the inclusion of a hierarchical pixel
decoder in H-SAM enhances its proficiency in capturing fine-grained and
localized details. This approach enables SAM to effectively integrate learned
medical priors, facilitating enhanced adaptation for medical image segmentation
with limited samples. Our H-SAM demonstrates a 4.78% improvement in average
Dice compared to existing prompt-free SAM variants for multi-organ segmentation
using only 10% of 2D slices. Notably, without using any unlabeled data, H-SAM
even outperforms state-of-the-art semi-supervised models relying on extensive
unlabeled training data across various medical datasets. Our code is available
at https://github.com/Cccccczh404/H-SAM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Deraining via Self-supervised Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He-Hao Liao, Yan-Tsung Peng, Wen-Tao Chu, Ping-Chun Hsieh, Chung-Chi Tsai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quality of images captured outdoors is often affected by the weather. One
factor that interferes with sight is rain, which can obstruct the view of
observers and computer vision applications that rely on those images. The work
aims to recover rain images by removing rain streaks via Self-supervised
Reinforcement Learning (RL) for image deraining (SRL-Derain). We locate rain
streak pixels from the input rain image via dictionary learning and use
pixel-wise RL agents to take multiple inpainting actions to remove rain
progressively. To our knowledge, this work is the first attempt where
self-supervised RL is applied to image deraining. Experimental results on
several benchmark image-deraining datasets show that the proposed SRL-Derain
performs favorably against state-of-the-art few-shot and self-supervised
deraining and denoising methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Branch-Tuning: Balancing Stability and Plasticity for Continual
  Self-Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenzhuo Liu, Fei Zhu, Cheng-Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) has emerged as an effective paradigm for
deriving general representations from vast amounts of unlabeled data. However,
as real-world applications continually integrate new content, the high
computational and resource demands of SSL necessitate continual learning rather
than complete retraining. This poses a challenge in striking a balance between
stability and plasticity when adapting to new information. In this paper, we
employ Centered Kernel Alignment for quantitatively analyzing model stability
and plasticity, revealing the critical roles of batch normalization layers for
stability and convolutional layers for plasticity. Motivated by this, we
propose Branch-tuning, an efficient and straightforward method that achieves a
balance between stability and plasticity in continual SSL. Branch-tuning
consists of branch expansion and compression, and can be easily applied to
various SSL methods without the need of modifying the original methods,
retaining old data or models. We validate our method through incremental
experiments on various benchmark datasets, demonstrating its effectiveness and
practical value in real-world scenarios. We hope our work offers new insights
for future continual self-supervised learning research. The code will be made
publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Interactive Regional Understanding in Vision-Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jungbeom Lee, Sanghyuk Chun, Sangdoo Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Vision-Language Pre-training (VLP) models have demonstrated
significant advancements. Nevertheless, these models heavily rely on image-text
pairs that capture only coarse and global information of an image, leading to a
limitation in their regional understanding ability. In this work, we introduce
\textbf{RegionVLM}, equipped with explicit regional modeling capabilities,
allowing them to understand user-indicated image regions. To achieve this, we
design a simple yet innovative architecture, requiring no modifications to the
model architecture or objective function. Additionally, we leverage a dataset
that contains a novel source of information, namely Localized Narratives, which
has been overlooked in previous VLP research. Our experiments demonstrate that
our single generalist model not only achieves an interactive dialogue system
but also exhibits superior performance on various zero-shot region
understanding tasks, without compromising its ability for global image
understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shifting to Machine Supervision: Annotation-Efficient Semi and
  Self-Supervised Learning for Automatic Medical Image Segmentation and
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10319v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10319v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Singh, Raviteja Chukkapalli, Shravan Chaudhari, Luoyao Chen, Mei Chen, Jinqian Pan, Craig Smuda, Jacopo Cirrone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in clinical treatment are increasingly constrained by the
limitations of supervised learning techniques, which depend heavily on large
volumes of annotated data. The annotation process is not only costly but also
demands substantial time from clinical specialists. Addressing this issue, we
introduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging)
pipeline, a novel approach that leverages advancements in self-supervised and
semi-supervised learning. These techniques engage in auxiliary tasks that do
not require labeling, thus simplifying the scaling of machine supervision
compared to fully-supervised methods. Our study benchmarks these techniques on
three distinct medical imaging datasets to evaluate their effectiveness in
classification and segmentation tasks. Notably, we observed that self
supervised learning significantly surpassed the performance of supervised
methods in the classification of all evaluated datasets. Remarkably, the
semi-supervised approach demonstrated superior outcomes in segmentation,
outperforming fully-supervised methods while using 50% fewer labels across all
datasets. In line with our commitment to contributing to the scientific
community, we have made the S4MI code openly accessible, allowing for broader
application and further development of these methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Seventeen pages (incl. references), five figures, and one table.
  (Under Review)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Object Detection with Zero-Shot Day-Night Domain Adaptation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01220v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01220v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhipeng Du, Miaojing Shi, Jiankang Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting objects in low-light scenarios presents a persistent challenge, as
detectors trained on well-lit data exhibit significant performance degradation
on low-light data due to low visibility. Previous methods mitigate this issue
by exploring image enhancement or object detection techniques with real
low-light image datasets. However, the progress is impeded by the inherent
difficulties about collecting and annotating low-light images. To address this
challenge, we propose to boost low-light object detection with zero-shot
day-night domain adaptation, which aims to generalize a detector from well-lit
scenarios to low-light ones without requiring real low-light data. Revisiting
Retinex theory in the low-level vision, we first design a reflectance
representation learning module to learn Retinex-based illumination invariance
in images with a carefully designed illumination invariance reinforcement
strategy. Next, an interchange-redecomposition-coherence procedure is
introduced to improve over the vanilla Retinex image decomposition process by
performing two sequential image decompositions and introducing a
redecomposition cohering loss. Extensive experiments on ExDark, DARK FACE, and
CODaN datasets show strong low-light generalizability of our method. Our code
is available at https://github.com/ZPDu/DAI-Net.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupled Data Consistency with Diffusion Purification for Image
  Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06054v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06054v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Soo Min Kwon, Ismail R. Alkhouri, Saiprasad Ravishankar, Qing Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently gained traction as a powerful class of deep
generative priors, excelling in a wide range of image restoration tasks due to
their exceptional ability to model data distributions. To solve image
restoration problems, many existing techniques achieve data consistency by
incorporating additional likelihood gradient steps into the reverse sampling
process of diffusion models. However, the additional gradient steps pose a
challenge for real-world practical applications as they incur a large
computational overhead, thereby increasing inference time. They also present
additional difficulties when using accelerated diffusion model samplers, as the
number of data consistency steps is limited by the number of reverse sampling
steps. In this work, we propose a novel diffusion-based image restoration
solver that addresses these issues by decoupling the reverse process from the
data consistency steps. Our method involves alternating between a
reconstruction phase to maintain data consistency and a refinement phase that
enforces the prior via diffusion purification. Our approach demonstrates
versatility, making it highly adaptable for efficient problem-solving in latent
space. Additionally, it reduces the necessity for numerous sampling steps
through the integration of consistency models. The efficacy of our approach is
validated through comprehensive experiments across various image restoration
tasks, including image denoising, deblurring, inpainting, and super-resolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretable machine learning for time-to-event prediction in medicine
  and healthcare 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09817v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09817v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hubert Baniecki, Bartlomiej Sobieski, Patryk Szatkowski, Przemyslaw Bombinski, Przemyslaw Biecek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-to-event prediction, e.g. cancer survival analysis or hospital length of
stay, is a highly prominent machine learning task in medical and healthcare
applications. However, only a few interpretable machine learning methods comply
with its challenges. To facilitate a comprehensive explanatory analysis of
survival models, we formally introduce time-dependent feature effects and
global feature importance explanations. We show how post-hoc interpretation
methods allow for finding biases in AI systems predicting length of stay using
a novel multi-modal dataset created from 1235 X-ray images with textual
radiology reports annotated by human experts. Moreover, we evaluate cancer
survival models beyond predictive performance to include the importance of
multi-omics feature groups based on a large-scale benchmark comprising 11
datasets from The Cancer Genome Atlas (TCGA). Model developers can use the
proposed methods to debug and improve machine learning algorithms, while
physicians can discover disease biomarkers and assess their significance. We
hope the contributed open data and code resources facilitate future work in the
emerging research direction of explainable survival analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An extended version of an AIME 2023 paper submitted to Artificial
  Intelligence in Medicine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simplified Diffusion Schrödinger Bridge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14623v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14623v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicong Tang, Tiankai Hang, Shuyang Gu, Dong Chen, Baining Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel theoretical simplification of the Diffusion
Schr\"odinger Bridge (DSB) that facilitates its unification with Score-based
Generative Models (SGMs), addressing the limitations of DSB in complex data
generation and enabling faster convergence and enhanced performance. By
employing SGMs as an initial solution for DSB, our approach capitalizes on the
strengths of both frameworks, ensuring a more efficient training process and
improving the performance of SGM. We also propose a reparameterization
technique that, despite theoretical approximations, practically improves the
network's fitting capabilities. Our extensive experimental evaluations confirm
the effectiveness of the simplified DSB, demonstrating its significant
improvements. We believe the contributions of this work pave the way for
advanced generative modeling. The code is available at
https://github.com/checkcrab/SDSB.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-supervised co-salient object detection via feature correspondence
  at multiple scales 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11107v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11107v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Souradeep Chakraborty, Dimitris Samaras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our paper introduces a novel two-stage self-supervised approach for detecting
co-occurring salient objects (CoSOD) in image groups without requiring
segmentation annotations. Unlike existing unsupervised methods that rely solely
on patch-level information (e.g. clustering patch descriptors) or on
computation heavy off-the-shelf components for CoSOD, our lightweight model
leverages feature correspondences at both patch and region levels,
significantly improving prediction performance. In the first stage, we train a
self-supervised network that detects co-salient regions by computing local
patch-level feature correspondences across images. We obtain the segmentation
predictions using confidence-based adaptive thresholding. In the next stage, we
refine these intermediate segmentations by eliminating the detected regions
(within each image) whose averaged feature representations are dissimilar to
the foreground feature representation averaged across all the cross-attention
maps (from the previous stage). Extensive experiments on three CoSOD benchmark
datasets show that our self-supervised model outperforms the corresponding
state-of-the-art models by a huge margin (e.g. on the CoCA dataset, our model
has a 13.7% F-measure gain over the SOTA unsupervised CoSOD model). Notably,
our self-supervised model also outperforms several recent fully supervised
CoSOD models on the three test datasets (e.g., on the CoCA dataset, our model
has a 4.6% F-measure gain over a recent supervised CoSOD model).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LION: Implicit Vision Prompt Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09992v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09992v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haixin Wang, Jianlong Chang, Xiao Luo, Jinan Sun, Zhouchen Lin, Qi Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent competitive performance across a range of vision tasks, vision
Transformers still have an issue of heavy computational costs. Recently, vision
prompt learning has provided an economic solution to this problem without
fine-tuning the whole large-scale models. However, the efficiency of existing
models are still far from satisfactory due to insertion of extensive prompts
blocks and trick prompt designs. In this paper, we propose an efficient vision
model named impLicit vIsion prOmpt tuNing (LION), which is motivated by deep
implicit models with stable memory costs for various complex tasks. In
particular, we merely insect two equilibrium implicit layers in two ends of the
pre-trained main backbone with parameters in the backbone frozen. Moreover, we
prune the parameters in these two layers according to lottery hypothesis. The
performance obtained by our LION are promising on a wide range of datasets. In
particular, our LION reduces up to 11.5% of training parameter numbers while
obtaining higher performance compared with the state-of-the-art baseline VPT,
especially under challenging scenes. Furthermore, we find that our proposed
LION had a good generalization performance, making it an easy way to boost
transfer learning in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024; 9 pages, 3 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Incorporating simulated spatial context information improves the
  effectiveness of contrastive learning models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15120v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15120v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lizhen Zhu, James Z. Wang, Wonseuk Lee, Brad Wyble
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual learning often occurs in a specific context, where an agent acquires
skills through exploration and tracking of its location in a consistent
environment. The historical spatial context of the agent provides a similarity
signal for self-supervised contrastive learning. We present a unique approach,
termed Environmental Spatial Similarity (ESS), that complements existing
contrastive learning methods. Using images from simulated, photorealistic
environments as an experimental setting, we demonstrate that ESS outperforms
traditional instance discrimination approaches. Moreover, sampling additional
data from the same environment substantially improves accuracy and provides new
augmentations. ESS allows remarkable proficiency in room classification and
spatial prediction tasks, especially in unfamiliar environments. This learning
paradigm has the potential to enable rapid visual learning in agents operating
in new environments with unique visual characteristics. Potentially
transformative applications span from robotics to space exploration. Our proof
of concept demonstrates improved efficiency over methods that rely on
extensive, disconnected datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12091v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12091v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yu, Danruo Deng, Furui Liu, Yueming Jin, Qi Dou, Guangyong Chen, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) methods assume that labeled data, unlabeled
data and test data are from the same distribution. Open-set semi-supervised
learning (Open-set SSL) considers a more practical scenario, where unlabeled
data and test data contain new categories (outliers) not observed in labeled
data (inliers). Most previous works focused on outlier detection via binary
classifiers, which suffer from insufficient scalability and inability to
distinguish different types of uncertainty. In this paper, we propose a novel
framework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these
limitations. Concretely, we first introduce evidential deep learning (EDL) as
an outlier detector to quantify different types of uncertainty, and design
different uncertainty metrics for self-training and inference. Furthermore, we
propose a novel adaptive negative optimization strategy, making EDL more
tailored to the unlabeled dataset containing both inliers and outliers. As
demonstrated empirically, our proposed method outperforms existing
state-of-the-art methods across four datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision Transformer-Based Deep Learning for Histologic Classification of
  Endometrial Cancer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08479v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08479v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manu Goyal, Laura J. Tafe, James X. Feng, Kristen E. Muller, Liesbeth Hondelink, Jessica L. Bentz, Saeed Hassanpour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Endometrial cancer, the fourth most common cancer in females in the United
States, with the lifetime risk for developing this disease is approximately
2.8% in women. Precise histologic evaluation and molecular classification of
endometrial cancer is important for effective patient management and
determining the best treatment modalities. This study introduces EndoNet, which
uses convolutional neural networks for extracting histologic features and a
vision transformer for aggregating these features and classifying slides based
on their visual characteristics into high- and low- grade. The model was
trained on 929 digitized hematoxylin and eosin-stained whole-slide images of
endometrial cancer from hysterectomy cases at Dartmouth-Health. It classifies
these slides into low-grade (Endometroid Grades 1 and 2) and high-grade
(endometroid carcinoma FIGO grade 3, uterine serous carcinoma, carcinosarcoma)
categories. EndoNet was evaluated on an internal test set of 110 patients and
an external test set of 100 patients from the public TCGA database. The model
achieved a weighted average F1-score of 0.91 (95% CI: 0.86-0.95) and an AUC of
0.95 (95% CI: 0.89-0.99) on the internal test, and 0.86 (95% CI: 0.80-0.94) for
F1-score and 0.86 (95% CI: 0.75-0.93) for AUC on the external test. Pending
further validation, EndoNet has the potential to support pathologists without
the need of manual annotations in classifying the grades of gynecologic
pathology tumors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 Tables and 3 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automated Construction of Time-Space Diagrams for Traffic Analysis Using
  Street-View Video Sequence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.06098v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.06098v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanay Rastogi, Mårten Björkman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-space diagrams are essential tools for analyzing traffic patterns and
optimizing transportation infrastructure and traffic management strategies.
Traditional data collection methods for these diagrams have limitations in
terms of temporal and spatial coverage. Recent advancements in camera
technology have overcome these limitations and provided extensive urban data.
In this study, we propose an innovative approach to constructing time-space
diagrams by utilizing street-view video sequences captured by cameras mounted
on moving vehicles. Using the state-of-the-art YOLOv5, StrongSORT, and
photogrammetry techniques for distance calculation, we can infer vehicle
trajectories from the video data and generate time-space diagrams. To evaluate
the effectiveness of our proposed method, we utilized datasets from the KITTI
computer vision benchmark suite. The evaluation results demonstrate that our
approach can generate trajectories from video data, although there are some
errors that can be mitigated by improving the performance of the detector,
tracker, and distance calculation components. In conclusion, the utilization of
street-view video sequences captured by cameras mounted on moving vehicles,
combined with state-of-the-art computer vision techniques, has immense
potential for constructing comprehensive time-space diagrams. These diagrams
offer valuable insights into traffic patterns and contribute to the design of
transportation infrastructure and traffic management strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is published in 2023 IEEE 26th International Conference on
  Intelligent Transportation Systems (ITSC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SOAC: Spatio-Temporal Overlap-Aware Multi-Sensor Calibration using
  Neural Radiance Fields <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15803v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15803v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quentin Herau, Nathan Piasco, Moussab Bennehar, Luis Roldão, Dzmitry Tsishkou, Cyrille Migniot, Pascal Vasseur, Cédric Demonceaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In rapidly-evolving domains such as autonomous driving, the use of multiple
sensors with different modalities is crucial to ensure high operational
precision and stability. To correctly exploit the provided information by each
sensor in a single common frame, it is essential for these sensors to be
accurately calibrated. In this paper, we leverage the ability of Neural
Radiance Fields (NeRF) to represent different sensors modalities in a common
volumetric representation to achieve robust and accurate spatio-temporal sensor
calibration. By designing a partitioning approach based on the visible part of
the scene for each sensor, we formulate the calibration problem using only the
overlapping areas. This strategy results in a more robust and accurate
calibration that is less prone to failure. We demonstrate that our approach
works on outdoor urban scenes by validating it on multiple established driving
datasets. Results show that our method is able to get better accuracy and
robustness compared to existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024. Project page: https://qherau.github.io/SOAC/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Point, Segment and Count: A Generalized Framework for Object Counting <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12386v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12386v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhizhong Huang, Mingliang Dai, Yi Zhang, Junping Zhang, Hongming Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-agnostic object counting aims to count all objects in an image with
respect to example boxes or class names, \emph{a.k.a} few-shot and zero-shot
counting. In this paper, we propose a generalized framework for both few-shot
and zero-shot object counting based on detection. Our framework combines the
superior advantages of two foundation models without compromising their
zero-shot capability: (\textbf{i}) SAM to segment all possible objects as mask
proposals, and (\textbf{ii}) CLIP to classify proposals to obtain accurate
object counts. However, this strategy meets the obstacles of efficiency
overhead and the small crowded objects that cannot be localized and
distinguished. To address these issues, our framework, termed PseCo, follows
three steps: point, segment, and count. Specifically, we first propose a
class-agnostic object localization to provide accurate but least point prompts
for SAM, which consequently not only reduces computation costs but also avoids
missing small objects. Furthermore, we propose a generalized object
classification that leverages CLIP image/text embeddings as the classifier,
following a hierarchical knowledge distillation to obtain discriminative
classifications among hierarchical mask proposals. Extensive experimental
results on FSC-147, COCO, and LVIS demonstrate that PseCo achieves
state-of-the-art performance in both few-shot/zero-shot object
counting/detection. Code: https://github.com/Hzzone/PseCo
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024. Camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech
  Gesture Generation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17532v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17532v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingqun Qi, Jiahao Pan, Peng Li, Ruibin Yuan, Xiaowei Chi, Mengfei Li, Wenhan Luo, <span class="highlight-author">Wei Xu</span>e, Shanghang Zhang, Qifeng Liu, Yike Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating vivid and emotional 3D co-speech gestures is crucial for virtual
avatar animation in human-machine interaction applications. While the existing
methods enable generating the gestures to follow a single emotion label, they
overlook that long gesture sequence modeling with emotion transition is more
practical in real scenes. In addition, the lack of large-scale available
datasets with emotional transition speech and corresponding 3D human gestures
also limits the addressing of this task. To fulfill this goal, we first
incorporate the ChatGPT-4 and an audio inpainting approach to construct the
high-fidelity emotion transition human speeches. Considering obtaining the
realistic 3D pose annotations corresponding to the dynamically inpainted
emotion transition audio is extremely difficult, we propose a novel weakly
supervised training strategy to encourage authority gesture transitions.
Specifically, to enhance the coordination of transition gestures w.r.t
different emotional ones, we model the temporal association representation
between two different emotional gesture sequences as style guidance and infuse
it into the transition generation. We further devise an emotion mixture
mechanism that provides weak supervision based on a learnable mixed emotion
label for transition gestures. Last, we present a keyframe sampler to supply
effective initial posture cues in long sequences, enabling us to generate
diverse gestures. Extensive experiments demonstrate that our method outperforms
the state-of-the-art models constructed by adapting single emotion-conditioned
counterparts on our newly defined emotion transition task and datasets. Our
code and dataset will be released on the project page:
https://xingqunqi-lab.github.io/Emo-Transition-Gesture/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning by Erasing: Conditional Entropy based Transferable
  Out-Of-Distribution Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.11041v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.11041v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Xing, Zhiyong Feng, Yong Su, Changjae Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection is essential to handle the distribution
shifts between training and test scenarios. For a new in-distribution (ID)
dataset, existing methods require retraining to capture the dataset-specific
feature representation or data distribution. In this paper, we propose a deep
generative models (DGM) based transferable OOD detection method, which is
unnecessary to retrain on a new ID dataset. We design an image erasing strategy
to equip exclusive conditional entropy distribution for each ID dataset, which
determines the discrepancy of DGM's posteriori ucertainty distribution on
different ID datasets. Owing to the powerful representation capacity of
convolutional neural networks, the proposed model trained on complex dataset
can capture the above discrepancy between ID datasets without retraining and
thus achieve transferable OOD detection. We validate the proposed method on
five datasets and verity that ours achieves comparable performance to the
state-of-the-art group based OOD detection methods that need to be retrained to
deploy on new ID datasets. Our code is available at
https://github.com/oOHCIOo/CETOOD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>update new experimental results</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual Structure-Aware Image Filterings for Semi-supervised Medical Image
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuliang Gu, Zhichao Sun, Tian Chen, Xin Xiao, Yepeng Liu, Yongchao Xu, Laurent Najman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised image segmentation has attracted great attention recently.
The key is how to leverage unlabeled images in the training process. Most
methods maintain consistent predictions of the unlabeled images under
variations (e.g., adding noise/perturbations, or creating alternative versions)
in the image and/or model level. In most image-level variation, medical images
often have prior structure information, which has not been well explored. In
this paper, we propose novel dual structure-aware image filterings (DSAIF) as
the image-level variations for semi-supervised medical image segmentation.
Motivated by connected filtering that simplifies image via filtering in
structure-aware tree-based image representation, we resort to the dual contrast
invariant Max-tree and Min-tree representation. Specifically, we propose a
novel connected filtering that removes topologically equivalent nodes (i.e.
connected components) having no siblings in the Max/Min-tree. This results in
two filtered images preserving topologically critical structure. Applying the
proposed DSAIF to mutually supervised networks decreases the consensus of their
erroneous predictions on unlabeled images. This helps to alleviate the
confirmation bias issue of overfitting to noisy pseudo labels of unlabeled
images, and thus effectively improves the segmentation performance. Extensive
experimental results on three benchmark datasets demonstrate that the proposed
method significantly/consistently outperforms some state-of-the-art methods.
The source codes will be publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decomposing Disease Descriptions for Enhanced Pathology Detection: A
  Multi-Aspect Vision-Language Pre-training Framework <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07636v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07636v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vu Minh Hieu Phan, Yutong Xie, Yuankai Qi, Lingqiao Liu, Liyang Liu, Bowen Zhang, Zhibin Liao, Qi Wu, Minh-Son To, Johan W. Verjans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical vision language pre-training (VLP) has emerged as a frontier of
research, enabling zero-shot pathological recognition by comparing the query
image with the textual descriptions for each disease. Due to the complex
semantics of biomedical texts, current methods struggle to align medical images
with key pathological findings in unstructured reports. This leads to the
misalignment with the target disease's textual representation. In this paper,
we introduce a novel VLP framework designed to dissect disease descriptions
into their fundamental aspects, leveraging prior knowledge about the visual
manifestations of pathologies. This is achieved by consulting a large language
model and medical experts. Integrating a Transformer module, our approach
aligns an input image with the diverse elements of a disease, generating
aspect-centric image representations. By consolidating the matches from each
aspect, we improve the compatibility between an image and its associated
disease. Additionally, capitalizing on the aspect-oriented representations, we
present a dual-head Transformer tailored to process known and unknown diseases,
optimizing the comprehensive detection efficacy. Conducting experiments on
seven downstream datasets, ours improves the accuracy of recent methods by up
to 8.56% and 17.0% for seen and unseen categories, respectively. Our code is
released at https://github.com/HieuPhan33/MAVL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR2024. Pre-print before final camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shapley Values-Powered Framework for Fair Reward Split in Content
  Produced by GenAI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09700v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09700v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Glinsky, Alexey Sokolsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is evident that, currently, generative models are surpassed in quality by
human professionals. However, with the advancements in Artificial Intelligence,
this gap will narrow, leading to scenarios where individuals who have dedicated
years of their lives to mastering a skill become obsolete due to their high
costs, which are inherently linked to the time they require to complete a task
-- a task that AI could accomplish in minutes or seconds. To avoid future
social upheavals, we must, even now, contemplate how to fairly assess the
contributions of such individuals in training generative models and how to
compensate them for the reduction or complete loss of their incomes. In this
work, we propose a method to structure collaboration between model developers
and data providers. To achieve this, we employ Shapley Values to quantify the
contribution of artist(s) in an image generated by the Stable Diffusion-v1.5
model and to equitably allocate the reward among them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 32 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ E4S: Fine-grained Face Swapping via Editing With Regional GAN Inversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15081v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15081v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maomao Li, Ge Yuan, Cairong Wang, Zhian Liu, Yong Zhang, Yongwei Nie, Jue Wang, Dong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel approach to face swapping from the perspective of
fine-grained facial editing, dubbed "editing for swapping" (E4S). The
traditional face swapping methods rely on global feature extraction and fail to
preserve the detailed source identity. In contrast, we propose a Regional GAN
Inversion (RGI) method, which allows the explicit disentanglement of shape and
texture. Specifically, our E4S performs face swapping in the latent space of a
pretrained StyleGAN, where a multi-scale mask-guided encoder is applied to
project the texture of each facial component into regional style codes and a
mask-guided injection module manipulating feature maps with the style codes.
Based on this disentanglement, face swapping can be simplified as style and
mask swapping. Besides, due to the large lighting condition gap, transferring
the source skin into the target image may lead to disharmony lighting. We
propose a re-coloring network to make the swapped face maintain the target
lighting condition while preserving the source skin. Further, to deal with the
potential mismatch areas during mask exchange, we design a face inpainting
module to refine the face shape. The extensive comparisons with
state-of-the-art methods demonstrate that our E4S outperforms existing methods
in preserving texture, shape, and lighting. Our implementation is available at
https://github.com/e4s2024/E4S2024.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://e4s2024.github.io/ ;. arXiv admin note: text
  overlap with arXiv:2211.14068</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> ViDA: Homeostatic Visual Domain Adapter for Continual Test Time
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04344v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04344v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Liu, Senqiao Yang, Peidong Jia, Renrui Zhang, Ming Lu, Yandong Guo, <span class="highlight-author">Wei Xu</span>e, Shanghang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since real-world machine systems are running in non-stationary environments,
Continual Test-Time Adaptation (CTTA) task is proposed to adapt the pre-trained
model to continually changing target domains. Recently, existing methods mainly
focus on model-based adaptation, which aims to leverage a self-training manner
to extract the target domain knowledge. However, pseudo labels can be noisy and
the updated model parameters are unreliable under dynamic data distributions,
leading to error accumulation and catastrophic forgetting in the continual
adaptation process. To tackle these challenges and maintain the model
plasticity, we design a Visual Domain Adapter (ViDA) for CTTA, explicitly
handling both domain-specific and domain-shared knowledge. Specifically, we
first comprehensively explore the different domain representations of the
adapters with trainable high-rank or low-rank embedding spaces. Then we inject
ViDAs into the pre-trained model, which leverages high-rank and low-rank
features to adapt the current domain distribution and maintain the continual
domain-shared knowledge, respectively. To exploit the low-rank and high-rank
ViDAs more effectively, we further propose a Homeostatic Knowledge Allotment
(HKA) strategy, which adaptively combines different knowledge from each ViDA.
Extensive experiments conducted on four widely used benchmarks demonstrate that
our proposed method achieves state-of-the-art performance in both
classification and segmentation CTTA tasks. Note that, our method can be
regarded as a novel transfer paradigm for large-scale models, delivering
promising results in adaptation to continually changing distributions. Project
page: https://sites.google.com/view/iclr2024-vida/home.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visually Guided Generative Text-Layout Pre-training for Document
  Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16516v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16516v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiming Mao, Haoli Bai, Lu Hou, Jiansheng Wei, Xin Jiang, Qun Liu, Kam-Fai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior study shows that pre-training techniques can boost the performance of
visual document understanding (VDU), which typically requires models to gain
abilities to perceive and reason both document texts and layouts (e.g.,
locations of texts and table-cells). To this end, we propose visually guided
generative text-layout pre-training, named ViTLP. Given a document image, the
model optimizes hierarchical language and layout modeling objectives to
generate the interleaved text and layout sequence. In addition, to address the
limitation of processing long documents by Transformers, we introduce a
straightforward yet effective multi-segment generative pre-training scheme,
facilitating ViTLP to process word-intensive documents of any length. ViTLP can
function as a native OCR model to localize and recognize texts of document
images. Besides, ViTLP can be effectively applied to various downstream VDU
tasks. Extensive experiments show that ViTLP achieves competitive performance
over existing baselines on benchmark VDU tasks, including information
extraction, document classification, and document question answering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024 main conference. The first version of this
  paper was submitted to OpenReview
  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intraoperative 2D/3D Image Registration via Differentiable X-ray
  Rendering <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06358v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06358v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivek Gopalakrishnan, Neel Dey, Polina Golland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical decisions are informed by aligning rapid portable 2D intraoperative
images (e.g., X-rays) to a high-fidelity 3D preoperative reference scan (e.g.,
CT). 2D/3D image registration often fails in practice: conventional
optimization methods are prohibitively slow and susceptible to local minima,
while neural networks trained on small datasets fail on new patients or require
impractical landmark supervision. We present DiffPose, a self-supervised
approach that leverages patient-specific simulation and differentiable
physics-based rendering to achieve accurate 2D/3D registration without relying
on manually labeled data. Preoperatively, a CNN is trained to regress the pose
of a randomly oriented synthetic X-ray rendered from the preoperative CT. The
CNN then initializes rapid intraoperative test-time optimization that uses the
differentiable X-ray renderer to refine the solution. Our work further proposes
several geometrically principled methods for sampling camera poses from
$\mathbf{SE}(3)$, for sparse differentiable rendering, and for driving
registration in the tangent space $\mathfrak{se}(3)$ with geodesic and
multiscale locality-sensitive losses. DiffPose achieves sub-millimeter accuracy
across surgical datasets at intraoperative speeds, improving upon existing
unsupervised methods by an order of magnitude and even outperforming supervised
baselines. Our code is available at https://github.com/eigenvivek/DiffPose.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Challenging Common Paradigms in Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04698v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04698v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cathrin Elich, Lukas Kirchdorfer, Jan M. Köhler, Lukas Schott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While multi-task learning (MTL) has gained significant attention in recent
years, its underlying mechanisms remain poorly understood. Recent methods did
not yield consistent performance improvements over single task learning (STL)
baselines, underscoring the importance of gaining more profound insights about
challenges specific to MTL. In our study, we challenge paradigms in MTL in the
context of STL: First, the impact of the choice of optimizer has only been
mildly investigated in MTL. We show the pivotal role of common STL tools such
as the Adam optimizer in MTL empirically in various experiments. To further
investigate Adam's effectiveness, we theoretical derive a partial loss-scale
invariance under mild assumptions. Second, the notion of gradient conflicts has
often been phrased as a specific problem in MTL. We delve into the role of
gradient conflicts in MTL and compare it to STL. For angular gradient alignment
we find no evidence that this is a unique problem in MTL. We emphasize
differences in gradient magnitude as the main distinguishing factor. Lastly, we
compare the transferability of features learned through MTL and STL on common
image corruptions, and find light evidence that MTL can lead to superior
transferability. Overall, we find surprising similarities between STL and MTL
suggesting to consider methods from both fields in a broader context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>-</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Fields for Interactive Visualization of Statistical Dependencies
  in 3D Simulation Ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02203v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02203v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemeh Farokhmanesh, Kevin Höhlein, Christoph Neuhauser, Tobias Necker, Martin Weissmann, Takemasa Miyoshi, Rüdiger Westermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the first neural network that has learned to compactly represent
and can efficiently reconstruct the statistical dependencies between the values
of physical variables at different spatial locations in large 3D simulation
ensembles. Going beyond linear dependencies, we consider mutual information as
a measure of non-linear dependence. We demonstrate learning and reconstruction
with a large weather forecast ensemble comprising 1000 members, each storing
multiple physical variables at a 250 x 352 x 20 simulation grid. By
circumventing compute-intensive statistical estimators at runtime, we
demonstrate significantly reduced memory and computation requirements for
reconstructing the major dependence structures. This enables embedding the
estimator into a GPU-accelerated direct volume renderer and interactively
visualizing all mutual dependencies for a selected domain point.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAR-Net: Multi-scale Direction-aware SAR Network via Global Information
  Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxiang Cao, Jie Lei, Weiying Xie, Jiaqing Zhang, Daixun Li, Yunsong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has driven significant progress in object detection using
Synthetic Aperture Radar (SAR) imagery. Existing methods, while achieving
promising results, often struggle to effectively integrate local and global
information, particularly direction-aware features. This paper proposes
SAR-Net, a novel framework specifically designed for global fusion of
direction-aware information in SAR object detection. SAR-Net leverages two key
innovations: the Unity Compensation Mechanism (UCM) and the Direction-aware
Attention Module (DAM). UCM facilitates the establishment of complementary
relationships among features across different scales, enabling efficient global
information fusion. Among them, Multi-scale Alignment Module (MAM) and distinct
Multi-level Fusion Module (MFM) enhance feature integration by capturing both
texture detail and semantic information. Then, Multi-feature Embedding Module
(MEM) feeds back global features into the primary branches, further improving
information transmission. Additionally, DAM, through bidirectional attention
polymerization, captures direction-aware information, effectively eliminating
background interference. Extensive experiments demonstrate the effectiveness of
SAR-Net, achieving state-of-the-art results on aircraft (SAR-AIRcraft-1.0) and
ship datasets (SSDD, HRSID), confirming its generalization capability and
robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose
  Estimation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Jialun Cai, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have been successfully applied in the field of video-based 3D
human pose estimation. However, the high computational costs of these video
pose transformers (VPTs) make them impractical on resource-constrained devices.
In this paper, we present a plug-and-play pruning-and-recovering framework,
called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose
estimation from videos. Our HoT begins with pruning pose tokens of redundant
frames and ends with recovering full-length tokens, resulting in a few pose
tokens in the intermediate transformer blocks and thus improving the model
efficiency. To effectively achieve this, we propose a token pruning cluster
(TPC) that dynamically selects a few representative tokens with high semantic
diversity while eliminating the redundancy of video frames. In addition, we
develop a token recovering attention (TRA) to restore the detailed
spatio-temporal information based on the selected tokens, thereby expanding the
network output to the original full-length temporal resolution for fast
inference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and
MPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and
estimation accuracy compared to the original VPT models. For instance, applying
to MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs
without sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,
respectively. Code and models are available at
https://github.com/NationalGAILab/HoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024, Open Sourced</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Object Coherence in Layout-to-Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10522v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10522v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibin Wang, Weizhong Zhang, Jianwei Zheng, Cheng Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Layout-to-image synthesis is an emerging technique in conditional image
generation. It aims to generate complex scenes, where users require fine
control over the layout of the objects in a scene. However, it remains
challenging to control the object coherence, including semantic coherence
(e.g., the cat looks at the flowers or not) and physical coherence (e.g., the
hand and the racket should not be misaligned). In this paper, we propose a
novel diffusion model with effective global semantic fusion (GSF) and
self-similarity feature enhancement modules to guide the object coherence for
this task. For semantic coherence, we argue that the image caption contains
rich information for defining the semantic relationship within the objects in
the images. Instead of simply employing cross-attention between captions and
generated images, which addresses the highly relevant layout restriction and
semantic coherence separately and thus leads to unsatisfying results shown in
our experiments, we develop GSF to fuse the supervision from the layout
restriction and semantic coherence requirement and exploit it to guide the
image synthesis process. Moreover, to improve the physical coherence, we
develop a Self-similarity Coherence Attention (SCA) module to explicitly
integrate local contextual physical coherence into each pixel's generation
process. Specifically, we adopt a self-similarity map to encode the coherence
restrictions and employ it to extract coherent features from text embedding.
Through visualization of our self-similarity map, we explore the essence of
SCA, revealing that its effectiveness is not only in capturing reliable
physical coherence patterns but also in enhancing complex texture generation.
Extensive experiments demonstrate the superiority of our proposed method in
both image generation quality and controllability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BEVUDA: Multi-geometric Space Alignments for Domain Adaptive BEV 3D
  Object Detection <span class="chip">ICRA2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.17126v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.17126v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Liu, Rongyu Zhang, Xiaoqi Li, Xiaowei Chi, Zehui Chen, Ming Lu, Yandong Guo, Shanghang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-centric bird-eye-view (BEV) perception has shown promising potential
in autonomous driving. Recent works mainly focus on improving efficiency or
accuracy but neglect the challenges when facing environment changing, resulting
in severe degradation of transfer performance. For BEV perception, we figure
out the significant domain gaps existing in typical real-world cross-domain
scenarios and comprehensively solve the Domain Adaption (DA) problem for
multi-view 3D object detection. Since BEV perception approaches are complicated
and contain several components, the domain shift accumulation on multiple
geometric spaces (i.e., 2D, 3D Voxel, BEV) makes BEV DA even challenging. In
this paper, we propose a Multi-space Alignment Teacher-Student (MATS) framework
to ease the domain shift accumulation, which consists of a Depth-Aware Teacher
(DAT) and a Geometric-space Aligned Student (GAS) model. DAT tactfully combines
target lidar and reliable depth prediction to construct depth-aware
information, extracting target domain-specific knowledge in Voxel and BEV
feature spaces. It then transfers the sufficient domain knowledge of multiple
spaces to the student model. In order to jointly alleviate the domain shift,
GAS projects multi-geometric space features to a shared geometric embedding
space and decreases data distribution distance between two domains. To verify
the effectiveness of our method, we conduct BEV 3D object detection experiments
on three cross-domain scenarios and achieve state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Back to 3D: Few-Shot 3D Keypoint Detection with Back-Projected 2D
  Features <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18113v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18113v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Wimmer, Peter Wonka, Maks Ovsjanikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the immense growth of dataset sizes and computing resources in recent
years, so-called foundation models have become popular in NLP and vision tasks.
In this work, we propose to explore foundation models for the task of keypoint
detection on 3D shapes. A unique characteristic of keypoint detection is that
it requires semantic and geometric awareness while demanding high localization
accuracy. To address this problem, we propose, first, to back-project features
from large pre-trained 2D vision models onto 3D shapes and employ them for this
task. We show that we obtain robust 3D features that contain rich semantic
information and analyze multiple candidate features stemming from different 2D
foundation models. Second, we employ a keypoint candidate optimization module
which aims to match the average observed distribution of keypoints on the shape
and is guided by the back-projected features. The resulting approach achieves a
new state of the art for few-shot keypoint detection on the KeyPointNet
dataset, almost doubling the performance of the previous best methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024, Project page:
  https://wimmerth.github.io/back-to-3d.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Dynamic 3D Object Generation from a Single-view Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08742v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08742v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijie Pan, Zeyu Yang, Xiatian Zhu, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating dynamic 3D object from a single-view video is challenging due to
the lack of 4D labeled data. Extending image-to-3D pipelines by transferring
off-the-shelf image generation models such as score distillation sampling,
existing methods tend to be slow and expensive to scale due to the need for
back-propagating the information-limited supervision signals through a large
pretrained model. To address this, we propose an efficient video-to-4D object
generation framework called Efficient4D. It generates high-quality
spacetime-consistent images under different camera views, and then uses them as
labeled data to directly train a novel 4D Gaussian splatting model with
explicit point cloud geometry, enabling real-time rendering under continuous
camera trajectories. Extensive experiments on synthetic and real videos show
that Efficient4D offers a remarkable 20-fold increase in speed when compared to
prior art alternatives while preserving the quality of novel view synthesis.
For example, Efficient4D takes only 6 mins to model a dynamic object, vs 120
mins by Consistent4D.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15098v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15098v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lan Feng, Mohammadhossein Bahari, Kaouther Messaoud Ben Amor, Éloi Zablocki, Matthieu Cord, Alexandre Alahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicle trajectory prediction has increasingly relied on data-driven
solutions, but their ability to scale to different data domains and the impact
of larger dataset sizes on their generalization remain under-explored. While
these questions can be studied by employing multiple datasets, it is
challenging due to several discrepancies, e.g., in data formats, map
resolution, and semantic annotation types. To address these challenges, we
introduce UniTraj, a comprehensive framework that unifies various datasets,
models, and evaluation criteria, presenting new opportunities for the vehicle
trajectory prediction field. In particular, using UniTraj, we conduct extensive
experiments and find that model performance significantly drops when
transferred to other datasets. However, enlarging data size and diversity can
substantially improve performance, leading to a new state-of-the-art result for
the nuScenes dataset. We provide insights into dataset characteristics to
explain these findings. The code can be found here:
https://github.com/vita-epfl/UniTraj
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLIP-DINOiser: Teaching CLIP a few DINO tricks for open-vocabulary
  semantic segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12359v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12359v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Monika Wysoczańska, Oriane Siméoni, Michaël Ramamonjisoa, Andrei Bursuc, Tomasz Trzciński, Patrick Pérez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The popular CLIP model displays impressive zero-shot capabilities thanks to
its seamless interaction with arbitrary text prompts. However, its lack of
spatial awareness makes it unsuitable for dense computer vision tasks, e.g.,
semantic segmentation, without an additional fine-tuning step that often uses
annotations and can potentially suppress its original open-vocabulary
properties. Meanwhile, self-supervised representation methods have demonstrated
good localization properties without human-made annotations nor explicit
supervision. In this work, we take the best of both worlds and propose an
open-vocabulary semantic segmentation method, which does not require any
annotations. We propose to locally improve dense MaskCLIP features, which are
computed with a simple modification of CLIP's last pooling layer, by
integrating localization priors extracted from self-supervised features. By
doing so, we greatly improve the performance of MaskCLIP and produce smooth
outputs. Moreover, we show that the used self-supervised feature properties can
directly be learnt from CLIP features. Our method CLIP-DINOiser needs only a
single forward pass of CLIP and two light convolutional layers at inference, no
extra supervision nor extra memory and reaches state-of-the-art results on
challenging and fine-grained benchmarks such as COCO, Pascal Context,
Cityscapes and ADE20k. The code to reproduce our results is available at
https://github.com/wysoczanska/clip_dinoiser.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual-MAE: Adaptive Distribution Masked Autoencoders for Continual
  Test-Time Adaptation <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12480v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12480v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Liu, Ran Xu, Senqiao Yang, Renrui Zhang, Qizhe Zhang, Zehui Chen, Yandong Guo, Shanghang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual Test-Time Adaptation (CTTA) is proposed to migrate a source
pre-trained model to continually changing target distributions, addressing
real-world dynamism. Existing CTTA methods mainly rely on entropy minimization
or teacher-student pseudo-labeling schemes for knowledge extraction in
unlabeled target domains. However, dynamic data distributions cause
miscalibrated predictions and noisy pseudo-labels in existing self-supervised
learning methods, hindering the effective mitigation of error accumulation and
catastrophic forgetting problems during the continual adaptation process. To
tackle these issues, we propose a continual self-supervised method, Adaptive
Distribution Masked Autoencoders (ADMA), which enhances the extraction of
target domain knowledge while mitigating the accumulation of distribution
shifts. Specifically, we propose a Distribution-aware Masking (DaM) mechanism
to adaptively sample masked positions, followed by establishing consistency
constraints between the masked target samples and the original target samples.
Additionally, for masked tokens, we utilize an efficient decoder to reconstruct
a hand-crafted feature descriptor (e.g., Histograms of Oriented Gradients),
leveraging its invariant properties to boost task-relevant representations.
Through conducting extensive experiments on four widely recognized benchmarks,
our proposed method attains state-of-the-art performance in both classification
and segmentation CTTA tasks. Our project page:
https://sites.google.com/view/continual-mae/home.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel
  Segmentation via Two-Phase Training Angiography-to-Venography Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Galati, Daniele Falcetta, Rosa Cortese, Barbara Casolla, Ferran Prados, Ninon Burgos, Maria A. Zuluaga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a semi-supervised domain adaptation framework for brain vessel
segmentation from different image modalities. Existing state-of-the-art methods
focus on a single modality, despite the wide range of available cerebrovascular
imaging techniques. This can lead to significant distribution shifts that
negatively impact the generalization across modalities. By relying on annotated
angiographies and a limited number of annotated venographies, our framework
accomplishes image-to-image translation and semantic segmentation, leveraging a
disentangled and semantically rich latent space to represent heterogeneous data
and perform image-level adaptation from source to target domains. Moreover, we
reduce the typical complexity of cycle-based architectures and minimize the use
of adversarial training, which allows us to build an efficient and intuitive
model with stable training. We evaluate our method on magnetic resonance
angiographies and venographies. While achieving state-of-the-art performance in
the source domain, our method attains a Dice score coefficient in the target
domain that is only 8.9% lower, highlighting its promising potential for robust
cerebrovascular image segmentation across different modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 34th British Machine Vision Conference (BMVC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Debiasing Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Fan Zhang, Weichen Yu, Qingsong Wen, Xue Wang, Zhang Zhang, Liang Wang, Rong Jin, Tieniu Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realms of computer vision and natural language processing, Large
Vision-Language Models (LVLMs) have become indispensable tools, proficient in
generating textual descriptions based on visual inputs. Despite their
advancements, our investigation reveals a noteworthy bias in the generated
content, where the output is primarily influenced by the underlying Large
Language Models (LLMs) prior rather than the input image. Our empirical
experiments underscore the persistence of this bias, as LVLMs often provide
confident answers even in the absence of relevant images or given incongruent
visual input. To rectify these biases and redirect the model's focus toward
vision information, we introduce two simple, training-free strategies. Firstly,
for tasks such as classification or multi-choice question-answering (QA), we
propose a ``calibration'' step through affine transformation to adjust the
output distribution. This ``Post-Hoc debias'' approach ensures uniform scores
for each answer when the image is absent, serving as an effective
regularization technique to alleviate the influence of LLM priors. For more
intricate open-ended generation tasks, we extend this method to ``Debias
sampling'', drawing inspirations from contrastive decoding methods.
Furthermore, our investigation sheds light on the instability of LVLMs across
various decoding configurations. Through systematic exploration of different
settings, we significantly enhance performance, surpassing reported results and
raising concerns about the fairness of existing evaluations. Comprehensive
experiments substantiate the effectiveness of our proposed strategies in
mitigating biases. These strategies not only prove beneficial in minimizing
hallucinations but also contribute to the generation of more helpful and
precise illustrations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SIGNeRF: Scene Integrated Generation for Neural Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01647v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01647v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan-Niklas Dihlmann, Andreas Engelhardt, Hendrik Lensch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in image diffusion models have recently led to notable improvements
in the generation of high-quality images. In combination with Neural Radiance
Fields (NeRFs), they enabled new opportunities in 3D generation. However, most
generative 3D approaches are object-centric and applying them to editing
existing photorealistic scenes is not trivial. We propose SIGNeRF, a novel
approach for fast and controllable NeRF scene editing and scene-integrated
object generation. A new generative update strategy ensures 3D consistency
across the edited images, without requiring iterative optimization. We find
that depth-conditioned diffusion models inherently possess the capability to
generate 3D consistent views by requesting a grid of images instead of single
views. Based on these insights, we introduce a multi-view reference sheet of
modified images. Our method updates an image collection consistently based on
the reference sheet and refines the original NeRF with the newly generated
image set in one go. By exploiting the depth conditioning mechanism of the
image diffusion model, we gain fine control over the spatial location of the
edit and enforce shape guidance by a selected region or an external mesh.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://signerf.jdihlmann.com</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LocalStyleFool: Regional Video Style Transfer Attack Using Segment
  Anything Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11656v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11656v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Cao, Jinghao Li, Xi Xiao, Derui Wang, Minhui Xue, Hao Ge, Wei Liu, Guangwu Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous work has shown that well-crafted adversarial perturbations can
threaten the security of video recognition systems. Attackers can invade such
models with a low query budget when the perturbations are semantic-invariant,
such as StyleFool. Despite the query efficiency, the naturalness of the minutia
areas still requires amelioration, since StyleFool leverages style transfer to
all pixels in each frame. To close the gap, we propose LocalStyleFool, an
improved black-box video adversarial attack that superimposes regional
style-transfer-based perturbations on videos. Benefiting from the popularity
and scalably usability of Segment Anything Model (SAM), we first extract
different regions according to semantic information and then track them through
the video stream to maintain the temporal consistency. Then, we add
style-transfer-based perturbations to several regions selected based on the
associative criterion of transfer-based gradient information and regional area.
Perturbation fine adjustment is followed to make stylized videos adversarial.
We demonstrate that LocalStyleFool can improve both intra-frame and inter-frame
naturalness through a human-assessed survey, while maintaining competitive
fooling rate and query efficiency. Successful experiments on the
high-resolution dataset also showcase that scrupulous segmentation of SAM helps
to improve the scalability of adversarial attacks under high-resolution data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 2024 IEEE Security and Privacy Workshops (SPW)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TULIP: Transformer for Upsampling of <span class="highlight-title">LiDAR</span> Point Cloud <span class="chip">CVPR20224</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06733v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06733v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Yang, Patrick Pfreundschuh, Roland Siegwart, Marco Hutter, Peyman Moghadam, Vaishakh Patil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR Upsampling is a challenging task for the perception systems of robots
and autonomous vehicles, due to the sparse and irregular structure of
large-scale scene contexts. Recent works propose to solve this problem by
converting LiDAR data from 3D Euclidean space into an image super-resolution
problem in 2D image space. Although their methods can generate high-resolution
range images with fine-grained details, the resulting 3D point clouds often
blur out details and predict invalid points. In this paper, we propose TULIP, a
new method to reconstruct high-resolution LiDAR point clouds from
low-resolution LiDAR input. We also follow a range image-based approach but
specifically modify the patch and window geometries of a Swin-Transformer-based
network to better fit the characteristics of range images. We conducted several
experiments on three public real-world and simulated datasets. TULIP
outperforms state-of-the-art methods in all relevant metrics and generates
robust and more realistic point clouds than prior works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper was accepted by CVPR20224</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D Face Reconstruction Using A Spectral-Based Graph Convolution Encoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05218v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05218v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxin Xu, Zezheng Zhao, Yuxin Cao, Chunyu Chen, Hao Ge, Ziyao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular 3D face reconstruction plays a crucial role in avatar generation,
with significant demand in web-related applications such as generating virtual
financial advisors in FinTech. Current reconstruction methods predominantly
rely on deep learning techniques and employ 2D self-supervision as a means to
guide model learning. However, these methods encounter challenges in capturing
the comprehensive 3D structural information of the face due to the utilization
of 2D images for model training purposes. To overcome this limitation and
enhance the reconstruction of 3D structural features, we propose an innovative
approach that integrates existing 2D features with 3D features to guide the
model learning process. Specifically, we introduce the 3D-ID Loss, which
leverages the high-dimensional structure features extracted from a
Spectral-Based Graph Convolution Encoder applied to the facial mesh. This
approach surpasses the sole reliance on the 3D information provided by the
facial mesh vertices coordinates. Our model is trained using 2D-3D data pairs
from a combination of datasets and achieves state-of-the-art performance on the
NoW benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures. Accepted to WWW 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AEROBLADE: Training-Free Detection of Latent Diffusion Images Using
  Autoencoder Reconstruction Error <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Ricker, Denis Lukovnikov, Asja Fischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With recent text-to-image models, anyone can generate deceptively realistic
images with arbitrary contents, fueling the growing threat of visual
disinformation. A key enabler for generating high-resolution images with low
computational cost has been the development of latent diffusion models (LDMs).
In contrast to conventional diffusion models, LDMs perform the denoising
process in the low-dimensional latent space of a pre-trained autoencoder (AE)
instead of the high-dimensional image space. Despite their relevance, the
forensic analysis of LDMs is still in its infancy. In this work we propose
AEROBLADE, a novel detection method which exploits an inherent component of
LDMs: the AE used to transform images between image and latent space. We find
that generated images can be more accurately reconstructed by the AE than real
images, allowing for a simple detection approach based on the reconstruction
error. Most importantly, our method is easy to implement and does not require
any training, yet nearly matches the performance of detectors that rely on
extensive training. We empirically demonstrate that AEROBLADE is effective
against state-of-the-art LDMs, including Stable Diffusion and Midjourney.
Beyond detection, our approach allows for the qualitative analysis of images,
which can be leveraged for identifying inpainted regions. We release our code
and data at https://github.com/jonasricker/aeroblade .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A citizen science toolkit to collect human perceptions of urban
  environments using open street view images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00174v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00174v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Danish, SM Labib, Britta Ricker, Marco Helbich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Street View-level Imagery (SVI) is a valuable data source for studies (e.g.,
environmental assessments, green space identification or land cover
classification). While commercial SVI is available, such providers commonly
restrict copying or reuse in ways necessary for research. Open SVI datasets are
readily available from less restrictive sources, such as Mapillary, but due to
the heterogeneity of the images, these require substantial preprocessing,
filtering, and careful quality checks. We present an efficient method for
automated downloading, processing, cropping, and filtering open SVI, to be used
in a survey of human perceptions of the streets portrayed in these images. We
demonstrate our open-source reusable SVI preparation and smartphone-friendly
perception-survey software with Amsterdam (Netherlands) as the case study.
Using a citizen science approach, we collected from 331 people 22,637 ratings
about their perceptions for various criteria. We have published our software in
a public repository for future re-use and reproducibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Chen, Chao Tang, Amir Aghabiglou, Chung San Chu, Yves Wiaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new approach for non-Cartesian magnetic resonance image
reconstruction. While unrolled architectures provide robustness via
data-consistency layers, embedding measurement operators in Deep Neural Network
(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)
approaches, where the denoising DNNs are blind to the measurement setting, are
not affected by this limitation and have also proven effective, but their
highly iterative nature also affects scalability. To address this scalability
challenge, we leverage the "Residual-to-Residual DNN series for high-Dynamic
range imaging (R2D2)" approach recently introduced in astronomical imaging.
R2D2's reconstruction is formed as a series of residual images, iteratively
estimated as outputs of DNNs taking the previous iteration's image estimate and
associated data residual as inputs. The method can be interpreted as a learned
version of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,
considering radial k-space sampling acquisition sequences. Our preliminary
results suggest that R2D2 achieves: (i) suboptimal performance compared to its
unrolled incarnation R2D2-Net, which is however non-scalable due to the
necessary embedding of NUFFT-based data-consistency layers; (ii) superior
reconstruction quality to a scalable version of R2D2-Net embedding an FFT-based
approximation for data consistency; (iii) superior reconstruction quality to
PnP, while only requiring few iterations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FoMo-Bench: a multi-modal, multi-scale and multi-task Forest Monitoring
  Benchmark for remote sensing foundation models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Ioannis Bountos, Arthur Ouaknine, David Rolnick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forests are an essential part of Earth's ecosystems and natural systems, as
well as providing services on which humanity depends, yet they are rapidly
changing as a result of land use decisions and climate change. Understanding
and mitigating negative effects requires parsing data on forests at global
scale from a broad array of sensory modalities, and recently many such problems
have been approached using machine learning algorithms for remote sensing. To
date, forest-monitoring problems have largely been addressed in isolation.
Inspired by the rise of foundation models for computer vision and remote
sensing, we here present the first unified Forest Monitoring Benchmark
(FoMo-Bench). FoMo-Bench consists of 15 diverse datasets encompassing
satellite, aerial, and inventory data, covering a variety of geographical
regions, and including multispectral, red-green-blue, synthetic aperture radar
(SAR) and LiDAR data with various temporal, spatial and spectral resolutions.
FoMo-Bench includes multiple types of forest-monitoring tasks, spanning
classification, segmentation, and object detection. To further enhance the
diversity of tasks and geographies represented in FoMo-Bench, we introduce a
novel global dataset, TalloS, combining satellite imagery with ground-based
annotations for tree species classification, encompassing 1,000+ categories
across multiple hierarchical taxonomic levels (species, genus, family).
Finally, we propose FoMo-Net, a baseline foundation model with the capacity to
process any combination of commonly used spectral bands in remote sensing,
across diverse ground sampling distances and geographical locations worldwide.
This work aims to inspire research collaborations between machine learning and
forest biology researchers in exploring scalable multi-modal and multi-task
models for forest monitoring. All code and data will be made publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-Augmented Generation for AI-Generated Content: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19473v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19473v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of Artificial Intelligence Generated Content (AIGC) has been
facilitated by advancements in model algorithms, the increasing scale of
foundation models, and the availability of ample high-quality datasets. While
AIGC has achieved remarkable performance, it still faces several challenges,
such as the difficulty of maintaining up-to-date and long-tail knowledge, the
risk of data leakage, and the high costs associated with training and
inference. Retrieval-Augmented Generation(RAG) has recently emerged as a
paradigm to address such challenges. In particular, RAG introduces the
information retrieval process, which enhances the generation process by
retrieving relevant objects from available data stores, leading to higher
accuracy and better robustness. In this paper, we comprehensively review
existing efforts that integrate RAG technique into AIGC scenarios. We first
classify RAG foundations according to how the retriever augments the generator,
distilling the fundamental abstractions of the augmentation methodologies for
various retrievers and generators. This unified perspective encompasses all RAG
scenarios, illuminating advancements and pivotal technologies that help with
potential future progress. We also summarize additional enhancements methods
for RAG, facilitating effective engineering and implementation of RAG systems.
Then from another view, we survey on practical applications of RAG across
different modalities and tasks, offering valuable references for researchers
and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss
the limitations of current RAG systems, and suggest potential directions for
future research.Project Repo: https://github.com/hymie122/RAG-Survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Citing 380 papers, 36 pages, 16 figures. Project:
  https://github.com/hymie122/RAG-Survey</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Concept-Based Causal Transition and Symbolic Reasoning for
  Visual Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilue Qian, Peiyu Yu, Ying Nian Wu, Yao Su, Wei Wang, Lifeng Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual planning simulates how humans make decisions to achieve desired goals
in the form of searching for visual causal transitions between an initial
visual state and a final visual goal state. It has become increasingly
important in egocentric vision with its advantages in guiding agents to perform
daily tasks in complex environments. In this paper, we propose an interpretable
and generalizable visual planning framework consisting of i) a novel
Substitution-based Concept Learner (SCL) that abstracts visual inputs into
disentangled concept representations, ii) symbol abstraction and reasoning that
performs task planning via the self-learned symbols, and iii) a Visual Causal
Transition model (ViCT) that grounds visual causal transitions to semantically
similar real-world actions. Given an initial state, we perform goal-conditioned
visual planning with a symbolic reasoning method fueled by the learned
representations and causal transitions to reach the goal state. To verify the
effectiveness of the proposed model, we collect a large-scale visual planning
dataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this
challenging dataset demonstrate the superior performance of our method in
visual task planning. Empirically, we show that our framework can generalize to
unseen task trajectories, unseen object categories, and real-world data.
Further details of this work are provided at
https://fqyqc.github.io/ConTranPlan/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Centered Masking for Language-Image Pre-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15837v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15837v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingliang Liang, Martha Larson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel,
straightforward, and effective technique for masking image patches during
pre-training of a vision-language model. GLIP builds on Fast Language-Image
Pre-Training (FLIP), which randomly masks image patches while training a CLIP
model. GLIP replaces random masking with centered masking, that uses a Gaussian
distribution and is inspired by the importance of image patches at the center
of the image. GLIP retains the same computational savings as FLIP, while
improving performance across a range of downstream datasets and tasks, as
demonstrated by our experimental results. We show the benefits of GLIP to be
easy to obtain, requiring no delicate tuning of the Gaussian, and also
applicable to data sets containing images without an obvious center focus.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physical 3D Adversarial Attacks against Monocular Depth Estimation in
  Autonomous Driving <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17301v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17301v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhao Zheng, Chenhao Lin, Jiahao Sun, Zhengyu Zhao, Qian Li, Chao Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based monocular depth estimation (MDE), extensively applied in
autonomous driving, is known to be vulnerable to adversarial attacks. Previous
physical attacks against MDE models rely on 2D adversarial patches, so they
only affect a small, localized region in the MDE map but fail under various
viewpoints. To address these limitations, we propose 3D Depth Fool
(3D$^2$Fool), the first 3D texture-based adversarial attack against MDE models.
3D$^2$Fool is specifically optimized to generate 3D adversarial textures
agnostic to model types of vehicles and to have improved robustness in bad
weather conditions, such as rain and fog. Experimental results validate the
superior performance of our 3D$^2$Fool across various scenarios, including
vehicles, MDE models, weather conditions, and viewpoints. Real-world
experiments with printed 3D textures on physical vehicle models further
demonstrate that our 3D$^2$Fool can cause an MDE error of over 10 meters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weakly-Supervised Conditional Embedding for Referred Visual Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02928v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02928v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Lepage, Jérémie Mary, David Picard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a new challenge for image similarity search in the
context of fashion, addressing the inherent ambiguity in this domain stemming
from complex images. We present Referred Visual Search (RVS), a task allowing
users to define more precisely the desired similarity, following recent
interest in the industry. We release a new large public dataset,
LAION-RVS-Fashion, consisting of 272k fashion products with 842k images
extracted from LAION, designed explicitly for this task. However, unlike
traditional visual search methods in the industry, we demonstrate that superior
performance can be achieved by bypassing explicit object detection and adopting
weakly-supervised conditional contrastive learning on image tuples. Our method
is lightweight and demonstrates robustness, reaching Recall at one superior to
strong detection-based baselines against 2M distractors. Code, data and models
are available at https://www.github.com/Simon-Lepage/CondViT-LRVSF .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 13 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-criteria Token Fusion with One-step-ahead Attention for Efficient
  Vision Transformers <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10030v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10030v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanghyeok Lee, Joonmyung Choi, Hyunwoo J. Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformer (ViT) has emerged as a prominent backbone for computer
vision. For more efficient ViTs, recent works lessen the quadratic cost of the
self-attention layer by pruning or fusing the redundant tokens. However, these
works faced the speed-accuracy trade-off caused by the loss of information.
Here, we argue that token fusion needs to consider diverse relations between
tokens to minimize information loss. In this paper, we propose a Multi-criteria
Token Fusion (MCTF), that gradually fuses the tokens based on multi-criteria
(e.g., similarity, informativeness, and size of fused tokens). Further, we
utilize the one-step-ahead attention, which is the improved approach to capture
the informativeness of the tokens. By training the model equipped with MCTF
using a token reduction consistency, we achieve the best speed-accuracy
trade-off in the image classification (ImageNet1K). Experimental results prove
that MCTF consistently surpasses the previous reduction methods with and
without training. Specifically, DeiT-T and DeiT-S with MCTF reduce FLOPs by
about 44% while improving the performance (+0.5%, and +0.3%) over the base
model, respectively. We also demonstrate the applicability of MCTF in various
Vision Transformers (e.g., T2T-ViT, LV-ViT), achieving at least 31% speedup
without performance degradation. Code is available at
https://github.com/mlvlab/MCTF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference on Computer Vision and Pattern Recognition (CVPR), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-Adaptive Saliency Guidance for Exemplar-free Class Incremental
  Learning <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xialei Liu, Jiang-Tian Zhai, Andrew D. Bagdanov, Ke Li, Ming-Ming Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exemplar-free Class Incremental Learning (EFCIL) aims to sequentially learn
tasks with access only to data from the current one. EFCIL is of interest
because it mitigates concerns about privacy and long-term storage of data,
while at the same time alleviating the problem of catastrophic forgetting in
incremental learning. In this work, we introduce task-adaptive saliency for
EFCIL and propose a new framework, which we call Task-Adaptive Saliency
Supervision (TASS), for mitigating the negative effects of saliency drift
between different tasks. We first apply boundary-guided saliency to maintain
task adaptivity and \textit{plasticity} on model attention. Besides, we
introduce task-agnostic low-level signals as auxiliary supervision to increase
the \textit{stability} of model attention. Finally, we introduce a module for
injecting and recovering saliency noise to increase the robustness of saliency
preservation. Our experiments demonstrate that our method can better preserve
saliency maps across tasks and achieve state-of-the-art results on the
CIFAR-100, Tiny-ImageNet, and ImageNet-Subset EFCIL benchmarks. Code is
available at \url{https://github.com/scok30/tass}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Effects of Mixed Sample Data Augmentation are Class Dependent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haeil Lee, Hansang Lee, Junmo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixed Sample Data Augmentation (MSDA) techniques, such as Mixup, CutMix, and
PuzzleMix, have been widely acknowledged for enhancing performance in a variety
of tasks. A previous study reported the class dependency of traditional data
augmentation (DA), where certain classes benefit disproportionately compared to
others. This paper reveals a class dependent effect of MSDA, where some classes
experience improved performance while others experience degraded performance.
This research addresses the issue of class dependency in MSDA and proposes an
algorithm to mitigate it. The approach involves training on a mixture of MSDA
and non-MSDA data, which not only mitigates the negative impact on the affected
classes, but also improves overall accuracy. Furthermore, we provide in-depth
analysis and discussion of why MSDA introduced class dependencies and which
classes are most likely to have them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 18 figures, Overall Revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18920v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18920v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongliang Cao, Marvin Eisenberger, Nafie El Amrani, Daniel Cremers, Florian Bernard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although 3D shape matching and interpolation are highly interrelated, they
are often studied separately and applied sequentially to relate different 3D
shapes, thus resulting in sub-optimal performance. In this work we present a
unified framework to predict both point-wise correspondences and shape
interpolation between 3D shapes. To this end, we combine the deep functional
map framework with classical surface deformation models to map shapes in both
spectral and spatial domains. On the one hand, by incorporating spatial maps,
our method obtains more accurate and smooth point-wise correspondences compared
to previous functional map methods for shape matching. On the other hand, by
introducing spectral maps, our method gets rid of commonly used but
computationally expensive geodesic distance constraints that are only valid for
near-isometric shape deformations. Furthermore, we propose a novel test-time
adaptation scheme to capture both pose-dominant and shape-dominant
deformations. Using different challenging datasets, we demonstrate that our
method outperforms previous state-of-the-art methods for both shape matching
and interpolation, even compared to supervised approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CEIMVEN: An Approach of Cutting Edge Implementation of Modified Versions
  of EfficientNet (V1-V2) Architecture for Breast Cancer Detection and
  Classification from Ultrasound Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.13356v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.13356v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheekar Banerjee, Md. Kamrul Hasan Monir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Undoubtedly breast cancer identifies itself as one of the most widespread and
terrifying cancers across the globe. Millions of women are getting affected
each year from it. Breast cancer remains the major one for being the reason of
largest number of demise of women. In the recent time of research, Medical
Image Computing and Processing has been playing a significant role for
detecting and classifying breast cancers from ultrasound images and mammograms,
along with the celestial touch of deep neural networks. In this research, we
focused mostly on our rigorous implementations and iterative result analysis of
different cutting-edge modified versions of EfficientNet architectures namely
EfficientNet-V1 (b0-b7) and EfficientNet-V2 (b0-b3) with ultrasound image,
named as CEIMVEN. We utilized transfer learning approach here for using the
pre-trained models of EfficientNet versions. We activated the hyper-parameter
tuning procedures, added fully connected layers, discarded the unprecedented
outliers and recorded the accuracy results from our custom modified
EfficientNet architectures. Our deep learning model training approach was
related to both identifying the cancer affected areas with region of interest
(ROI) techniques and multiple classifications (benign, malignant and normal).
The approximate testing accuracies we got from the modified versions of
EfficientNet-V1 (b0- 99.15%, b1- 98.58%, b2- 98.43%, b3- 98.01%, b4- 98.86%,
b5- 97.72%, b6- 97.72%, b7- 98.72%) and EfficientNet-V2 (b0- 99.29%, b1-
99.01%, b2- 98.72%, b3- 99.43%) are showing very bright future and strong
potentials of deep learning approach for the successful detection and
classification of breast cancers from the ultrasound images at a very early
stage. The code for this research is available here:
https://github.com/ac005sheekar/CEIMVEN-Breast.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ViT-CoMer: Vision Transformer with Convolutional Multi-scale Feature
  Interaction for Dense Predictions <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07392v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07392v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunlong Xia, Xinliang Wang, Feng Lv, Xin Hao, Yifeng Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Vision Transformer (ViT) has achieved significant success in
computer vision, it does not perform well in dense prediction tasks due to the
lack of inner-patch information interaction and the limited diversity of
feature scale. Most existing studies are devoted to designing vision-specific
transformers to solve the above problems, which introduce additional
pre-training costs. Therefore, we present a plain, pre-training-free, and
feature-enhanced ViT backbone with Convolutional Multi-scale feature
interaction, named ViT-CoMer, which facilitates bidirectional interaction
between CNN and transformer. Compared to the state-of-the-art, ViT-CoMer has
the following advantages: (1) We inject spatial pyramid multi-receptive field
convolutional features into the ViT architecture, which effectively alleviates
the problems of limited local information interaction and single-feature
representation in ViT. (2) We propose a simple and efficient CNN-Transformer
bidirectional fusion interaction module that performs multi-scale fusion across
hierarchical features, which is beneficial for handling dense prediction tasks.
(3) We evaluate the performance of ViT-CoMer across various dense prediction
tasks, different frameworks, and multiple advanced pre-training. Notably, our
ViT-CoMer-L achieves 64.3% AP on COCO val2017 without extra training data, and
62.1% mIoU on ADE20K val, both of which are comparable to state-of-the-art
methods. We hope ViT-CoMer can serve as a new backbone for dense prediction
tasks to facilitate future research. The code will be released at
https://github.com/Traffic-X/ViT-CoMer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InterControl: Generate Human Motion Interactions by Controlling Every
  Joint 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15864v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15864v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenzhi Wang, Jingbo Wang, Yixuan Li, Dahua Lin, Bo Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-conditioned human motion synthesis has made remarkable progress with the
emergence of diffusion models in recent research. However, the majority of
these motion diffusion models are primarily designed for a single character and
overlook multi-human interactions. In our approach, we strive to explore this
problem by synthesizing human motion with interactions for a group of
characters of any size. The key aspect of our approach is the adaptation of
human-wise interactions as pairs of human joints that can be either in contact
or separated by a desired distance. In contrast to existing methods that
necessitate training motion generation models on multi-human motion datasets
with a fixed number of characters, our approach inherently possesses the
flexibility to model human interactions involving an arbitrary number of
individuals, thereby transcending the limitations imposed by the training data.
We introduce a novel controllable motion generation method, InterControl, to
encourage the synthesized motions maintaining the desired distance between
joint pairs. It consists of a motion controller and an inverse kinematics
guidance module that realistically and accurately aligns the joints of
synthesized characters to the desired location. Furthermore, we demonstrate
that the distance between joint pairs for human-wise interactions can be
generated using an off-the-shelf Large Language Model (LLM). Experimental
results highlight the capability of our framework to generate interactions with
multiple human characters and its potential to work with off-the-shelf
physics-based character simulators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Generate human interactions with only single-person data via joint
  contact pairs, code https://github.com/zhenzhiwang/intercontrol</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SSM Meets Video Diffusion Models: Efficient Video Generation with
  Structured State Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07711v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07711v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuta Oshima, Shohei Taniguchi, Masahiro Suzuki, Yutaka Matsuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the remarkable achievements in image generation through diffusion
models, the research community has shown increasing interest in extending these
models to video generation. Recent diffusion models for video generation have
predominantly utilized attention layers to extract temporal features. However,
attention layers are limited by their memory consumption, which increases
quadratically with the length of the sequence. This limitation presents
significant challenges when attempting to generate longer video sequences using
diffusion models. To overcome this challenge, we propose leveraging state-space
models (SSMs). SSMs have recently gained attention as viable alternatives due
to their linear memory consumption relative to sequence length. In the
experiments, we first evaluate our SSM-based model with UCF101, a standard
benchmark of video generation. In addition, to investigate the potential of
SSMs for longer video generation, we perform an experiment using the MineRL
Navigate dataset, varying the number of frames to 64, 200, and 400. In these
settings, our SSM-based model can considerably save memory consumption for
longer sequences, while maintaining competitive FVD scores to the
attention-based models. Our codes are available at
https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as workshop paper at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rotation-Invariant Transformer for Point Cloud Matching <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08231v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08231v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yu, Zheng Qin, Ji Hou, Mahdi Saleh, Dongsheng Li, Benjamin Busam, Slobodan Ilic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The intrinsic rotation invariance lies at the core of matching point clouds
with handcrafted descriptors. However, it is widely despised by recent deep
matchers that obtain the rotation invariance extrinsically via data
augmentation. As the finite number of augmented rotations can never span the
continuous SO(3) space, these methods usually show instability when facing
rotations that are rarely seen. To this end, we introduce RoITr, a
Rotation-Invariant Transformer to cope with the pose variations in the point
cloud matching task. We contribute both on the local and global levels.
Starting from the local level, we introduce an attention mechanism embedded
with Point Pair Feature (PPF)-based coordinates to describe the pose-invariant
geometry, upon which a novel attention-based encoder-decoder architecture is
constructed. We further propose a global transformer with rotation-invariant
cross-frame spatial awareness learned by the self-attention mechanism, which
significantly improves the feature distinctiveness and makes the model robust
with respect to the low overlap. Experiments are conducted on both the rigid
and non-rigid public benchmarks, where RoITr outperforms all the
state-of-the-art models by a considerable margin in the low-overlapping
scenarios. Especially when the rotations are enlarged on the challenging
3DLoMatch benchmark, RoITr surpasses the existing methods by at least 13 and 5
percentage points in terms of Inlier Ratio and Registration Recall,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extend Your Own Correspondences: Unsupervised Distant Point Cloud
  Registration by Progressive Distance Extension <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03532v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03532v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Liu, Hongzi Zhu, Zhenxi Wang, Yunsong Zhou, Shan Chang, Minyi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Registration of point clouds collected from a pair of distant vehicles
provides a comprehensive and accurate 3D view of the driving scenario, which is
vital for driving safety related applications, yet existing literature suffers
from the expensive pose label acquisition and the deficiency to generalize to
new data distributions. In this paper, we propose EYOC, an unsupervised distant
point cloud registration method that adapts to new point cloud distributions on
the fly, requiring no global pose labels. The core idea of EYOC is to train a
feature extractor in a progressive fashion, where in each round, the feature
extractor, trained with near point cloud pairs, can label slightly farther
point cloud pairs, enabling self-supervision on such far point cloud pairs.
This process continues until the derived extractor can be used to register
distant point clouds. Particularly, to enable high-fidelity correspondence
label generation, we devise an effective spatial filtering scheme to select the
most representative correspondences to register a point cloud pair, and then
utilize the aligned point clouds to discover more correct correspondences.
Experiments show that EYOC can achieve comparable performance with
state-of-the-art supervised methods at a lower training cost. Moreover, it
outwits supervised methods regarding generalization performance on new data
distributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Foundation Model Makes Clustering A Better Initialization For Cold-Start
  Active Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Yuan, Chuan Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active learning selects the most informative samples from the unlabelled
dataset to annotate in the context of a limited annotation budget. While
numerous methods have been proposed for subsequent sample selection based on an
initialized model, scant attention has been paid to the indispensable phase of
active learning: selecting samples for model cold-start initialization. Most of
the previous studies resort to random sampling or naive clustering. However,
random sampling is prone to fluctuation, and naive clustering suffers from
convergence speed, particularly when dealing with high-dimensional data such as
imaging data. In this work, we propose to integrate foundation models with
clustering methods to select samples for cold-start active learning
initialization. Foundation models refer to those trained on massive datasets by
the self-supervised paradigm and capable of generating informative and
compacted embeddings for various downstream tasks. Leveraging these embeddings
to replace raw features such as pixel values, clustering quickly converges and
identifies better initial samples. For a comprehensive comparison, we included
a classic ImageNet-supervised model to acquire embeddings. Experiments on two
clinical tasks of image classification and segmentation demonstrated that
foundation model-based clustering efficiently pinpointed informative initial
samples, leading to models showcasing enhanced performance than the baseline
methods. We envisage that this study provides an effective paradigm for future
cold-start active learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DifFlow3D: Toward Robust Uncertainty-Aware Scene Flow Estimation with
  Iterative Diffusion-Based Refinement <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17456v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17456v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiuming Liu, Guangming Wang, Weicai Ye, Chaokang Jiang, Jinru Han, Zhe Liu, Guofeng Zhang, Dalong Du, Hesheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene flow estimation, which aims to predict per-point 3D displacements of
dynamic scenes, is a fundamental task in the computer vision field. However,
previous works commonly suffer from unreliable correlation caused by locally
constrained searching ranges, and struggle with accumulated inaccuracy arising
from the coarse-to-fine structure. To alleviate these problems, we propose a
novel uncertainty-aware scene flow estimation network (DifFlow3D) with the
diffusion probabilistic model. Iterative diffusion-based refinement is designed
to enhance the correlation robustness and resilience to challenging cases, e.g.
dynamics, noisy inputs, repetitive patterns, etc. To restrain the generation
diversity, three key flow-related features are leveraged as conditions in our
diffusion model. Furthermore, we also develop an uncertainty estimation module
within diffusion to evaluate the reliability of estimated scene flow. Our
DifFlow3D achieves state-of-the-art performance, with 24.0% and 29.1% EPE3D
reduction respectively on FlyingThings3D and KITTI 2015 datasets. Notably, our
method achieves an unprecedented millimeter-level accuracy (0.0078m in EPE3D)
on the KITTI dataset. Additionally, our diffusion-based refinement paradigm can
be readily integrated as a plug-and-play module into existing scene flow
networks, significantly increasing their estimation accuracy. Codes are
released at https://github.com/IRMVLab/DifFlow3D.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version of CVPR 2024. Codes are released at
  https://github.com/IRMVLab/DifFlow3D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-wise Sampling Convolutions for Arbitrary-Oriented Object Detection
  in Aerial Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.02200v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.02200v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanchao Huang, Wei Li, Xiang-Gen Xia, Hao Wang, Ran Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Arbitrary-oriented object detection (AOOD) has been widely applied to locate
and classify objects with diverse orientations in remote sensing images.
However, the inconsistent features for the localization and classification
tasks in AOOD models may lead to ambiguity and low-quality object predictions,
which constrains the detection performance. In this article, an AOOD method
called task-wise sampling convolutions (TS-Conv) is proposed. TS-Conv
adaptively samples task-wise features from respective sensitive regions and
maps these features together in alignment to guide a dynamic label assignment
for better predictions. Specifically, sampling positions of the localization
convolution in TS-Conv are supervised by the oriented bounding box (OBB)
prediction associated with spatial coordinates, while sampling positions and
convolutional kernel of the classification convolution are designed to be
adaptively adjusted according to different orientations for improving the
orientation robustness of features. Furthermore, a dynamic
task-consistent-aware label assignment (DTLA) strategy is developed to select
optimal candidate positions and assign labels dynamically according to ranked
task-aware scores obtained from TS-Conv. Extensive experiments on several
public datasets covering multiple scenes, multimodal images, and multiple
categories of objects demonstrate the effectiveness, scalability, and superior
performance of the proposed TS-Conv.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 13 figures, 11 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-26T00:00:00Z">2024-03-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">59</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SLEDGE: Synthesizing Simulation Environments for Driving Agents with
  Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kashyap Chitta, Daniel Dauner, Andreas Geiger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SLEDGE is the first generative simulator for vehicle motion planning trained
on real-world driving logs. Its core component is a learned model that is able
to generate agent bounding boxes and lane graphs. The model's outputs serve as
an initial state for traffic simulation. The unique properties of the entities
to be generated for SLEDGE, such as their connectivity and variable count per
scene, render the naive application of most modern generative models to this
task non-trivial. Therefore, together with a systematic study of existing lane
graph representations, we introduce a novel raster-to-vector autoencoder
(RVAE). It encodes agents and the lane graph into distinct channels in a
rasterized latent map. This facilitates both lane-conditioned agent generation
and combined generation of lanes and agents with a Diffusion Transformer. Using
generated entities in SLEDGE enables greater control over the simulation, e.g.
upsampling turns or increasing traffic density. Further, SLEDGE can support
500m long routes, a capability not found in existing data-driven simulators
like nuPlan. It presents new challenges for planning algorithms, evidenced by
failure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,
when tested on hard routes and dense traffic generated by our model. Compared
to nuPlan, SLEDGE requires 500$\times$ less storage to set up (<4GB), making it
a more accessible option and helping with democratizing future research in this
field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Agent Clarity-Aware Dynamic Coverage with Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17917v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17917v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devansh R. Agrawal, Dimitra Panagou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents two algorithms for multi-agent dynamic coverage in
spatiotemporal environments, where the coverage algorithms are informed by the
method of data assimilation. In particular, we show that by considering the
information assimilation algorithm, here a Numerical Gaussian Process Kalman
Filter, the influence of measurements taken at one position on the uncertainty
of the estimate at another location can be computed. We use this relationship
to propose new coverage algorithms. Furthermore, we show that the controllers
naturally extend to the multi-agent context, allowing for a distributed-control
central-information paradigm for multi-agent coverage. Finally, we demonstrate
the algorithms through a realistic simulation of a team of UAVs collecting wind
data over a region in Austria.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures, submitted to CDC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CMP: Cooperative Motion Prediction with Multi-Agent Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyuan Wu, Yuping Wang, Hengbo Ma, Zhaowei Li, Hang Qiu, Jiachen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The confluence of the advancement of Autonomous Vehicles (AVs) and the
maturity of Vehicle-to-Everything (V2X) communication has enabled the
capability of cooperative connected and automated vehicles (CAVs). Building on
top of cooperative perception, this paper explores the feasibility and
effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR
signals as input to enhance tracking and prediction capabilities. Unlike
previous work that focuses separately on either cooperative perception or
motion prediction, our framework, to the best of our knowledge, is the first to
address the unified problem where CAVs share information in both perception and
prediction modules. Incorporated into our design is the unique capability to
tolerate realistic V2X bandwidth limitations and transmission delays, while
dealing with bulky perception representations. We also propose a prediction
aggregation module, which unifies the predictions obtained by different CAVs
and generates the final prediction. Through extensive experiments and ablation
studies, we demonstrate the effectiveness of our method in cooperative
perception, tracking, and motion prediction tasks. In particular, CMP reduces
the average prediction error by 17.2\% with fewer missing detections compared
with the no cooperation setting. Our work marks a significant step forward in
the cooperative capabilities of CAVs, showcasing enhanced performance in
complex scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi Agent Pathfinding for Noise Restricted Hybrid Fuel Unmanned Aerial
  Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Drew Scott, Satyanarayana G. Manyam, David W. Casbeer, Manish Kumar, Isaac E. Weintraub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi Agent Path Finding (MAPF) seeks the optimal set of paths for multiple
agents from respective start to goal locations such that no paths conflict. We
address the MAPF problem for a fleet of hybrid-fuel unmanned aerial vehicles
which are subject to location-dependent noise restrictions. We solve this
problem by searching a constraint tree for which the subproblem at each node is
a set of shortest path problems subject to the noise and fuel constraints and
conflict zone avoidance. A labeling algorithm is presented to solve this
subproblem, including the conflict zones which are treated as dynamic
obstacles. We present the experimental results of the algorithms for various
graph sizes and number of agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrhman Werby, Chenguang Huang, Martin Büchner, Abhinav Valada, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent open-vocabulary robot mapping methods enrich dense geometric maps with
pre-trained visual-language features. While these maps allow for the prediction
of point-wise saliency maps when queried for a certain language concept,
large-scale environments and abstract queries beyond the object level still
pose a considerable hurdle, ultimately limiting language-grounded robotic
navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D
scene graph mapping approach for language-grounded robot navigation. Leveraging
open-vocabulary vision foundation models, we first obtain state-of-the-art
open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene
graph hierarchy consisting of floor, room, and object concepts, each enriched
with open-vocabulary features. Our approach is able to represent multi-story
buildings and allows robotic traversal of those using a cross-floor Voronoi
graph. HOV-SG is evaluated on three distinct datasets and surpasses previous
baselines in open-vocabulary semantic accuracy on the object, room, and floor
level while producing a 75% reduction in representation size compared to dense
open-vocabulary maps. In order to prove the efficacy and generalization
capabilities of HOV-SG, we showcase successful long-horizon
language-conditioned robot navigation within real-world multi-storage
environments. We provide code and trial video data at http://hovsg.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and video are available at http://hovsg.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scenario-Based Curriculum Generation for Multi-Agent Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Brunnbauer, Luigi Berducci, Peter Priller, Dejan Nickovic, Radu Grosu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automated generation of diverse and complex training scenarios has been
an important ingredient in many complex learning tasks. Especially in
real-world application domains, such as autonomous driving, auto-curriculum
generation is considered vital for obtaining robust and general policies.
However, crafting traffic scenarios with multiple, heterogeneous agents is
typically considered as a tedious and time-consuming task, especially in more
complex simulation environments. In our work, we introduce MATS-Gym, a
Multi-Agent Traffic Scenario framework to train agents in CARLA, a
high-fidelity driving simulator. MATS-Gym is a multi-agent training framework
for autonomous driving that uses partial scenario specifications to generate
traffic scenarios with variable numbers of agents. This paper unifies various
existing approaches to traffic scenario description into a single training
framework and demonstrates how it can be integrated with techniques from
unsupervised environment design to automate the generation of adaptive
auto-curricula. The code is available at
https://github.com/AutonomousDrivingExaminer/mats-gym.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 Pages, Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ System Calibration of a Field Phenotyping Robot with Multiple
  High-Precision Profile Laser Scanners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Esser, Gereon Tombrink, Andre Cornelißen, Lasse Klingbeil, Heiner Kuhlmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The creation of precise and high-resolution crop point clouds in agricultural
fields has become a key challenge for high-throughput phenotyping applications.
This work implements a novel calibration method to calibrate the laser scanning
system of an agricultural field robot consisting of two industrial-grade laser
scanners used for high-precise 3D crop point cloud creation. The calibration
method optimizes the transformation between the scanner origins and the robot
pose by minimizing 3D point omnivariances within the point cloud. Moreover, we
present a novel factor graph-based pose estimation method that fuses total
station prism measurements with IMU and GNSS heading information for
high-precise pose determination during calibration. The root-mean-square error
of the distances to a georeferenced ground truth point cloud results in 0.8 cm
after parameter optimization. Furthermore, our results show the importance of a
reference point cloud in the calibration method needed to estimate the vertical
translation of the calibration. Challenges arise due to non-static parameters
while the robot moves, indicated by systematic deviations to a ground truth
terrestrial laser scan.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optical Flow Based Detection and Tracking of Moving Objects for
  Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        MReza Alipour Sormoli, Mehrdad Dianati, Sajjad Mozaffari, Roger woodman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate velocity estimation of surrounding moving objects and their
trajectories are critical elements of perception systems in
Automated/Autonomous Vehicles (AVs) with a direct impact on their safety. These
are non-trivial problems due to the diverse types and sizes of such objects and
their dynamic and random behaviour. Recent point cloud based solutions often
use Iterative Closest Point (ICP) techniques, which are known to have certain
limitations. For example, their computational costs are high due to their
iterative nature, and their estimation error often deteriorates as the relative
velocities of the target objects increase (>2 m/sec). Motivated by such
shortcomings, this paper first proposes a novel Detection and Tracking of
Moving Objects (DATMO) for AVs based on an optical flow technique, which is
proven to be computationally efficient and highly accurate for such problems.
\textcolor{black}{This is achieved by representing the driving scenario as a
vector field and applying vector calculus theories to ensure spatiotemporal
continuity.} We also report the results of a comprehensive performance
evaluation of the proposed DATMO technique, carried out in this study using
synthetic and real-world data. The results of this study demonstrate the
superiority of the proposed technique, compared to the DATMO techniques in the
literature, in terms of estimation accuracy and processing time in a wide range
of relative velocities of moving objects. Finally, we evaluate and discuss the
sensitivity of the estimation error of the proposed DATMO technique to various
system and environmental parameters, as well as the relative velocities of the
moving objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript has been accepted as a regular paper in Transactions
  on Intelligent Transportation Systems (DOI: 10.1109/TITS.2024.3382495)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LiDAR</span>-Based Crop Row Detection Algorithm for Over-Canopy Autonomous
  Navigation in Agriculture Fields <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiji Liu, Francisco Yandun, George Kantor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous navigation is crucial for various robotics applications in
agriculture. However, many existing methods depend on RTK-GPS systems, which
are expensive and susceptible to poor signal coverage. This paper introduces a
state-of-the-art LiDAR-based navigation system that can achieve over-canopy
autonomous navigation in row-crop fields, even when the canopy fully blocks the
interrow spacing. Our crop row detection algorithm can detect crop rows across
diverse scenarios, encompassing various crop types, growth stages, weed
presence, and discontinuities within the crop rows. Without utilizing the
global localization of the robot, our navigation system can perform autonomous
navigation in these challenging scenarios, detect the end of the crop rows, and
navigate to the next crop row autonomously, providing a crop-agnostic approach
to navigate the whole row-crop field. This navigation system has undergone
tests in various simulated agricultural fields, achieving an average of
$2.98cm$ autonomous driving accuracy without human intervention on the custom
Amiga robot. In addition, the qualitative results of our crop row detection
algorithm from the actual soybean fields validate our LiDAR-based crop row
detection algorithm's potential for practical agricultural applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 9 figures, submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Goal-Directed Object Pushing in Cluttered Scenes with
  Location-Based Attention <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nils Dengler, Juan Del Aguila Ferrandis, João Moura, Sethu Vijayakumar, Maren Bennewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-prehensile planar pushing is a challenging task due to its underactuated
nature with hybrid-dynamics, where a robot needs to reason about an object's
long-term behaviour and contact-switching, while being robust to contact
uncertainty. The presence of clutter in the environment further complicates
this task, introducing the need to include more sophisticated spatial analysis
to avoid collisions. Building upon prior work on reinforcement learning (RL)
with multimodal categorical exploration for planar pushing, in this paper we
incorporate location-based attention to enable robust navigation through
clutter. Unlike previous RL literature addressing this obstacle avoidance
pushing task, our framework requires no predefined global paths and considers
the target orientation of the manipulated object. Our results demonstrate that
the learned policies successfully navigate through a wide range of complex
obstacle configurations, including dynamic obstacles, with smooth motions,
achieving the desired target object pose. We also validate the transferability
of the learned policies to robotic hardware using the KUKA iiwa robot arm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object
  Detection with Sparse <span class="highlight-title">LiDAR</span> and Large Domain Gaps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej K Wozniak, Mattias Hansson, Marko Thiel, Patric Jensfelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we address a gap in existing unsupervised domain adaptation
approaches on LiDAR-based 3D object detection, which have predominantly
concentrated on adapting between established, high-density autonomous driving
datasets. We focus on sparser point clouds, capturing scenarios from different
perspectives: not just from vehicles on the road but also from mobile robots on
sidewalks, which encounter significantly different environmental conditions and
sensor configurations. We introduce Unsupervised Adversarial Domain Adaptation
for 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source
models or teacher-student architectures. Instead, it uses an adversarial
approach to directly learn domain-invariant features. We demonstrate its
efficacy in various adaptation scenarios, showing significant improvements in
both self-driving car and mobile robot domains. Our code is open-source and
will be available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Tree Reconstruction and Forest Inventory on a Mobile Robotic
  System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonard Freißmuth, Matias Mattamala, Nived Chebrolu, Simon Schaefer, Stefan Leutenegger, Maurice Fallon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Terrestrial laser scanning (TLS) is the standard technique used to create
accurate point clouds for digital forest inventories. However, the measurement
process is demanding, requiring up to two days per hectare for data collection,
significant data storage, as well as resource-heavy post-processing of 3D data.
In this work, we present a real-time mapping and analysis system that enables
online generation of forest inventories using mobile laser scanners that can be
mounted e.g. on mobile robots. Given incrementally created and locally accurate
submaps-data payloads-our approach extracts tree candidates using a custom,
Voronoi-inspired clustering algorithm. Tree candidates are reconstructed using
an adapted Hough algorithm, which enables robust modeling of the tree stem.
Further, we explicitly incorporate the incremental nature of the data
collection by consistently updating the database using a pose graph LiDAR SLAM
system. This enables us to refine our estimates of the tree traits if an area
is revisited later during a mission. We demonstrate competitive accuracy to TLS
or manual measurements using laser scanners that we mounted on backpacks or
mobile robots operating in conifer, broad-leaf and mixed forests. Our results
achieve RMSE of 1.93 cm, a bias of 0.65 cm and a standard deviation of 1.81 cm
(averaged across these sequences)-with no post-processing required after the
mission is complete.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interactive Identification of Granular Materials using Force
  Measurements <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuli Hynninen, Tran Nguyen Le, Ville Kyrki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to identify granular materials facilitates the emergence of
various new applications in robotics, ranging from cooking at home to truck
loading at mining sites. However, granular material identification remains a
challenging and underexplored area. In this work, we present a novel
interactive material identification framework that enables robots to identify a
wide range of granular materials using only a force-torque sensor for
perception. Our framework, comprising interactive exploration, feature
extraction, and classification stages, prioritizes simplicity and transparency
for seamless integration into various manipulation pipelines. We evaluate the
proposed approach through extensive experiments with a real-world dataset
comprising 11 granular materials, which we also make publicly available.
Additionally, we conducted a comprehensive qualitative analysis of the dataset
to offer deeper insights into its nature, aiding future development. Our
results show that the proposed method is capable of accurately identifying a
wide range of granular materials solely relying on force measurements obtained
from direct interaction with the materials. Code and dataset are available at:
https://irobotics.aalto.fi/indentify_granular/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to 2024 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aerial Robots Carrying Flexible Cables: Dynamic Shape Optimal Control
  via Spectral Method Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaolei Shen, Chiara Gabellieri, Antonio Franchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a model-based optimal boundary control design for an
aerial robotic system composed of a quadrotor carrying a flexible cable. The
whole system is modeled by partial differential equations (PDEs) combined with
boundary conditions described by ordinary differential equations (ODEs). The
proper orthogonal decomposition (POD) method is adopted to project the original
infinite-dimensional system on a subspace spanned by orthogonal basis
functions. Based on the reduced order model, nonlinear model predictive control
(NMPC) is implemented online to realize shape trajectory tracking of the
flexible cable in an optimal predictive fashion. The proposed reduced modeling
and optimal control paradigms are numerically verified against an accurate
high-dimensional FDM-based model in different scenarios and the controller's
superior performance is shown compared to an optimally tuned PID controller.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time-Optimal Flight with Safety Constraints and Data-driven Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Krinner, Angel Romero, Leonard Bauersfeld, Melanie Zeilinger, Andrea Carron, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-optimal quadrotor flight is an extremely challenging problem due to the
limited control authority encountered at the limit of handling. Model
Predictive Contouring Control (MPCC) has emerged as a leading model-based
approach for time optimization problems such as drone racing. However, the
standard MPCC formulation used in quadrotor racing introduces the notion of the
gates directly in the cost function, creating a multi-objective optimization
that continuously trades off between maximizing progress and tracking the path
accurately. This paper introduces three key components that enhance the MPCC
approach for drone racing. First and foremost, we provide safety guarantees in
the form of a constraint and terminal set. The safety set is designed as a
spatial constraint which prevents gate collisions while allowing for
time-optimization only in the cost function. Second, we augment the existing
first principles dynamics with a residual term that captures complex
aerodynamic effects and thrust forces learned directly from real world data.
Third, we use Trust Region Bayesian Optimization (TuRBO), a state of the art
global Bayesian Optimization algorithm, to tune the hyperparameters of the MPC
controller given a sparse reward based on lap time minimization. The proposed
approach achieves similar lap times to the best state-of-the-art RL and
outperforms the best time-optimal controller while satisfying constraints. In
both simulation and real-world, our approach consistently prevents gate crashes
with 100\% success rate, while pushing the quadrotor to its physical limit
reaching speeds of more than 80km/h.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepMIF: Deep Monotonic Implicit Fields for Large-Scale <span class="highlight-title">LiDAR</span> 3D <span class="highlight-title">Mapping</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kutay Yılmaz, Matthias Nießner, Anastasiia Kornilova, Alexey Artemov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, significant progress has been achieved in sensing real large-scale
outdoor 3D environments, particularly by using modern acquisition equipment
such as LiDAR sensors. Unfortunately, they are fundamentally limited in their
ability to produce dense, complete 3D scenes. To address this issue, recent
learning-based methods integrate neural implicit representations and
optimizable feature grids to approximate surfaces of 3D scenes. However,
naively fitting samples along raw LiDAR rays leads to noisy 3D mapping results
due to the nature of sparse, conflicting LiDAR measurements. Instead, in this
work we depart from fitting LiDAR data exactly, instead letting the network
optimize a non-metric monotonic implicit field defined in 3D space. To fit our
field, we design a learning system integrating a monotonicity loss that enables
optimizing neural monotonic fields and leverages recent progress in large-scale
3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as
captured by multiple quantitative and perceptual measures and visual results
obtained for Mai City, Newer College, and KITTI benchmarks. The code of our
approach will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design and Preliminary Evaluation of a Torso Stabiliser for Individuals
  with Spinal Cord Injury 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rejin John Varghese, Man-Yan Tong, Isabella Szczech, Peter Bryan, Dario Farina, Etienne Burdet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spinal cord injuries (SCIs) generally result in sensory and mobility
impairments, with torso instability being particularly debilitating. Existing
torso stabilisers are often rigid and restrictive. This paper presents an early
investigation into a non-restrictive 1 degree-of-freedom (DoF) mechanical torso
stabiliser inspired by devices such as centrifugal clutches and seat-belt
mechanisms. Firstly, the paper presents a motion-capture (MoCap) and
OpenSim-based kinematic analysis of the cable-based system to understand
requisite device characteristics. The simulated evaluation resulted in the
cable-based device to require 55-60cm of unrestricted travel, and to lock at a
threshold cable velocity of 80-100cm/sec. Next, the developed 1-DoF device is
introduced. The proposed mechanical device is transparent during activities of
daily living, and transitions to compliant blocking when incipient fall is
detected. Prototype behaviour was then validated using a MoCap-based kinematic
analysis to verify non-restrictive movement, reliable transition to blocking,
and compliance of the blocking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 4 figures, 10 references. Submitted to IEEE EMBC 2024
  conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-Power, Flexible, Robust Hand: Development of Musculoskeletal Hand
  Using Machined Springs and Realization of Self-Weight Supporting Motion with
  Humanoid <span class="chip">IROS2017</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shogo Makino, Kento Kawaharazuka, Masaya Kawamura, Yuki Asano, Kei Okada, Masayuki Inaba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human can not only support their body during standing or walking, but also
support them by hand, so that they can dangle a bar and others. But most
humanoid robots support their body only in the foot and they use their hand
just to manipulate objects because their hands are too weak to support their
body. Strong hands are supposed to enable humanoid robots to act in much
broader scene. Therefore, we developed new life-size five-fingered hand that
can support the body of life-size humanoid robot. It is tendon-driven and
underactuated hand and actuators in forearms produce large gripping force. This
hand has flexible joints using machined springs, which can be designed
integrally with the attachment. Thus, it has both structural strength and
impact resistance in spite of small size. As other characteristics, this hand
has force sensors to measure external force and the fingers can be flexed along
objects though the number of actuators to flex fingers is less than that of
fingers. We installed the developed hand on musculoskeletal humanoid "Kengoro"
and achieved two self-weight supporting motions: push-up motion and dangling
motion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at IROS2017</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Five-fingered Hand with Wide Range of Thumb Using Combination of
  Machined Springs and Variable Stiffness Joints <span class="chip">IROS2018</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shogo Makino, Kento Kawaharazuka, Ayaka Fujii, Masaya Kawamura, Tasuku Makabe, Moritaka Onitsuka, Yuki Asano, Kei Okada, Koji Kawasaki, Masayuki Inaba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human hands can not only grasp objects of various shape and size and
manipulate them in hands but also exert such a large gripping force that they
can support the body in the situations such as dangling a bar and climbing a
ladder. On the other hand, it is difficult for most robot hands to manage both.
Therefore in this paper we developed the hand which can grasp various objects
and exert large gripping force. To develop such hand, we focused on the thumb
CM joint with wide range of motion and the MP joints of four fingers with the
DOF of abduction and adduction. Based on the hand with large gripping force and
flexibility using machined spring, we applied above mentioned joint mechanism
to the hand. The thumb CM joint has wide range of motion because of the
combination of three machined springs and MP joints of four fingers have
variable rigidity mechanism instead of driving each joint independently in
order to move joint in limited space and by limited actuators. Using the
developed hand, we achieved the grasping of various objects, supporting a large
load and several motions with an arm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at IROS2018</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Line-Of-Sight guidance law based on vector fields path
  following for underactuated unmanned surface vehicle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Qi, Ronghua Wang, Nailong Wu, Yuxin Fan, Jigang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The focus of this paper is to develop a methodology that enables an unmanned
surface vehicle (USV) to efficiently track a planned path. The introduction of
a vector field-based adaptive line-of-sight guidance law (VFALOS) for accurate
trajectory tracking and minimizing the overshoot response time during USV
tracking of curved paths improves the overall line-of-sight (LOS) guidance
method. These improvements contribute to faster convergence to the desired
path, reduce oscillations, and can mitigate the effects of persistent external
disturbances. It is shown that the proposed guidance law exhibits k-exponential
stability when converging to the desired path consisting of straight and curved
lines. The results in the paper show that the proposed method effectively
improves the accuracy of the USV tracking the desired path while ensuring the
safety of the USV work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive <span class="highlight-title">LiDAR</span>-Radar Fusion for Outdoor <span class="highlight-title">Odometry</span> Across Dense Smoke
  Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chiyun Noh, Ayoung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust odometry estimation in perceptually degraded environments represents a
key challenge in the field of robotics. In this paper, we propose a LiDAR-radar
fusion method for robust odometry for adverse environment with LiDAR
degeneracy. By comparing the LiDAR point cloud with the radar static point
cloud obtained through preprocessing module, it is possible to identify
instances of LiDAR degeneracy to overcome perceptual limits. We demonstrate the
effectiveness of our method in challenging conditions such as dense smoke,
showcasing its ability to reliably estimate odometry and identify/remove
dynamic points prone to LiDAR degeneracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cyclic pursuit formation control for arbitrary desired shapes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Fujioka, Masaki Ogura, Naoki Wakamiya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A multi-agent system comprises numerous agents that autonomously make
decisions to collectively accomplish tasks, drawing significant attention for
their wide-ranging applications. Within this context, formation control emerges
as a prominent task, wherein agents collaboratively shape and maneuver while
preserving formation integrity. Our focus centers on cyclic pursuit, a method
facilitating the formation of circles, ellipses, and figure-eights under the
assumption that agents can only perceive the relative positions of those
preceding them. However, this method's scope has been restricted to these
specific shapes, leaving the feasibility of forming other shapes uncertain. In
response, our study proposes a novel method based on cyclic pursuit capable of
forming a broader array of shapes, enabling agents to individually shape while
pursuing preceding agents, thereby extending the repertoire of achievable
formations. We present two scenarios concerning the information available to
agents and devise formation control methods tailored to each scenario. Through
extensive simulations, we demonstrate the efficacy of our proposed method in
forming multiple shapes, including those represented as Fourier series, thereby
underscoring the versatility and effectiveness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural-artificial hybrid swarm: Cyborg-insect group navigation in
  unknown obstructed soft terrain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Bai, Phuoc Thanh Tran Ngoc, Huu Duoc Nguyen, Duc Long Le, Quang Huy Ha, Kazuki Kai, Yu Xiang See To, Yaosheng Deng, Jie Song, Naoki Wakamiya, Hirotaka Sato, Masaki Ogura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigating multi-robot systems in complex terrains has always been a
challenging task. This is due to the inherent limitations of traditional robots
in collision avoidance, adaptation to unknown environments, and sustained
energy efficiency. In order to overcome these limitations, this research
proposes a solution by integrating living insects with miniature electronic
controllers to enable robotic-like programmable control, and proposing a novel
control algorithm for swarming. Although these creatures, called cyborg
insects, have the ability to instinctively avoid collisions with neighbors and
obstacles while adapting to complex terrains, there is a lack of literature on
the control of multi-cyborg systems. This research gap is due to the difficulty
in coordinating the movements of a cyborg system under the presence of insects'
inherent individual variability in their reactions to control input. In
response to this issue, we propose a novel swarm navigation algorithm
addressing these challenges. The effectiveness of the algorithm is demonstrated
through an experimental validation in which a cyborg swarm was successfully
navigated through an unknown sandy field with obstacles and hills. This
research contributes to the domain of swarm robotics and showcases the
potential of integrating biological organisms with robotics and control theory
to create more intelligent autonomous systems with real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RoboDuet: A Framework Affording Mobile-Manipulation and Cross-Embodiment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoping Pan, Qingwei Ben, Zhecheng Yuan, Guangqi Jiang, Yandong Ji, Jiangmiao Pang, Houde Liu, Huazhe Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining the mobility of legged robots with the manipulation skills of arms
has the potential to significantly expand the operational range and enhance the
capabilities of robotic systems in performing various mobile manipulation
tasks. Existing approaches are confined to imprecise six degrees of freedom
(DoF) manipulation and possess a limited arm workspace. In this paper, we
propose a novel framework, RoboDuet, which employs two collaborative policies
to realize locomotion and manipulation simultaneously, achieving whole-body
control through interactions between each other. Surprisingly, going beyond the
large-range pose tracking, we find that the two-policy framework may enable
cross-embodiment deployment such as using different quadrupedal robots or other
arms. Our experiments demonstrate that the policies trained through RoboDuet
can accomplish stable gaits, agile 6D end-effector pose tracking, and zero-shot
exchange of legged robots, and can be deployed in the real world to perform
various mobile manipulation tasks. Our project page with demo videos is at
https://locomanip-duet.github.io .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Objective Trajectory Planning with Dual-Encoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beibei Zhang, Tian Xiang, Chentao Mao, Yuhua Zheng, Shuai Li, Haoyi Niu, Xiangming Xi, Wenyuan Bai, Feng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-jerk optimal trajectory planning is crucial in advancing robotic arms'
performance in dynamic tasks. Traditional methods rely on solving complex
nonlinear programming problems, bringing significant delays in generating
optimized trajectories. In this paper, we propose a two-stage approach to
accelerate time-jerk optimal trajectory planning. Firstly, we introduce a
dual-encoder based transformer model to establish a good preliminary
trajectory. This trajectory is subsequently refined through sequential
quadratic programming to improve its optimality and robustness. Our approach
outperforms the state-of-the-art by up to 79.72\% in reducing trajectory
planning time. Compared with existing methods, our method shrinks the
optimality gap with the objective function value decreasing by up to 29.9\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Path and Gait Planning for Safe Bipedal Robot Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyang Peng, Victor Paredes, Ayonga Hereid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe path and gait planning are essential for bipedal robots to navigate
complex real-world environments. The prevailing approaches often plan the path
and gait separately in a hierarchical fashion, potentially resulting in unsafe
movements due to neglecting the physical constraints of walking robots. A
safety-critical path must not only avoid obstacles but also ensure that the
robot's gaits are subject to its dynamic and kinematic constraints. This work
presents a novel approach that unifies path planning and gait planning via a
Model Predictive Control (MPC) using the Linear Inverted Pendulum (LIP) model
representing bipedal locomotion. This approach considers environmental
constraints, such as obstacles, and the robot's kinematics and dynamics
constraints. By using discrete-time Control Barrier Functions for obstacle
avoidance, our approach generates the next foot landing position, ensuring
robust walking gaits and a safe navigation path within clustered environments.
We validated our proposed approach in simulation using a Digit robot in 20
randomly created environments. The results demonstrate improved performance in
terms of safety and robustness when compared to hierarchical path and gait
planning frameworks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Symmetry in RL-based Legged Locomotion Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi Su, Xiaoyu Huang, Daniel Ordoñez-Apraez, Yunfei Li, Zhongyu Li, Qiayuan Liao, Giulio Turrisi, Massimiliano Pontil, Claudio Semini, Yi Wu, Koushil Sreenath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model-free reinforcement learning is a promising approach for autonomously
solving challenging robotics control problems, but faces exploration difficulty
without information of the robot's kinematics and dynamics morphology. The
under-exploration of multiple modalities with symmetric states leads to
behaviors that are often unnatural and sub-optimal. This issue becomes
particularly pronounced in the context of robotic systems with morphological
symmetries, such as legged robots for which the resulting asymmetric and
aperiodic behaviors compromise performance, robustness, and transferability to
real hardware. To mitigate this challenge, we can leverage symmetry to guide
and improve the exploration in policy learning via equivariance/invariance
constraints. In this paper, we investigate the efficacy of two approaches to
incorporate symmetry: modifying the network architectures to be strictly
equivariant/invariant, and leveraging data augmentation to approximate
equivariant/invariant actor-critics. We implement the methods on challenging
loco-manipulation and bipedal locomotion tasks and compare with an
unconstrained baseline. We find that the strictly equivariant policy
consistently outperforms other methods in sample efficiency and task
performance in simulation. In addition, symmetry-incorporated approaches
exhibit better gait quality, higher robustness and can be deployed zero-shot in
real-world experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse-Graph-Enabled Formation Planning for Large-Scale Aerial Swarms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Zhou, Lun Quan, Chao Xu, Guangtong Xu, Fei Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The formation trajectory planning using complete graphs to model
collaborative constraints becomes computationally intractable as the number of
drones increases due to the curse of dimensionality. To tackle this issue, this
paper presents a sparse graph construction method for formation planning to
realize better efficiency-performance trade-off. Firstly, a sparsification
mechanism for complete graphs is designed to ensure the global rigidity of
sparsified graphs, which is a necessary condition for uniquely corresponding to
a geometric shape. Secondly, a good sparse graph is constructed to preserve the
main structural feature of complete graphs sufficiently. Since the graph-based
formation constraint is described by Laplacian matrix, the sparse graph
construction problem is equivalent to submatrix selection, which has
combinatorial time complexity and needs a scoring metric. Via comparative
simulations, the Max-Trace matrix-revealing metric shows the promising
performance. The sparse graph is integrated into the formation planning.
Simulation results with 72 drones in complex environments demonstrate that when
preserving 30\% connection edges, our method has comparative formation error
and recovery performance w.r.t. complete graphs. Meanwhile, the planning
efficiency is improved by approximate an order of magnitude. Benchmark
comparisons and ablation studies are conducted to fully validate the merits of
our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Code Generation for Conic Model-Predictive Control on Microcontrollers
  with TinyMPC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Schoedel, Khai Nguyen, Elakhya Nedumaran, Brian Plancher, Zachary Manchester
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conic constraints appear in many important control applications like legged
locomotion, robotic manipulation, and autonomous rocket landing. However,
current solvers for conic optimization problems have relatively heavy
computational demands in terms of both floating-point operations and memory
footprint, making them impractical for use on small embedded devices. We extend
TinyMPC, an open-source, high-speed solver targeting low-power embedded control
applications, to handle second-order cone constraints. We also present
code-generation software to enable deployment of TinyMPC on a variety of
microcontrollers. We benchmark our generated code against state-of-the-art
embedded QP and SOCP solvers, demonstrating a two-order-of-magnitude speed
increase over ECOS while consuming less memory. Finally, we demonstrate
TinyMPC's efficacy on the Crazyflie, a lightweight, resource-constrained
quadrotor with fast dynamics. TinyMPC and its code-generation tools are
publicly available at https://tinympc.org.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to CDC, 2024. First two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Real-Time Rescheduling Algorithm for Multi-robot Plan Execution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Feng, Adittyo Paul, Zhe Chen, Jiaoyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One area of research in multi-agent path finding is to determine how
replanning can be efficiently achieved in the case of agents being delayed
during execution. One option is to reschedule the passing order of agents,
i.e., the sequence in which agents visit the same location. In response, we
propose Switchable-Edge Search (SES), an A*-style algorithm designed to find
optimal passing orders. We prove the optimality of SES and evaluate its
efficiency via simulations. The best variant of SES takes less than 1 second
for small- and medium-sized problems and runs up to 4 times faster than
baselines for large-sized problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICAPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Multi-Band Temporal Video Filter for Reducing Human-Robot
  Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lawrence O'Gorman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although mobile robots have on-board sensors to perform navigation, their
efficiency in completing paths can be enhanced by planning to avoid human
interaction. Infrastructure cameras can capture human activity continuously for
the purpose of compiling activity analytics to choose efficient times and
routes. We describe a cascade temporal filtering method to efficiently extract
short- and long-term activity in two time dimensions, isochronal and
chronological, for use in global path planning and local navigation
respectively. The temporal filter has application either independently, or, if
object recognition is also required, it can be used as a pre-filter to perform
activity-gating of the more computationally expensive neural network
processing. For a testbed 32-camera network, we show how this hybrid approach
can achieve over 8 times improvement in frames per second throughput and 6.5
times reduction of system power use. We also show how the cost map of static
objects in the ROS robot software development framework is augmented with
dynamic regions determined from the temporal filter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Path Integral Control with Rollout Clustering and Dynamic Obstacles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Patrick, Efstathios Bakolas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model Predictive Path Integral (MPPI) control has proven to be a powerful
tool for the control of uncertain systems (such as systems subject to
disturbances and systems with unmodeled dynamics). One important limitation of
the baseline MPPI algorithm is that it does not utilize simulated trajectories
to their fullest extent. For one, it assumes that the average of all
trajectories weighted by their performance index will be a safe trajectory. In
this paper, multiple examples are shown where the previous assumption does not
hold, and a trajectory clustering technique is presented that reduces the
chances of the weighted average crossing in an unsafe region. Secondly, MPPI
does not account for dynamic obstacles, so the authors put forward a novel cost
function that accounts for dynamic obstacles without adding significant
computation time to the overall algorithm. The novel contributions proposed in
this paper were evaluated with extensive simulations to demonstrate
improvements upon the state-of-the-art MPPI techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, extended version of ACC 2024 submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ShapeGrasp: Zero-Shot Task-Oriented Grasping with Large Language Models
  through Geometric Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Li, Sarthak Bhagat, Joseph Campbell, Yaqi Xie, Woojun Kim, Katia Sycara, Simon Stepputtis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task-oriented grasping of unfamiliar objects is a necessary skill for robots
in dynamic in-home environments. Inspired by the human capability to grasp such
objects through intuition about their shape and structure, we present a novel
zero-shot task-oriented grasping method leveraging a geometric decomposition of
the target object into simple, convex shapes that we represent in a graph
structure, including geometric attributes and spatial relationships. Our
approach employs minimal essential information - the object's name and the
intended task - to facilitate zero-shot task-oriented grasping. We utilize the
commonsense reasoning capabilities of large language models to dynamically
assign semantic meaning to each decomposed part and subsequently reason over
the utility of each part for the intended task. Through extensive experiments
on a real-world robotics platform, we demonstrate that our grasping approach's
decomposition and reasoning pipeline is capable of selecting the correct part
in 92% of the cases and successfully grasping the object in 82% of the tasks we
evaluate. Additional videos, experiments, code, and data are available on our
project website: https://shapegrasp.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Piecewise Residuals of Control Barrier Functions for Safety of
  Switching Systems using Multi-Output Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Aali, Jun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Control barrier functions (CBFs) have recently been introduced as a
systematic tool to ensure safety by establishing set invariance. When combined
with a control Lyapunov function (CLF), they form a safety-critical control
mechanism. However, the effectiveness of CBFs and CLFs is closely tied to the
system model. In practice, model uncertainty can jeopardize safety and
stability guarantees and may lead to undesirable performance. In this paper, we
develop a safe learning-based control strategy for switching systems in the
face of uncertainty. We focus on the case that a nominal model is available for
a true underlying switching system. This uncertainty results in piecewise
residuals for each switching surface, impacting the CLF and CBF constraints. We
introduce a batch multi-output Gaussian process (MOGP) framework to approximate
these piecewise residuals, thereby mitigating the adverse effects of
uncertainty. A particular structure of the covariance function enables us to
convert the MOGP-based chance constraints CLF and CBF into second-order cone
constraints, which leads to a convex optimization. We analyze the feasibility
of the resulting optimization and provide the necessary and sufficient
conditions for feasibility. The effectiveness of the proposed strategy is
validated through a simulation of a switching adaptive cruise control system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2403.09573</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpectralWaste <span class="highlight-title">Dataset</span>: Multimodal Data for Waste Sorting Automation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Casao, Fernando Peña, Alberto Sabater, Rosa Castillón, Darío Suárez, Eduardo Montijano, Ana C. Murillo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increase in non-biodegradable waste is a worldwide concern. Recycling
facilities play a crucial role, but their automation is hindered by the complex
characteristics of waste recycling lines like clutter or object deformation. In
addition, the lack of publicly available labeled data for these environments
makes developing robust perception systems challenging. Our work explores the
benefits of multimodal perception for object segmentation in real waste
management scenarios. First, we present SpectralWaste, the first dataset
collected from an operational plastic waste sorting facility that provides
synchronized hyperspectral and conventional RGB images. This dataset contains
labels for several categories of objects that commonly appear in sorting plants
and need to be detected and separated from the main trash flow for several
reasons, such as security in the management line or reuse. Additionally, we
propose a pipeline employing different object segmentation architectures and
evaluate the alternatives on our dataset, conducting an extensive analysis for
both multimodal and unimodal alternatives. Our evaluation pays special
attention to efficiency and suitability for real-time processing and
demonstrates how HSI can bring a boost to RGB-only perception in these
realistic industrial settings without much computational overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Study on the Use of Simulation in Synthesizing Path-Following Control
  Policies for Autonomous Ground Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harry Zhang, Stefan Caldararu, Aaron Young, Alexis Ruiz, Huzaifa Unjhawala, Ishaan Mahajan, Sriram Ashokkumar, Nevindu Batagoda, Zhenhao Zhou, Luning Bakke, Dan Negrut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We report results obtained and insights gained while answering the following
question: how effective is it to use a simulator to establish path following
control policies for an autonomous ground robot? While the quality of the
simulator conditions the answer to this question, we found that for the
simulation platform used herein, producing four control policies for path
planning was straightforward once a digital twin of the controlled robot was
available. The control policies established in simulation and subsequently
demonstrated in the real world are PID control, MPC, and two neural network
(NN) based controllers. Training the two NN controllers via imitation learning
was accomplished expeditiously using seven simple maneuvers: follow three
circles clockwise, follow the same circles counter-clockwise, and drive
straight. A test randomization process that employs random micro-simulations is
used to rank the ``goodness'' of the four control policies. The policy ranking
noted in simulation correlates well with the ranking observed when the control
policies were tested in the real world. The simulation platform used is
publicly available and BSD3-released as open source; a public Docker image is
available for reproducibility studies. It contains a dynamics engine, a sensor
simulator, a ROS2 bridge, and a ROS2 autonomy stack the latter employed both in
the simulator and the real world experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Constructive Method for Designing Safe Multirate Controllers for
  Differentially-Flat Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devansh R. Agrawal, Hardik Parwana, Ryan K. Cosner, Ugo Rosolia, Aaron D. Ames, Dimitra Panagou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a multi-rate control architecture that leverages fundamental
properties of differential flatness to synthesize controllers for
safety-critical nonlinear dynamical systems. We propose a two-layer
architecture, where the high-level generates reference trajectories using a
linear Model Predictive Controller, and the low-level tracks this reference
using a feedback controller. The novelty lies in how we couple these layers, to
achieve formal guarantees on recursive feasibility of the MPC problem, and
safety of the nonlinear system. Furthermore, using differential flatness, we
provide a constructive means to synthesize the multi-rate controller, thereby
removing the need to search for suitable Lyapunov or barrier functions, or to
approximately linearize/discretize nonlinear dynamics. We show the synthesized
controller is a convex optimization problem, making it amenable to real-time
implementations. The method is demonstrated experimentally on a ground rover
and a quadruped robotic system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, accepted at IEEE Control Systems Letters 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe Explicable Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03773v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03773v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akkamahadevi Hanni, Andrew Boateng, Yu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human expectations arise from their understanding of others and the world. In
the context of human-AI interaction, this understanding may not align with
reality, leading to the AI agent failing to meet expectations and compromising
team performance. Explicable planning, introduced as a method to bridge this
gap, aims to reconcile human expectations with the agent's optimal behavior,
facilitating interpretable decision-making. However, an unresolved critical
issue is ensuring safety in explicable planning, as it could result in
explicable behaviors that are unsafe. To address this, we propose Safe
Explicable Planning (SEP), which extends the prior work to support the
specification of a safety bound. The goal of SEP is to find behaviors that
align with human expectations while adhering to the specified safety criterion.
Our approach generalizes the consideration of multiple objectives stemming from
multiple models rather than a single model, yielding a Pareto set of safe
explicable policies. We present both an exact method, guaranteeing finding the
Pareto set, and a more efficient greedy method that finds one of the policies
in the Pareto set. Additionally, we offer approximate solutions based on state
aggregation to improve scalability. We provide formal proofs that validate the
desired theoretical properties of these methods. Evaluation through simulations
and physical robot experiments confirms the effectiveness of our approach for
safe explicable planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Resilient source seeking with robot swarms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Acuaviva, Jesus Bautista, Weijia Yao, Juan Jimenez, Hector Garcia de Marina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a solution for locating the source, or maximum, of an unknown
scalar field using a swarm of mobile robots. Unlike relying on the traditional
gradient information, the swarm determines an ascending direction to approach
the source with arbitrary precision. The ascending direction is calculated from
measurements of the field strength at the robot locations and their relative
positions concerning the centroid. Rather than focusing on individual robots,
we focus the analysis on the density of robots per unit area to guarantee a
more resilient swarm, i.e., the functionality remains even if individuals go
missing or are misplaced during the mission. We reinforce the robustness of the
algorithm by providing sufficient conditions for the swarm shape so that the
ascending direction is almost parallel to the gradient. The swarm can respond
to an unexpected environment by morphing its shape and exploiting the existence
of multiple ascending directions. Finally, we validate our approach numerically
with hundreds of robots. The fact that a large number of robots always
calculate an ascending direction compensates for the loss of individuals and
mitigates issues arising from the actuator and sensor noises.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, submitted to CDC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tuning-free Quasi-stiffness Control Framework of a Powered Transfemoral
  Prosthesis for Task-adaptive Walking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15030v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15030v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teng Ma, Shucong Yin, Zhimin Hou, Binxin Huang, Haoyong Yu, Chenglong Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Impedance-based control represents a prevalent strategy in the development of
powered transfemoral prostheses. However, creating a task-adaptive, tuning-free
controller that effectively generalizes across diverse locomotion modes and
terrain conditions continues to be a significant challenge. This letter
proposes a tuning-free and task-adaptive quasi-stiffness control framework for
powered prostheses that generalizes across various walking tasks, including the
torque-angle relationship reconstruction part and the quasi-stiffness
controller design part. A Gaussian Process Regression (GPR) model is introduced
to predict the target features of the human joint angle and torque in a new
task. Subsequently, a Kernelized Movement Primitives (KMP) is employed to
reconstruct the torque-angle relationship of the new task from multiple human
reference trajectories and estimated target features. Based on the torque-angle
relationship of the new task, a quasi-stiffness control approach is designed
for a powered prosthesis. Finally, the proposed framework is validated through
practical examples, including varying speeds and inclines walking tasks.
Notably, the proposed framework not only aligns with but frequently surpasses
the performance of a benchmark finite state machine impedance controller
(FSMIC) without necessitating manual impedance tuning and has the potential to
expand to variable walking tasks in daily life for the transfemoral amputees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 10 figures. This work has been submitted to the IEEE-RAL for
  possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Randomization via Entropy Maximization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01885v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01885v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Tiboni, Pascal Klink, Jan Peters, Tatiana Tommasi, Carlo D'Eramo, Georgia Chalvatzaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Varying dynamics parameters in simulation is a popular Domain Randomization
(DR) approach for overcoming the reality gap in Reinforcement Learning (RL).
Nevertheless, DR heavily hinges on the choice of the sampling distribution of
the dynamics parameters, since high variability is crucial to regularize the
agent's behavior but notoriously leads to overly conservative policies when
randomizing excessively. In this paper, we propose a novel approach to address
sim-to-real transfer, which automatically shapes dynamics distributions during
training in simulation without requiring real-world data. We introduce DOmain
RAndomization via Entropy MaximizatiON (DORAEMON), a constrained optimization
problem that directly maximizes the entropy of the training distribution while
retaining generalization capabilities. In achieving this, DORAEMON gradually
increases the diversity of sampled dynamics parameters as long as the
probability of success of the current policy is sufficiently high. We
empirically validate the consistent benefits of DORAEMON in obtaining highly
adaptive and generalizable policies, i.e. solving the task at hand across the
widest range of dynamics parameters, as opposed to representative baselines
from the DR literature. Notably, we also demonstrate the Sim2Real applicability
of DORAEMON through its successful zero-shot transfer in a robotic manipulation
setup under unknown real-world parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2024. Project website at
  https://gabrieletiboni.github.io/doraemon/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SGS-<span class="highlight-title">SLAM</span>: Semantic Gaussian Splatting For Neural Dense <span class="highlight-title">SLAM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03246v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03246v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingrui Li, Shuhong Liu, Heng Zhou, Guohao Zhu, Na Cheng, Tianchen Deng, Hongyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SGS-SLAM, the first semantic visual SLAM system based on Gaussian
Splatting. It incorporates appearance, geometry, and semantic features through
multi-channel optimization, addressing the oversmoothing limitations of neural
implicit SLAM systems in high-quality rendering, scene understanding, and
object-level geometry. We introduce a unique semantic feature loss that
effectively compensates for the shortcomings of traditional depth and color
losses in object optimization. Through a semantic-guided keyframe selection
strategy, we prevent erroneous reconstructions caused by cumulative errors.
Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art
performance in camera pose estimation, map reconstruction, precise semantic
segmentation, and object-level geometric accuracy, while ensuring real-time
rendering capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Source-free Domain Adaptive Semantic Segmentation via
  Importance-aware and Prototype-contrast Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01598v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01598v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Cao, Hui Zhang, Xiao Lu, Zheng Xiao, Kailun Yang, Yaonan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain adaptive semantic segmentation enables robust pixel-wise understanding
in real-world driving scenes. Source-free domain adaptation, as a more
practical technique, addresses the concerns of data privacy and storage
limitations in typical unsupervised domain adaptation methods, making it
especially relevant in the context of intelligent vehicles. It utilizes a
well-trained source model and unlabeled target data to achieve adaptation in
the target domain. However, in the absence of source data and target labels,
current solutions cannot sufficiently reduce the impact of domain shift and
fully leverage the information from the target data. In this paper, we propose
an end-to-end source-free domain adaptation semantic segmentation method via
Importance-Aware and Prototype-Contrast (IAPC) learning. The proposed IAPC
framework effectively extracts domain-invariant knowledge from the well-trained
source model and learns domain-specific knowledge from the unlabeled target
domain. Specifically, considering the problem of domain shift in the prediction
of the target domain by the source model, we put forward an importance-aware
mechanism for the biased target prediction probability distribution to extract
domain-invariant knowledge from the source model. We further introduce a
prototype-contrast strategy, which includes a prototype-symmetric cross-entropy
loss and a prototype-enhanced cross-entropy loss, to learn target intra-domain
knowledge without relying on labels. A comprehensive variety of experiments on
two domain adaptive semantic segmentation benchmarks demonstrates that the
proposed end-to-end IAPC solution outperforms existing state-of-the-art
methods. The source code is publicly available at
https://github.com/yihong-97/Source-free-IAPC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Intelligent Vehicles (T-IV). The
  source code is publicly available at
  https://github.com/yihong-97/Source-free-IAPC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Robotics Meets Wireless Communications: An Introductory Tutorial 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.02021v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.02021v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Bonilla Licea, Mounir Ghogho, Martin Saska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The importance of ground Mobile Robots (MRs) and Unmanned Aerial Vehicles
(UAVs) within the research community, industry, and society is growing fast.
Many of these agents are nowadays equipped with communication systems that are,
in some cases, essential to successfully achieve certain tasks. In this
context, we have begun to witness the development of a new interdisciplinary
research field at the intersection of robotics and communications. This
research field has been boosted by the intention of integrating UAVs within the
5G and 6G communication networks. This research will undoubtedly lead to many
important applications in the near future. Nevertheless, one of the main
obstacles to the development of this research area is that most researchers
address these problems by oversimplifying either the robotics or the
communications aspect. This impedes the ability of reaching the full potential
of this new interdisciplinary research area. In this tutorial, we present some
of the modelling tools necessary to address problems involving both robotics
and communication from an interdisciplinary perspective. As an illustrative
example of such problems, we focus in this tutorial on the issue of
communication-aware trajectory planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 192 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motion Generation from Fine-grained Textual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunhang Li, Yansong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of text2motion is to generate human motion sequences from given
textual descriptions, where the model explores diverse mappings from natural
language instructions to human body movements. While most existing works are
confined to coarse-grained motion descriptions, e.g., "A man squats.",
fine-grained descriptions specifying movements of relevant body parts are
barely explored. Models trained with coarse-grained texts may not be able to
learn mappings from fine-grained motion-related words to motion primitives,
resulting in the failure to generate motions from unseen descriptions. In this
paper, we build a large-scale language-motion dataset specializing in
fine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with
step-by-step instructions with pseudo-code compulsory checks. Accordingly, we
design a new text2motion model, FineMotionDiffuse, making full use of
fine-grained textual information. Our quantitative evaluation shows that
FineMotionDiffuse trained on FineHumanML3D improves FID by a large margin of
0.38, compared with competitive baselines. According to the qualitative
evaluation and case study, our model outperforms MotionDiffuse in generating
spatially or chronologically composite motions, by learning the implicit
mappings from fine-grained descriptions to the corresponding basic motions. We
release our data at https://github.com/KunhangL/finemotiondiffuse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attention-based Estimation and Prediction of Human Intent to augment
  Haptic Glove aided Control of Robotic Hand 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.07953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.07953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muneeb Ahmed, Rajesh Kumar, Qaim Abbas, Brejesh Lall, Arzad A. Kherani, Sudipto Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The letter focuses on Haptic Glove (HG) based control of a Robotic Hand (RH)
executing in-hand manipulation of certain objects of interest. The high
dimensional motion signals in HG and RH possess intrinsic variability of
kinematics resulting in difficulty to establish a direct mapping of the motion
signals from HG onto the RH. An estimation mechanism is proposed to quantify
the motion signal acquired from the human controller in relation to the
intended goal pose of the object being held by the robotic hand. A control
algorithm is presented to transform the synthesized intent at the RH and allow
relocation of the object to the expected goal pose. The lag in synthesis of the
intent in the presence of communication delay leads to a requirement of
predicting the estimated intent. We leverage an attention-based convolutional
neural network encoder to predict the trajectory of intent for a certain
lookahead to compensate for the delays. The proposed methodology is evaluated
across objects of different shapes, mass, and materials. We present a
comparative performance of the estimation and prediction mechanisms on
5G-driven real-world robotic setup against benchmark methodologies. The
test-MSE in prediction of human intent is reported to yield ~ 97.3 -98.7%
improvement of accuracy in comparison to LSTM-based benchmark
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guessing human intentions to avoid dangerous situations in caregiving
  robots <span class="chip">IROS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16291v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16291v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noé Zapata, Gerardo Pérez, Lucas Bonilla, Pedro Núñez, Pilar Bachiller, Pablo Bustos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For robots to interact socially, they must interpret human intentions and
anticipate their potential outcomes accurately. This is particularly important
for social robots designed for human care, which may face potentially dangerous
situations for people, such as unseen obstacles in their way, that should be
avoided. This paper explores the Artificial Theory of Mind (ATM) approach to
inferring and interpreting human intentions. We propose an algorithm that
detects risky situations for humans, selecting a robot action that removes the
danger in real time. We use the simulation-based approach to ATM and adopt the
'like-me' policy to assign intentions and actions to people. Using this
strategy, the robot can detect and act with a high rate of success under
time-constrained situations. The algorithm has been implemented as part of an
existing robotics cognitive architecture and tested in simulation scenarios.
Three experiments have been conducted to test the implementation's robustness,
precision and real-time response, including a simulated scenario, a
human-in-the-loop hybrid configuration and a real-world scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures. Submitted to IROS2024. For associated mpeg file
  see https://youtu.be/87UEB8P97KY</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Full Attitude Intelligent Controller Design of a Heliquad under Complete
  Failure of an Actuator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.07529v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.07529v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eeshan Kulkarni, Suresh Sundaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we design a reliable Heliquad and develop an intelligent
controller to handle one actuators complete failure. Heliquad is a multi-copter
similar to Quadcopter, with four actuators diagonally symmetric from the
center. Each actuator has two control inputs; the first input changes the
propeller blades collective pitch (also called variable pitch), and the other
input changes the rotation speed. For reliable operation and high torque
characteristic requirement for yaw control, a cambered airfoil is used to
design propeller blades. A neural network-based control allocation is designed
to provide complete control authority even under a complete loss of one
actuator. Nonlinear quaternion based outer loop position control, with
proportional-derivative inner loop for attitude control and neural
network-based control allocation is used in controller design. The proposed
controller and Heliquad designs performance is evaluated using a
software-in-loop simulation to track the position reference command under
failure. The results clearly indicate that the Heliquad with an intelligent
controller provides necessary tracking performance even under a complete loss
of one actuator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, For video go to
  https://indianinstituteofscience-my.sharepoint.com/:v:/g/personal/eeshank_iisc_ac_in/EcMg2uTtE91AsHDejNkb6YMBNckaXGjeh_YMzDV6sAHZAQ?e=DrRqmN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Massive Interaction with Generalist Robotics: A Systematic
  <span class="highlight-title">Review</span> of XR-enabled Remote Human-Robot Interaction Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11384v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11384v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xian Wang, Luyao Shen, Lik-Hang Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rising interest of generalist robots seek to create robots with
versatility to handle multiple tasks in a variety of environments, and human
will interact with such robots through immersive interfaces. In the context of
human-robot interaction (HRI), this survey provides an exhaustive review of the
applications of extended reality (XR) technologies in the field of remote HRI.
We developed a systematic search strategy based on the PRISMA methodology. From
the initial 2,561 articles selected, 100 research papers that met our inclusion
criteria were included. We categorized and summarized the domain in detail,
delving into XR technologies, including augmented reality (AR), virtual reality
(VR), and mixed reality (MR), and their applications in facilitating intuitive
and effective remote control and interaction with robotic systems. The survey
highlights existing articles on the application of XR technologies, user
experience enhancement, and various interaction designs for XR in remote HRI,
providing insights into current trends and future directions. We also
identified potential gaps and opportunities for future research to improve
remote HRI systems through XR technology to guide and inform future XR and
robotics research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autonomous Hook-Based Grasping and Transportation with Quadcopters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02444v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02444v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Péter Antal, Tamás Péni, Roland Tóth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Payload grasping and transportation with quadcopters is an active research
area that has rapidly developed over the last decade. To grasp a payload
without human interaction, most state-of-the-art approaches apply robotic arms
that are attached to the quadcopter body. However, due to the large weight and
power consumption of these aerial manipulators, their agility and flight time
are limited. This paper proposes a motion control and planning method for
transportation with a lightweight, passive manipulator structure that consists
of a hook attached to a quadrotor using a 1 DoF revolute joint. To perform
payload grasping, transportation, and release, first, time-optimal reference
trajectories are designed through specific waypoints to ensure the fast and
reliable execution of the tasks. Then, a two-stage motion control approach is
developed based on a robust geometric controller for precise and reliable
reference tracking and a linear--quadratic payload regulator for rapid setpoint
stabilization of the payload swing. Furthermore, stability of the closed-loop
system is mathematically proven to give safety guarantee for its operation. The
proposed control architecture and design are evaluated in a high-fidelity
physical simulator, and also in real flight experiments, using a custom-made
quadrotor--hook manipulator platform.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robustness Evaluation of Localization Techniques for Autonomous Racing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07658v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07658v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Yi Lim, Edoardo Ghignone, Nicolas Baumann, Michele Magno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces SynPF, an MCL-based algorithm tailored for high-speed
racing environments. Benchmarked against Cartographer, a state-of-the-art
pose-graph SLAM algorithm, SynPF leverages synergies from previous
particle-filtering methods and synthesizes them for the high-performance racing
domain. Our extensive in-field evaluations reveal that while Cartographer
excels under nominal conditions, it struggles when subjected to wheel-slip, a
common phenomenon in a racing scenario due to varying grip levels and
aggressive driving behaviour. Conversely, SynPF demonstrates robustness in
these challenging conditions and a low-latency computation time of 1.25 ms on
on-board computers without a GPU. Using the F1TENTH platform, a 1:10 scaled
autonomous racing vehicle, this work not only highlights the vulnerabilities of
existing algorithms in high-speed scenarios, tested up until 7.6 m/s, but also
emphasizes the potential of SynPF as a viable alternative, especially in
deteriorating odometry conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Design, Automation and Test in Europe Conference 2024
  as an extended abstract</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OCC-VO: Dense <span class="highlight-title">Mapping</span> via 3D Occupancy-Based Visual <span class="highlight-title">Odometry</span> for
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11011v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11011v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Li, Yifan Duan, Xinran Zhang, Haiyi Liu, Jianmin Ji, Yanyong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Odometry (VO) plays a pivotal role in autonomous systems, with a
principal challenge being the lack of depth information in camera images. This
paper introduces OCC-VO, a novel framework that capitalizes on recent advances
in deep learning to transform 2D camera images into 3D semantic occupancy,
thereby circumventing the traditional need for concurrent estimation of ego
poses and landmark locations. Within this framework, we utilize the TPV-Former
to convert surround view cameras' images into 3D semantic occupancy. Addressing
the challenges presented by this transformation, we have specifically tailored
a pose estimation and mapping algorithm that incorporates Semantic Label
Filter, Dynamic Object Filter, and finally, utilizes Voxel PFilter for
maintaining a consistent global semantic map. Evaluations on the Occ3D-nuScenes
not only showcase a 20.6% improvement in Success Ratio and a 29.6% enhancement
in trajectory accuracy against ORB-SLAM3, but also emphasize our ability to
construct a comprehensive map. Our implementation is open-sourced and available
at: https://github.com/USTCLH/OCC-VO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motion Planning Diffusion: Learning and Planning of Robot Motions with
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.01557v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.01557v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joao Carvalho, An T. Le, Mark Baierl, Dorothea Koert, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning priors on trajectory distributions can help accelerate robot motion
planning optimization. Given previously successful plans, learning trajectory
generative models as priors for a new planning problem is highly desirable.
Prior works propose several ways on utilizing this prior to bootstrapping the
motion planning problem. Either sampling the prior for initializations or using
the prior distribution in a maximum-a-posterior formulation for trajectory
optimization. In this work, we propose learning diffusion models as priors. We
then can sample directly from the posterior trajectory distribution conditioned
on task goals, by leveraging the inverse denoising process of diffusion models.
Furthermore, diffusion has been recently shown to effectively encode data
multimodality in high-dimensional settings, which is particularly well-suited
for large trajectory dataset. To demonstrate our method efficacy, we compare
our proposed method - Motion Planning Diffusion - against several baselines in
simulated planar robot and 7-dof robot arm manipulator environments. To assess
the generalization capabilities of our method, we test it in environments with
previously unseen obstacles. Our experiments show that diffusion models are
strong priors to encode high-dimensional trajectory distributions of robot
motions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feeling Optimistic? Ambiguity Attitudes for Online Decision Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04225v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04225v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jared J. Beard, R. Michael Butts, Yu Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the complexity of many decision making problems, tree search
algorithms often have inadequate information to produce accurate transition
models. Robust methods, designed to make safe decisions when faced with these
uncertainties, often overlook the impact expressions of uncertainty have on how
the decision is made. This work introduces the Ambiguity Attitude Graph Search
(AAGS), advocating for more precise representation of ambiguities (uncertainty
from a set of plausible models) in decision making. Additionally, AAGS allows
users to adjust their ambiguity attitude (or preference), promoting exploration
and improving users' ability to control how an agent should respond when faced
with a set of valid alternatives. Simulation in a dynamic sailing environment
shows how highly stochastic environments can lead robust methods to fail.
Results further demonstrate how adjusting ambiguity attitudes better fulfills
objectives while mitigating this failure mode of robust approaches. Because
this approach is a generalization of the robust framework, these results
further demonstrate how algorithms focused on ambiguity have applicability
beyond safety-critical systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, 2 algorithms. Submitted to the 2024 IEEE/RSJ
  International Conference on Intelligent Robots and Systems in Abu Dhabi, UAE
  (Oct 14-18, 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Whole-Body Control for Legged Loco-Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghuan Liu, Zixuan Chen, Xuxin Cheng, Yandong Ji, Ruihan Yang, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of mobile manipulation using legged robots equipped with
an arm, namely legged loco-manipulation. The robot legs, while usually utilized
for mobility, offer an opportunity to amplify the manipulation capabilities by
conducting whole-body control. That is, the robot can control the legs and the
arm at the same time to extend its workspace. We propose a framework that can
conduct the whole-body control autonomously with visual observations. Our
approach, namely Visual Whole-Body Control(VBC), is composed of a low-level
policy using all degrees of freedom to track the end-effector manipulator
position and a high-level policy proposing the end-effector position based on
visual inputs. We train both levels of policies in simulation and perform
Sim2Real transfer for real robot deployment. We perform extensive experiments
and show significant improvements over baselines in picking up diverse objects
in different configurations (heights, locations, orientations) and
environments. Project page: https://wholebody-b1.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contribute equally. Project page:
  https://wholebody-b1.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Point Cloud to Mesh Reconstruction for Deformable Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02749v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02749v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elham Amin Mansour, Hehui Zheng, Robert K. Katzschmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The world around us is full of soft objects we perceive and deform with
dexterous hand movements. For a robotic hand to control soft objects, it has to
acquire online state feedback of the deforming object. While RGB-D cameras can
collect occluded point clouds at a rate of 30Hz, this does not represent a
continuously trackable object surface. Hence, in this work, we developed a
method that takes as input a template mesh which is the mesh of an object in
its non-deformed state and a deformed point cloud of the same object, and then
shapes the template mesh such that it matches the deformed point cloud. The
reconstruction of meshes from point clouds has long been studied in the field
of Computer graphics under 3D reconstruction and 4D reconstruction, however,
both lack the speed and generalizability needed for robotics applications. Our
model is designed using a point cloud auto-encoder and a Real-NVP architecture.
Our trained model can perform mesh reconstruction and tracking at a rate of
58Hz on a template mesh of 3000 vertices and a deformed point cloud of 5000
points and is generalizable to the deformations of six different object
categories which are assumed to be made of soft material in our experiments
(scissors, hammer, foam brick, cleanser bottle, orange, and dice). The object
meshes are taken from the YCB benchmark dataset. An instance of a downstream
application can be the control algorithm for a robotic hand that requires
online feedback from the state of the manipulated object which would allow
online grasp adaptation in a closed-loop manner. Furthermore, the tracking
capacity of our method can help in the system identification of deforming
objects in a marker-free approach. In future work, we will extend our trained
model to generalize beyond six object categories and additionally to real-world
deforming point clouds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages with appendix,16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Grasping with a Learned Meta-Controller 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08463v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08463v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinsen Jia, Jingxi Xu, Dinesh Jayaraman, Shuran Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grasping moving objects is a challenging task that requires multiple
submodules such as object pose predictor, arm motion planner, etc. Each
submodule operates under its own set of meta-parameters. For example, how far
the pose predictor should look into the future (i.e., look-ahead time) and the
maximum amount of time the motion planner can spend planning a motion (i.e.,
time budget). Many previous works assign fixed values to these parameters;
however, at different moments within a single episode of dynamic grasping, the
optimal values should vary depending on the current scene. In this work, we
propose a dynamic grasping pipeline with a meta-controller that controls the
look-ahead time and time budget dynamically. We learn the meta-controller
through reinforcement learning with a sparse reward. Our experiments show the
meta-controller improves the grasping success rate (up to 28% in the most
cluttered environment) and reduces grasping time, compared to the strongest
baseline. Our meta-controller learns to reason about the reachable workspace
and maintain the predicted pose within the reachable region. In addition, it
assigns a small but sufficient time budget for the motion planner. Our method
can handle different objects, trajectories, and obstacles. Despite being
trained only with 3-6 random cuboidal obstacles, our meta-controller
generalizes well to 7-9 obstacles and more realistic out-of-domain household
setups with unseen obstacle shapes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D <span class="highlight-title">SLAM</span> <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02126v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02126v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Keetha, Jay Karhade, Krishna Murthy Jatavallabhula, Gengshan Yang, Sebastian Scherer, Deva Ramanan, Jonathon Luiten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense simultaneous localization and mapping (SLAM) is crucial for robotics
and augmented reality applications. However, current methods are often hampered
by the non-volumetric or implicit way they represent a scene. This work
introduces SplaTAM, an approach that, for the first time, leverages explicit
volumetric representations, i.e., 3D Gaussians, to enable high-fidelity
reconstruction from a single unposed RGB-D camera, surpassing the capabilities
of existing methods. SplaTAM employs a simple online tracking and mapping
system tailored to the underlying Gaussian representation. It utilizes a
silhouette mask to elegantly capture the presence of scene density. This
combination enables several benefits over prior representations, including fast
rendering and dense optimization, quickly determining if areas have been
previously mapped, and structured map expansion by adding more Gaussians.
Extensive experiments show that SplaTAM achieves up to 2x superior performance
in camera pose estimation, map construction, and novel-view synthesis over
existing methods, paving the way for more immersive high-fidelity SLAM
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Website: https://spla-tam.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08344v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08344v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Wen, Wei Yang, Jan Kautz, Stan Birchfield
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present FoundationPose, a unified foundation model for 6D object pose
estimation and tracking, supporting both model-based and model-free setups. Our
approach can be instantly applied at test-time to a novel object without
fine-tuning, as long as its CAD model is given, or a small number of reference
images are captured. We bridge the gap between these two setups with a neural
implicit representation that allows for effective novel view synthesis, keeping
the downstream pose estimation modules invariant under the same unified
framework. Strong generalizability is achieved via large-scale synthetic
training, aided by a large language model (LLM), a novel transformer-based
architecture, and contrastive learning formulation. Extensive evaluation on
multiple public datasets involving challenging scenarios and objects indicate
our unified approach outperforms existing methods specialized for each task by
a large margin. In addition, it even achieves comparable results to
instance-level methods despite the reduced assumptions. Project page:
https://nvlabs.github.io/FoundationPose/
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Video Object Segmentation via Modulated Cross-Attention Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Shaker, Syed Talal Wasim, Martin Danelljan, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, transformer-based approaches have shown promising results for
semi-supervised video object segmentation. However, these approaches typically
struggle on long videos due to increased GPU memory demands, as they frequently
expand the memory bank every few frames. We propose a transformer-based
approach, named MAVOS, that introduces an optimized and dynamic long-term
modulated cross-attention (MCA) memory to model temporal smoothness without
requiring frequent memory expansion. The proposed MCA effectively encodes both
local and global features at various levels of granularity while efficiently
maintaining consistent speed regardless of the video length. Extensive
experiments on multiple benchmarks, LVOS, Long-Time Video, and DAVIS 2017,
demonstrate the effectiveness of our proposed contributions leading to
real-time inference and markedly reduced memory demands without any degradation
in segmentation accuracy on long videos. Compared to the best existing
transformer-based approach, our MAVOS increases the speed by 7.6x, while
significantly reducing the GPU memory by 87% with comparable segmentation
performance on short and long video datasets. Notably on the LVOS dataset, our
MAVOS achieves a J&F score of 63.3% while operating at 37 frames per second
(FPS) on a single V100 GPU. Our code and models will be publicly available at:
https://github.com/Amshaker/MAVOS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture
  Synthesis <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Hamza Mughal, Rishabh Dabral, Ikhsanul Habibie, Lucia Donatelli, Marc Habermann, Christian Theobalt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gestures play a key role in human communication. Recent methods for co-speech
gesture generation, while managing to generate beat-aligned motions, struggle
generating gestures that are semantically aligned with the utterance. Compared
to beat gestures that align naturally to the audio signal, semantically
coherent gestures require modeling the complex interactions between the
language and human motion, and can be controlled by focusing on certain words.
Therefore, we present ConvoFusion, a diffusion-based approach for multi-modal
gesture synthesis, which can not only generate gestures based on multi-modal
speech inputs, but can also facilitate controllability in gesture synthesis.
Our method proposes two guidance objectives that allow the users to modulate
the impact of different conditioning modalities (e.g. audio vs text) as well as
to choose certain words to be emphasized during gesturing. Our method is
versatile in that it can be trained either for generating monologue gestures or
even the conversational gestures. To further advance the research on
multi-party interactive gestures, the DnD Group Gesture dataset is released,
which contains 6 hours of gesture data showing 5 people interacting with one
another. We compare our method with several recent works and demonstrate
effectiveness of our method on a variety of tasks. We urge the reader to watch
our supplementary video at our website.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Project Page:
  https://vcai.mpi-inf.mpg.de/projects/ConvoFusion/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniVid: A Generative Framework for Universal Video Understanding <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17935v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17935v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junke Wang, Dongdong Chen, Chong Luo, Bo He, Lu Yuan, Zuxuan Wu, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The core of video understanding tasks, such as recognition, captioning, and
tracking, is to automatically detect objects or actions in a video and analyze
their temporal evolution. Despite sharing a common goal, different tasks often
rely on distinct model architectures and annotation formats. In contrast,
natural language processing benefits from a unified output space, i.e., text
sequences, which simplifies the training of powerful foundational language
models, such as GPT-3, with extensive training corpora. Inspired by this, we
seek to unify the output space of video understanding tasks by using languages
as labels and additionally introducing time and box tokens. In this way, a
variety of video tasks could be formulated as video-grounded token generation.
This enables us to address various types of video tasks, including
classification (such as action recognition), captioning (covering clip
captioning, video question answering, and dense video captioning), and
localization tasks (such as visual object tracking) within a fully shared
encoder-decoder architecture, following a generative framework. Through
comprehensive experiments, we demonstrate such a simple and straightforward
idea is quite effective and can achieve state-of-the-art or competitive results
on seven video benchmarks, providing a novel perspective for more universal
video understanding. Code is available at https://github.com/wangjk666/OmniVid.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AiOS: All-in-One-Stage Expressive Human Pose and Shape Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingping Sun, Yanjun Wang, Ailing Zeng, Wanqi Yin, Chen Wei, Wenjia Wang, Haiyi Mei, Chi Sing Leung, Ziwei Liu, Lei Yang, Zhongang Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expressive human pose and shape estimation (a.k.a. 3D whole-body mesh
recovery) involves the human body, hand, and expression estimation. Most
existing methods have tackled this task in a two-stage manner, first detecting
the human body part with an off-the-shelf detection model and inferring the
different human body parts individually. Despite the impressive results
achieved, these methods suffer from 1) loss of valuable contextual information
via cropping, 2) introducing distractions, and 3) lacking inter-association
among different persons and body parts, inevitably causing performance
degradation, especially for crowded scenes. To address these issues, we
introduce a novel all-in-one-stage framework, AiOS, for multiple expressive
human pose and shape recovery without an additional human detection step.
Specifically, our method is built upon DETR, which treats multi-person
whole-body mesh recovery task as a progressive set prediction problem with
various sequential detection. We devise the decoder tokens and extend them to
our task. Specifically, we first employ a human token to probe a human location
in the image and encode global features for each instance, which provides a
coarse location for the later transformer block. Then, we introduce a
joint-related token to probe the human joint in the image and encoder a
fine-grained local feature, which collaborates with the global feature to
regress the whole-body mesh. This straightforward but effective model
outperforms previous state-of-the-art methods by a 9% reduction in NMVE on
AGORA, a 30% reduction in PVE on EHF, a 10% reduction in PVE on ARCTIC, and a
3% reduction in PVE on EgoBody.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Homepage: https://ttxskk.github.io/AiOS/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SLEDGE: Synthesizing Simulation Environments for Driving Agents with
  Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kashyap Chitta, Daniel Dauner, Andreas Geiger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SLEDGE is the first generative simulator for vehicle motion planning trained
on real-world driving logs. Its core component is a learned model that is able
to generate agent bounding boxes and lane graphs. The model's outputs serve as
an initial state for traffic simulation. The unique properties of the entities
to be generated for SLEDGE, such as their connectivity and variable count per
scene, render the naive application of most modern generative models to this
task non-trivial. Therefore, together with a systematic study of existing lane
graph representations, we introduce a novel raster-to-vector autoencoder
(RVAE). It encodes agents and the lane graph into distinct channels in a
rasterized latent map. This facilitates both lane-conditioned agent generation
and combined generation of lanes and agents with a Diffusion Transformer. Using
generated entities in SLEDGE enables greater control over the simulation, e.g.
upsampling turns or increasing traffic density. Further, SLEDGE can support
500m long routes, a capability not found in existing data-driven simulators
like nuPlan. It presents new challenges for planning algorithms, evidenced by
failure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,
when tested on hard routes and dense traffic generated by our model. Compared
to nuPlan, SLEDGE requires 500$\times$ less storage to set up (<4GB), making it
a more accessible option and helping with democratizing future research in this
field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Track Everything Everywhere Fast and Robustly 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunzhou Song, Jiahui Lei, Ziyun Wang, Lingjie Liu, Kostas Daniilidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel test-time optimization approach for efficiently and
robustly tracking any pixel at any time in a video. The latest state-of-the-art
optimization-based tracking technique, OmniMotion, requires a prohibitively
long optimization time, rendering it impractical for downstream applications.
OmniMotion is sensitive to the choice of random seeds, leading to unstable
convergence. To improve efficiency and robustness, we introduce a novel
invertible deformation network, CaDeX++, which factorizes the function
representation into a local spatial-temporal feature grid and enhances the
expressivity of the coupling blocks with non-linear functions. While CaDeX++
incorporates a stronger geometric bias within its architectural design, it also
takes advantage of the inductive bias provided by the vision foundation models.
Our system utilizes monocular depth estimation to represent scene geometry and
enhances the objective by incorporating DINOv2 long-term semantics to regulate
the optimization process. Our experiments demonstrate a substantial improvement
in training speed (more than \textbf{10 times} faster), robustness, and
accuracy in tracking over the SoTA optimization-based method OmniMotion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://timsong412.github.io/FastOmniTrack/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Explaining Hypercomplex Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eleonora Lopez, Eleonora Grassucci, Debora Capriotti, Danilo Comminiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hypercomplex neural networks are gaining increasing interest in the deep
learning community. The attention directed towards hypercomplex models
originates from several aspects, spanning from purely theoretical and
mathematical characteristics to the practical advantage of lightweight models
over conventional networks, and their unique properties to capture both global
and local relations. In particular, a branch of these architectures,
parameterized hypercomplex neural networks (PHNNs), has also gained popularity
due to their versatility across a multitude of application domains.
Nonetheless, only few attempts have been made to explain or interpret their
intricacies. In this paper, we propose inherently interpretable PHNNs and
quaternion-like networks, thus without the need for any post-hoc method. To
achieve this, we define a type of cosine-similarity transform within the
parameterized hypercomplex domain. This PHB-cos transform induces weight
alignment with relevant input features and allows to reduce the model into a
single linear transform, rendering it directly interpretable. In this work, we
start to draw insights into how this unique branch of neural models operates.
We observe that hypercomplex networks exhibit a tendency to concentrate on the
shape around the main object of interest, in addition to the shape of the
object itself. We provide a thorough analysis, studying single neurons of
different layers and comparing them against how real-valued networks learn. The
code of the paper is available at https://github.com/ispamm/HxAI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted at IEEE WCCI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastCAR: Fast Classification And Regression Multi-Task Learning via Task
  Consolidation for Modelling a Continuous Property Variable of Object Classes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anoop Kini, Andreas Jansche, Timo Bernthaler, Gerhard Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  FastCAR is a novel task consolidation approach in Multi-Task Learning (MTL)
for a classification and a regression task, despite task heterogeneity with
only subtle correlation. It addresses object classification and continuous
property variable regression, a crucial use case in science and engineering.
FastCAR involves a labeling transformation approach that can be used with a
single-task regression network architecture. FastCAR outperforms traditional
MTL model families, parametrized in the landscape of architecture and loss
weighting schemes, when learning of both tasks are collectively considered
(classification accuracy of 99.54%, regression mean absolute percentage error
of 2.3%). The experiments performed used an Advanced Steel Property dataset
contributed by us. The dataset comprises 4536 images of 224x224 pixels,
annotated with object classes and hardness properties that take continuous
values. With the labeling transformation and single-task regression network
architecture, FastCAR achieves reduced latency and time efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AID: Attention Interpolation of Text-to-Image Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyuan He, Jinghao Wang, Ziwei Liu, Angela Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conditional diffusion models can create unseen images in various settings,
aiding image interpolation. Interpolation in latent spaces is well-studied, but
interpolation with specific conditions like text or poses is less understood.
Simple approaches, such as linear interpolation in the space of conditions,
often result in images that lack consistency, smoothness, and fidelity. To that
end, we introduce a novel training-free technique named Attention Interpolation
via Diffusion (AID). Our key contributions include 1) proposing an inner/outer
interpolated attention layer; 2) fusing the interpolated attention with
self-attention to boost fidelity; and 3) applying beta distribution to
selection to increase smoothness. We also present a variant, Prompt-guided
Attention Interpolation via Diffusion (PAID), that considers interpolation as a
condition-dependent generative process. This method enables the creation of new
images with greater consistency, smoothness, and efficiency, and offers control
over the exact path of interpolation. Our approach demonstrates effectiveness
for conceptual and spatial interpolation. Code and demo are available at
https://github.com/QY-H00/attention-interpolation-diffusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TC4D: Trajectory-Conditioned Text-to-4D Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sherwin Bahmani, Xian Liu, Yifan Wang, Ivan Skorokhodov, Victor Rong, Ziwei Liu, Xihui Liu, Jeong Joon Park, Sergey Tulyakov, Gordon Wetzstein, Andrea Tagliasacchi, David B. Lindell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent techniques for text-to-4D generation synthesize dynamic 3D scenes
using supervision from pre-trained text-to-video models. However, existing
representations for motion, such as deformation models or time-dependent neural
representations, are limited in the amount of motion they can generate-they
cannot synthesize motion extending far beyond the bounding box used for volume
rendering. The lack of a more flexible motion model contributes to the gap in
realism between 4D generation methods and recent, near-photorealistic video
generation models. Here, we propose TC4D: trajectory-conditioned text-to-4D
generation, which factors motion into global and local components. We represent
the global motion of a scene's bounding box using rigid transformation along a
trajectory parameterized by a spline. We learn local deformations that conform
to the global trajectory using supervision from a text-to-video model. Our
approach enables the synthesis of scenes animated along arbitrary trajectories,
compositional scene generation, and significant improvements to the realism and
amount of generated motion, which we evaluate qualitatively and through a user
study. Video results can be viewed on our website:
https://sherwinbahmani.github.io/tc4d.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://sherwinbahmani.github.io/tc4d</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CMP: Cooperative Motion Prediction with Multi-Agent Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyuan Wu, Yuping Wang, Hengbo Ma, Zhaowei Li, Hang Qiu, Jiachen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The confluence of the advancement of Autonomous Vehicles (AVs) and the
maturity of Vehicle-to-Everything (V2X) communication has enabled the
capability of cooperative connected and automated vehicles (CAVs). Building on
top of cooperative perception, this paper explores the feasibility and
effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR
signals as input to enhance tracking and prediction capabilities. Unlike
previous work that focuses separately on either cooperative perception or
motion prediction, our framework, to the best of our knowledge, is the first to
address the unified problem where CAVs share information in both perception and
prediction modules. Incorporated into our design is the unique capability to
tolerate realistic V2X bandwidth limitations and transmission delays, while
dealing with bulky perception representations. We also propose a prediction
aggregation module, which unifies the predictions obtained by different CAVs
and generates the final prediction. Through extensive experiments and ablation
studies, we demonstrate the effectiveness of our method in cooperative
perception, tracking, and motion prediction tasks. In particular, CMP reduces
the average prediction error by 17.2\% with fewer missing detections compared
with the no cooperation setting. Our work marks a significant step forward in
the cooperative capabilities of CAVs, showcasing enhanced performance in
complex scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Near-Field Lighting for Monocular Depth Estimation from
  Endoscopy Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshay Paruchuri, Samuel Ehrenstein, Shuxian Wang, Inbar Fried, Stephen M. Pizer, Marc Niethammer, Roni Sengupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular depth estimation in endoscopy videos can enable assistive and
robotic surgery to obtain better coverage of the organ and detection of various
health issues. Despite promising progress on mainstream, natural image depth
estimation, techniques perform poorly on endoscopy images due to a lack of
strong geometric features and challenging illumination effects. In this paper,
we utilize the photometric cues, i.e., the light emitted from an endoscope and
reflected by the surface, to improve monocular depth estimation. We first
create two novel loss functions with supervised and self-supervised variants
that utilize a per-pixel shading representation. We then propose a novel depth
refinement network (PPSNet) that leverages the same per-pixel shading
representation. Finally, we introduce teacher-student transfer learning to
produce better depth maps from both synthetic data with supervision and
clinical data with self-supervision. We achieve state-of-the-art results on the
C3VD dataset while estimating high-quality depth maps from clinical data. Our
code, pre-trained models, and supplementary materials can be found on our
project page: https://ppsnet.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 7 tables, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELGC-Net: Efficient Local-Global Context Aggregation for Remote Sensing
  Change Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mubashir Noman, Mustansar Fiaz, Hisham Cholakkal, Salman Khan, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has shown remarkable success in remote sensing change detection
(CD), aiming to identify semantic change regions between co-registered
satellite image pairs acquired at distinct time stamps. However, existing
convolutional neural network and transformer-based frameworks often struggle to
accurately segment semantic change regions. Moreover, transformers-based
methods with standard self-attention suffer from quadratic computational
complexity with respect to the image resolution, making them less practical for
CD tasks with limited training data. To address these issues, we propose an
efficient change detection framework, ELGC-Net, which leverages rich contextual
information to precisely estimate change regions while reducing the model size.
Our ELGC-Net comprises a Siamese encoder, fusion modules, and a decoder. The
focus of our design is the introduction of an Efficient Local-Global Context
Aggregator module within the encoder, capturing enhanced global context and
local spatial information through a novel pooled-transpose (PT) attention and
depthwise convolution, respectively. The PT attention employs pooling
operations for robust feature extraction and minimizes computational cost with
transposed attention. Extensive experiments on three challenging CD datasets
demonstrate that ELGC-Net outperforms existing methods. Compared to the recent
transformer-based CD approach (ChangeFormer), ELGC-Net achieves a 1.4% gain in
intersection over union metric on the LEVIR-CD dataset, while significantly
reducing trainable parameters. Our proposed ELGC-Net sets a new
state-of-the-art performance in remote sensing change detection benchmarks.
Finally, we also introduce ELGC-Net-LW, a lighter variant with significantly
reduced computational complexity, suitable for resource-constrained settings,
while achieving comparable performance. Project url
https://github.com/techmn/elgcnet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at IEEE TGRS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Yiwei, Tang Chao, Aghabiglou Amir, Chu Chung San, Wiaux Yves
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new approach for non-Cartesian magnetic resonance image
reconstruction. While unrolled architectures provide robustness via
data-consistency layers, embedding measurement operators in Deep Neural Network
(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)
approaches, where the denoising DNNs are blind to the measurement setting, are
not affected by this limitation and have also proven effective, but their
highly iterative nature also affects scalability. To address this scalability
challenge, we leverage the "Residual-to-Residual DNN series for high-Dynamic
range imaging (R2D2)" approach recently introduced in astronomical imaging.
R2D2's reconstruction is formed as a series of residual images, iteratively
estimated as outputs of DNNs taking the previous iteration's image estimate and
associated data residual as inputs. The method can be interpreted as a learned
version of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,
considering radial k-space sampling acquisition sequences. Our preliminary
results suggest that R2D2 achieves: (i) suboptimal performance compared to its
unrolled incarnation R2D2-Net, which is however non-scalable due to the
necessary embedding of NUFFT-based data-consistency layers; (ii) superior
reconstruction quality to a scalable version of R2D2-Net embedding an FFT-based
approximation for data consistency; (iii) superior reconstruction quality to
PnP, while only requiring few iterations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Serpent: Scalable and Efficient Image Restoration via Multi-scale
  Structured State Space Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Shahab Sepehri, Zalan Fabian, Mahdi Soltanolkotabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The landscape of computational building blocks of efficient image restoration
architectures is dominated by a combination of convolutional processing and
various attention mechanisms. However, convolutional filters are inherently
local and therefore struggle at modeling long-range dependencies in images. On
the other hand, attention excels at capturing global interactions between
arbitrary image regions, however at a quadratic cost in image dimension. In
this work, we propose Serpent, an architecture that leverages recent advances
in state space models (SSMs) in its core computational block. SSMs, originally
introduced for sequence modeling, can maintain a global receptive field with a
favorable linear scaling in input size. Our preliminary results demonstrate
that Serpent can achieve reconstruction quality on par with state-of-the-art
techniques, while requiring orders of magnitude less compute (up to $150$ fold
reduction in FLOPS) and a factor of up to $5\times$ less GPU memory while
maintaining a compact model size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, preliminary workshop submission of a
  comprehensive work to be released soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D
  Gaussians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kerui Ren, Lihan Jiang, Tao Lu, Mulin Yu, Linning Xu, Zhangkai Ni, Bo Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering
fidelity and efficiency compared to NeRF-based neural scene representations.
While demonstrating the potential for real-time rendering, 3D-GS encounters
rendering bottlenecks in large scenes with complex details due to an excessive
number of Gaussian primitives located within the viewing frustum. This
limitation is particularly noticeable in zoom-out views and can lead to
inconsistent rendering speeds in scenes with varying details. Moreover, it
often struggles to capture the corresponding level of details at different
scales with its heuristic density control operation. Inspired by the
Level-of-Detail (LOD) techniques, we introduce Octree-GS, featuring an
LOD-structured 3D Gaussian approach supporting level-of-detail decomposition
for scene representation that contributes to the final rendering results. Our
model dynamically selects the appropriate level from the set of
multi-resolution anchor points, ensuring consistent rendering performance with
adaptive LOD adjustments while maintaining high-fidelity rendering results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://city-super.github.io/octree-gs/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on 3D Egocentric Human Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Mushfiqur Azam, Kevin Desai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Egocentric human pose estimation aims to estimate human body poses and
develop body representations from a first-person camera perspective. It has
gained vast popularity in recent years because of its wide range of
applications in sectors like XR-technologies, human-computer interaction, and
fitness tracking. However, to the best of our knowledge, there is no systematic
literature review based on the proposed solutions regarding egocentric 3D human
pose estimation. To that end, the aim of this survey paper is to provide an
extensive overview of the current state of egocentric pose estimation research.
In this paper, we categorize and discuss the popular datasets and the different
pose estimation models, highlighting the strengths and weaknesses of different
methods by comparative analysis. This survey can be a valuable resource for
both researchers and practitioners in the field, offering insights into key
concepts and cutting-edge solutions in egocentric pose estimation, its
wide-ranging applications, as well as the open problems with future scope.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, Shenghua Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has recently revolutionized radiance field
reconstruction, achieving high quality novel view synthesis and fast rendering
speed without baking. However, 3DGS fails to accurately represent surfaces due
to the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian
Splatting (2DGS), a novel approach to model and reconstruct geometrically
accurate radiance fields from multi-view images. Our key idea is to collapse
the 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D
Gaussians, 2D Gaussians provide view-consistent geometry while modeling
surfaces intrinsically. To accurately recover thin surfaces and achieve stable
optimization, we introduce a perspective-accurate 2D splatting process
utilizing ray-splat intersection and rasterization. Additionally, we
incorporate depth distortion and normal consistency terms to further enhance
the quality of the reconstructions. We demonstrate that our differentiable
renderer allows for noise-free and detailed geometry reconstruction while
maintaining competitive appearance quality, fast training speed, and real-time
rendering. Our code will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sen2Fire: A Challenging Benchmark <span class="highlight-title">Dataset</span> for Wildfire Detection using
  Sentinel Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghao Xu, Amanda Berg, Leif Haglund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing satellite imagery for wildfire detection presents substantial
potential for practical applications. To advance the development of machine
learning algorithms in this domain, our study introduces the \textit{Sen2Fire}
dataset--a challenging satellite remote sensing dataset tailored for wildfire
detection. This dataset is curated from Sentinel-2 multi-spectral data and
Sentinel-5P aerosol product, comprising a total of 2466 image patches. Each
patch has a size of 512$\times$512 pixels with 13 bands. Given the distinctive
sensitivities of various wavebands to wildfire responses, our research focuses
on optimizing wildfire detection by evaluating different wavebands and
employing a combination of spectral indices, such as normalized burn ratio
(NBR) and normalized difference vegetation index (NDVI). The results suggest
that, in contrast to using all bands for wildfire detection, selecting specific
band combinations yields superior performance. Additionally, our study
underscores the positive impact of integrating Sentinel-5 aerosol data for
wildfire detection. The code and dataset are available online
(https://zenodo.org/records/10881058).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Superior and Pragmatic Talking Face Generation with Teacher-Student
  Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Liang, Jianwen Jiang, Tianyun Zhong, Gaojie Lin, Zhengkun Rong, Jiaqi Yang, Yongming Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Talking face generation technology creates talking videos from arbitrary
appearance and motion signal, with the "arbitrary" offering ease of use but
also introducing challenges in practical applications. Existing methods work
well with standard inputs but suffer serious performance degradation with
intricate real-world ones. Moreover, efficiency is also an important concern in
deployment. To comprehensively address these issues, we introduce SuperFace, a
teacher-student framework that balances quality, robustness, cost and
editability. We first propose a simple but effective teacher model capable of
handling inputs of varying qualities to generate high-quality results. Building
on this, we devise an efficient distillation strategy to acquire an
identity-specific student model that maintains quality with significantly
reduced computational load. Our experiments validate that SuperFace offers a
more comprehensive solution than existing methods for the four mentioned
objectives, especially in reducing FLOPs by 99\% with the student model.
SuperFace can be driven by both video and audio and allows for localized facial
attributes editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deepfake Generation and Detection: A Benchmark and <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gan Pei, Jiangning Zhang, Menghan Hu, Guangtao Zhai, Chengjie Wang, Zhenyu Zhang, Jian Yang, Chunhua Shen, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In addition to the advancements in deepfake generation, corresponding
detection technologies need to continuously evolve to regulate the potential
misuse of deepfakes, such as for privacy invasion and phishing attacks. This
survey comprehensively reviews the latest developments in deepfake generation
and detection, summarizing and analyzing the current state of the art in this
rapidly evolving field. We first unify task definitions, comprehensively
introduce datasets and metrics, and discuss the development of generation and
detection technology frameworks. Then, we discuss the development of several
related sub-fields and focus on researching four mainstream deepfake fields:
popular face swap, face reenactment, talking face generation, and facial
attribute editing, as well as foreign detection. Subsequently, we
comprehensively benchmark representative methods on popular datasets for each
field, fully evaluating the latest and influential works published in top
conferences/journals. Finally, we analyze the challenges and future research
directions of the discussed fields. We closely follow the latest developments
in https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Latency Neural Stereo Streaming <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiqi Hou, Farzad Farhadzadeh, Amir Said, Guillaume Sautiere, Hoang Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of new video modalities like virtual reality or autonomous driving
has increased the demand for efficient multi-view video compression methods,
both in terms of rate-distortion (R-D) performance and in terms of delay and
runtime. While most recent stereo video compression approaches have shown
promising performance, they compress left and right views sequentially, leading
to poor parallelization and runtime performance. This work presents Low-Latency
neural codec for Stereo video Streaming (LLSS), a novel parallel stereo video
coding method designed for fast and efficient low-latency stereo video
streaming. Instead of using a sequential cross-view motion compensation like
existing methods, LLSS introduces a bidirectional feature shifting module to
directly exploit mutual information among views and encode them effectively
with a joint cross-view prior model for entropy coding. Thanks to this design,
LLSS processes left and right views in parallel, minimizing latency; all while
substantially improving R-D performance compared to both existing neural and
conventional codecs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Diffusion Models with Moving Average Sampling in Frequency
  Domain <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yurui Qian, Qi Cai, Yingwei Pan, Yehao Li, Ting Yao, Qibin Sun, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently brought a powerful revolution in image
generation. Despite showing impressive generative capabilities, most of these
models rely on the current sample to denoise the next one, possibly resulting
in denoising instability. In this paper, we reinterpret the iterative denoising
process as model optimization and leverage a moving average mechanism to
ensemble all the prior samples. Instead of simply applying moving average to
the denoised samples at different timesteps, we first map the denoised samples
to data space and then perform moving average to avoid distribution shift
across timesteps. In view that diffusion models evolve the recovery from
low-frequency components to high-frequency details, we further decompose the
samples into different frequency components and execute moving average
separately on each component. We name the complete approach "Moving Average
Sampling in Frequency domain (MASF)". MASF could be seamlessly integrated into
mainstream pre-trained diffusion models and sampling schedules. Extensive
experiments on both unconditional and conditional diffusion models demonstrate
that our MASF leads to superior performances compared to the baselines, with
almost negligible additional complexity cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ To Supervise or Not to Supervise: Understanding and Addressing the Key
  Challenges of 3D Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Souhail Hadgi, Lei Li, Maks Ovsjanikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transfer learning has long been a key factor in the advancement of many
fields including 2D image analysis. Unfortunately, its applicability in 3D data
processing has been relatively limited. While several approaches for 3D
transfer learning have been proposed in recent literature, with contrastive
learning gaining particular prominence, most existing methods in this domain
have only been studied and evaluated in limited scenarios. Most importantly,
there is currently a lack of principled understanding of both when and why 3D
transfer learning methods are applicable. Remarkably, even the applicability of
standard supervised pre-training is poorly understood. In this work, we conduct
the first in-depth quantitative and qualitative investigation of supervised and
contrastive pre-training strategies and their utility in downstream 3D tasks.
We demonstrate that layer-wise analysis of learned features provides
significant insight into the downstream utility of trained networks. Informed
by this analysis, we propose a simple geometric regularization strategy, which
improves the transferability of supervised pre-training. Our work thus sheds
light onto both the specific challenges of 3D transfer learning, as well as
strategies to overcome them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrhman Werby, Chenguang Huang, Martin Büchner, Abhinav Valada, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent open-vocabulary robot mapping methods enrich dense geometric maps with
pre-trained visual-language features. While these maps allow for the prediction
of point-wise saliency maps when queried for a certain language concept,
large-scale environments and abstract queries beyond the object level still
pose a considerable hurdle, ultimately limiting language-grounded robotic
navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D
scene graph mapping approach for language-grounded robot navigation. Leveraging
open-vocabulary vision foundation models, we first obtain state-of-the-art
open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene
graph hierarchy consisting of floor, room, and object concepts, each enriched
with open-vocabulary features. Our approach is able to represent multi-story
buildings and allows robotic traversal of those using a cross-floor Voronoi
graph. HOV-SG is evaluated on three distinct datasets and surpasses previous
baselines in open-vocabulary semantic accuracy on the object, room, and floor
level while producing a 75% reduction in representation size compared to dense
open-vocabulary maps. In order to prove the efficacy and generalization
capabilities of HOV-SG, we showcase successful long-horizon
language-conditioned robot navigation within real-world multi-storage
environments. We provide code and trial video data at http://hovsg.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and video are available at http://hovsg.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReMamber: Referring Image Segmentation with Mamba Twister 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhuan Yang, Chaofan Ma, Jiangchao Yao, Zhun Zhong, Ya Zhang, Yanfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring Image Segmentation (RIS) leveraging transformers has achieved great
success on the interpretation of complex visual-language tasks. However, the
quadratic computation cost makes it resource-consuming in capturing long-range
visual-language dependencies. Fortunately, Mamba addresses this with efficient
linear complexity in processing. However, directly applying Mamba to
multi-modal interactions presents challenges, primarily due to inadequate
channel interactions for the effective fusion of multi-modal data. In this
paper, we propose ReMamber, a novel RIS architecture that integrates the power
of Mamba with a multi-modal Mamba Twister block. The Mamba Twister explicitly
models image-text interaction, and fuses textual and visual features through
its unique channel and spatial twisting mechanism. We achieve the
state-of-the-art on three challenging benchmarks. Moreover, we conduct thorough
analyses of ReMamber and discuss other fusion designs using Mamba. These
provide valuable perspectives for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GTA-HDR: A Large-Scale Synthetic <span class="highlight-title">Dataset</span> for HDR Image Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hrishav Bakul Barua, Kalin Stefanov, KokSheik Wong, Abhinav Dhall, Ganesh Krishnasamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range
of applications. However, capturing HDR content from real-world scenes is
expensive and time- consuming. Therefore, the challenging task of
reconstructing visually accurate HDR images from their Low Dynamic Range (LDR)
counterparts is gaining attention in the vision research community. A major
challenge in this research problem is the lack of datasets, which capture
diverse scene conditions (e.g., lighting, shadows, weather, locations,
landscapes, objects, humans, buildings) and various image features (e.g.,
color, contrast, saturation, hue, luminance, brightness, radiance). To address
this gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset
of photo-realistic HDR images sampled from the GTA-V video game. We perform
thorough evaluation of the proposed dataset, which demonstrates significant
qualitative and quantitative improvements of the state-of-the-art HDR image
reconstruction methods. Furthermore, we demonstrate the effectiveness of the
proposed dataset and its impact on additional computer vision tasks including
3D human pose estimation, human body part segmentation, and holistic scene
segmentation. The dataset, data collection pipeline, and evaluation code are
available at: https://github.com/HrishavBakulBarua/GTA-HDR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A foundation model utilizing chest CT volumes and radiology reports for
  supervised-level zero-shot detection of abnormalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ibrahim Ethem Hamamci, Sezgin Er, Furkan Almas, Ayse Gulnihan Simsek, Sevval Nil Esirgun, Irem Dogan, Muhammed Furkan Dasdelen, Bastian Wittmann, Enis Simsar, Mehmet Simsar, Emine Bensu Erdemir, Abdullah Alanbay, Anjany Sekuboyina, Berkan Lafci, Mehmet K. Ozdemir, Bjoern Menze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major challenge in computational research in 3D medical imaging is the lack
of comprehensive datasets. Addressing this issue, our study introduces CT-RATE,
the first 3D medical imaging dataset that pairs images with textual reports.
CT-RATE consists of 25,692 non-contrast chest CT volumes, expanded to 50,188
through various reconstructions, from 21,304 unique patients, along with
corresponding radiology text reports. Leveraging CT-RATE, we developed CT-CLIP,
a CT-focused contrastive language-image pre-training framework. As a versatile,
self-supervised model, CT-CLIP is designed for broad application and does not
require task-specific training. Remarkably, CT-CLIP outperforms
state-of-the-art, fully supervised methods in multi-abnormality detection
across all key metrics, thus eliminating the need for manual annotation. We
also demonstrate its utility in case retrieval, whether using imagery or
textual queries, thereby advancing knowledge dissemination. The open-source
release of CT-RATE and CT-CLIP marks a significant advancement in medical AI,
enhancing 3D imaging analysis and fostering innovation in healthcare.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessment of Multimodal Large Language Models in Alignment with Human
  Values 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhelun Shi, Zhipin Wang, Hongxing Fan, Zaibin Zhang, Lijun Li, Yongting Zhang, Zhenfei Yin, Lu Sheng, Yu Qiao, Jing Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) aim to serve as versatile assistants aligned
with human values, as defined by the principles of being helpful, honest, and
harmless (hhh). However, in terms of Multimodal Large Language Models (MLLMs),
despite their commendable performance in perception and reasoning tasks, their
alignment with human values remains largely unexplored, given the complexity of
defining hhh dimensions in the visual world and the difficulty in collecting
relevant data that accurately mirrors real-world situations. To address this
gap, we introduce Ch3Ef, a Compreh3ensive Evaluation dataset and strategy for
assessing alignment with human expectations. Ch3Ef dataset contains 1002
human-annotated data samples, covering 12 domains and 46 tasks based on the hhh
principle. We also present a unified evaluation strategy supporting assessment
across various scenarios and different perspectives. Based on the evaluation
results, we summarize over 10 key findings that deepen the understanding of
MLLM capabilities, limitations, and the dynamic relationships between
evaluation levels, guiding future advancements in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2311.02692</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from
  Textual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sammy Christen, Shreyas Hampali, Fadime Sener, Edoardo Remelli, Tomas Hodan, Eric Sauser, Shugao Ma, Bugra Tekin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating natural hand-object interactions in 3D is challenging as the
resulting hand and object motions are expected to be physically plausible and
semantically meaningful. Furthermore, generalization to unseen objects is
hindered by the limited scale of available hand-object interaction datasets. We
propose DiffH2O, a novel method to synthesize realistic, one or two-handed
object interactions from provided text prompts and geometry of the object. The
method introduces three techniques that enable effective learning from limited
data. First, we decompose the task into a grasping stage and a text-based
interaction stage and use separate diffusion models for each. In the grasping
stage, the model only generates hand motions, whereas in the interaction phase
both hand and object poses are synthesized. Second, we propose a compact
representation that tightly couples hand and object poses. Third, we propose
two different guidance schemes to allow more control of the generated motions:
grasp guidance and detailed textual guidance. Grasp guidance takes a single
target grasping pose and guides the diffusion model to reach this grasp at the
end of the grasping stage, which provides control over the grasping pose. Given
a grasping motion from this stage, multiple different actions can be prompted
in the interaction phase. For textual guidance, we contribute comprehensive
text descriptions to the GRAB dataset and show that they enable our method to
have more fine-grained control over hand-object interactions. Our quantitative
and qualitative evaluation demonstrates that the proposed method outperforms
baseline methods and leads to natural hand-object motions. Moreover, we
demonstrate the practicality of our framework by utilizing a hand pose estimate
from an off-the-shelf pose estimator for guidance, and then sampling multiple
different actions in the interaction stage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://diffh2o.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Image Pre-Training with Siamese Cropped Masked Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Eymaël, Renaud Vandeghen, Anthony Cioppa, Silvio Giancola, Bernard Ghanem, Marc Van Droogenbroeck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised pre-training of image encoders is omnipresent in the
literature, particularly following the introduction of Masked autoencoders
(MAE). Current efforts attempt to learn object-centric representations from
motion in videos. In particular, SiamMAE recently introduced a Siamese network,
training a shared-weight encoder from two frames of a video with a high
asymmetric masking ratio (95%). In this work, we propose CropMAE, an
alternative approach to the Siamese pre-training introduced by SiamMAE. Our
method specifically differs by exclusively considering pairs of cropped images
sourced from the same image but cropped differently, deviating from the
conventional pairs of frames extracted from a video. CropMAE therefore
alleviates the need for video datasets, while maintaining competitive
performances and drastically reducing pre-training time. Furthermore, we
demonstrate that CropMAE learns similar object-centric representations without
explicit motion, showing that current self-supervised learning methods do not
learn objects from motion, but rather thanks to the Siamese architecture.
Finally, CropMAE achieves the highest masking ratio to date (98.5%), enabling
the reconstruction of images using only two visible patches. Our code is
available at https://github.com/alexandre-eymael/CropMAE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures, 3 tables, 1 page of supplementary material</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matias Turkulainen, Xuqian Ren, Iaroslav Melekhov, Otto Seiskari, Esa Rahtu, Juho Kannala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian splatting, a novel differentiable rendering technique, has
achieved state-of-the-art novel view synthesis results with high rendering
speeds and relatively low training times. However, its performance on scenes
commonly seen in indoor datasets is poor due to the lack of geometric
constraints during optimization. We extend 3D Gaussian splatting with depth and
normal cues to tackle challenging indoor datasets and showcase techniques for
efficient mesh extraction, an important downstream application. Specifically,
we regularize the optimization procedure with depth information, enforce local
smoothness of nearby Gaussians, and use the geometry of the 3D Gaussians
supervised by normal cues to achieve better alignment with the true scene
geometry. We improve depth estimation and novel view synthesis results over
baselines and show how this simple yet effective regularization technique can
be used to directly extract meshes from the Gaussian representation yielding
more physically accurate reconstructions on indoor scenes. Our code will be
released in https://github.com/maturk/dn-splatter.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Annotated Biomedical Video Generation using Denoising Diffusion
  Probabilistic Models and Flow Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rüveyda Yilmaz, Dennis Eschweiler, Johannes Stegmaier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The segmentation and tracking of living cells play a vital role within the
biomedical domain, particularly in cancer research, drug development, and
developmental biology. These are usually tedious and time-consuming tasks that
are traditionally done by biomedical experts. Recently, to automatize these
processes, deep learning based segmentation and tracking methods have been
proposed. These methods require large-scale datasets and their full potential
is constrained by the scarcity of annotated data in the biomedical imaging
domain. To address this limitation, we propose Biomedical Video Diffusion Model
(BVDM), capable of generating realistic-looking synthetic microscopy videos.
Trained only on a single real video, BVDM can generate videos of arbitrary
length with pixel-level annotations that can be used for training data-hungry
models. It is composed of a denoising diffusion probabilistic model (DDPM)
generating high-fidelity synthetic cell microscopy images and a flow prediction
model (FPM) predicting the non-rigid transformation between consecutive video
frames. During inference, initially, the DDPM imposes realistic cell textures
on synthetic cell masks which are generated based on real data statistics. The
flow prediction model predicts the flow field between consecutive masks and
applies that to the DDPM output from the previous time frame to create the next
one while keeping temporal consistency. BVDM outperforms state-of-the-art
synthetic live cell microscopy video generation models. Furthermore, we
demonstrate that a sufficiently large synthetic dataset enhances the
performance of cell segmentation and tracking models compared to using a
limited amount of available real data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Text-to-Image Consistency via Automatic Prompt Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oscar Mañas, Pietro Astolfi, Melissa Hall, Candace Ross, Jack Urbanek, Adina Williams, Aishwarya Agrawal, Adriana Romero-Soriano, Michal Drozdzal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Impressive advances in text-to-image (T2I) generative models have yielded a
plethora of high performing models which are able to generate aesthetically
appealing, photorealistic images. Despite the progress, these models still
struggle to produce images that are consistent with the input prompt,
oftentimes failing to capture object quantities, relations and attributes
properly. Existing solutions to improve prompt-image consistency suffer from
the following challenges: (1) they oftentimes require model fine-tuning, (2)
they only focus on nearby prompt samples, and (3) they are affected by
unfavorable trade-offs among image quality, representation diversity, and
prompt-image consistency. In this paper, we address these challenges and
introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a
large language model (LLM) to improve prompt-image consistency in T2I models.
Our framework starts from a user prompt and iteratively generates revised
prompts with the goal of maximizing a consistency score. Our extensive
validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost
the initial consistency score by up to 24.9% in terms of DSG score while
preserving the FID and increasing the recall between generated and real data.
Our work paves the way toward building more reliable and robust T2I systems by
harnessing the power of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards 3D Vision with Low-Cost Single-Photon Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangzhou Mu, Carter Sifferman, Sacha Jungerman, Yiquan Li, Mark Han, Michael Gleicher, Mohit Gupta, Yin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method for reconstructing 3D shape of arbitrary Lambertian
objects based on measurements by miniature, energy-efficient, low-cost
single-photon cameras. These cameras, operating as time resolved image sensors,
illuminate the scene with a very fast pulse of diffuse light and record the
shape of that pulse as it returns back from the scene at a high temporal
resolution. We propose to model this image formation process, account for its
non-idealities, and adapt neural rendering to reconstruct 3D geometry from a
set of spatially distributed sensors with known poses. We show that our
approach can successfully recover complex 3D shapes from simulated data. We
further demonstrate 3D object reconstruction from real-world captures,
utilizing measurements from a commodity proximity sensor. Our work draws a
connection between image-based modeling and active range scanning and is a step
towards 3D vision with single-photon cameras.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models
  Versus Fine-Tuned Vision Transformers in Image-Based Security Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fouad Trad, Ali Chehab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of Large Language Models (LLMs) has led to a parallel rise in the
development of Large Multimodal Models (LMMs), such as Gemini-pro, which have
begun to transform a variety of applications. These sophisticated multimodal
models are designed to interpret and analyze complex data, integrating both
textual and visual information on a scale previously unattainable, opening new
avenues for a range of applications. This paper investigates the applicability
and effectiveness of prompt-engineered Gemini-pro LMMs versus fine-tuned Vision
Transformer (ViT) models in addressing critical security challenges. We focus
on two distinct tasks: a visually evident task of detecting simple triggers,
such as small squares in images, indicative of potential backdoors, and a
non-visually evident task of malware classification through visual
representations. Our results highlight a significant divergence in performance,
with Gemini-pro falling short in accuracy and reliability when compared to
fine-tuned ViT models. The ViT models, on the other hand, demonstrate
exceptional accuracy, achieving near-perfect performance on both tasks. This
study not only showcases the strengths and limitations of prompt-engineered
LMMs in cybersecurity applications but also emphasizes the unmatched efficacy
of fine-tuned ViT models for precise and dependable tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenesisTex: Adapting Image Denoising Diffusion to Texture Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenjian Gao, Boyan Jiang, Xinghui Li, Yingpeng Zhang, Qian Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present GenesisTex, a novel method for synthesizing textures for 3D
geometries from text descriptions. GenesisTex adapts the pretrained image
diffusion model to texture space by texture space sampling. Specifically, we
maintain a latent texture map for each viewpoint, which is updated with
predicted noise on the rendering of the corresponding viewpoint. The sampled
latent texture maps are then decoded into a final texture map. During the
sampling process, we focus on both global and local consistency across multiple
viewpoints: global consistency is achieved through the integration of style
consistency mechanisms within the noise prediction network, and low-level
consistency is achieved by dynamically aligning latent textures. Finally, we
apply reference-based inpainting and img2img on denser views for texture
refinement. Our approach overcomes the limitations of slow optimization in
distillation-based methods and instability in inpainting-based methods.
Experiments on meshes from various sources demonstrate that our method
surpasses the baseline methods quantitatively and qualitatively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CT Synthesis with Conditional Diffusion Models for Abdominal Lymph Node
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongrui Yu, Hanyu Chen, Zitian Zhang, Qiong Xiao, Wenhui Lei, Linrui Dai, Yu Fu, Hui Tan, Guan Wang, Peng Gao, Xiaofan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the significant success achieved by deep learning methods in medical
image segmentation, researchers still struggle in the computer-aided diagnosis
of abdominal lymph nodes due to the complex abdominal environment, small and
indistinguishable lesions, and limited annotated data. To address these
problems, we present a pipeline that integrates the conditional diffusion model
for lymph node generation and the nnU-Net model for lymph node segmentation to
improve the segmentation performance of abdominal lymph nodes through
synthesizing a diversity of realistic abdominal lymph node data. We propose
LN-DDPM, a conditional denoising diffusion probabilistic model (DDPM) for lymph
node (LN) generation. LN-DDPM utilizes lymph node masks and anatomical
structure masks as model conditions. These conditions work in two conditioning
mechanisms: global structure conditioning and local detail conditioning, to
distinguish between lymph nodes and their surroundings and better capture lymph
node characteristics. The obtained paired abdominal lymph node images and masks
are used for the downstream segmentation task. Experimental results on the
abdominal lymph node datasets demonstrate that LN-DDPM outperforms other
generative methods in the abdominal lymph node image synthesis and better
assists the downstream abdominal lymph node segmentation task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MUTE-<span class="highlight-title">SLAM</span>: Real-Time Neural <span class="highlight-title">SLAM</span> with Multiple Tri-Plane Hash
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Yan, Ruomin He, Zhenghua Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MUTE-SLAM, a real-time neural RGB-D SLAM system employing
multiple tri-plane hash-encodings for efficient scene representation. MUTE-SLAM
effectively tracks camera positions and incrementally builds a scalable
multi-map representation for both small and large indoor environments. It
dynamically allocates sub-maps for newly observed local regions, enabling
constraint-free mapping without prior scene information. Unlike traditional
grid-based methods, we use three orthogonal axis-aligned planes for
hash-encoding scene properties, significantly reducing hash collisions and the
number of trainable parameters. This hybrid approach not only speeds up
convergence but also enhances the fidelity of surface reconstruction.
Furthermore, our optimization strategy concurrently optimizes all sub-maps
intersecting with the current camera frustum, ensuring global consistency.
Extensive testing on both real-world and synthetic datasets has shown that
MUTE-SLAM delivers state-of-the-art surface reconstruction quality and
competitive tracking performance across diverse indoor settings. The code will
be made public upon acceptance of the paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Makeup Prior Models for 3D Facial Makeup Estimation and Applications <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingchao Yang, Takafumi Taketomi, Yuki Endo, Yoshihiro Kanamori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce two types of makeup prior models to extend
existing 3D face prior models: PCA-based and StyleGAN2-based priors. The
PCA-based prior model is a linear model that is easy to construct and is
computationally efficient. However, it retains only low-frequency information.
Conversely, the StyleGAN2-based model can represent high-frequency information
with relatively higher computational cost than the PCA-based model. Although
there is a trade-off between the two models, both are applicable to 3D facial
makeup estimation and related applications. By leveraging makeup prior models
and designing a makeup consistency module, we effectively address the
challenges that previous methods faced in robustly estimating makeup,
particularly in the context of handling self-occluded faces. In experiments, we
demonstrate that our approach reduces computational costs by several orders of
magnitude, achieving speeds up to 180 times faster. In addition, by improving
the accuracy of the estimated makeup, we confirm that our methods are highly
advantageous for various 3D facial makeup applications such as 3D makeup face
reconstruction, user-friendly makeup editing, makeup transfer, and
interpolation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024. Project: https://yangxingchao.github.io/makeup-priors-page</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Noise2Noise Denoising of CRISM Hyperspectral Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Platt, Rossella Arcucci, Cédric John
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral data acquired by the Compact Reconnaissance Imaging
Spectrometer for Mars (CRISM) have allowed for unparalleled mapping of the
surface mineralogy of Mars. Due to sensor degradation over time, a significant
portion of the recently acquired data is considered unusable. Here a new
data-driven model architecture, Noise2Noise4Mars (N2N4M), is introduced to
remove noise from CRISM images. Our model is self-supervised and does not
require zero-noise target data, making it well suited for use in Planetary
Science applications where high quality labelled data is scarce. We demonstrate
its strong performance on synthetic-noise data and CRISM images, and its impact
on downstream classification performance, outperforming benchmark methods on
most metrics. This allows for detailed analysis for critical sites of interest
on the Martian surface, including proposed lander sites.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures. Accepted as a conference paper at the ICLR 2024
  ML4RS Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DataCook: Crafting Anti-Adversarial Examples for Healthcare Data
  Copyright Protection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sihan Shang, Jiancheng Yang, Zhenglong Sun, Pascal Fua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of healthcare, the challenges of copyright protection and
unauthorized third-party misuse are increasingly significant. Traditional
methods for data copyright protection are applied prior to data distribution,
implying that models trained on these data become uncontrollable. This paper
introduces a novel approach, named DataCook, designed to safeguard the
copyright of healthcare data during the deployment phase. DataCook operates by
"cooking" the raw data before distribution, enabling the development of models
that perform normally on this processed data. However, during the deployment
phase, the original test data must be also "cooked" through DataCook to ensure
normal model performance. This process grants copyright holders control over
authorization during the deployment phase. The mechanism behind DataCook is by
crafting anti-adversarial examples (AntiAdv), which are designed to enhance
model confidence, as opposed to standard adversarial examples (Adv) that aim to
confuse models. Similar to Adv, AntiAdv introduces imperceptible perturbations,
ensuring that the data processed by DataCook remains easily understandable. We
conducted extensive experiments on MedMNIST datasets, encompassing both 2D/3D
data and the high-resolution variants. The outcomes indicate that DataCook
effectively meets its objectives, preventing models trained on AntiAdv from
analyzing unauthorized data effectively, without compromising the validity and
accuracy of the data in legitimate scenarios. Code and data are available at
https://github.com/MedMNIST/DataCook.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Task Dense Prediction via Mixture of Low-Rank Experts <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Yang, Peng-Tao Jiang, Qibin Hou, Hao Zhang, Jinwei Chen, Bo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous multi-task dense prediction methods based on the Mixture of Experts
(MoE) have received great performance but they neglect the importance of
explicitly modeling the global relations among all tasks. In this paper, we
present a novel decoder-focused method for multi-task dense prediction, called
Mixture-of-Low-Rank-Experts (MLoRE). To model the global task relationships,
MLoRE adds a generic convolution path to the original MoE structure, where each
task feature can go through this path for explicit parameter sharing.
Furthermore, to control the parameters and computational cost brought by the
increase in the number of experts, we take inspiration from LoRA and propose to
leverage the low-rank format of a vanilla convolution in the expert network.
Since the low-rank experts have fewer parameters and can be dynamically
parameterized into the generic convolution, the parameters and computational
cost do not change much with the increase of experts. Benefiting from this
design, we increase the number of experts and its reception field to enlarge
the representation capacity, facilitating multiple dense tasks learning in a
unified network. Extensive experiments on the PASCAL-Context and NYUD-v2
benchmarks show that our MLoRE achieves superior performance compared to
previous state-of-the-art methods on all metrics. Our code is available at
https://github.com/YuqiYang213/MLoRE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Paired Diffusion: Generation of related, synthetic PET-CT-Segmentation
  scans using Linked Denoising Diffusion Probabilistic Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rowan Bradbury, Katherine A. Vallis, Bartlomiej W. Papiez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Artificial Intelligence (AI) in biomedical imaging
and radiotherapy is hindered by the limited availability of large imaging data
repositories. With recent research and improvements in denoising diffusion
probabilistic models (DDPM), high quality synthetic medical scans are now
possible. Despite this, there is currently no way of generating multiple
related images, such as a corresponding ground truth which can be used to train
models, so synthetic scans are often manually annotated before use. This
research introduces a novel architecture that is able to generate multiple,
related PET-CT-tumour mask pairs using paired networks and conditional
encoders. Our approach includes innovative, time step-controlled mechanisms and
a `noise-seeding' strategy to improve DDPM sampling consistency. While our
model requires a modified perceptual loss function to ensure accurate feature
alignment we show generation of clearly aligned synthetic images and
improvement in segmentation accuracy with generated images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in IEEE International Symposium on Biomedical Imaging
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastPerson: Enhancing Video Learning through Effective Video
  Summarization that Preserves Linguistic and Visual Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuki Kawamura, Jun Rekimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quickly understanding lengthy lecture videos is essential for learners with
limited time and interest in various topics to improve their learning
efficiency. To this end, video summarization has been actively researched to
enable users to view only important scenes from a video. However, these studies
focus on either the visual or audio information of a video and extract
important segments in the video. Therefore, there is a risk of missing
important information when both the teacher's speech and visual information on
the blackboard or slides are important, such as in a lecture video. To tackle
this issue, we propose FastPerson, a video summarization approach that
considers both the visual and auditory information in lecture videos.
FastPerson creates summary videos by utilizing audio transcriptions along with
on-screen images and text, minimizing the risk of overlooking crucial
information for learners. Further, it provides a feature that allows learners
to switch between the summary and original videos for each chapter of the
video, enabling them to adjust the pace of learning based on their interests
and level of understanding. We conducted an evaluation with 40 participants to
assess the effectiveness of our method and confirmed that it reduced viewing
time by 53\% at the same level of comprehension as that when using traditional
video playback methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for Segmentation of Cracks in High-Resolution Images of
  Steel Bridges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrii Kompanets, Gautam Pai, Remco Duits, Davide Leonetti, Bert Snijder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automating the current bridge visual inspection practices using drones and
image processing techniques is a prominent way to make these inspections more
effective, robust, and less expensive. In this paper, we investigate the
development of a novel deep-learning method for the detection of fatigue cracks
in high-resolution images of steel bridges. First, we present a novel and
challenging dataset comprising of images of cracks in steel bridges. Secondly,
we integrate the ConvNext neural network with a previous state- of-the-art
encoder-decoder network for crack segmentation. We study and report, the
effects of the use of background patches on the network performance when
applied to high-resolution images of cracks in steel bridges. Finally, we
introduce a loss function that allows the use of more background patches for
the training process, which yields a significant reduction in false positive
rates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Invisible Gas Detection: An RGB-Thermal Cross Attention Network and A
  New Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jue Wang, Yuxiang Lin, Qi Zhao, Dong Luo, Shuaibao Chen, Wei Chen, Xiaojiang Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread use of various chemical gases in industrial processes
necessitates effective measures to prevent their leakage during transportation
and storage, given their high toxicity. Thermal infrared-based computer vision
detection techniques provide a straightforward approach to identify gas leakage
areas. However, the development of high-quality algorithms has been challenging
due to the low texture in thermal images and the lack of open-source datasets.
In this paper, we present the RGB-Thermal Cross Attention Network (RT-CAN),
which employs an RGB-assisted two-stream network architecture to integrate
texture information from RGB images and gas area information from thermal
images. Additionally, to facilitate the research of invisible gas detection, we
introduce Gas-DB, an extensive open-source gas detection database including
about 1.3K well-annotated RGB-thermal images with eight variant collection
scenes. Experimental results demonstrate that our method successfully leverages
the advantages of both modalities, achieving state-of-the-art (SOTA)
performance among RGB-thermal methods, surpassing single-stream SOTA models in
terms of accuracy, Intersection of Union (IoU), and F2 metrics by 4.86%, 5.65%,
and 4.88%, respectively. The code and data will be made available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Groupwise Query Specialization and Quality-Aware Multi-Assignment for
  Transformer-based Visual Relationship Detection <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jongha Kim, Jihwan Park, Jinyoung Park, Jinyoung Kim, Sehyung Kim, Hyunwoo J. Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Relationship Detection (VRD) has seen significant advancements with
Transformer-based architectures recently. However, we identify two key
limitations in a conventional label assignment for training Transformer-based
VRD models, which is a process of mapping a ground-truth (GT) to a prediction.
Under the conventional assignment, an unspecialized query is trained since a
query is expected to detect every relation, which makes it difficult for a
query to specialize in specific relations. Furthermore, a query is also
insufficiently trained since a GT is assigned only to a single prediction,
therefore near-correct or even correct predictions are suppressed by being
assigned no relation as a GT. To address these issues, we propose Groupwise
Query Specialization and Quality-Aware Multi-Assignment (SpeaQ). Groupwise
Query Specialization trains a specialized query by dividing queries and
relations into disjoint groups and directing a query in a specific query group
solely toward relations in the corresponding relation group. Quality-Aware
Multi-Assignment further facilitates the training by assigning a GT to multiple
predictions that are significantly close to a GT in terms of a subject, an
object, and the relation in between. Experimental results and analyses show
that SpeaQ effectively trains specialized queries, which better utilize the
capacity of a model, resulting in consistent performance gains with zero
additional inference cost across multiple VRD models and benchmarks. Code is
available at https://github.com/mlvlab/SpeaQ.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Panonut360: A Head and Eye Tracking <span class="highlight-title">Dataset</span> for Panoramic Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Xu, Junhao Du, Jiahe Wang, Yuwei Ning, Sihan Zhou Yang Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development and widespread application of VR/AR technology,
maximizing the quality of immersive panoramic video services that match users'
personal preferences and habits has become a long-standing challenge.
Understanding the saliency region where users focus, based on data collected
with HMDs, can promote multimedia encoding, transmission, and quality
assessment. At the same time, large-scale datasets are essential for
researchers and developers to explore short/long-term user behavior patterns
and train AI models related to panoramic videos. However, existing panoramic
video datasets often include low-frequency user head or eye movement data
through short-term videos only, lacking sufficient data for analyzing users'
Field of View (FoV) and generating video saliency regions.
  Driven by these practical factors, in this paper, we present a head and eye
tracking dataset involving 50 users (25 males and 25 females) watching 15
panoramic videos. The dataset provides details on the viewport and gaze
attention locations of users. Besides, we present some statistics samples
extracted from the dataset. For example, the deviation between head and eye
movements challenges the widely held assumption that gaze attention decreases
from the center of the FoV following a Gaussian distribution. Our analysis
reveals a consistent downward offset in gaze fixations relative to the FoV in
experimental settings involving multiple users and videos. That's why we name
the dataset Panonut, a saliency weighting shaped like a donut. Finally, we also
provide a script that generates saliency distributions based on given head or
eye coordinates and pre-generated saliency distribution map sets of each video
from the collected eye tracking data.
  The dataset is available on website: https://dianvrlab.github.io/Panonut360/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages,ACM MMSys'24 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Solution for the CVPR 2023 1st foundation model challenge-Track2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Xu, Yurui Huang, Sishun Pan, Zhihao Guan, Yi Xu, Yang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a solution for cross-modal transportation
retrieval. Due to the cross-domain problem of traffic images, we divide the
problem into two sub-tasks of pedestrian retrieval and vehicle retrieval
through a simple strategy. In pedestrian retrieval tasks, we use IRRA as the
base model and specifically design an Attribute Classification to mine the
knowledge implied by attribute labels. More importantly, We use the strategy of
Inclusion Relation Matching to make the image-text pairs with inclusion
relation have similar representation in the feature space. For the vehicle
retrieval task, we use BLIP as the base model. Since aligning the color
attributes of vehicles is challenging, we introduce attribute-based object
detection techniques to add color patch blocks to vehicle images for color data
augmentation. This serves as strong prior information, helping the model
perform the image-text alignment. At the same time, we incorporate labeled
attributes into the image-text alignment loss to learn fine-grained alignment
and prevent similar images and texts from being incorrectly separated. Our
approach ranked first in the final B-board test with a score of 70.9.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical
  Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Tang, Lianglun Cheng, Guoheng Huang, Zhengguang Tan, Junhao Lu, Kaihong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image segmentation holds a vital position in the realms of diagnosis and
treatment within the medical domain. Traditional convolutional neural networks
(CNNs) and Transformer models have made significant advancements in this realm,
but they still encounter challenges because of limited receptive field or high
computing complexity. Recently, State Space Models (SSMs), particularly Mamba
and its variants, have demonstrated notable performance in the field of vision.
However, their feature extraction methods may not be sufficiently effective and
retain some redundant structures, leaving room for parameter reduction.
Motivated by previous spatial and channel attention methods, we propose Triplet
Mamba-UNet. The method leverages residual VSS Blocks to extract intensive
contextual features, while Triplet SSM is employed to fuse features across
spatial and channel dimensions. We conducted experiments on ISIC17, ISIC18,
CVC-300, CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB, and Kvasir-Instrument datasets,
demonstrating the superior segmentation performance of our proposed TM-UNet.
Additionally, compared to the previous VM-UNet, our model achieves a one-third
reduction in parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhongyi Yang, Zehui Chen, Miguel Espinosa, Linus Ericsson, Zhenyu Wang, Jiaming Liu, Elliot J. Crowley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PlainMamba: a simple non-hierarchical state space model (SSM)
designed for general visual recognition. The recent Mamba model has shown how
SSMs can be highly competitive with other architectures on sequential data and
initial attempts have been made to apply it to images. In this paper, we
further adapt the selective scanning process of Mamba to the visual domain,
enhancing its ability to learn features from two-dimensional images by (i) a
continuous 2D scanning process that improves spatial continuity by ensuring
adjacency of tokens in the scanning sequence, and (ii) direction-aware updating
which enables the model to discern the spatial relations of tokens by encoding
directional information. Our architecture is designed to be easy to use and
easy to scale, formed by stacking identical PlainMamba blocks, resulting in a
model with constant width throughout all layers. The architecture is further
simplified by removing the need for special tokens. We evaluate PlainMamba on a
variety of visual recognition tasks including image classification, semantic
segmentation, object detection, and instance segmentation. Our method achieves
performance gains over previous non-hierarchical models and is competitive with
hierarchical alternatives. For tasks requiring high-resolution inputs, in
particular, PlainMamba requires much less computing while maintaining high
performance. Code and models are available at
https://github.com/ChenhongyiYang/PlainMamba
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huawei Wei, Zejun Yang, Zhisheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose AniPortrait, a novel framework for generating
high-quality animation driven by audio and a reference portrait image. Our
methodology is divided into two stages. Initially, we extract 3D intermediate
representations from audio and project them into a sequence of 2D facial
landmarks. Subsequently, we employ a robust diffusion model, coupled with a
motion module, to convert the landmark sequence into photorealistic and
temporally consistent portrait animation. Experimental results demonstrate the
superiority of AniPortrait in terms of facial naturalness, pose diversity, and
visual quality, thereby offering an enhanced perceptual experience. Moreover,
our methodology exhibits considerable potential in terms of flexibility and
controllability, which can be effectively applied in areas such as facial
motion editing or face reenactment. We release code and model weights at
https://github.com/scutzzj/AniPortrait
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Manifold-Guided Lyapunov Control with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amartya Mukherjee, Thanin Quartz, Jun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to generating stabilizing controllers
for a large class of dynamical systems using diffusion models. The core
objective is to develop stabilizing control functions by identifying the
closest asymptotically stable vector field relative to a predetermined manifold
and adjusting the control function based on this finding. To achieve this, we
employ a diffusion model trained on pairs consisting of asymptotically stable
vector fields and their corresponding Lyapunov functions. Our numerical results
demonstrate that this pre-trained model can achieve stabilization over
previously unseen systems efficiently and rapidly, showcasing the potential of
our approach in fast zero-shot control and generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to
  Inform GenAI Copyright Disputes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uri Hacohen, Adi Haviv, Shahar Sarfaty, Bruria Friedman, Niva Elkin-Koren, Roi Livni, Amit H Bermano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Generative Artificial Intelligence (GenAI) models, including
GitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content
creation, enabling non-professionals to produce high-quality content across
various domains. This transformative technology has led to a surge of synthetic
content and sparked legal disputes over copyright infringement. To address
these challenges, this paper introduces a novel approach that leverages the
learning capacity of GenAI models for copyright legal analysis, demonstrated
with GPT2 and Stable Diffusion models. Copyright law distinguishes between
original expressions and generic ones (Sc\`enes \`a faire), protecting the
former and permitting reproduction of the latter. However, this distinction has
historically been challenging to make consistently, leading to over-protection
of copyrighted works. GenAI offers an unprecedented opportunity to enhance this
legal analysis by revealing shared patterns in preexisting works. We propose a
data-driven approach to identify the genericity of works created by GenAI,
employing "data-driven bias" to assess the genericity of expressive
compositions. This approach aids in copyright scope determination by utilizing
the capabilities of GenAI to identify and prioritize expressive elements and
rank them according to their frequency in the model's dataset. The potential
implications of measuring expressive genericity for copyright law are profound.
Such scoring could assist courts in determining copyright scope during
litigation, inform the registration practices of Copyright Offices, allowing
registration of only highly original synthetic works, and help copyright owners
signal the value of their works and facilitate fairer licensing deals. More
generally, this approach offers valuable insights to policymakers grappling
with adapting copyright law to the challenges posed by the era of GenAI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at ACM CSLAW 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Light Transformer Ensembles for Multimodal Trajectory
  Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrien Lafage, Mathieu Barbier, Gianni Franchi, David Filliat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate trajectory forecasting is crucial for the performance of various
systems, such as advanced driver-assistance systems and self-driving vehicles.
These forecasts allow to anticipate events leading to collisions and,
therefore, to mitigate them. Deep Neural Networks have excelled in motion
forecasting, but issues like overconfidence and uncertainty quantification
persist. Deep Ensembles address these concerns, yet applying them to multimodal
distributions remains challenging. In this paper, we propose a novel approach
named Hierarchical Light Transformer Ensembles (HLT-Ens), aimed at efficiently
training an ensemble of Transformer architectures using a novel hierarchical
loss function. HLT-Ens leverages grouped fully connected layers, inspired by
grouped convolution techniques, to capture multimodal distributions,
effectively. Through extensive experimentation, we demonstrate that HLT-Ens
achieves state-of-the-art performance levels, offering a promising avenue for
improving trajectory forecasting techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Perceived Gloss: Do Weak Labels Suffice? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julia Guerrero-Viu, J. Daniel Subias, Ana Serrano, Katherine R. Storrs, Roland W. Fleming, Belen Masia, Diego Gutierrez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating perceptual attributes of materials directly from images is a
challenging task due to their complex, not fully-understood interactions with
external factors, such as geometry and lighting. Supervised deep learning
models have recently been shown to outperform traditional approaches, but rely
on large datasets of human-annotated images for accurate perception
predictions. Obtaining reliable annotations is a costly endeavor, aggravated by
the limited ability of these models to generalise to different aspects of
appearance. In this work, we show how a much smaller set of human annotations
("strong labels") can be effectively augmented with automatically derived "weak
labels" in the context of learning a low-dimensional image-computable gloss
metric. We evaluate three alternative weak labels for predicting human gloss
perception from limited annotated data. Incorporating weak labels enhances our
gloss prediction beyond the current state of the art. Moreover, it enables a
substantial reduction in human annotation costs without sacrificing accuracy,
whether working with rendered images or real photographs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Computer Graphics Forum (Eurographics 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffFAE: Advancing High-fidelity One-shot Facial Appearance Editing with
  Space-sensitive Customization and Semantic Preservation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qilin Wang, Jiangning Zhang, Chengming Xu, Weijian Cao, Ying Tai, Yue Han, Yanhao Ge, Hong Gu, Chengjie Wang, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial Appearance Editing (FAE) aims to modify physical attributes, such as
pose, expression and lighting, of human facial images while preserving
attributes like identity and background, showing great importance in
photograph. In spite of the great progress in this area, current researches
generally meet three challenges: low generation fidelity, poor attribute
preservation, and inefficient inference. To overcome above challenges, this
paper presents DiffFAE, a one-stage and highly-efficient diffusion-based
framework tailored for high-fidelity FAE. For high-fidelity query attributes
transfer, we adopt Space-sensitive Physical Customization (SPC), which ensures
the fidelity and generalization ability by utilizing rendering texture derived
from 3D Morphable Model (3DMM). In order to preserve source attributes, we
introduce the Region-responsive Semantic Composition (RSC). This module is
guided to learn decoupled source-regarding features, thereby better preserving
the identity and alleviating artifacts from non-facial attributes such as hair,
clothes, and background. We further introduce a consistency regularization for
our pipeline to enhance editing controllability by leveraging prior knowledge
in the attention matrices of diffusion model. Extensive experiments demonstrate
the superiority of DiffFAE over existing methods, achieving state-of-the-art
performance in facial appearance editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Dynamic Transformer for Efficient Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawen Zhu, Xin Chen, Haiwen Diao, Shuai Li, Jun-Yan He, Chenyang Li, Bin Luo, Dong Wang, Huchuan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The speed-precision trade-off is a critical problem for visual object
tracking which usually requires low latency and deployment on constrained
resources. Existing solutions for efficient tracking mainly focus on adopting
light-weight backbones or modules, which nevertheless come at the cost of a
sacrifice in precision. In this paper, inspired by dynamic network routing, we
propose DyTrack, a dynamic transformer framework for efficient tracking.
Real-world tracking scenarios exhibit diverse levels of complexity. We argue
that a simple network is sufficient for easy frames in video sequences, while
more computation could be assigned to difficult ones. DyTrack automatically
learns to configure proper reasoning routes for various inputs, gaining better
utilization of the available computational budget. Thus, it can achieve higher
performance with the same running speed. We formulate instance-specific
tracking as a sequential decision problem and attach terminating branches to
intermediate layers of the entire model. Especially, to fully utilize the
computations, we introduce the feature recycling mechanism to reuse the outputs
of predecessors. Furthermore, a target-aware self-distillation strategy is
designed to enhance the discriminating capabilities of early predictions by
effectively mimicking the representation pattern of the deep model. Extensive
experiments on multiple benchmarks demonstrate that DyTrack achieves promising
speed-precision trade-offs with only a single model. For instance, DyTrack
obtains 64.9% AUC on LaSOT with a speed of 256 fps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-Resolution Image Translation Model Based on Grayscale Redefinition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xixian Wu, Dian Chao, Yang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-to-image translation is a technique that focuses on transferring images
from one domain to another while maintaining the essential content
representations. In recent years, image-to-image translation has gained
significant attention and achieved remarkable advancements due to its diverse
applications in computer vision and image processing tasks. In this work, we
propose an innovative method for image translation between different domains.
For high-resolution image translation tasks, we use a grayscale adjustment
method to achieve pixel-level translation. For other tasks, we utilize the
Pix2PixHD model with a coarse-to-fine generator, multi-scale discriminator, and
improved loss to enhance the image translation performance. On the other hand,
to tackle the issue of sparse training data, we adopt model weight
initialization from other task to optimize the performance of the current task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning with Unreliability: Fast Few-shot Voxel Radiance Fields with
  Relative Geometric Consistency <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingjie Xu, Bangzhen Liu, Hao Tang, Bailin Deng, Shengfeng He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a voxel-based optimization framework, ReVoRF, for few-shot
radiance fields that strategically address the unreliability in pseudo novel
view synthesis. Our method pivots on the insight that relative depth
relationships within neighboring regions are more reliable than the absolute
color values in disoccluded areas. Consequently, we devise a bilateral
geometric consistency loss that carefully navigates the trade-off between color
fidelity and geometric accuracy in the context of depth consistency for
uncertain regions. Moreover, we present a reliability-guided learning strategy
to discern and utilize the variable quality across synthesized views,
complemented by a reliability-aware voxel smoothing algorithm that smoothens
the transition between reliable and unreliable data patches. Our approach
allows for a more nuanced use of all available data, promoting enhanced
learning from regions previously considered unsuitable for high-quality
reconstruction. Extensive experiments across diverse datasets reveal that our
approach attains significant gains in efficiency and accuracy, delivering
rendering speeds of 3 FPS, 7 mins to train a $360^\circ$ scene, and a 5\%
improvement in PSNR over existing few-shot methods. Code is available at
https://github.com/HKCLynn/ReVoRF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024 final version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object
  Detection with Sparse <span class="highlight-title">LiDAR</span> and Large Domain Gaps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej K Wozniak, Mattias Hansson, Marko Thiel, Patric Jensfelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we address a gap in existing unsupervised domain adaptation
approaches on LiDAR-based 3D object detection, which have predominantly
concentrated on adapting between established, high-density autonomous driving
datasets. We focus on sparser point clouds, capturing scenarios from different
perspectives: not just from vehicles on the road but also from mobile robots on
sidewalks, which encounter significantly different environmental conditions and
sensor configurations. We introduce Unsupervised Adversarial Domain Adaptation
for 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source
models or teacher-student architectures. Instead, it uses an adversarial
approach to directly learn domain-invariant features. We demonstrate its
efficacy in various adaptation scenarios, showing significant improvements in
both self-driving car and mobile robot domains. Our code is open-source and
will be available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AniArtAvatar: Animatable 3D Art Avatar from a Single Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoxu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach for generating animatable 3D-aware art avatars
from a single image, with controllable facial expressions, head poses, and
shoulder movements. Unlike previous reenactment methods, our approach utilizes
a view-conditioned 2D diffusion model to synthesize multi-view images from a
single art portrait with a neutral expression. With the generated colors and
normals, we synthesize a static avatar using an SDF-based neural surface. For
avatar animation, we extract control points, transfer the motion with these
points, and deform the implicit canonical space. Firstly, we render the front
image of the avatar, extract the 2D landmarks, and project them to the 3D space
using a trained SDF network. We extract 3D driving landmarks using 3DMM and
transfer the motion to the avatar landmarks. To animate the avatar pose, we
manually set the body height and bound the head and torso of an avatar with two
cages. The head and torso can be animated by transforming the two cages. Our
approach is a one-shot pipeline that can be applied to various styles.
Experiments demonstrate that our method can generate high-quality 3D art
avatars with desired control over different motions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grad-CAMO: Learning Interpretable Single-Cell Morphological Profiles
  from 3D Cell Painting Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivek Gopalakrishnan, Jingzhe Ma, Zhiyong Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their black-box nature, deep learning models are extensively used in
image-based drug discovery to extract feature vectors from single cells in
microscopy images. To better understand how these networks perform
representation learning, we employ visual explainability techniques (e.g.,
Grad-CAM). Our analyses reveal several mechanisms by which supervised models
cheat, exploiting biologically irrelevant pixels when extracting morphological
features from images, such as noise in the background. This raises doubts
regarding the fidelity of learned single-cell representations and their
relevance when investigating downstream biological questions. To address this
misalignment between researcher expectations and machine behavior, we introduce
Grad-CAMO, a novel single-cell interpretability score for supervised feature
extractors. Grad-CAMO measures the proportion of a model's attention that is
concentrated on the cell of interest versus the background. This metric can be
assessed per-cell or averaged across a validation set, offering a tool to audit
individual features vectors or guide the improved design of deep learning
architectures. Importantly, Grad-CAMO seamlessly integrates into existing
workflows, requiring no dataset or model modifications, and is compatible with
both 2D and 3D Cell Painting data. Additional results are available at
https://github.com/eigenvivek/Grad-CAMO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMVP: A Multimodal MoCap <span class="highlight-title">Dataset</span> with Vision and Pressure Sensors <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Zhang, Shenghao Ren, Haolei Yuan, Jianhui Zhao, Fan Li, Shuangpeng Sun, Zhenghao Liang, Tao Yu, Qiu Shen, Xun Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foot contact is an important cue not only for human motion capture but also
for motion understanding and physically plausible motion generation. However,
most of the foot-contact annotations in existing datasets are estimated by
purely visual matching and distance thresholding, which results in low accuracy
and coarse granularity. Even though existing multimodal datasets
synergistically capture plantar pressure (foot contact) and visual signals,
they are specifically designed for small-range and slow motion such as Taiji
Quan and Yoga. Therefore, there is still a lack of a vision-pressure multimodal
dataset with large-range and fast human motion, as well as accurate and dense
foot-contact annotation. To fill this gap, we propose a Multimodal MoCap
Dataset with Vision and Pressure sensors, named MMVP. MMVP provides accurate
and dense plantar pressure signals synchronized with RGBD observations, which
is especially useful for both plausible shape estimation, robust pose fitting
without foot drifting, and accurate global translation tracking. To validate
the dataset, we propose an RGBD-P SMPL fitting method and also a
monocular-video-based baseline framework, VP-MoCap, for human motion capture.
Experiments demonstrate that our RGBD-P SMPL Fitting results significantly
outperform pure visual motion capture. Moreover, VP-MoCap outperforms SOTA
methods in foot-contact and global translation estimation accuracy. We believe
the configuration of the dataset and the baseline frameworks will stimulate the
research in this direction and also provide a good reference for MoCap
applications in various domains. Project page:
https://haolyuan.github.io/MMVP-Dataset/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fake or JPEG? Revealing Common Biases in Generated Image Detection
  <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Grommelt, Louis Weiss, Franz-Josef Pfreundt, Janis Keuper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread adoption of generative image models has highlighted the urgent
need to detect artificial content, which is a crucial step in combating
widespread manipulation and misinformation. Consequently, numerous detectors
and associated datasets have emerged. However, many of these datasets
inadvertently introduce undesirable biases, thereby impacting the effectiveness
and evaluation of detectors. In this paper, we emphasize that many datasets for
AI-generated image detection contain biases related to JPEG compression and
image size. Using the GenImage dataset, we demonstrate that detectors indeed
learn from these undesired factors. Furthermore, we show that removing the
named biases substantially increases robustness to JPEG compression and
significantly alters the cross-generator performance of evaluated detectors.
Specifically, it leads to more than 11 percentage points increase in
cross-generator performance for ResNet50 and Swin-T detectors on the GenImage
dataset, achieving state-of-the-art results.
  We provide the dataset and source codes of this paper on the anonymous
website: https://www.unbiased-genimage.org
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Memory Networks: A Versatile Adaptation Approach for
  Vision-Language Models <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yabin Zhang, Wenjie Zhu, Hui Tang, Zhiyuan Ma, Kaiyang Zhou, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of pre-trained vision-language models like CLIP, how to
adapt them to various downstream classification tasks has garnered significant
attention in recent research. The adaptation strategies can be typically
categorized into three paradigms: zero-shot adaptation, few-shot adaptation,
and the recently-proposed training-free few-shot adaptation. Most existing
approaches are tailored for a specific setting and can only cater to one or two
of these paradigms. In this paper, we introduce a versatile adaptation approach
that can effectively work under all three settings. Specifically, we propose
the dual memory networks that comprise dynamic and static memory components.
The static memory caches training data knowledge, enabling training-free
few-shot adaptation, while the dynamic memory preserves historical test
features online during the testing process, allowing for the exploration of
additional data insights beyond the training set. This novel capability
enhances model performance in the few-shot setting and enables model usability
in the absence of training data. The two memory networks employ the same
flexible memory interactive strategy, which can operate in a training-free mode
and can be further enhanced by incorporating learnable projection layers. Our
approach is tested across 11 datasets under the three task settings.
Remarkably, in the zero-shot scenario, it outperforms existing methods by over
3\% and even shows superior results against methods utilizing external training
data. Additionally, our method exhibits robust performance against natural
distribution shifts. Codes are available at \url{https://github.com/YBZh/DMN}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024; Codes are available at \url{https://github.com/YBZh/DMN}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepMIF: Deep Monotonic Implicit Fields for Large-Scale <span class="highlight-title">LiDAR</span> 3D <span class="highlight-title">Mapping</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kutay Yılmaz, Matthias Nießner, Anastasiia Kornilova, Alexey Artemov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, significant progress has been achieved in sensing real large-scale
outdoor 3D environments, particularly by using modern acquisition equipment
such as LiDAR sensors. Unfortunately, they are fundamentally limited in their
ability to produce dense, complete 3D scenes. To address this issue, recent
learning-based methods integrate neural implicit representations and
optimizable feature grids to approximate surfaces of 3D scenes. However,
naively fitting samples along raw LiDAR rays leads to noisy 3D mapping results
due to the nature of sparse, conflicting LiDAR measurements. Instead, in this
work we depart from fitting LiDAR data exactly, instead letting the network
optimize a non-metric monotonic implicit field defined in 3D space. To fit our
field, we design a learning system integrating a monotonicity loss that enables
optimizing neural monotonic fields and leverages recent progress in large-scale
3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as
captured by multiple quantitative and perceptual measures and visual results
obtained for Mai City, Newer College, and KITTI benchmarks. The code of our
approach will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Practical Applications of Advanced Cloud Services and Generative AI
  Systems in Medical Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyu Xu, Binbin Wu, Jiaxin Huang, Yulu Gong, Yifan Zhang, Bo Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The medical field is one of the important fields in the application of
artificial intelligence technology. With the explosive growth and
diversification of medical data, as well as the continuous improvement of
medical needs and challenges, artificial intelligence technology is playing an
increasingly important role in the medical field. Artificial intelligence
technologies represented by computer vision, natural language processing, and
machine learning have been widely penetrated into diverse scenarios such as
medical imaging, health management, medical information, and drug research and
development, and have become an important driving force for improving the level
and quality of medical services.The article explores the transformative
potential of generative AI in medical imaging, emphasizing its ability to
generate syntheticACM-2 data, enhance images, aid in anomaly detection, and
facilitate image-to-image translation. Despite challenges like model
complexity, the applications of generative models in healthcare, including
Med-PaLM 2 technology, show promising results. By addressing limitations in
dataset size and diversity, these models contribute to more accurate diagnoses
and improved patient outcomes. However, ethical considerations and
collaboration among stakeholders are essential for responsible implementation.
Through experiments leveraging GANs to augment brain tumor MRI datasets, the
study demonstrates how generative AI can enhance image quality and diversity,
ultimately advancing medical diagnostics and patient care.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Gaze-grounded Visual Question Answering <span class="highlight-title">Dataset</span> for Clarifying
  Ambiguous Japanese Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shun Inadumi, Seiya Kawano, Akishige Yuguchi, Yasutomo Kawanishi, Koichiro Yoshino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Situated conversations, which refer to visual information as visual question
answering (VQA), often contain ambiguities caused by reliance on directive
information. This problem is exacerbated because some languages, such as
Japanese, often omit subjective or objective terms. Such ambiguities in
questions are often clarified by the contexts in conversational situations,
such as joint attention with a user or user gaze information. In this study, we
propose the Gaze-grounded VQA dataset (GazeVQA) that clarifies ambiguous
questions using gaze information by focusing on a clarification process
complemented by gaze information. We also propose a method that utilizes gaze
target estimation results to improve the accuracy of GazeVQA tasks. Our
experimental results showed that the proposed method improved the performance
in some cases of a VQA system on GazeVQA and identified some typical problems
of GazeVQA tasks that need to be improved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WordRobe: Text-Guided Generation of Textured 3D Garments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Astitva Srivastava, Pranav Manu, Amit Raj, Varun Jampani, Avinash Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we tackle a new and challenging problem of text-driven
generation of 3D garments with high-quality textures. We propose "WordRobe", a
novel framework for the generation of unposed & textured 3D garment meshes from
user-friendly text prompts. We achieve this by first learning a latent
representation of 3D garments using a novel coarse-to-fine training strategy
and a loss for latent disentanglement, promoting better latent interpolation.
Subsequently, we align the garment latent space to the CLIP embedding space in
a weakly supervised manner, enabling text-driven 3D garment generation and
editing. For appearance modeling, we leverage the zero-shot generation
capability of ControlNet to synthesize view-consistent texture maps in a single
feed-forward inference step, thereby drastically decreasing the generation time
as compared to existing methods. We demonstrate superior performance over
current SOTAs for learning 3D garment latent space, garment interpolation, and
text-driven texture synthesis, supported by quantitative evaluation and
qualitative user study. The unposed 3D garment meshes generated using WordRobe
can be directly fed to standard cloth simulation & animation pipelines without
any post-processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeRF-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using
  Heuristics-Guided Segmentation <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Chen, Yipeng Qin, Lingjie Liu, Jiangbo Lu, Guanbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Field (NeRF) has been widely recognized for its excellence in
novel view synthesis and 3D scene reconstruction. However, their effectiveness
is inherently tied to the assumption of static scenes, rendering them
susceptible to undesirable artifacts when confronted with transient distractors
such as moving objects or shadows. In this work, we propose a novel paradigm,
namely "Heuristics-Guided Segmentation" (HuGS), which significantly enhances
the separation of static scenes from transient distractors by harmoniously
combining the strengths of hand-crafted heuristics and state-of-the-art
segmentation models, thus significantly transcending the limitations of
previous solutions. Furthermore, we delve into the meticulous design of
heuristics, introducing a seamless fusion of Structure-from-Motion (SfM)-based
heuristics and color residual heuristics, catering to a diverse range of
texture profiles. Extensive experiments demonstrate the superiority and
robustness of our method in mitigating transient distractors for NeRFs trained
in non-static scenes. Project page: https://cnhaox.github.io/NeRF-HuGS/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Few-Shot Learning with Disentangled Self-Supervised Learning
  and Meta-Learning for Medical Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eva Pachetti, Sotirios A. Tsaftaris, Sara Colantonio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background and objective: Employing deep learning models in critical domains
such as medical imaging poses challenges associated with the limited
availability of training data. We present a strategy for improving the
performance and generalization capabilities of models trained in low-data
regimes. Methods: The proposed method starts with a pre-training phase, where
features learned in a self-supervised learning setting are disentangled to
improve the robustness of the representations for downstream tasks. We then
introduce a meta-fine-tuning step, leveraging related classes between
meta-training and meta-testing phases but varying the granularity level. This
approach aims to enhance the model's generalization capabilities by exposing it
to more challenging classification tasks during meta-training and evaluating it
on easier tasks but holding greater clinical relevance during meta-testing. We
demonstrate the effectiveness of the proposed approach through a series of
experiments exploring several backbones, as well as diverse pre-training and
fine-tuning schemes, on two distinct medical tasks, i.e., classification of
prostate cancer aggressiveness from MRI data and classification of breast
cancer malignity from microscopic images. Results: Our results indicate that
the proposed approach consistently yields superior performance w.r.t. ablation
experiments, maintaining competitiveness even when a distribution shift between
training and evaluation data occurs. Conclusion: Extensive experiments
demonstrate the effectiveness and wide applicability of the proposed approach.
We hope that this work will add another solution to the arsenal of addressing
learning issues in data-scarce imaging domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 4 figures, 4 tables. Submitted to Elsevier on 25 March 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Equipping Sketch Patches with Context-Aware Positional Encoding for
  Graphic Sketch Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicong Zang, Zhijun Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The drawing order of a sketch records how it is created stroke-by-stroke by a
human being. For graphic sketch representation learning, recent studies have
injected sketch drawing orders into graph edge construction by linking each
patch to another in accordance to a temporal-based nearest neighboring
strategy. However, such constructed graph edges may be unreliable, since a
sketch could have variants of drawings. In this paper, we propose a
variant-drawing-protected method by equipping sketch patches with context-aware
positional encoding (PE) to make better use of drawing orders for learning
graphic sketch representation. Instead of injecting sketch drawings into graph
edges, we embed these sequential information into graph nodes only. More
specifically, each patch embedding is equipped with a sinusoidal absolute PE to
highlight the sequential position in the drawing order. And its neighboring
patches, ranked by the values of self-attention scores between patch
embeddings, are equipped with learnable relative PEs to restore the contextual
positions within a neighborhood. During message aggregation via graph
convolutional networks, a node receives both semantic contents from patch
embeddings and contextual patterns from PEs by its neighbors, arriving at
drawing-order-enhanced sketch representations. Experimental results indicate
that our method significantly improves sketch healing and controllable sketch
synthesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Adversarial Training via Fisher-Rao Norm-based Regularization <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Yin, Wenjie Ruan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial training is extensively utilized to improve the adversarial
robustness of deep neural networks. Yet, mitigating the degradation of standard
generalization performance in adversarial-trained models remains an open
problem. This paper attempts to resolve this issue through the lens of model
complexity. First, We leverage the Fisher-Rao norm, a geometrically invariant
metric for model complexity, to establish the non-trivial bounds of the
Cross-Entropy Loss-based Rademacher complexity for a ReLU-activated Multi-Layer
Perceptron. Then we generalize a complexity-related variable, which is
sensitive to the changes in model width and the trade-off factors in
adversarial training. Moreover, intensive empirical evidence validates that
this variable highly correlates with the generalization gap of Cross-Entropy
loss between adversarial-trained and standard-trained models, especially during
the initial and final phases of the training process. Building upon this
observation, we propose a novel regularization framework, called Logit-Oriented
Adversarial Training (LOAT), which can mitigate the trade-off between
robustness and accuracy while imposing only a negligible increase in
computational overhead. Our extensive experiments demonstrate that the proposed
regularization strategy can boost the performance of the prevalent adversarial
training algorithms, including PGD-AT, TRADES, TRADES (LSE), MART, and DM-AT,
across various network architectures. Our code will be available at
https://github.com/TrustAI/LOAT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Random-coupled Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Liu, Mingzhe Liu, Peng Li, Jiahui Wu, Xin Jiang, Zhuo Zuo, Bingqi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving the efficiency of current neural networks and modeling them in
biological neural systems have become popular research directions in recent
years. Pulse-coupled neural network (PCNN) is a well applicated model for
imitating the computation characteristics of the human brain in computer vision
and neural network fields. However, differences between the PCNN and biological
neural systems remain: limited neural connection, high computational cost, and
lack of stochastic property. In this study, random-coupled neural network
(RCNN) is proposed. It overcomes these difficulties in PCNN's neuromorphic
computing via a random inactivation process. This process randomly closes some
neural connections in the RCNN model, realized by the random inactivation
weight matrix of link input. This releases the computational burden of PCNN,
making it affordable to achieve vast neural connections. Furthermore, the image
and video processing mechanisms of RCNN are researched. It encodes constant
stimuli as periodic spike trains and periodic stimuli as chaotic spike trains,
the same as biological neural information encoding characteristics. Finally,
the RCNN is applicated to image segmentation, fusion, and pulse shape
discrimination subtasks. It is demonstrated to be robust, efficient, and highly
anti-noised, with outstanding performance in all applications mentioned above.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DS-AL: A Dual-Stream Analytic Learning for Exemplar-Free
  Class-Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiping Zhuang, Run He, Kai Tong, Ziqian Zeng, Cen Chen, Zhiping Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental learning (CIL) under an exemplar-free constraint has
presented a significant challenge. Existing methods adhering to this constraint
are prone to catastrophic forgetting, far more so than replay-based techniques
that retain access to past samples. In this paper, to solve the exemplar-free
CIL problem, we propose a Dual-Stream Analytic Learning (DS-AL) approach. The
DS-AL contains a main stream offering an analytical (i.e., closed-form) linear
solution, and a compensation stream improving the inherent under-fitting
limitation due to adopting linear mapping. The main stream redefines the CIL
problem into a Concatenated Recursive Least Squares (C-RLS) task, allowing an
equivalence between the CIL and its joint-learning counterpart. The
compensation stream is governed by a Dual-Activation Compensation (DAC) module.
This module re-activates the embedding with a different activation function
from the main stream one, and seeks fitting compensation by projecting the
embedding to the null space of the main stream's linear mapping. Empirical
results demonstrate that the DS-AL, despite being an exemplar-free technique,
delivers performance comparable with or better than that of replay-based
methods across various datasets, including CIFAR-100, ImageNet-100 and
ImageNet-Full. Additionally, the C-RLS' equivalent property allows the DS-AL to
execute CIL in a phase-invariant manner. This is evidenced by a
never-before-seen 500-phase CIL ImageNet task, which performs on a level
identical to a 5-phase one. Our codes are available at
https://github.com/ZHUANGHP/Analytic-continual-learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SeNM-VAE: Semi-Supervised Noise Modeling with Hierarchical Variational
  Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dihan Zheng, Yihang Zou, Xiaowen Zhang, Chenglong Bao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The data bottleneck has emerged as a fundamental challenge in learning based
image restoration methods. Researchers have attempted to generate synthesized
training data using paired or unpaired samples to address this challenge. This
study proposes SeNM-VAE, a semi-supervised noise modeling method that leverages
both paired and unpaired datasets to generate realistic degraded data. Our
approach is based on modeling the conditional distribution of degraded and
clean images with a specially designed graphical model. Under the variational
inference framework, we develop an objective function for handling both paired
and unpaired data. We employ our method to generate paired training samples for
real-world image denoising and super-resolution tasks. Our approach excels in
the quality of synthetic degraded images compared to other unpaired and paired
noise modeling methods. Furthermore, our approach demonstrates remarkable
performance in downstream image restoration tasks, even with limited paired
data. With more paired data, our method achieves the best performance on the
SIDD dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sharing the Cost of Success: A Game for Evaluating and Learning
  Collaborative Multi-Agent Instruction Giving and Following Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Sadler, Sherzod Hakimov, David Schlangen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In collaborative goal-oriented settings, the participants are not only
interested in achieving a successful outcome, but do also implicitly negotiate
the effort they put into the interaction (by adapting to each other). In this
work, we propose a challenging interactive reference game that requires two
players to coordinate on vision and language observations. The learning signal
in this game is a score (given after playing) that takes into account the
achieved goal and the players' assumed efforts during the interaction. We show
that a standard Proximal Policy Optimization (PPO) setup achieves a high
success rate when bootstrapped with heuristic partner behaviors that implement
insights from the analysis of human-human interactions. And we find that a
pairing of neural partners indeed reduces the measured joint effort when
playing together repeatedly. However, we observe that in comparison to a
reasonable heuristic pairing there is still room for improvement -- which
invites further research in the direction of cost-sharing in collaborative
interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dr.Hair: Reconstructing Scalp-Connected Hair Strands without
  Pre-training via Differentiable Rendering of Line Segments <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Takimoto, Hikari Takehara, Hiroyuki Sato, Zihao Zhu, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the film and gaming industries, achieving a realistic hair appearance
typically involves the use of strands originating from the scalp. However,
reconstructing these strands from observed surface images of hair presents
significant challenges. The difficulty in acquiring Ground Truth (GT) data has
led state-of-the-art learning-based methods to rely on pre-training with
manually prepared synthetic CG data. This process is not only labor-intensive
and costly but also introduces complications due to the domain gap when
compared to real-world data. In this study, we propose an optimization-based
approach that eliminates the need for pre-training. Our method represents hair
strands as line segments growing from the scalp and optimizes them using a
novel differentiable rendering algorithm. To robustly optimize a substantial
number of slender explicit geometries, we introduce 3D orientation estimation
utilizing global optimization, strand initialization based on Laplace's
equation, and reparameterization that leverages geometric connectivity and
spatial proximity. Unlike existing optimization-based methods, our method is
capable of reconstructing internal hair flow in an absolute direction. Our
method exhibits robust and accurate inverse rendering, surpassing the quality
of existing methods and significantly improving processing speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffGaze: A Diffusion Model for Continuous Gaze Sequence Generation on
  360° Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuhan Jiao, Yao Wang, Guanhua Zhang, Mihai Bâce, Zhiming Hu, Andreas Bulling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DiffGaze, a novel method for generating realistic and diverse
continuous human gaze sequences on 360{\deg} images based on a conditional
score-based denoising diffusion model. Generating human gaze on 360{\deg}
images is important for various human-computer interaction and computer
graphics applications, e.g. for creating large-scale eye tracking datasets or
for realistic animation of virtual humans. However, existing methods are
limited to predicting discrete fixation sequences or aggregated saliency maps,
thereby neglecting crucial parts of natural gaze behaviour. Our method uses
features extracted from 360{\deg} images as condition and uses two transformers
to model the temporal and spatial dependencies of continuous human gaze. We
evaluate DiffGaze on two 360{\deg} image benchmarks for gaze sequence
generation as well as scanpath prediction and saliency prediction. Our
evaluations show that DiffGaze outperforms state-of-the-art methods on all
tasks on both benchmarks. We also report a 21-participant user study showing
that our method generates gaze sequences that are indistinguishable from real
human sequences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated
  Image Detection <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunpeng Luo, Junlong Du, Ke Yan, Shouhong Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of Diffusion Models has dramatically improved image generation
quality, making it increasingly difficult to differentiate between real and
generated images. This development, while impressive, also raises significant
privacy and security concerns. In response to this, we propose a novel Latent
REconstruction error guided feature REfinement method (LaRE^2) for detecting
the diffusion-generated images. We come up with the Latent Reconstruction Error
(LaRE), the first reconstruction-error based feature in the latent space for
generated image detection. LaRE surpasses existing methods in terms of feature
extraction efficiency while preserving crucial cues required to differentiate
between the real and the fake. To exploit LaRE, we propose an Error-Guided
feature REfinement module (EGRE), which can refine the image feature guided by
LaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an
align-then-refine mechanism, which effectively refines the image feature for
generated-image detection from both spatial and channel perspectives. Extensive
experiments on the large-scale GenImage benchmark demonstrate the superiority
of our LaRE^2, which surpasses the best SoTA method by up to 11.9%/12.1%
average ACC/AP across 8 different image generators. LaRE also surpasses
existing methods in terms of feature extraction cost, delivering an impressive
speed enhancement of 8 times.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building Bridges across Spatial and Temporal Resolutions:
  Reference-Based Super-Resolution via Change Priors and Conditional Diffusion
  Model <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runmin Dong, Shuai Yuan, Bin Luo, Mengxuan Chen, Jinxiao Zhang, Lixian Zhang, Weijia Li, Juepeng Zheng, Haohuan Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reference-based super-resolution (RefSR) has the potential to build bridges
across spatial and temporal resolutions of remote sensing images. However,
existing RefSR methods are limited by the faithfulness of content
reconstruction and the effectiveness of texture transfer in large scaling
factors. Conditional diffusion models have opened up new opportunities for
generating realistic high-resolution images, but effectively utilizing
reference images within these models remains an area for further exploration.
Furthermore, content fidelity is difficult to guarantee in areas without
relevant reference information. To solve these issues, we propose a
change-aware diffusion model named Ref-Diff for RefSR, using the land cover
change priors to guide the denoising process explicitly. Specifically, we
inject the priors into the denoising model to improve the utilization of
reference information in unchanged areas and regulate the reconstruction of
semantically relevant content in changed areas. With this powerful guidance, we
decouple the semantics-guided denoising and reference texture-guided denoising
processes to improve the model performance. Extensive experiments demonstrate
the superior effectiveness and robustness of the proposed method compared with
state-of-the-art RefSR methods in both quantitative and qualitative
evaluations. The code and data are available at
https://github.com/dongrunmin/RefDiff.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chain of Compression: A Systematic Approach to Combinationally Compress
  Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingtao Shen, Minqing Sun, Jie Zhao, An Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) have achieved significant popularity,
but their computational and memory intensity poses challenges for
resource-constrained computing systems, particularly with the prerequisite of
real-time performance. To release this burden, model compression has become an
important research focus. Many approaches like quantization, pruning, early
exit, and knowledge distillation have demonstrated the effect of reducing
redundancy in neural networks. Upon closer examination, it becomes apparent
that each approach capitalizes on its unique features to compress the neural
network, and they can also exhibit complementary behavior when combined. To
explore the interactions and reap the benefits from the complementary features,
we propose the Chain of Compression, which works on the combinational sequence
to apply these common techniques to compress the neural network. Validated on
the image-based regression and classification networks across different data
sets, our proposed Chain of Compression can significantly compress the
computation cost by 100-1000 times with ignorable accuracy loss compared with
the baseline model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Mamba Sequence Model and Hierarchical Upsampling Network for
  Accurate Semantic Segmentation of Multiple Sclerosis Legion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazi Shahriar Sanjid, Md. Tanzim Hossain, Md. Shakib Shahariar Junayed, Dr. Mohammad Monir Uddin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating components from convolutional neural networks and state space
models in medical image segmentation presents a compelling approach to enhance
accuracy and efficiency. We introduce Mamba HUNet, a novel architecture
tailored for robust and efficient segmentation tasks. Leveraging strengths from
Mamba UNet and the lighter version of Hierarchical Upsampling Network (HUNet),
Mamba HUNet combines convolutional neural networks local feature extraction
power with state space models long range dependency modeling capabilities. We
first converted HUNet into a lighter version, maintaining performance parity
and then integrated this lighter HUNet into Mamba HUNet, further enhancing its
efficiency. The architecture partitions input grayscale images into patches,
transforming them into 1D sequences for processing efficiency akin to Vision
Transformers and Mamba models. Through Visual State Space blocks and patch
merging layers, hierarchical features are extracted while preserving spatial
information. Experimental results on publicly available Magnetic Resonance
Imaging scans, notably in Multiple Sclerosis lesion segmentation, demonstrate
Mamba HUNet's effectiveness across diverse segmentation tasks. The model's
robustness and flexibility underscore its potential in handling complex
anatomical structures. These findings establish Mamba HUNet as a promising
solution in advancing medical image segmentation, with implications for
improving clinical decision making processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Test-time Adaptation Meets Image Enhancement: Improving Accuracy via
  Uncertainty-aware Logit Switching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shohei Enomoto, Naoya Hasegawa, Kazuki Adachi, Taku Sasaki, Shin'ya Yamaguchi, Satoshi Suzuki, Takeharu Eda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have achieved remarkable success in a variety of
computer vision applications. However, there is a problem of degrading accuracy
when the data distribution shifts between training and testing. As a solution
of this problem, Test-time Adaptation~(TTA) has been well studied because of
its practicality. Although TTA methods increase accuracy under distribution
shift by updating the model at test time, using high-uncertainty predictions is
known to degrade accuracy. Since the input image is the root of the
distribution shift, we incorporate a new perspective on enhancing the input
image into TTA methods to reduce the prediction's uncertainty. We hypothesize
that enhancing the input image reduces prediction's uncertainty and increase
the accuracy of TTA methods. On the basis of our hypothesis, we propose a novel
method: Test-time Enhancer and Classifier Adaptation~(TECA). In TECA, the
classification model is combined with the image enhancement model that
transforms input images into recognition-friendly ones, and these models are
updated by existing TTA methods. Furthermore, we found that the prediction from
the enhanced image does not always have lower uncertainty than the prediction
from the original image. Thus, we propose logit switching, which compares the
uncertainty measure of these predictions and outputs the lower one. In our
experiments, we evaluate TECA with various TTA methods and show that TECA
reduces prediction's uncertainty and increases accuracy of TTA methods despite
having no hyperparameters and little parameter overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IJCNN2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse
  Diffusion <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihyun Lee, Shunsuke Saito, Giljoo Nam, Minhyuk Sung, Tae-Kyun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present InterHandGen, a novel framework that learns the generative prior
of two-hand interaction. Sampling from our model yields plausible and diverse
two-hand shapes in close interaction with or without an object. Our prior can
be incorporated into any optimization or learning methods to reduce ambiguity
in an ill-posed setup. Our key observation is that directly modeling the joint
distribution of multiple instances imposes high learning complexity due to its
combinatorial nature. Thus, we propose to decompose the modeling of joint
distribution into the modeling of factored unconditional and conditional single
instance distribution. In particular, we introduce a diffusion model that
learns the single-hand distribution unconditional and conditional to another
hand via conditioning dropout. For sampling, we combine anti-penetration and
classifier-free guidance to enable plausible generation. Furthermore, we
establish the rigorous evaluation protocol of two-hand synthesis, where our
method significantly outperforms baseline generative models in terms of
plausibility and diversity. We also demonstrate that our diffusion prior can
boost the performance of two-hand reconstruction from monocular in-the-wild
images, achieving new state-of-the-art accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024, project page:
  https://jyunlee.github.io/projects/interhandgen/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Visually Localize Sound Sources from Mixtures without Prior
  Source Knowledge <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongjin Kim, Sung Jin Um, Sangmin Lee, Jung Uk Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of the multi-sound source localization task is to localize sound
sources from the mixture individually. While recent multi-sound source
localization methods have shown improved performance, they face challenges due
to their reliance on prior information about the number of objects to be
separated. In this paper, to overcome this limitation, we present a novel
multi-sound source localization method that can perform localization without
prior knowledge of the number of sound sources. To achieve this goal, we
propose an iterative object identification (IOI) module, which can recognize
sound-making objects in an iterative manner. After finding the regions of
sound-making objects, we devise object similarity-aware clustering (OSC) loss
to guide the IOI module to effectively combine regions of the same object but
also distinguish between different objects and backgrounds. It enables our
method to perform accurate localization of sound-making objects without any
prior knowledge. Extensive experimental results on the MUSIC and VGGSound
benchmarks show the significant performance improvements of the proposed method
over the existing methods for both single and multi-source. Our code is
available at: https://github.com/VisualAIKHU/NoPrior_MultiSSL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Clustering based Visual Representation Learning <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guikun Chen, Xia Li, Yi Yang, Wenguan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate a fundamental aspect of machine vision: the measurement of
features, by revisiting clustering, one of the most classic approaches in
machine learning and data analysis. Existing visual feature extractors,
including ConvNets, ViTs, and MLPs, represent an image as rectangular regions.
Though prevalent, such a grid-style paradigm is built upon engineering practice
and lacks explicit modeling of data distribution. In this work, we propose
feature extraction with clustering (FEC), a conceptually elegant yet
surprisingly ad-hoc interpretable neural clustering framework, which views
feature extraction as a process of selecting representatives from data and thus
automatically captures the underlying data distribution. Given an image, FEC
alternates between grouping pixels into individual clusters to abstract
representatives and updating the deep features of pixels with current
representatives. Such an iterative working mechanism is implemented in the form
of several neural layers and the final representatives can be used for
downstream tasks. The cluster assignments across layers, which can be viewed
and inspected by humans, make the forward process of FEC fully transparent and
empower it with promising ad-hoc interpretability. Extensive experiments on
various visual recognition models and tasks verify the effectiveness,
generality, and interpretability of FEC. We expect this work will provoke a
rethink of the current de facto grid-style paradigm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Code: https://github.com/guikunchen/FEC/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SSF3D: Strict Semi-Supervised 3D Object Detection with Switching Filter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songbur Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SSF3D modified the semi-supervised 3D object detection (SS3DOD) framework,
which designed specifically for point cloud data. Leveraging the
characteristics of non-coincidence and weak correlation of target objects in
point cloud, we adopt a strategy of retaining only the truth-determining pseudo
labels and trimming the other fuzzy labels with points, instead of pursuing a
balance between the quantity and quality of pseudo labels. Besides, we notice
that changing the filter will make the model meet different distributed
targets, which is beneficial to break the training bottleneck. Two mechanism
are introduced to achieve above ideas: strict threshold and filter switching.
The experiments are conducted to analyze the effectiveness of above approaches
and their impact on the overall performance of the system. Evaluating on the
KITTI dataset, SSF3D exhibits superior performance compared to the current
state-of-the-art methods. The code will be released here.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoupled Pseudo-labeling for Semi-Supervised Monocular 3D Object
  Detection <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Zhang, Jiaming Li, Xiangru Lin, Wei Zhang, Xiao Tan, Junyu Han, Errui Ding, Jingdong Wang, Guanbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We delve into pseudo-labeling for semi-supervised monocular 3D object
detection (SSM3OD) and discover two primary issues: a misalignment between the
prediction quality of 3D and 2D attributes and the tendency of depth
supervision derived from pseudo-labels to be noisy, leading to significant
optimization conflicts with other reliable forms of supervision. We introduce a
novel decoupled pseudo-labeling (DPL) approach for SSM3OD. Our approach
features a Decoupled Pseudo-label Generation (DPG) module, designed to
efficiently generate pseudo-labels by separately processing 2D and 3D
attributes. This module incorporates a unique homography-based method for
identifying dependable pseudo-labels in BEV space, specifically for 3D
attributes. Additionally, we present a DepthGradient Projection (DGP) module to
mitigate optimization conflicts caused by noisy depth supervision of
pseudo-labels, effectively decoupling the depth gradient and removing
conflicting gradients. This dual decoupling strategy-at both the pseudo-label
generation and gradient levels-significantly improves the utilization of
pseudo-labels in SSM3OD. Our comprehensive experiments on the KITTI benchmark
demonstrate the superiority of our method over existing approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok Jang, Jungwoo Kim, SeonHwa Kim, Hyun Hee Park, Kyong Hwan Jin, Seungryong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have demonstrated that diffusion models are capable of
generating high-quality samples, but their quality heavily depends on sampling
guidance techniques, such as classifier guidance (CG) and classifier-free
guidance (CFG). These techniques are often not applicable in unconditional
generation or in various downstream tasks such as image restoration. In this
paper, we propose a novel sampling guidance, called Perturbed-Attention
Guidance (PAG), which improves diffusion sample quality across both
unconditional and conditional settings, achieving this without requiring
additional training or the integration of external modules. PAG is designed to
progressively enhance the structure of samples throughout the denoising
process. It involves generating intermediate samples with degraded structure by
substituting selected self-attention maps in diffusion U-Net with an identity
matrix, by considering the self-attention mechanisms' ability to capture
structural information, and guiding the denoising process away from these
degraded samples. In both ADM and Stable Diffusion, PAG surprisingly improves
sample quality in conditional and even unconditional scenarios. Moreover, PAG
significantly improves the baseline performance in various downstream tasks
where existing guidances such as CG or CFG cannot be fully utilized, including
ControlNet with empty prompts and image restoration such as inpainting and
deblurring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page is available at
  https://ku-cvlab.github.io/Perturbed-Attention-Guidance</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiVa-360: The Dynamic Visual <span class="highlight-title">Dataset</span> for Immersive Neural Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.16897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.16897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng-You Lu, Peisen Zhou, Angela Xing, Chandradeep Pokhariya, Arnab Dey, Ishaan Shah, Rugved Mavidipalli, Dylan Hu, Andrew Comport, Kefan Chen, Srinath Sridhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in neural fields are enabling high-fidelity capture of the shape and
appearance of dynamic 3D scenes. However, their capabilities lag behind those
offered by conventional representations such as 2D videos because of
algorithmic challenges and the lack of large-scale multi-view real-world
datasets. We address the dataset limitation with DiVa-360, a real-world 360
dynamic visual dataset that contains synchronized high-resolution and
long-duration multi-view video sequences of table-scale scenes captured using a
customized low-cost system with 53 cameras. It contains 21 object-centric
sequences categorized by different motion types, 25 intricate hand-object
interaction sequences, and 8 long-duration sequences for a total of 17.4 M
image frames. In addition, we provide foreground-background segmentation masks,
synchronized audio, and text descriptions. We benchmark the state-of-the-art
dynamic neural field methods on DiVa-360 and provide insights about existing
methods and future challenges on long-duration neural field capture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HoloVIC: Large-scale <span class="highlight-title">Dataset</span> and Benchmark for Multi-Sensor Holographic
  Intersection and Vehicle-Infrastructure Cooperative <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02640v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02640v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Ma, Lei Qiao, Chengkai Zhu, Kai Liu, Zelong Kong, Qing Li, Xueqi Zhou, Yuheng Kan, Wei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicle-to-everything (V2X) is a popular topic in the field of Autonomous
Driving in recent years. Vehicle-infrastructure cooperation (VIC) becomes one
of the important research area. Due to the complexity of traffic conditions
such as blind spots and occlusion, it greatly limits the perception
capabilities of single-view roadside sensing systems. To further enhance the
accuracy of roadside perception and provide better information to the vehicle
side, in this paper, we constructed holographic intersections with various
layouts to build a large-scale multi-sensor holographic vehicle-infrastructure
cooperation dataset, called HoloVIC. Our dataset includes 3 different types of
sensors (Camera, Lidar, Fisheye) and employs 4 sensor-layouts based on the
different intersections. Each intersection is equipped with 6-18 sensors to
capture synchronous data. While autonomous vehicles pass through these
intersections for collecting VIC data. HoloVIC contains in total on 100k+
synchronous frames from different sensors. Additionally, we annotated 3D
bounding boxes based on Camera, Fisheye, and Lidar. We also associate the IDs
of the same objects across different devices and consecutive frames in
sequence. Based on HoloVIC, we formulated four tasks to facilitate the
development of related research. We also provide benchmarks for these tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to CVPR 2024, Benchmark Website: https://holovic.net</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06003v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06003v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linus Franke, Darius Rückert, Laura Fink, Marc Stamminger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point-based radiance field rendering has demonstrated impressive results for
novel view synthesis, offering a compelling blend of rendering quality and
computational efficiency. However, also latest approaches in this domain are
not without their shortcomings. 3D Gaussian Splatting [Kerbl and Kopanas et al.
2023] struggles when tasked with rendering highly detailed scenes, due to
blurring and cloudy artifacts. On the other hand, ADOP [R\"uckert et al. 2022]
can accommodate crisper images, but the neural reconstruction network decreases
performance, it grapples with temporal instability and it is unable to
effectively address large gaps in the point cloud.
  In this paper, we present TRIPS (Trilinear Point Splatting), an approach that
combines ideas from both Gaussian Splatting and ADOP. The fundamental concept
behind our novel technique involves rasterizing points into a screen-space
image pyramid, with the selection of the pyramid layer determined by the
projected point size. This approach allows rendering arbitrarily large points
using a single trilinear write. A lightweight neural network is then used to
reconstruct a hole-free image including detail beyond splat resolution.
Importantly, our render pipeline is entirely differentiable, allowing for
automatic optimization of both point sizes and positions.
  Our evaluation demonstrate that TRIPS surpasses existing state-of-the-art
methods in terms of rendering quality while maintaining a real-time frame rate
of 60 frames per second on readily available hardware. This performance extends
to challenging scenarios, such as scenes featuring intricate geometry,
expansive landscapes, and auto-exposed footage.
  The project page is located at: https://lfranke.github.io/trips/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semi-Supervised Crowd Counting from Unlabeled Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.13969v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.13969v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Duan, Fan Wan, Rui Sun, Zeyu Wang, Varun Ojha, Yu Guan, Hubert P. H. Shum, Bingzhang Hu, Yang Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Crowd behavior analysis can be applied to effectively help the
daily transportation statistics and planning, which helps the smart city
construction. As one of the most important keys, crowd counting has drawn
increasing attention. Recent works achieved promising performance but relied on
the supervised paradigm with expensive crowd annotations. To alleviate the
annotation cost in real-world transportation scenarios, in this work we
proposed a semi-supervised learning framework $S^{4}\textit{Crowd}$, which can
leverage both unlabeled/labeled data for robust crowd counting. In the
unsupervised pathway, two \textit{self-supervised losses} were proposed to
simulate the crowd variations such as scale, illumination, based on which
supervised information pseudo labels were generated and gradually refined. We
also proposed a crowd-driven recurrent unit \textit{Gated-Crowd-Recurrent-Unit
(GCRU)}, which can preserve discriminant crowd information by extracting
second-order statistics, yielding pseudo labels with improved quality. A joint
loss including both unsupervised/supervised information was proposed, and a
dynamic weighting strategy was employed to balance the importance of the
unsupervised loss and supervised loss at different training stages. We
conducted extensive experiments on four popular crowd counting datasets in
semi-supervised settings. Experimental results supported the effectiveness of
each proposed component in our $S^{4}$Crowd framework. Our method achieved
competitive performance in semi-supervised learning approaches on these crowd
counting datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Pre-training for Localized Instruction Generation of Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil Batra, Davide Moltisanti, Laura Sevilla-Lara, Marcus Rohrbach, Frank Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedural videos show step-by-step demonstrations of tasks like recipe
preparation. Understanding such videos is challenging, involving the precise
localization of steps and the generation of textual instructions. Manually
annotating steps and writing instructions is costly, which limits the size of
current datasets and hinders effective learning. Leveraging large but noisy
video-transcript datasets for pre-training can boost performance, but demands
significant computational resources. Furthermore, transcripts contain
irrelevant content and exhibit style variation compared to instructions written
by human annotators. To mitigate both issues, we propose a technique,
Sieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters
irrelevant transcripts and (ii) Swap enhances the quality of the text
instruction by automatically replacing the transcripts with human-written
instructions from a text-only recipe dataset. The curated dataset, three orders
of magnitude smaller than current web-scale datasets, enables efficient
training of large-scale models with competitive performance. We complement our
Sieve-\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step
localization and instruction generation for procedural videos. When this model
is pre-trained on our curated dataset, it achieves state-of-the-art performance
in zero-shot and finetuning settings on YouCook2 and Tasty, while using a
fraction of the computational resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has some missing experiments and elaborative technical
  details</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SimLVSeg: Simplifying Left Ventricular Segmentation in 2D+Time
  Echocardiograms with Self- and Weakly-Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00454v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00454v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fadillah Maani, Asim Ukaye, Nada Saadi, Numan Saeed, Mohammad Yaqub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Echocardiography has become an indispensable clinical imaging modality for
general heart health assessment. From calculating biomarkers such as ejection
fraction to the probability of a patient's heart failure, accurate segmentation
of the heart structures allows doctors to assess the heart's condition and
devise treatments with greater precision and accuracy. However, achieving
accurate and reliable left ventricle segmentation is time-consuming and
challenging due to different reasons. Hence, clinicians often rely on
segmenting the left ventricular (LV) in two specific echocardiogram frames to
make a diagnosis. This limited coverage in manual LV segmentation poses a
challenge for developing automatic LV segmentation with high temporal
consistency, as the resulting dataset is typically annotated sparsely. In
response to this challenge, this work introduces SimLVSeg, a novel paradigm
that enables video-based networks for consistent LV segmentation from sparsely
annotated echocardiogram videos. SimLVSeg consists of self-supervised
pre-training with temporal masking, followed by weakly supervised learning
tailored for LV segmentation from sparse annotations. We demonstrate how
SimLVSeg outperforms the state-of-the-art solutions by achieving a 93.32%
(95%CI 93.21-93.43%) dice score on the largest 2D+time echocardiography dataset
(EchoNet-Dynamic) while being more efficient. SimLVSeg is compatible with two
types of video segmentation networks: 2D super image and 3D segmentation. To
show the effectiveness of our approach, we provide extensive ablation studies,
including pre-training settings and various deep learning backbones. We further
conduct an out-of-distribution test to showcase SimLVSeg's generalizability on
unseen distribution (CAMUS dataset). The code is publicly available at
https://github.com/fadamsyah/SimLVSeg.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HIMap: HybrId Representation Learning for End-to-end Vectorized HD Map
  Construction <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08639v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08639v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Zhou, Hui Zhang, Jiaqian Yu, Yifan Yang, Sangil Jung, Seung-In Park, ByungIn Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vectorized High-Definition (HD) map construction requires predictions of the
category and point coordinates of map elements (e.g. road boundary, lane
divider, pedestrian crossing, etc.). State-of-the-art methods are mainly based
on point-level representation learning for regressing accurate point
coordinates. However, this pipeline has limitations in obtaining element-level
information and handling element-level failures, e.g. erroneous element shape
or entanglement between elements. To tackle the above issues, we propose a
simple yet effective HybrId framework named HIMap to sufficiently learn and
interact both point-level and element-level information. Concretely, we
introduce a hybrid representation called HIQuery to represent all map elements,
and propose a point-element interactor to interactively extract and encode the
hybrid information of elements, e.g. point position and element shape, into the
HIQuery. Additionally, we present a point-element consistency constraint to
enhance the consistency between the point-level and element-level information.
Finally, the output point-element integrated HIQuery can be directly converted
into map elements' class, point coordinates, and mask. We conduct extensive
experiments and consistently outperform previous methods on both nuScenes and
Argoverse2 datasets. Notably, our method achieves $77.8$ mAP on the nuScenes
dataset, remarkably superior to previous SOTAs by $8.3$ mAP at least.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Semantic Reconstruction to Mitigate Hallucinations in
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16167v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16167v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minchan Kim, Minyeong Kim, Junik Bae, Suhwan Choi, Sungkyung Kim, Buru Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations in vision-language models pose a significant challenge to
their reliability, particularly in the generation of long captions. Current
methods fall short of accurately identifying and mitigating these
hallucinations. To address this issue, we introduce ESREAL, a novel
unsupervised learning framework designed to suppress the generation of
hallucinations through accurate localization and penalization of hallucinated
tokens. Initially, ESREAL creates a reconstructed image based on the generated
caption and aligns its corresponding regions with those of the original image.
This semantic reconstruction aids in identifying both the presence and type of
token-level hallucinations within the generated caption. Subsequently, ESREAL
computes token-level hallucination scores by assessing the semantic similarity
of aligned regions based on the type of hallucination. Finally, ESREAL employs
a proximal policy optimization algorithm, where it selectively penalizes
hallucinated tokens according to their token-level hallucination scores. Our
framework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2
by 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved
solely through signals derived from the image itself, without the need for any
image-text pairs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pushing Auto-regressive Models for 3D Shape Generation at Capacity and
  Scalability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12225v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12225v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuelin Qian, Yu Wang, Simian Luo, Yinda Zhang, Ying Tai, Zhenyu Zhang, Chengjie Wang, Xiangyang Xue, Bo Zhao, Tiejun Huang, Yunsheng Wu, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Auto-regressive models have achieved impressive results in 2D image
generation by modeling joint distributions in grid space. In this paper, we
extend auto-regressive models to 3D domains, and seek a stronger ability of 3D
shape generation by improving auto-regressive models at capacity and
scalability simultaneously. Firstly, we leverage an ensemble of publicly
available 3D datasets to facilitate the training of large-scale models. It
consists of a comprehensive collection of approximately 900,000 objects, with
multiple properties of meshes, points, voxels, rendered images, and text
captions. This diverse labeled dataset, termed Objaverse-Mix, empowers our
model to learn from a wide range of object variations. However, directly
applying 3D auto-regression encounters critical challenges of high
computational demands on volumetric grids and ambiguous auto-regressive order
along grid dimensions, resulting in inferior quality of 3D shapes. To this end,
we then present a novel framework Argus3D in terms of capacity. Concretely, our
approach introduces discrete representation learning based on a latent vector
instead of volumetric grids, which not only reduces computational costs but
also preserves essential geometric details by learning the joint distributions
in a more tractable order. The capacity of conditional generation can thus be
realized by simply concatenating various conditioning inputs to the latent
vector, such as point clouds, categories, images, and texts. In addition,
thanks to the simplicity of our model architecture, we naturally scale up our
approach to a larger model with an impressive 3.6 billion parameters, further
enhancing the quality of versatile 3D generation. Extensive experiments on four
generation tasks demonstrate that Argus3D can synthesize diverse and faithful
shapes across multiple categories, achieving remarkable performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://argus-3d.github.io/ . Datasets:
  https://huggingface.co/datasets/BAAI/Objaverse-MIX. arXiv admin note:
  substantial text overlap with arXiv:2303.14700</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReMoS: 3D Motion-Conditioned Reaction Synthesis for Two-Person
  Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17057v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17057v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, Philipp Slusallek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current approaches for 3D human motion synthesis generate high-quality
animations of digital humans performing a wide variety of actions and gestures.
However, a notable technological gap exists in addressing the complex dynamics
of multi-human interactions within this paradigm. In this work, we present
ReMoS, a denoising diffusion-based model that synthesizes full-body reactive
motion of a person in a two-person interaction scenario. Assuming the motion of
one person is given, we employ a combined spatio-temporal cross-attention
mechanism to synthesize the reactive body and hand motion of the second person,
thereby completing the interactions between the two. We demonstrate ReMoS
across challenging two-person scenarios such as pair-dancing, Ninjutsu,
kickboxing, and acrobatics, where one person's movements have complex and
diverse influences on the other. We also contribute the ReMoCap dataset for
two-person interactions containing full-body and finger motions. We evaluate
ReMoS through multiple quantitative metrics, qualitative visualizations, and a
user study, and also indicate usability in interactive motion editing
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15585v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15585v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mai A. Shaaban, Adnan Khan, Mohammad Yaqub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chest X-ray images are commonly used for predicting acute and chronic
cardiopulmonary conditions, but efforts to integrate them with structured
clinical data face challenges due to incomplete electronic health records
(EHR). This paper introduces \textbf{MedPromptX}, the first model to integrate
multimodal large language models (MLLMs), few-shot prompting (FP) and visual
grounding (VG) to combine imagery with EHR data for chest X-ray diagnosis. A
pre-trained MLLM is utilized to complement the missing EHR information,
providing a comprehensive understanding of patients' medical history.
Additionally, FP reduces the necessity for extensive training of MLLMs while
effectively tackling the issue of hallucination. Nevertheless, the process of
determining the optimal number of few-shot examples and selecting high-quality
candidates can be burdensome, yet it profoundly influences model performance.
Hence, we propose a new technique that dynamically refines few-shot data for
real-time adjustment to new patient scenarios. Moreover, VG aids in focusing
the model's attention on relevant regions of interest in X-ray images,
enhancing the identification of abnormalities. We release MedPromptX-VQA, a new
in-context visual question answering dataset encompassing interleaved image and
EHR data derived from MIMIC-IV and MIMIC-CXR databases. Results demonstrate the
SOTA performance of MedPromptX, achieving an 11% improvement in F1-score
compared to the baselines. Code and data are available at
https://github.com/BioMedIA-MBZUAI/MedPromptX
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-Guided Variational Image Generation for Industrial Anomaly
  Detection and Segmentation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Lee, Jongwon Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a text-guided variational image generation method to address the
challenge of getting clean data for anomaly detection in industrial
manufacturing. Our method utilizes text information about the target object,
learned from extensive text library documents, to generate non-defective data
images resembling the input image. The proposed framework ensures that the
generated non-defective images align with anticipated distributions derived
from textual and image-based knowledge, ensuring stability and generality.
Experimental results demonstrate the effectiveness of our approach, surpassing
previous methods even with limited non-defective data. Our approach is
validated through generalization tests across four baseline models and three
distinct datasets. We present an additional analysis to enhance the
effectiveness of anomaly detection models by utilizing the generated images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identity-aware Dual-constraint Network for Cloth-Changing Person
  Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peini Guo, Mengyuan Liu, Hong Liu, Ruijia Fan, Guoquan Wang, Bin He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloth-Changing Person Re-Identification (CC-ReID) aims to accurately identify
the target person in more realistic surveillance scenarios, where pedestrians
usually change their clothing. Despite great progress, limited cloth-changing
training samples in existing CC-ReID datasets still prevent the model from
adequately learning cloth-irrelevant features. In addition, due to the absence
of explicit supervision to keep the model constantly focused on
cloth-irrelevant areas, existing methods are still hampered by the disruption
of clothing variations. To solve the above issues, we propose an Identity-aware
Dual-constraint Network (IDNet) for the CC-ReID task. Specifically, to help the
model extract cloth-irrelevant clues, we propose a Clothes Diversity
Augmentation (CDA), which generates more realistic cloth-changing samples by
enriching the clothing color while preserving the texture. In addition, a
Multi-scale Constraint Block (MCB) is designed, which extracts fine-grained
identity-related features and effectively transfers cloth-irrelevant knowledge.
Moreover, a Counterfactual-guided Attention Module (CAM) is presented, which
learns cloth-irrelevant features from channel and space dimensions and utilizes
the counterfactual intervention for supervising the attention map to highlight
identity-related regions. Finally, a Semantic Alignment Constraint (SAC) is
designed to facilitate high-level semantic feature interaction. Comprehensive
experiments on four CC-ReID datasets indicate that our method outperforms prior
state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Pitfalls of Knowledge Editing for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02129v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02129v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the cost associated with fine-tuning Large Language Models (LLMs)
continues to rise, recent research efforts have pivoted towards developing
methodologies to edit implicit knowledge embedded within LLMs. Yet, there's
still a dark cloud lingering overhead -- will knowledge editing trigger
butterfly effect? since it is still unclear whether knowledge editing might
introduce side effects that pose potential risks or not. This paper pioneers
the investigation into the potential pitfalls associated with knowledge editing
for LLMs. To achieve this, we introduce new benchmark datasets and propose
innovative evaluation metrics. Our results underline two pivotal concerns: (1)
Knowledge Conflict: Editing groups of facts that logically clash can magnify
the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)
Knowledge Distortion: Altering parameters with the aim of editing factual
knowledge can irrevocably warp the innate knowledge structure of LLMs.
Experimental results vividly demonstrate that knowledge editing might
inadvertently cast a shadow of unintended consequences on LLMs, which warrant
attention and efforts for future works. Code and data are available at
https://github.com/zjunlp/PitfallsKnowledgeEditing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bi'an Du, Xiang Gao, Wei Hu, Renjie Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative 3D part assembly involves understanding part relationships and
predicting their 6-DoF poses for assembling a realistic 3D shape. Prior work
often focus on the geometry of individual parts, neglecting part-whole
hierarchies of objects. Leveraging two key observations: 1) super-part poses
provide strong hints about part poses, and 2) predicting super-part poses is
easier due to fewer superparts, we propose a part-whole-hierarchy message
passing network for efficient 3D part assembly. We first introduce super-parts
by grouping geometrically similar parts without any semantic labels. Then we
employ a part-whole hierarchical encoder, wherein a super-part encoder predicts
latent super-part poses based on input parts. Subsequently, we transform the
point cloud using the latent poses, feeding it to the part encoder for
aggregating super-part information and reasoning about part relationships to
predict all part poses. In training, only ground-truth part poses are required.
During inference, the predicted latent poses of super-parts enhance
interpretability. Experimental results on the PartNet dataset show that our
method achieves state-of-the-art performance in part and connectivity accuracy
and enables an interpretable hierarchical part assembly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InNeRF360: Text-Guided 3D-Consistent Object Inpainting on 360-degree
  Neural Radiance Fields <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15094v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15094v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqing Wang, Tong Zhang, Alaa Abboud, Sabine Süsstrunk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose InNeRF360, an automatic system that accurately removes
text-specified objects from 360-degree Neural Radiance Fields (NeRF). The
challenge is to effectively remove objects while inpainting perceptually
consistent content for the missing regions, which is particularly demanding for
existing NeRF models due to their implicit volumetric representation. Moreover,
unbounded scenes are more prone to floater artifacts in the inpainted region
than frontal-facing scenes, as the change of object appearance and background
across views is more sensitive to inaccurate segmentations and inconsistent
inpainting. With a trained NeRF and a text description, our method efficiently
removes specified objects and inpaints visually consistent content without
artifacts. We apply depth-space warping to enforce consistency across multiview
text-encoded segmentations, and then refine the inpainted NeRF model using
perceptual priors and 3D diffusion-based geometric priors to ensure visual
plausibility. Through extensive experiments in segmentation and inpainting on
360-degree and frontal-facing NeRFs, we show that our approach is effective and
enhances NeRF's editability. Project page: https://ivrl.github.io/InNeRF360.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Passive Non-Line-of-Sight Imaging with Light Transport Modulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16014v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16014v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Zhang, Ruixu Geng, Xiaolong Du, Yan Chen, Houqiang Li, Yang Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Passive non-line-of-sight (NLOS) imaging has witnessed rapid development in
recent years, due to its ability to image objects that are out of sight. The
light transport condition plays an important role in this task since changing
the conditions will lead to different imaging models. Existing learning-based
NLOS methods usually train independent models for different light transport
conditions, which is computationally inefficient and impairs the practicality
of the models. In this work, we propose NLOS-LTM, a novel passive NLOS imaging
method that effectively handles multiple light transport conditions with a
single network. We achieve this by inferring a latent light transport
representation from the projection image and using this representation to
modulate the network that reconstructs the hidden image from the projection
image. We train a light transport encoder together with a vector quantizer to
obtain the light transport representation. To further regulate this
representation, we jointly learn both the reconstruction network and the
reprojection network during training. A set of light transport modulation
blocks is used to modulate the two jointly trained networks in a multi-scale
way. Extensive experiments on a large-scale passive NLOS dataset demonstrate
the superiority of the proposed method. The code is available at
https://github.com/JerryOctopus/NLOS-LTM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ViT-Lens: Towards Omni-modal Representations <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16081v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16081v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixian Lei, Yixiao Ge, Kun Yi, Jianfeng Zhang, Difei Gao, Dylan Sun, Yuying Ge, Ying Shan, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aiming to advance AI agents, large foundation models significantly improve
reasoning and instruction execution, yet the current focus on vision and
language neglects the potential of perceiving diverse modalities in open-world
environments. However, the success of data-driven vision and language models is
costly or even infeasible to be reproduced for rare modalities. In this paper,
we present ViT-Lens-2 that facilitates efficient omni-modal representation
learning by perceiving novel modalities with a pretrained ViT and aligning them
to a pre-defined space. Specifically, the modality-specific lens is tuned to
project any-modal signals to an intermediate embedding space, which are then
processed by a strong ViT with pre-trained visual knowledge. The encoded
representations are optimized toward aligning with the modal-independent space,
pre-defined by off-the-shelf foundation models. ViT-Lens-2 provides a unified
solution for representation learning of increasing modalities with two
appealing advantages: (i) Unlocking the great potential of pretrained ViTs to
novel modalities effectively with efficient data regime; (ii) Enabling emergent
downstream capabilities through modality alignment and shared ViT parameters.
We tailor ViT-Lens-2 to learn representations for 3D point cloud, depth, audio,
tactile and EEG, and set new state-of-the-art results across various
understanding tasks, such as zero-shot classification. By seamlessly
integrating ViT-Lens-2 into Multimodal Foundation Models, we enable
Any-modality to Text and Image Generation in a zero-shot manner. Code and
models are available at https://github.com/TencentARC/ViT-Lens.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work is a follow-up of arXiv:2308.10185. Accepted to CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implicit Discriminative Knowledge Learning for Visible-Infrared Person
  Re-Identification <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11708v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11708v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaijie Ren, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visible-Infrared Person Re-identification (VI-ReID) is a challenging
cross-modal pedestrian retrieval task, due to significant intra-class
variations and cross-modal discrepancies among different cameras. Existing
works mainly focus on embedding images of different modalities into a unified
space to mine modality-shared features. They only seek distinctive information
within these shared features, while ignoring the identity-aware useful
information that is implicit in the modality-specific features. To address this
issue, we propose a novel Implicit Discriminative Knowledge Learning (IDKL)
network to uncover and leverage the implicit discriminative information
contained within the modality-specific. First, we extract modality-specific and
modality-shared features using a novel dual-stream network. Then, the
modality-specific features undergo purification to reduce their modality style
discrepancies while preserving identity-aware discriminative knowledge.
Subsequently, this kind of implicit knowledge is distilled into the
modality-shared feature to enhance its distinctiveness. Finally, an alignment
loss is proposed to minimize modality discrepancy on enhanced modality-shared
features. Extensive experiments on multiple public datasets demonstrate the
superiority of IDKL network over the state-of-the-art methods. Code is
available at https://github.com/1KK077/IDKL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In Search of a Data Transformation That Accelerates Neural Field
  Training <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17094v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17094v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junwon Seo, Sangyoon Lee, Kwang In Kim, Jaeho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural field is an emerging paradigm in data representation that trains a
neural network to approximate the given signal. A key obstacle that prevents
its widespread adoption is the encoding speed-generating neural fields requires
an overfitting of a neural network, which can take a significant number of SGD
steps to reach the desired fidelity level. In this paper, we delve into the
impacts of data transformations on the speed of neural field training,
specifically focusing on how permuting pixel locations affect the convergence
speed of SGD. Counterintuitively, we find that randomly permuting the pixel
locations can considerably accelerate the training. To explain this phenomenon,
we examine the neural field training through the lens of PSNR curves, loss
landscapes, and error patterns. Our analyses suggest that the random pixel
permutations remove the easy-to-fit patterns, which facilitate easy
optimization in the early stage but hinder capturing fine details of the
signal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation
  with Unified Audio-Visual Speech Representation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongsoo Choi, Se Jin Park, Minsu Kim, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech
Translation (AV2AV) framework, where the input and output of the system are
multimodal (i.e., audio and visual speech). With the proposed AV2AV, two key
advantages can be brought: 1) We can perform real-like conversations with
individuals worldwide in a virtual meeting by utilizing our own primary
languages. In contrast to Speech-to-Speech Translation (A2A), which solely
translates between audio modalities, the proposed AV2AV directly translates
between audio-visual speech. This capability enhances the dialogue experience
by presenting synchronized lip movements along with the translated speech. 2)
We can improve the robustness of the spoken language translation system. By
employing the complementary information of audio-visual speech, the system can
effectively translate spoken language even in the presence of acoustic noise,
showcasing robust performance. To mitigate the problem of the absence of a
parallel AV2AV translation dataset, we propose to train our spoken language
translation system with the audio-only dataset of A2A. This is done by learning
unified audio-visual speech representations through self-supervised learning in
advance to train the translation system. Moreover, we propose an AV-Renderer
that can generate raw audio and video in parallel. It is designed with
zero-shot speaker modeling, thus the speaker in source audio-visual speech can
be maintained at the target translated audio-visual speech. The effectiveness
of AV2AV is evaluated with extensive experiments in a many-to-many language
translation setting. Demo page is available on
https://choijeongsoo.github.io/av2av.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Code & Demo: https://choijeongsoo.github.io/av2av</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SINC: Spatial Composition of 3D Human Motions for Simultaneous Action
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.10417v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.10417v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikos Athanasiou, Mathis Petrovich, Michael J. Black, Gül Varol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our goal is to synthesize 3D human motions given textual inputs describing
simultaneous actions, for example 'waving hand' while 'walking' at the same
time. We refer to generating such simultaneous movements as performing 'spatial
compositions'. In contrast to temporal compositions that seek to transition
from one action to another, spatial compositing requires understanding which
body parts are involved in which action, to be able to move them
simultaneously. Motivated by the observation that the correspondence between
actions and body parts is encoded in powerful language models, we extract this
knowledge by prompting GPT-3 with text such as "what are the body parts
involved in the action <action name>?", while also providing the parts list and
few-shot examples. Given this action-part mapping, we combine body parts from
two motions together and establish the first automated method to spatially
compose two actions. However, training data with compositional actions is
always limited by the combinatorics. Hence, we further create synthetic data
with this approach, and use it to train a new state-of-the-art text-to-motion
generation model, called SINC ("SImultaneous actioN Compositions for 3D human
motions"). In our experiments, that training with such GPT-guided synthetic
data improves spatial composition generation over baselines. Our code is
publicly available at https://sinc.is.tue.mpg.de/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Teaser Fixed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Powerful Lossy Compression for Noisy Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14135v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14135v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shilv Cai, Xiaoguo Liang, Shuning Cao, Luxin Yan, Sheng Zhong, Liqun Chen, Xu Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image compression and denoising represent fundamental challenges in image
processing with many real-world applications. To address practical demands,
current solutions can be categorized into two main strategies: 1) sequential
method; and 2) joint method. However, sequential methods have the disadvantage
of error accumulation as there is information loss between multiple individual
models. Recently, the academic community began to make some attempts to tackle
this problem through end-to-end joint methods. Most of them ignore that
different regions of noisy images have different characteristics. To solve
these problems, in this paper, our proposed signal-to-noise ratio~(SNR) aware
joint solution exploits local and non-local features for image compression and
denoising simultaneously. We design an end-to-end trainable network, which
includes the main encoder branch, the guidance branch, and the signal-to-noise
ratio~(SNR) aware branch. We conducted extensive experiments on both synthetic
and real-world datasets, demonstrating that our joint solution outperforms
existing state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICME 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ViT-Lens: Initiating Omni-Modal Exploration through 3D Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixian Lei, Yixiao Ge, Jianfeng Zhang, Dylan Sun, Kun Yi, Ying Shan, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Though the success of CLIP-based training recipes in vision-language models,
their scalability to more modalities (e.g., 3D, audio, etc.) is limited to
large-scale data, which is expensive or even inapplicable for rare modalities.
In this paper, we present ViT-Lens that facilitates efficient omni-modal
representation learning by perceiving novel modalities with a pretrained ViT
and aligning to a pre-defined space. Specifically, the modality-specific lens
is tuned to project multimodal signals to the shared embedding space, which are
then processed by a strong ViT that carries pre-trained image knowledge. The
encoded multimodal representations are optimized toward aligning with the
modal-independent space, pre-defined by off-the-shelf foundation models. A
well-trained lens with a ViT backbone has the potential to serve as one of
these foundation models, supervising the learning of subsequent modalities.
ViT-Lens provides a unified solution for representation learning of increasing
modalities with two appealing benefits: (i) Exploiting the pretrained ViT
across tasks and domains effectively with efficient data regime; (ii) Emergent
downstream capabilities of novel modalities are demonstrated due to the
modality alignment space. We evaluate ViT-Lens in the context of 3D as an
initial verification. In zero-shot 3D classification, ViT-Lens achieves
substantial improvements over previous state-of-the-art, showing 52.0% accuracy
on Objaverse-LVIS, 87.4% on ModelNet40, and 60.6% on ScanObjectNN. Furthermore,
we enable zero-shot 3D question-answering by simply integrating the trained 3D
lens into the InstructBLIP model without any adaptation. We will release the
results of ViT-Lens on more modalities in the near future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4 figures and 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TP2O: Creative Text Pair-to-Object Generation using Balance
  Swap-Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01819v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01819v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Li, Zedong Zhang, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating creative combinatorial objects from two seemingly unrelated object
texts is a challenging task in text-to-image synthesis, often hindered by a
focus on emulating existing data distributions. In this paper, we develop a
straightforward yet highly effective method, called \textbf{balance
swap-sampling}. First, we propose a swapping mechanism that generates a novel
combinatorial object image set by randomly exchanging intrinsic elements of two
text embeddings through a cutting-edge diffusion model. Second, we introduce a
balance swapping region to efficiently sample a small subset from the newly
generated image set by balancing CLIP distances between the new images and
their original generations, increasing the likelihood of accepting the
high-quality combinations. Last, we employ a segmentation method to compare
CLIP distances among the segmented components, ultimately selecting the most
promising object from the sampled subset. Extensive experiments demonstrate
that our approach outperforms recent SOTA T2I methods. Surprisingly, our
results even rival those of human artists, such as frog-broccoli.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://tp2o.github.io/anon/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Segment and Caption Anything <span class="chip">CVPR 24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00869v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00869v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoke Huang, Jianfeng Wang, Yansong Tang, Zheng Zhang, Han Hu, Jiwen Lu, Lijuan Wang, Zicheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method to efficiently equip the Segment Anything Model (SAM)
with the ability to generate regional captions. SAM presents strong
generalizability to segment anything while is short for semantic understanding.
By introducing a lightweight query-based feature mixer, we align the
region-specific features with the embedding space of language models for later
caption generation. As the number of trainable parameters is small (typically
in the order of tens of millions), it costs less computation, less memory
usage, and less communication bandwidth, resulting in both fast and scalable
training. To address the scarcity problem of regional caption data, we propose
to first pre-train our model on objection detection and segmentation tasks. We
call this step weak supervision pretraining since the pre-training data only
contains category names instead of full-sentence descriptions. The weak
supervision pretraining allows us to leverage many publicly available object
detection and segmentation datasets. We conduct extensive experiments to
demonstrate the superiority of our method and validate each design choice. This
work serves as a stepping stone towards scaling up regional captioning data and
sheds light on exploring efficient ways to augment SAM with regional semantics.
The project page, along with the associated code, can be accessed via
https://xk-huang.github.io/segment-caption-anything/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The project page, along with the associated code, can be accessed via
  https://xk-huang.github.io/segment-caption-anything/; Update author
  information; Accepted by CVPR 24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TagAlign: Improving Vision-Language Alignment with Multi-Tag
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14149v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14149v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinying Liu, Wei Wu, Kecheng Zheng, Zhan Tong, Jiawei Liu, Yu Liu, Wei Chen, Zilei Wang, Yujun Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The crux of learning vision-language models is to extract semantically
aligned information from visual and linguistic data. Existing attempts usually
face the problem of coarse alignment, e.g., the vision encoder struggles in
localizing an attribute-specified object. In this work, we propose an
embarrassingly simple approach to better align image and text features with no
need of additional data formats other than image-text pairs. Concretely, given
an image and its paired text, we manage to parse objects (e.g., cat) and
attributes (e.g., black) from the description, which are highly likely to exist
in the image. It is noteworthy that the parsing pipeline is fully automatic and
thus enjoys good scalability. With these parsed semantics as supervision
signals, we can complement the commonly used image-text contrastive loss with
the multi-tag classification loss. Extensive experimental results on a broad
suite of semantic segmentation datasets substantiate the average 5.2\%
improvement of our framework over existing alternatives. Furthermore, the
visualization results indicate that attribute supervision makes vision-language
models accurately localize attribute-specified objects. Project page can be
found at https://qinying-liu.github.io/Tag-Align.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SGS-<span class="highlight-title">SLAM</span>: Semantic Gaussian Splatting For Neural Dense <span class="highlight-title">SLAM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03246v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03246v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingrui Li, Shuhong Liu, Heng Zhou, Guohao Zhu, Na Cheng, Tianchen Deng, Hongyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SGS-SLAM, the first semantic visual SLAM system based on Gaussian
Splatting. It incorporates appearance, geometry, and semantic features through
multi-channel optimization, addressing the oversmoothing limitations of neural
implicit SLAM systems in high-quality rendering, scene understanding, and
object-level geometry. We introduce a unique semantic feature loss that
effectively compensates for the shortcomings of traditional depth and color
losses in object optimization. Through a semantic-guided keyframe selection
strategy, we prevent erroneous reconstructions caused by cumulative errors.
Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art
performance in camera pose estimation, map reconstruction, precise semantic
segmentation, and object-level geometric accuracy, while ensuring real-time
rendering capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ArtAdapter: Text-to-Image Style Transfer using Multi-Level Style Encoder
  and Explicit Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dar-Yen Chen, Hamish Tennent, Ching-Wen Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces ArtAdapter, a transformative text-to-image (T2I) style
transfer framework that transcends traditional limitations of color,
brushstrokes, and object shape, capturing high-level style elements such as
composition and distinctive artistic expression. The integration of a
multi-level style encoder with our proposed explicit adaptation mechanism
enables ArtAdapter to achieve unprecedented fidelity in style transfer,
ensuring close alignment with textual descriptions. Additionally, the
incorporation of an Auxiliary Content Adapter (ACA) effectively separates
content from style, alleviating the borrowing of content from style references.
Moreover, our novel fast finetuning approach could further enhance zero-shot
style representation while mitigating the risk of overfitting. Comprehensive
evaluations confirm that ArtAdapter surpasses current state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Clean-image Backdoor Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15010v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15010v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dazhong Rong, Guoyao Yu, Shuheng Shen, Xinyi Fu, Peng Qian, Jianhai Chen, Qinming He, Xing Fu, Weiqiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To gather a significant quantity of annotated training data for
high-performance image classification models, numerous companies opt to enlist
third-party providers to label their unlabeled data. This practice is widely
regarded as secure, even in cases where some annotated errors occur, as the
impact of these minor inaccuracies on the final performance of the models is
negligible and existing backdoor attacks require attacker's ability to poison
the training images. Nevertheless, in this paper, we propose clean-image
backdoor attacks which uncover that backdoors can still be injected via a
fraction of incorrect labels without modifying the training images.
Specifically, in our attacks, the attacker first seeks a trigger feature to
divide the training images into two parts: those with the feature and those
without it. Subsequently, the attacker falsifies the labels of the former part
to a backdoor class. The backdoor will be finally implanted into the target
model after it is trained on the poisoned data. During the inference phase, the
attacker can activate the backdoor in two ways: slightly modifying the input
image to obtain the trigger feature, or taking an image that naturally has the
trigger feature as input. We conduct extensive experiments to demonstrate the
effectiveness and practicality of our attacks. According to the experimental
results, we conclude that our attacks seriously jeopardize the fairness and
robustness of image classification models, and it is necessary to be vigilant
about the incorrect labels in outsourced labeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transferring Relative Monocular Depth to Surgical Vision with Temporal
  Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06683v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06683v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charlie Budd, Tom Vercauteren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relative monocular depth, inferring depth up to shift and scale from a single
image, is an active research topic. Recent deep learning models, trained on
large and varied meta-datasets, now provide excellent performance in the domain
of natural images. However, few datasets exist which provide ground truth depth
for endoscopic images, making training such models from scratch unfeasible.
This work investigates the transfer of these models into the surgical domain,
and presents an effective and simple way to improve on standard supervision
through the use of temporal consistency self-supervision. We show temporal
consistency significantly improves supervised training alone when transferring
to the low-data regime of endoscopy, and outperforms the prevalent
self-supervision technique for this task. In addition we show our method
drastically outperforms the state-of-the-art method from within the domain of
endoscopy. We also release our code, model and ensembled meta-dataset,
Meta-MED, establishing a strong benchmark for future work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Source-free Domain Adaptive Semantic Segmentation via
  Importance-aware and Prototype-contrast Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01598v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01598v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Cao, Hui Zhang, Xiao Lu, Zheng Xiao, Kailun Yang, Yaonan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain adaptive semantic segmentation enables robust pixel-wise understanding
in real-world driving scenes. Source-free domain adaptation, as a more
practical technique, addresses the concerns of data privacy and storage
limitations in typical unsupervised domain adaptation methods, making it
especially relevant in the context of intelligent vehicles. It utilizes a
well-trained source model and unlabeled target data to achieve adaptation in
the target domain. However, in the absence of source data and target labels,
current solutions cannot sufficiently reduce the impact of domain shift and
fully leverage the information from the target data. In this paper, we propose
an end-to-end source-free domain adaptation semantic segmentation method via
Importance-Aware and Prototype-Contrast (IAPC) learning. The proposed IAPC
framework effectively extracts domain-invariant knowledge from the well-trained
source model and learns domain-specific knowledge from the unlabeled target
domain. Specifically, considering the problem of domain shift in the prediction
of the target domain by the source model, we put forward an importance-aware
mechanism for the biased target prediction probability distribution to extract
domain-invariant knowledge from the source model. We further introduce a
prototype-contrast strategy, which includes a prototype-symmetric cross-entropy
loss and a prototype-enhanced cross-entropy loss, to learn target intra-domain
knowledge without relying on labels. A comprehensive variety of experiments on
two domain adaptive semantic segmentation benchmarks demonstrates that the
proposed end-to-end IAPC solution outperforms existing state-of-the-art
methods. The source code is publicly available at
https://github.com/yihong-97/Source-free-IAPC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Intelligent Vehicles (T-IV). The
  source code is publicly available at
  https://github.com/yihong-97/Source-free-IAPC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SD4Match: Learning to Prompt Stable Diffusion Model for Semantic
  Matching <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17569v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17569v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghui Li, Jingyi Lu, Kai Han, Victor Prisacariu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the challenge of matching semantically similar
keypoints across image pairs. Existing research indicates that the intermediate
output of the UNet within the Stable Diffusion (SD) can serve as robust image
feature maps for such a matching task. We demonstrate that by employing a basic
prompt tuning technique, the inherent potential of Stable Diffusion can be
harnessed, resulting in a significant enhancement in accuracy over previous
approaches. We further introduce a novel conditional prompting module that
conditions the prompt on the local details of the input image pairs, leading to
a further improvement in performance. We designate our approach as SD4Match,
short for Stable Diffusion for Semantic Matching. Comprehensive evaluations of
SD4Match on the PF-Pascal, PF-Willow, and SPair-71k datasets show that it sets
new benchmarks in accuracy across all these datasets. Particularly, SD4Match
outperforms the previous state-of-the-art by a margin of 12 percentage points
on the challenging SPair-71k dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024. Project website:
  https://sd4match.active.vision/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ObjectCompose: Evaluating Resilience of Vision-Based Models on
  Object-to-Background Compositional Changes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04701v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04701v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hashmat Shadab Malik, Muhammad Huzaifa, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the large-scale multi-modal training of recent vision-based models and
their generalization capabilities, understanding the extent of their robustness
is critical for their real-world deployment. In this work, we evaluate the
resilience of current vision-based models against diverse object-to-background
context variations. The majority of robustness evaluation methods have
introduced synthetic datasets to induce changes to object characteristics
(viewpoints, scale, color) or utilized image transformation techniques
(adversarial changes, common corruptions) on real images to simulate shifts in
distributions. Recent works have explored leveraging large language models and
diffusion models to generate changes in the background. However, these methods
either lack in offering control over the changes to be made or distort the
object semantics, making them unsuitable for the task. Our method, on the other
hand, can induce diverse object-to-background changes while preserving the
original semantics and appearance of the object. To achieve this goal, we
harness the generative capabilities of text-to-image, image-to-text, and
image-to-segment models to automatically generate a broad spectrum of
object-to-background changes. We induce both natural and adversarial background
changes by either modifying the textual prompts or optimizing the latents and
textual embedding of text-to-image models. We produce various versions of
standard vision datasets (ImageNet, COCO), incorporating either diverse and
realistic backgrounds into the images or introducing color, texture, and
adversarial changes in the background. We conduct extensive experiment to
analyze the robustness of vision-based models against object-to-background
context variations across diverse tasks. Code
https://github.com/Muhammad-Huzaifaa/ObjectCompose.git
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motion Generation from Fine-grained Textual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunhang Li, Yansong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of text2motion is to generate human motion sequences from given
textual descriptions, where the model explores diverse mappings from natural
language instructions to human body movements. While most existing works are
confined to coarse-grained motion descriptions, e.g., "A man squats.",
fine-grained descriptions specifying movements of relevant body parts are
barely explored. Models trained with coarse-grained texts may not be able to
learn mappings from fine-grained motion-related words to motion primitives,
resulting in the failure to generate motions from unseen descriptions. In this
paper, we build a large-scale language-motion dataset specializing in
fine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with
step-by-step instructions with pseudo-code compulsory checks. Accordingly, we
design a new text2motion model, FineMotionDiffuse, making full use of
fine-grained textual information. Our quantitative evaluation shows that
FineMotionDiffuse trained on FineHumanML3D improves FID by a large margin of
0.38, compared with competitive baselines. According to the qualitative
evaluation and case study, our model outperforms MotionDiffuse in generating
spatially or chronologically composite motions, by learning the implicit
mappings from fine-grained descriptions to the corresponding basic motions. We
release our data at https://github.com/KunhangL/finemotiondiffuse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Low-Energy Adaptive Personalization for Resource-Constrained
  Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushan Huang, Josh Millar, Yuxuan Long, Yuchen Zhao, Hamed Hadaddi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The personalization of machine learning (ML) models to address data drift is
a significant challenge in the context of Internet of Things (IoT)
applications. Presently, most approaches focus on fine-tuning either the full
base model or its last few layers to adapt to new data, while often neglecting
energy costs. However, various types of data drift exist, and fine-tuning the
full base model or the last few layers may not result in optimal performance in
certain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energy
adaptive personalization framework designed for resource-constrained devices.
We categorize data drift and personalization into three types: input-level,
feature-level, and output-level. For each type, we fine-tune different blocks
of the model to achieve optimal performance with reduced energy costs.
Specifically, input-, feature-, and output-level correspond to fine-tuning the
front, middle, and rear blocks of the model. We evaluate TBFT on a ResNet
model, three datasets, three different training sizes, and a Raspberry Pi.
Compared with the $Block Avg$, where each block is fine-tuned individually and
their performance improvements are averaged, TBFT exhibits an improvement in
model accuracy by an average of 15.30% whilst saving 41.57% energy consumption
on average compared with full fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepetd to The 4th Workshop on Machine Learning and Systems
  (EuroMLSys '24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FPT: Fine-grained Prompt Tuning for Parameter and Memory Efficient Fine
  Tuning in High-resolution Medical Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07576v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07576v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijin Huang, Pujin Cheng, Roger Tam, Xiaoying Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning (PEFT) is proposed as a cost-effective way to
transfer pre-trained models to downstream tasks, avoiding the high cost of
updating entire large-scale pre-trained models (LPMs). In this work, we present
Fine-grained Prompt Tuning (FPT), a novel PEFT method for medical image
classification. FPT significantly reduces memory consumption compared to other
PEFT methods, especially in high-resolution contexts. To achieve this, we first
freeze the weights of the LPM and construct a learnable lightweight side
network. The frozen LPM takes high-resolution images as input to extract
fine-grained features, while the side network is fed low-resolution images to
reduce memory usage. To allow the side network to access pre-trained knowledge,
we introduce fine-grained prompts that summarize information from the LPM
through a fusion module. Important tokens selection and preloading techniques
are employed to further reduce training cost and memory requirements. We
evaluate FPT on four medical datasets with varying sizes, modalities, and
complexities. Experimental results demonstrate that FPT achieves comparable
performance to fine-tuning the entire LPM while using only 1.8% of the
learnable parameters and 13% of the memory costs of an encoder ViT-B model with
a 512 x 512 input resolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SegVol: Universal and Interactive Volumetric Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13385v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13385v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Du, Fan Bai, Tiejun Huang, Bo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise image segmentation provides clinical study with instructive
information. Despite the remarkable progress achieved in medical image
segmentation, there is still an absence of 3D foundation segmentation model
that can segment a wide range of anatomical categories with easy user
interaction. In this paper, we propose a 3D foundation segmentation model,
named SegVol, supporting universal and interactive volumetric medical image
segmentation. By scaling up training data to 90K unlabeled Computed Tomography
(CT) volumes and 6K labeled CT volumes, this foundation model supports the
segmentation of over 200 anatomical categories using semantic and spatial
prompts. Extensive experiments on 10 internal validation tasks and 18 external
validation tasks verify that SegVol outperforms the state of the art by a large
margin. Through its capacity to provide precise volumetric segmentation across
various anatomical categories, SegVol has the potential to accelerate
advancements in medical imaging diagnosis and facilitate treatment
optimization. The model and code are publicly available at:
https://github.com/BAAI-DCAI/SegVol.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DreamComposer: Controllable 3D Object Generation via Multi-View
  Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03611v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03611v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhan Yang, Yukun Huang, Xiaoyang Wu, Yuan-Chen Guo, Song-Hai Zhang, Hengshuang Zhao, Tong He, Xihui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing pre-trained 2D large-scale generative models, recent works are
capable of generating high-quality novel views from a single in-the-wild image.
However, due to the lack of information from multiple views, these works
encounter difficulties in generating controllable novel views. In this paper,
we present DreamComposer, a flexible and scalable framework that can enhance
existing view-aware diffusion models by injecting multi-view conditions.
Specifically, DreamComposer first uses a view-aware 3D lifting module to obtain
3D representations of an object from multiple views. Then, it renders the
latent features of the target view from 3D representations with the multi-view
feature fusion module. Finally the target view features extracted from
multi-view inputs are injected into a pre-trained diffusion model. Experiments
show that DreamComposer is compatible with state-of-the-art diffusion models
for zero-shot novel view synthesis, further enhancing them to generate
high-fidelity novel view images with multi-view conditions, ready for
controllable 3D object reconstruction and various other applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://yhyang-myron.github.io/DreamComposer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regularizing Self-supervised 3D Scene Flows with Surface Awareness and
  Cyclic Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrik Vacek, David Hurych, Karel Zimmermann, Patrick Perez, Tomas Svoboda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning without supervision how to predict 3D scene flows from point clouds
is essential to many perception systems. We propose a novel learning framework
for this task which improves the necessary regularization. Relying on the
assumption that scene elements are mostly rigid, current smoothness losses are
built on the definition of ``rigid clusters" in the input point clouds. The
definition of these clusters is challenging and has a significant impact on the
quality of predicted flows. We introduce two new consistency losses that
enlarge clusters while preventing them from spreading over distinct objects. In
particular, we enforce \emph{temporal} consistency with a forward-backward
cyclic loss and \emph{spatial} consistency by considering surface orientation
similarity in addition to spatial proximity. The proposed losses are
model-independent and can thus be used in a plug-and-play fashion to
significantly improve the performance of existing models, as demonstrated on
two most widely used architectures. We also showcase the effectiveness and
generalization capability of our framework on four standard sensor-unique
driving datasets, achieving state-of-the-art performance in 3D scene flow
estimation. Our codes are available on https://github.com/ctu-vras/sac-flow.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ P2ANet: A <span class="highlight-title">Dataset</span> and Benchmark for Dense Action Detection from Table
  Tennis Match Broadcasting Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.12730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.12730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang Bian, Xuhong Li, Tao Wang, Qingzhong Wang, Jun Huang, Chen Liu, Jun Zhao, Feixiang Lu, Dejing Dou, Haoyi Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning has been widely used for video analytics, such as video
classification and action detection, dense action detection with fast-moving
subjects from sports videos is still challenging. In this work, we release yet
another sports video benchmark \TheName{} for \emph{\underline{P}}ing
\emph{\underline{P}}ong-\emph{\underline{A}}ction detection, which consists of
2,721 video clips collected from the broadcasting videos of professional table
tennis matches in World Table Tennis Championships and Olympiads. We work with
a crew of table tennis professionals and referees on a specially designed
annotation toolbox to obtain fine-grained action labels (in 14 classes) for
every ping-pong action that appeared in the dataset, and formulate two sets of
action detection problems -- \emph{action localization} and \emph{action
recognition}. We evaluate a number of commonly-seen action recognition (e.g.,
TSM, TSN, Video SwinTransformer, and Slowfast) and action localization models
(e.g., BSN, BSN++, BMN, TCANet), using \TheName{} for both problems, under
various settings. These models can only achieve 48\% area under the AR-AN curve
for localization and 82\% top-one accuracy for recognition since the ping-pong
actions are dense with fast-moving subjects but broadcasting videos are with
only 25 FPS. The results confirm that \TheName{} is still a challenging task
and can be used as a special benchmark for dense action detection from videos.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Semantic Segmentation Through Depth-Guided Feature
  Correlation and Sampling <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12378v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12378v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Sick, Dominik Engel, Pedro Hermosilla, Timo Ropinski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, training neural networks to perform semantic segmentation
required expensive human-made annotations. But more recently, advances in the
field of unsupervised learning have made significant progress on this issue and
towards closing the gap to supervised algorithms. To achieve this, semantic
knowledge is distilled by learning to correlate randomly sampled features from
images across an entire dataset. In this work, we build upon these advances by
incorporating information about the structure of the scene into the training
process through the use of depth information. We achieve this by (1) learning
depth-feature correlation by spatially correlate the feature maps with the
depth maps to induce knowledge about the structure of the scene and (2)
implementing farthest-point sampling to more effectively select relevant
features by utilizing 3D sampling techniques on depth information of the scene.
Finally, we demonstrate the effectiveness of our technical contributions
through extensive experimentation and present significant improvements in
performance across multiple benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Reflectance Map: Single-Image Stochastic Inverse Rendering of
  Illumination and Reflectance <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04529v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04529v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuto Enyo, Ko Nishino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reflectance bounds the frequency spectrum of illumination in the object
appearance. In this paper, we introduce the first stochastic inverse rendering
method, which recovers the attenuated frequency spectrum of an illumination
jointly with the reflectance of an object of known geometry from a single
image. Our key idea is to solve this blind inverse problem in the reflectance
map, an appearance representation invariant to the underlying geometry, by
learning to reverse the image formation with a novel diffusion model which we
refer to as the Diffusion Reflectance Map Network (DRMNet). Given an observed
reflectance map converted and completed from the single input image, DRMNet
generates a reflectance map corresponding to a perfect mirror sphere while
jointly estimating the reflectance. The forward process can be understood as
gradually filtering a natural illumination with lower and lower frequency
reflectance and additive Gaussian noise. DRMNet learns to invert this process
with two subnetworks, IllNet and RefNet, which work in concert towards this
joint estimation. The network is trained on an extensive synthetic dataset and
is demonstrated to generalize to real images, showing state-of-the-art accuracy
on established datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProMamba: Prompt-Mamba for polyp segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13660v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13660v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhao Xie, Ruofan Liao, Ziang Zhang, Sida Yi, Yuesheng Zhu, Guibo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting polyps through colonoscopy is an important task in medical image
segmentation, which provides significant assistance and reference value for
clinical surgery. However, accurate segmentation of polyps is a challenging
task due to two main reasons. Firstly, polyps exhibit various shapes and
colors. Secondly, the boundaries between polyps and their normal surroundings
are often unclear. Additionally, significant differences between different
datasets lead to limited generalization capabilities of existing methods. To
address these issues, we propose a segmentation model based on Prompt-Mamba,
which incorporates the latest Vision-Mamba and prompt technologies. Compared to
previous models trained on the same dataset, our model not only maintains high
segmentation accuracy on the validation part of the same dataset but also
demonstrates superior accuracy on unseen datasets, exhibiting excellent
generalization capabilities. Notably, we are the first to apply the
Vision-Mamba architecture to polyp segmentation and the first to utilize prompt
technology in a polyp segmentation model. Our model efficiently accomplishes
segmentation tasks, surpassing previous state-of-the-art methods by an average
of 5% across six datasets. Furthermore, we have developed multiple versions of
our model with scaled parameter counts, achieving better performance than
previous models even with fewer parameters. Our code and trained weights will
be released soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures,3 tabels</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SocialCircle: Learning the Angle-based Social Interaction Representation
  for Pedestrian Trajectory Prediction <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Conghao Wong, Beihao Xia, Ziqian Zou, Yulong Wang, Xinge You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing and forecasting trajectories of agents like pedestrians and cars in
complex scenes has become more and more significant in many intelligent systems
and applications. The diversity and uncertainty in socially interactive
behaviors among a rich variety of agents make this task more challenging than
other deterministic computer vision tasks. Researchers have made a lot of
efforts to quantify the effects of these interactions on future trajectories
through different mathematical models and network structures, but this problem
has not been well solved. Inspired by marine animals that localize the
positions of their companions underwater through echoes, we build a new
anglebased trainable social interaction representation, named SocialCircle, for
continuously reflecting the context of social interactions at different angular
orientations relative to the target agent. We validate the effect of the
proposed SocialCircle by training it along with several newly released
trajectory prediction models, and experiments show that the SocialCircle not
only quantitatively improves the prediction performance, but also qualitatively
helps better simulate social interactions when forecasting pedestrian
trajectories in a way that is consistent with human intuitions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emotic Masked Autoencoder with Attention Fusion for Facial Expression
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13039v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13039v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bach Nguyen-Xuan, Thien Nguyen-Hoang, Nhu Tai-Do
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial Expression Recognition (FER) is a critical task within computer vision
with diverse applications across various domains. Addressing the challenge of
limited FER datasets, which hampers the generalization capability of expression
recognition models, is imperative for enhancing performance. Our paper presents
an innovative approach integrating the MAE-Face self-supervised learning (SSL)
method and Fusion Attention mechanism for expression classification,
particularly showcased in the 6th Affective Behavior 32 pages harvmac; added
references for section 5Analysis in-the-wild (ABAW) competition. Additionally,
we propose preprocessing techniques to emphasize essential facial features,
thereby enhancing model performance on both training and validation sets,
notably demonstrated on the Aff-wild2 dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages; added references for section 1; corrected typo for email
  author</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning User Embeddings from Human Gaze for Personalised Saliency
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13653v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13653v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Strohm, Mihai Bâce, Andreas Bulling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reusable embeddings of user behaviour have shown significant performance
improvements for the personalised saliency prediction task. However, prior
works require explicit user characteristics and preferences as input, which are
often difficult to obtain. We present a novel method to extract user embeddings
from pairs of natural images and corresponding saliency maps generated from a
small amount of user-specific eye tracking data. At the core of our method is a
Siamese convolutional neural encoder that learns the user embeddings by
contrasting the image and personal saliency map pairs of different users.
Evaluations on two public saliency datasets show that the generated embeddings
have high discriminative power, are effective at refining universal saliency
maps to the individual users, and generalise well across users and images.
Finally, based on our model's ability to encode individual user
characteristics, our work points towards other applications that can benefit
from reusable embeddings of gaze behaviour.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VRP-SAM: SAM with Visual Reference Prompt <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17726v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17726v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanpeng Sun, Jiahui Chen, Shan Zhang, Xinyu Zhang, Qiang Chen, Gang Zhang, Errui Ding, Jingdong Wang, Zechao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel Visual Reference Prompt (VRP) encoder that
empowers the Segment Anything Model (SAM) to utilize annotated reference images
as prompts for segmentation, creating the VRP-SAM model. In essence, VRP-SAM
can utilize annotated reference images to comprehend specific objects and
perform segmentation of specific objects in target image. It is note that the
VRP encoder can support a variety of annotation formats for reference images,
including \textbf{point}, \textbf{box}, \textbf{scribble}, and \textbf{mask}.
VRP-SAM achieves a breakthrough within the SAM framework by extending its
versatility and applicability while preserving SAM's inherent strengths, thus
enhancing user-friendliness. To enhance the generalization ability of VRP-SAM,
the VRP encoder adopts a meta-learning strategy. To validate the effectiveness
of VRP-SAM, we conducted extensive empirical studies on the Pascal and COCO
datasets. Remarkably, VRP-SAM achieved state-of-the-art performance in visual
reference segmentation with minimal learnable parameters. Furthermore, VRP-SAM
demonstrates strong generalization capabilities, allowing it to perform
segmentation of unseen objects and enabling cross-domain segmentation. The
source code and models will be available at
\url{https://github.com/syp2ysy/VRP-SAM}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024; The camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeFFeC: Semantic Facial Feature Control for Fine-grained Face Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13972v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13972v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Strohm, Mihai Bâce, Markus Kaltenecker, Andreas Bulling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Semantic Facial Feature Control (SeFFeC) - a novel method for
fine-grained face shape editing. Our method enables the manipulation of
human-understandable, semantic face features, such as nose length or mouth
width, which are defined by different groups of facial landmarks. In contrast
to existing methods, the use of facial landmarks enables precise measurement of
the facial features, which then enables training SeFFeC without any manually
annotated labels. SeFFeC consists of a transformer-based encoder network that
takes a latent vector of a pre-trained generative model and a facial feature
embedding as input, and learns to modify the latent vector to perform the
desired face edit operation. To ensure that the desired feature measurement is
changed towards the target value without altering uncorrelated features, we
introduced a novel semantic face feature loss. Qualitative and quantitative
results show that SeFFeC enables precise and fine-grained control of 23 facial
features, some of which could not previously be controlled by other methods,
without requiring manual annotations. Unlike existing methods, SeFFeC also
provides deterministic control over the exact values of the facial features and
more localised and disentangled face edits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual Prototype Attention for Unsupervised Video Object Segmentation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12036v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12036v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suhwan Cho, Minhyeok Lee, Seunghoon Lee, Dogyoon Lee, Heeseung Choi, Ig-Jae Kim, Sangyoun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised video object segmentation (VOS) aims to detect and segment the
most salient object in videos. The primary techniques used in unsupervised VOS
are 1) the collaboration of appearance and motion information; and 2) temporal
fusion between different frames. This paper proposes two novel prototype-based
attention mechanisms, inter-modality attention (IMA) and inter-frame attention
(IFA), to incorporate these techniques via dense propagation across different
modalities and frames. IMA densely integrates context information from
different modalities based on a mutual refinement. IFA injects global context
of a video to the query frame, enabling a full utilization of useful properties
from multiple frames. Experimental results on public benchmark datasets
demonstrate that our proposed approach outperforms all existing methods by a
substantial margin. The proposed two components are also thoroughly validated
via ablative study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Pretext to Purpose: Batch-Adaptive Self-Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09974v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09974v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiansong Zhang, Linlin Shen, Peizhong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, self-supervised contrastive learning has emerged as a
distinguished paradigm in the artificial intelligence landscape. It facilitates
unsupervised feature learning through contrastive delineations at the instance
level. However, crafting an effective self-supervised paradigm remains a
pivotal challenge within this field. This paper delves into two crucial factors
impacting self-supervised contrastive learning-bach size and pretext tasks, and
from a data processing standpoint, proposes an adaptive technique of batch
fusion. The proposed method, via dimensionality reduction and reconstruction of
batch data, enables formerly isolated individual data to partake in intra-batch
communication through the Embedding Layer. Moreover, it adaptively amplifies
the self-supervised feature encoding capability as the training progresses. We
conducted a linear classification test of this method based on the classic
contrastive learning framework on ImageNet-1k. The empirical findings
illustrate that our approach achieves state-of-the-art performance under
equitable comparisons. Benefiting from its "plug-and-play" characteristics, we
further explored other contrastive learning methods. On the ImageNet-100,
compared to the original performance, the top1 has seen a maximum increase of
1.25%. We suggest that the proposed method may contribute to the advancement of
data-driven self-supervised learning research, bringing a fresh perspective to
this community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 2 figures, the code of this paper will be released soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLaFS: When Large Language Models Meet Few-Shot Segmentation <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16926v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16926v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lanyun Zhu, Tianrun Chen, Deyi Ji, Jieping Ye, Jun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes LLaFS, the first attempt to leverage large language
models (LLMs) in few-shot segmentation. In contrast to the conventional
few-shot segmentation methods that only rely on the limited and biased
information from the annotated support images, LLaFS leverages the vast prior
knowledge gained by LLM as an effective supplement and directly uses the LLM to
segment images in a few-shot manner. To enable the text-based LLM to handle
image-related tasks, we carefully design an input instruction that allows the
LLM to produce segmentation results represented as polygons, and propose a
region-attribute table to simulate the human visual mechanism and provide
multi-modal guidance. We also synthesize pseudo samples and use curriculum
learning for pretraining to augment data and achieve better optimization. LLaFS
achieves state-of-the-art results on multiple datasets, showing the potential
of using LLMs for few-shot computer vision tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EcoSense: Energy-Efficient Intelligent Sensing for In-Shore Ship
  Detection through Edge-Cloud Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjun Huang, Hanning Chen, Yang Ni, Arghavan Rezvani, Sanggeon Yun, Sungheon Jeon, Eric Pedley, Mohsen Imani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting marine objects inshore presents challenges owing to algorithmic
intricacies and complexities in system deployment. We propose a
difficulty-aware edge-cloud collaborative sensing system that splits the task
into object localization and fine-grained classification. Objects are
classified either at the edge or within the cloud, based on their estimated
difficulty. The framework comprises a low-power device-tailored front-end model
for object localization, classification, and difficulty estimation, along with
a transformer-graph convolutional network-based back-end model for fine-grained
classification. Our system demonstrates superior performance (mAP@0.5 +4.3%})
on widely used marine object detection datasets, significantly reducing both
data transmission volume (by 95.43%) and energy consumption (by 72.7%}) at the
system level. We validate the proposed system across various embedded system
platforms and in real-world scenarios involving drone deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision Transformers with Hierarchical Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.03180v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.03180v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun Liu, Yu-Huan Wu, Guolei Sun, Le Zhang, Ajad Chhatkuli, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper tackles the high computational/space complexity associated with
Multi-Head Self-Attention (MHSA) in vanilla vision transformers. To this end,
we propose Hierarchical MHSA (H-MHSA), a novel approach that computes
self-attention in a hierarchical fashion. Specifically, we first divide the
input image into patches as commonly done, and each patch is viewed as a token.
Then, the proposed H-MHSA learns token relationships within local patches,
serving as local relationship modeling. Then, the small patches are merged into
larger ones, and H-MHSA models the global dependencies for the small number of
the merged tokens. At last, the local and global attentive features are
aggregated to obtain features with powerful representation capacity. Since we
only calculate attention for a limited number of tokens at each step, the
computational load is reduced dramatically. Hence, H-MHSA can efficiently model
global relationships among tokens without sacrificing fine-grained information.
With the H-MHSA module incorporated, we build a family of
Hierarchical-Attention-based Transformer Networks, namely HAT-Net. To
demonstrate the superiority of HAT-Net in scene understanding, we conduct
extensive experiments on fundamental vision tasks, including image
classification, semantic segmentation, object detection, and instance
segmentation. Therefore, HAT-Net provides a new perspective for vision
transformers. Code and pretrained models are available at
https://github.com/yun-liu/HAT-Net.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Machine Intelligence Research (MIR), DOI: 10.1007/s11633-024-1393-8</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07728v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07728v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seokhyeon Ha, Sunbeom Jung, Jungwoo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained neural network models has become a widely adopted
approach across various domains. However, it can lead to the distortion of
pre-trained feature extractors that already possess strong generalization
capabilities. Mitigating feature distortion during adaptation to new target
domains is crucial. Recent studies have shown promising results in handling
feature distortion by aligning the head layer on in-distribution datasets
before performing fine-tuning. Nonetheless, a significant limitation arises
from the treatment of batch normalization layers during fine-tuning, leading to
suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning
(DAFT), a novel approach that incorporates batch normalization conversion and
the integration of linear probing and fine-tuning. Our batch normalization
conversion method effectively mitigates feature distortion by reducing
modifications to the neural network during fine-tuning. Additionally, we
introduce the integration of linear probing and fine-tuning to optimize the
head layer with gradual adaptation of the feature extractor. By leveraging
batch normalization layers and integrating linear probing and fine-tuning, our
DAFT significantly mitigates feature distortion and achieves improved model
performance on both in-distribution and out-of-distribution datasets. Extensive
experiments demonstrate that our method outperforms other baseline methods,
demonstrating its effectiveness in not only improving performance but also
mitigating feature distortion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeuS-PIR: Learning Relightable Neural Surface using Pre-Integrated
  Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07632v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07632v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Mao, Chenming Wu, Zhelun Shen, Yifan Wang, Dayan Wu, Liangjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a method, namely NeuS-PIR, for recovering relightable
neural surfaces using pre-integrated rendering from multi-view images or video.
Unlike methods based on NeRF and discrete meshes, our method utilizes implicit
neural surface representation to reconstruct high-quality geometry, which
facilitates the factorization of the radiance field into two components: a
spatially varying material field and an all-frequency lighting representation.
This factorization, jointly optimized using an adapted differentiable
pre-integrated rendering framework with material encoding regularization, in
turn addresses the ambiguity of geometry reconstruction and leads to better
disentanglement and refinement of each scene property. Additionally, we
introduced a method to distil indirect illumination fields from the learned
representations, further recovering the complex illumination effect like
inter-reflection. Consequently, our method enables advanced applications such
as relighting, which can be seamlessly integrated with modern graphics engines.
Qualitative and quantitative experiments have shown that NeuS-PIR outperforms
existing methods across various tasks on both synthetic and real datasets.
Source code is available at https://github.com/Sheldonmao/NeuSPIR
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaze-guided Hand-Object Interaction Synthesis: Benchmark and Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16169v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16169v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Tian, Lingxiao Yang, Ran Ji, Yuexin Ma, Lan Xu, Jingyi Yu, Ye Shi, Jingya Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaze plays a crucial role in revealing human attention and intention,
shedding light on the cognitive processes behind human actions. The integration
of gaze guidance with the dynamics of hand-object interactions boosts the
accuracy of human motion prediction. However, the lack of datasets that capture
the intricate relationship and consistency among gaze, hand, and object
movements remains a substantial hurdle. In this paper, we introduce the first
Gaze-guided Hand-Object Interaction dataset, GazeHOI, and present a novel task
for synthesizing gaze-guided hand-object interactions. Our dataset, GazeHOI,
features simultaneous 3D modeling of gaze, hand, and object interactions,
comprising 479 sequences with an average duration of 19.1 seconds, 812
sub-sequences, and 33 objects of various sizes. We propose a hierarchical
framework centered on a gaze-guided hand-object interaction diffusion model,
named GHO-Diffusion. In the pre-diffusion phase, we separate gaze conditions
into spatial-temporal features and goal pose conditions at different levels of
information granularity. During the diffusion phase, two gaze-conditioned
diffusion models are stacked to simplify the complex synthesis of hand-object
motions. Here, the object motion diffusion model generates sequences of object
motions based on gaze conditions, while the hand motion diffusion model
produces hand motions based on the generated object motion. To improve
fine-grained goal pose alignment, we introduce a Spherical Gaussian constraint
to guide the denoising step. In the subsequent post-diffusion phase, we
optimize the generated hand motions using contact consistency. Our extensive
experiments highlight the uniqueness of our dataset and the effectiveness of
our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning-based Axial Video Motion Magnification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kwon Byung-Ki, Oh Hyun-Bin, Kim Jun-Seong, Hyunwoo Ha, Tae-Hyun Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video motion magnification amplifies invisible small motions to be
perceptible, which provides humans with a spatially dense and holistic
understanding of small motions in the scene of interest. This is based on the
premise that magnifying small motions enhances the legibility of motions. In
the real world, however, vibrating objects often possess convoluted systems
that have complex natural frequencies, modes, and directions. Existing motion
magnification often fails to improve legibility since the intricate motions
still retain complex characteristics even after being magnified, which may
distract us from analyzing them. In this work, we focus on improving legibility
by proposing a new concept, axial motion magnification, which magnifies
decomposed motions along the user-specified direction. Axial motion
magnification can be applied to various applications where motions of specific
axes are critical, by providing simplified and easily readable motion
information. To achieve this, we propose a novel Motion Separation Module that
enables to disentangle and magnify the motion representation along axes of
interest. Furthermore, we build a new synthetic training dataset for the axial
motion magnification task. Our proposed method improves the legibility of
resulting motions along certain axes by adding a new feature: user
controllability. Axial motion magnification is a more generalized concept;
thus, our method can be directly adapted to the generic motion magnification
and achieves favorable performance against competing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>main paper: 12 pages, supplementary: 10 pages, 20 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-25T00:00:00Z">2024-03-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">85</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Priors from 3D Diffusion Models for RGB-Based One-Shot View
  Planning <span class="chip">IROS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicong Pan, Liren Jin, Xuying Huang, Cyrill Stachniss, Marija Popović, Maren Bennewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object reconstruction is relevant for many autonomous robotic tasks that
require interaction with the environment. A key challenge in such scenarios is
planning view configurations to collect informative measurements for
reconstructing an initially unknown object. One-shot view planning enables
efficient data collection by predicting view configurations and planning the
globally shortest path connecting all views at once. However, geometric priors
about the object are required to conduct one-shot view planning. In this work,
we propose a novel one-shot view planning approach that utilizes the powerful
3D generation capabilities of diffusion models as priors. By incorporating such
geometric priors into our pipeline, we achieve effective one-shot view planning
starting with only a single RGB image of the object to be reconstructed. Our
planning experiments in simulation and real-world setups indicate that our
approach balances well between object reconstruction quality and movement cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Sicong Pan and Liren Jin have equal contribution. Submitted to IROS
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CurbNet: Curb Detection Framework Based on <span class="highlight-title">LiDAR</span> Point Cloud
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoyang Zhao, Fulong Ma, Yuxuan Liu, Weiqing Qi, Ming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Curb detection is an important function in intelligent driving and can be
used to determine drivable areas of the road. However, curbs are difficult to
detect due to the complex road environment. This paper introduces CurbNet, a
novel framework for curb detection, leveraging point cloud segmentation.
Addressing the dearth of comprehensive curb datasets and the absence of 3D
annotations, we have developed the 3D-Curb dataset, encompassing 7,100 frames,
which represents the largest and most categorically diverse collection of curb
point clouds currently available. Recognizing that curbs are primarily
characterized by height variations, our approach harnesses spatially-rich 3D
point clouds for training. To tackle the challenges presented by the uneven
distribution of curb features on the xy-plane and their reliance on z-axis
high-frequency features, we introduce the multi-scale and channel attention
(MSCA) module, a bespoke solution designed to optimize detection performance.
Moreover, we propose an adaptive weighted loss function group, specifically
formulated to counteract the imbalance in the distribution of curb point clouds
relative to other categories. Our extensive experimentation on 2 major datasets
has yielded results that surpass existing benchmarks set by leading curb
detection and point cloud segmentation models. By integrating multi-clustering
and curve fitting techniques in our post-processing stage, we have
substantially reduced noise in curb detection, thereby enhancing precision to
0.8744. Notably, CurbNet has achieved an exceptional average metrics of over
0.95 at a tolerance of just 0.15m, thereby establishing a new benchmark.
Furthermore, corroborative real-world experiments and dataset analyzes mutually
validate each other, solidifying CurbNet's superior detection proficiency and
its robust generalizability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DBPF: A Framework for Efficient and Robust Dynamic Bin-Picking <span class="chip">RA-L</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichuan Li, Junkai Zhao, Yixiao Li, Zheng Wu, Rui Cao, Masayoshi Tomizuka, Yunhui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficiency and reliability are critical in robotic bin-picking as they
directly impact the productivity of automated industrial processes. However,
traditional approaches, demanding static objects and fixed collisions, lead to
deployment limitations, operational inefficiencies, and process unreliability.
This paper introduces a Dynamic Bin-Picking Framework (DBPF) that challenges
traditional static assumptions. The DBPF endows the robot with the reactivity
to pick multiple moving arbitrary objects while avoiding dynamic obstacles,
such as the moving bin. Combined with scene-level pose generation, the proposed
pose selection metric leverages the Tendency-Aware Manipulability Network
optimizing suction pose determination. Heuristic task-specific designs like
velocity-matching, dynamic obstacle avoidance, and the resight policy, enhance
the picking success rate and reliability. Empirical experiments demonstrate the
importance of these components. Our method achieves an average 84% success
rate, surpassing the 60% of the most comparable baseline, crucially, with zero
collisions. Further evaluations under diverse dynamic scenarios showcase DBPF's
robust performance in dynamic bin-picking. Results suggest that our framework
offers a promising solution for efficient and reliable robotic bin-picking
under dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures. This paper has been accepted by IEEE RA-L on
  2024-03-24. See the supplementary video at youtube:
  https://youtu.be/n5af2VsKhkg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Action Planning with Multiple Heterogeneous Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martina Lippi, Michael C. Welle, Marco Moletta, Alessandro Marino, Andrea Gasparri, Danica Kragic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual planning methods are promising to handle complex settings where
extracting the system state is challenging. However, none of the existing works
tackles the case of multiple heterogeneous agents which are characterized by
different capabilities and/or embodiment. In this work, we propose a method to
realize visual action planning in multi-agent settings by exploiting a roadmap
built in a low-dimensional structured latent space and used for planning. To
enable multi-agent settings, we infer possible parallel actions from a dataset
composed of tuples associated with individual actions. Next, we evaluate
feasibility and cost of them based on the capabilities of the multi-agent
system and endow the roadmap with this information, building a capability
latent space roadmap (C-LSR). Additionally, a capability suggestion strategy is
designed to inform the human operator about possible missing capabilities when
no paths are found. The approach is validated in a simulated burger cooking
task and a real-world box packing task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Cost Teleoperation with Haptic Feedback through Vision-based Tactile
  Sensors for Rigid and Soft Object Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martina Lippi, Michael C. Welle, Maciej K. Wozniak, Andrea Gasparri, Danica Kragic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Haptic feedback is essential for humans to successfully perform complex and
delicate manipulation tasks. A recent rise in tactile sensors has enabled
robots to leverage the sense of touch and expand their capability drastically.
However, many tasks still need human intervention/guidance. For this reason, we
present a teleoperation framework designed to provide haptic feedback to human
operators based on the data from camera-based tactile sensors mounted on the
robot gripper. Partial autonomy is introduced to prevent slippage of grasped
objects during task execution. Notably, we rely exclusively on low-cost
off-the-shelf hardware to realize an affordable solution. We demonstrate the
versatility of the framework on nine different objects ranging from rigid to
soft and fragile ones, using three different operators on real hardware.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://vision-tactile-manip.github.io/teleop/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Robotic Skill Learning System Built Upon Diffusion Policies and
  Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nils Ingelhag, Jesper Munkeby, Jonne van Haastregt, Anastasia Varava, Michael C. Welle, Danica Kragic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we build upon two major recent developments in the field,
Diffusion Policies for visuomotor manipulation and large pre-trained multimodal
foundational models to obtain a robotic skill learning system. The system can
obtain new skills via the behavioral cloning approach of visuomotor diffusion
policies given teleoperated demonstrations. Foundational models are being used
to perform skill selection given the user's prompt in natural language. Before
executing a skill the foundational model performs a precondition check given an
observation of the workspace. We compare the performance of different
foundational models to this end as well as give a detailed experimental
evaluation of the skills taught by the user in simulation and the real world.
Finally, we showcase the combined system on a challenging food serving scenario
in the real world. Videos of all experimental executions, as well as the
process of teaching new skills in simulation and the real world, are available
on the project's website.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://roboskillframework.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BatDeck: Advancing Nano-drone Navigation with Low-power Ultrasound-based
  Obstacle Avoidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanna Müller, Victor Kartsch, Michele Magno, Luca Benini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nano-drones, distinguished by their agility, minimal weight, and
cost-effectiveness, are particularly well-suited for exploration in confined,
cluttered and narrow spaces. Recognizing transparent, highly reflective or
absorbing materials, such as glass and metallic surfaces is challenging, as
classical sensors, such as cameras or laser rangers, often do not detect them.
Inspired by bats, which can fly at high speeds in complete darkness with the
help of ultrasound, this paper introduces \textit{BatDeck}, a pioneering
sensor-deck employing a lightweight and low-power ultrasonic sensor for
nano-drone autonomous navigation. This paper first provides insights about
sensor characteristics, highlighting the influence of motor noise on the
ultrasound readings, then it introduces the results of extensive experimental
tests for obstacle avoidance (OA) in a diverse environment. Results show that
\textit{BatDeck} allows exploration for a flight time of 8 minutes while
covering 136m on average before crash in a challenging environment with
transparent and reflective obstacles, proving the effectiveness of ultrasonic
sensors for OA on nano-drones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synapse: Learning Preferential Concepts from Visual Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadanand Modak, Noah Patton, Isil Dillig, Joydeep Biswas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of preference learning, which aims to learn
user-specific preferences (e.g., "good parking spot", "convenient drop-off
location") from visual input. Despite its similarity to learning factual
concepts (e.g., "red cube"), preference learning is a fundamentally harder
problem due to its subjective nature and the paucity of person-specific
training data. We address this problem using a new framework called Synapse,
which is a neuro-symbolic approach designed to efficiently learn preferential
concepts from limited demonstrations. Synapse represents preferences as
neuro-symbolic programs in a domain-specific language (DSL) that operates over
images, and leverages a novel combination of visual parsing, large language
models, and program synthesis to learn programs representing individual
preferences. We evaluate Synapse through extensive experimentation including a
user case study focusing on mobility-related concepts in mobile robotics and
autonomous driving. Our evaluation demonstrates that Synapse significantly
outperforms existing baselines as well as its own ablations. The code and other
details can be found on the project website https://amrl.cs.utexas.edu/synapse .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 7 figures; Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Adaptive Detection of MAVs: A Benchmark and Noise Suppression
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yin Zhang, Jinhong Deng, Peidong Liu, Wen Li, Shiyu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual detection of Micro Air Vehicles (MAVs) has attracted increasing
attention in recent years due to its important application in various tasks.
The existing methods for MAV detection assume that the training set and testing
set have the same distribution. As a result, when deployed in new domains, the
detectors would have a significant performance degradation due to domain
discrepancy. In this paper, we study the problem of cross-domain MAV detection.
The contributions of this paper are threefold. 1) We propose a
Multi-MAV-Multi-Domain (M3D) dataset consisting of both simulation and
realistic images. Compared to other existing datasets, the proposed one is more
comprehensive in the sense that it covers rich scenes, diverse MAV types, and
various viewing angles. A new benchmark for cross-domain MAV detection is
proposed based on the proposed dataset. 2) We propose a Noise Suppression
Network (NSN) based on the framework of pseudo-labeling and a large-to-small
training procedure. To reduce the challenging pseudo-label noises, two novel
modules are designed in this network. The first is a prior-based curriculum
learning module for allocating adaptive thresholds for pseudo labels with
different difficulties. The second is a masked copy-paste augmentation module
for pasting truly-labeled MAVs on unlabeled target images and thus decreasing
pseudo-label noises. 3) Extensive experimental results verify the superior
performance of the proposed method compared to the state-of-the-art ones. In
particular, it achieves mAP of 46.9%(+5.8%), 50.5%(+3.7%), and 61.5%(+11.3%) on
the tasks of simulation-to-real adaptation, cross-scene adaptation, and
cross-camera adaptation, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 11 figures. Accepted by IEEE Transactions on Automation
  Science and Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Skill Q-Network: Learning Adaptive Skill Ensemble for Mapless Navigation
  in Unknown Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunki Seong, David Hyunchul Shim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on the acquisition of mapless navigation skills within
unknown environments. We introduce the Skill Q-Network (SQN), a novel
reinforcement learning method featuring an adaptive skill ensemble mechanism.
Unlike existing methods, our model concurrently learns a high-level skill
decision process alongside multiple low-level navigation skills, all without
the need for prior knowledge. Leveraging a tailored reward function for mapless
navigation, the SQN is capable of learning adaptive maneuvers that incorporate
both exploration and goal-directed skills, enabling effective navigation in new
environments. Our experiments demonstrate that our SQN can effectively navigate
complex environments, exhibiting a 40% higher performance compared to baseline
models. Without explicit guidance, SQN discovers how to combine low-level skill
policies, showcasing both goal-directed navigations to reach destinations and
exploration maneuvers to escape from local minimum regions in challenging
scenarios. Remarkably, our adaptive skill ensemble method enables zero-shot
transfer to out-of-distribution domains, characterized by unseen observations
from non-convex obstacles or uneven, subterranean-like environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trajectory Planning of Robotic Manipulator in Dynamic Environment
  Exploiting DRL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Osama Ahmad, Zawar Hussain, Hammad Naeem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study is about the implementation of a reinforcement learning algorithm
in the trajectory planning of manipulators. We have a 7-DOF robotic arm to pick
and place the randomly placed block at a random target point in an unknown
environment. The obstacle is randomly moving which creates a hurdle in picking
the object. The objective of the robot is to avoid the obstacle and pick the
block with constraints to a fixed timestamp. In this literature, we have
applied a deep deterministic policy gradient (DDPG) algorithm and compared the
model's efficiency with dense and sparse rewards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICIESTR-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the Sim-to-Real Gap with Bayesian Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Rothfuss, Bhavya Sukhija, Lenart Treven, Florian Dörfler, Stelian Coros, Andreas Krause
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SIM-FSVGD for learning robot dynamics from data. As opposed to
traditional methods, SIM-FSVGD leverages low-fidelity physical priors, e.g., in
the form of simulators, to regularize the training of neural network models.
While learning accurate dynamics already in the low data regime, SIM-FSVGD
scales and excels also when more data is available. We empirically show that
learning with implicit physical priors results in accurate mean model
estimation as well as precise uncertainty quantification. We demonstrate the
effectiveness of SIM-FSVGD in bridging the sim-to-real gap on a
high-performance RC racecar system. Using model-based RL, we demonstrate a
highly dynamic parking maneuver with drifting, using less than half the data
compared to the state of the art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symbolic and User-friendly Geometric Algebra Routines (SUGAR) for
  Computations in Matlab 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manel Velasco, Isiah Zaplana, Arnau Dória-Cerezo, Pau Martí
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geometric algebra (GA) is a mathematical tool for geometric computing,
providing a framework that allows a unified and compact approach to geometric
relations which in other mathematical systems are typically described using
different more complicated elements. This fact has led to an increasing
adoption of GA in applied mathematics and engineering problems. However, the
scarcity of symbolic implementations of GA and its inherent complexity,
requiring a specific mathematical background, make it challenging and less
intuitive for engineers to work with. This prevents wider adoption among more
applied professionals. To address this challenge, this paper introduces SUGAR
(Symbolic and User-friendly Geometric Algebra Routines), an open-source toolbox
designed for Matlab and licensed under the MIT License. SUGAR facilitates the
translation of GA concepts into Matlab and provides a collection of
user-friendly functions tailored for GA computations, including support for
symbolic operations. It supports both numeric and symbolic computations in
high-dimensional GAs. Specifically tailored for applied mathematics and
engineering applications, SUGAR has been meticulously engineered to represent
geometric elements and transformations within two and three-dimensional
projective and conformal geometric algebras, aligning with established
computational methodologies in the literature. Furthermore, SUGAR efficiently
handles functions of multivectors, such as exponential, logarithmic,
sinusoidal, and cosine functions, enhancing its applicability across various
engineering domains, including robotics, control systems, and power
electronics. Finally, this work includes four distinct validation examples,
demonstrating SUGAR's capabilities across the above-mentioned fields and its
practical utility in addressing real-world applied mathematics and engineering
problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 6 figures, journal paper submitted to ACM TOMS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Technical Development of a Semi-Autonomous Robotic Partition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binh Vinh Duc Nguyen, Andrew Vande Moere
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This technical description details the design and engineering process of a
semi-autonomous robotic partition. This robotic partition prototype was
subsequently employed in a longer-term evaluation in-the-wild study conducted
by the authors in a real-world office setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ROXIE: Defining a Robotic eXplanation and Interpretability Engine <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco J. Rodríguez-Lera, Miguel A. González-Santamarta, Alejandro González-Cantón, Laura Fernández-Becerra, David Sobrín-Hidalgo, Angel Manuel Guerrero-Higueras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an era where autonomous robots increasingly inhabit public spaces, the
imperative for transparency and interpretability in their decision-making
processes becomes paramount. This paper presents the overview of a Robotic
eXplanation and Interpretability Engine (ROXIE), which addresses this critical
need, aiming to demystify the opaque nature of complex robotic behaviors. This
paper elucidates the key features and requirements needed for providing
information and explanations about robot decision-making processes. It also
overviews the suite of software components and libraries available for
deployment with ROS 2, empowering users to provide comprehensive explanations
and interpretations of robot processes and behaviors, thereby fostering trust
and collaboration in human-robot interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures, 1 tables, Submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Research Challenges for Adaptive Architecture: Empowering Occupants of
  Multi-Occupancy Buildings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binh Vinh Duc Nguyen, Andrew Vande Moere
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This positional paper outlines our vision of 'adaptive architecture', which
involves the integration of robotic technology to physically change an
architectural space in supporting the changing needs of its occupants, in
response to the CHI'24 workshop "HabiTech - Inhabiting Buildings, Data &
Technology" call on "How do new technologies enable and empower the inhabitants
of multi-occupancy buildings?". Specifically, while adaptive architecture holds
promise for enhancing occupant satisfaction, comfort, and overall health and
well-being, there remains a range of research challenges of (1) how it can
effectively support individual occupants, while (2) mediating the conflicting
needs of collocated others, and (3) integrating meaningfully into the
sociocultural characteristics of their building community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Adaptive Workplace: Orchestrating Architectural Services around the
  Wellbeing of Individual Occupants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Vande Moere, Sara Arko, Alena Safrova Drasilova, Tomáš Ondráček, Ilaria Pigliautile, Benedetta Pioppi, Anna Laura Pisello, Jakub Prochazka, Paula Acuna Roncancio, Davide Schaumann, Marcel Schweiker, Binh Vinh Duc Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the academic consortia members of the EU Horizon project SONATA
("Situation-aware OrchestratioN of AdapTive Architecture"), we respond to the
workshop call for "Office Wellbeing by Design: Don't Stand for Anything Less"
by proposing the "Adaptive Workplace" concept. In essence, our vision aims to
adapt a workplace to the ever-changing needs of individual occupants, instead
of that occupants are expected to adapt to their workplace.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counter-example guided Imitation Learning of Feedback Controllers from
  Temporal Logic Specifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thao Dang, Alexandre Donzé, Inzemamul Haque, Nikolaos Kekatos, Indranil Saha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel method for imitation learning for control requirements
expressed using Signal Temporal Logic (STL). More concretely we focus on the
problem of training a neural network to imitate a complex controller. The
learning process is guided by efficient data aggregation based on
counter-examples and a coverage measure. Moreover, we introduce a method to
evaluate the performance of the learned controller via parameterization and
parameter estimation of the STL requirements. We demonstrate our approach with
a flying robot case study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Admittance Control with Iterative Learning for General-Purpose
  Contact-Rich Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Zhou, Yuyao Sun, Wenbo Liu, Ruixuan Jiao, Fang Fang, Shihua Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Force interaction is inevitable when robots face multiple operation
scenarios. How to make the robot competent in force control for generalized
operations such as multi-tasks still remains a challenging problem. Aiming at
the reproducibility of interaction tasks and the lack of a generalized force
control framework for multi-task scenarios, this paper proposes a novel hybrid
control framework based on active admittance control with iterative learning
parameters-tunning mechanism. The method adopts admittance control as the
underlying algorithm to ensure flexibility, and iterative learning as the
high-level algorithm to regulate the parameters of the admittance model. The
whole algorithm has flexibility and learning ability, which is capable of
achieving the goal of excellent versatility. Four representative interactive
robot manipulation tasks are chosen to investigate the consistency and
generalisability of the proposed method. Experiments are designed to verify the
effectiveness of the whole framework, and an average of 98.21% and 91.52%
improvement of RMSE is obtained relative to the traditional admittance control
as well as the model-free adaptive control, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Arm-Constrained Curriculum Learning for Loco-Manipulation of the
  Wheel-Legged Robot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifan Wang, Yufei Jia, Lu Shi, Haoyu Wang, Haizhou Zhao, Xueyang Li, Jinni Zhou, Jun Ma, Guyue Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating a robotic manipulator into a wheel-legged robot enhances its
agility and expands its potential for practical applications. However, the
presence of potential instability and uncertainties presents additional
challenges for control objectives. In this paper, we introduce an
arm-constrained curriculum learning architecture to tackle the issues
introduced by adding the manipulator. Firstly, we develop an arm-constrained
reinforcement learning algorithm to ensure safety and stability in control
performance. Additionally, to address discrepancies in reward settings between
the arm and the base, we propose a reward-aware curriculum learning method. The
policy is first trained in Isaac gym and transferred to the physical robot to
do dynamic grasping tasks, including the door-opening task, fan-twitching task
and the relay-baton-picking and following task. The results demonstrate that
our proposed approach effectively controls the arm-equipped wheel-legged robot
to master dynamic grasping skills, allowing it to chase and catch a moving
object while in motion. The code can be found at
https://github.com/aCodeDog/legged-robots-manipulation. To view the
supplemental video, please visit https://youtu.be/sNXT-rwPNMM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hallucination Detection in Foundation Models for Decision-Making: A
  Flexible Definition and <span class="highlight-title">Review</span> of the State of the Art 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neeloy Chakraborty, Melkior Ornik, Katherine Driggs-Campbell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to
agricultural field robots, and from health care assistants to the entertainment
industry. The majority of these systems are developed with modular
sub-components for decision-making, planning, and control that may be
hand-engineered or learning-based. While these existing approaches have been
shown to perform well under the situations they were specifically designed for,
they can perform especially poorly in rare, out-of-distribution scenarios that
will undoubtedly arise at test-time. The rise of foundation models trained on
multiple tasks with impressively large datasets from a variety of fields has
led researchers to believe that these models may provide common sense reasoning
that existing planners are missing. Researchers posit that this common sense
reasoning will bridge the gap between algorithm development and deployment to
out-of-distribution tasks, like how humans adapt to unexpected scenarios. Large
language models have already penetrated the robotics and autonomous systems
domains as researchers are scrambling to showcase their potential use cases in
deployment. While this application direction is very promising empirically,
foundation models are known to hallucinate and generate decisions that may
sound reasonable, but are in fact poor. We argue there is a need to step back
and simultaneously design systems that can quantify the certainty of a model's
decision, and detect when it may be hallucinating. In this work, we discuss the
current use cases of foundation models for decision-making tasks, provide a
general definition for hallucinations with examples, discuss existing
approaches to hallucination detection and mitigation with a focus on decision
problems, and explore areas for further research in this exciting field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatially temporally distributed informative path planning for
  multi-robot systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binh Nguyen, Linh Nguyen, Truong X. Nghiem, Hung La, Jose Baca, Pablo Rangel, Miguel Cid Montoya, Thang Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the problem of informative path planning for a mobile
robotic sensor network in spatially temporally distributed mapping. The robots
are able to gather noisy measurements from an area of interest during their
movements to build a Gaussian Process (GP) model of a spatio-temporal field.
The model is then utilized to predict the spatio-temporal phenomenon at
different points of interest. To spatially and temporally navigate the group of
robots so that they can optimally acquire maximal information gains while their
connectivity is preserved, we propose a novel multistep prediction informative
path planning optimization strategy employing our newly defined local cost
functions. By using the dual decomposition method, it is feasible and practical
to effectively solve the optimization problem in a distributed manner. The
proposed method was validated through synthetic experiments utilizing
real-world data sets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Model Predictive Control with Zonotope-Based Neural Networks
  for Bipedal Social Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdulaziz Shamsah, Krishanu Agarwal, Shreyas Kousik, Ye Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the challenge of bipedal navigation in a dynamic
human-crowded environment, a research area that remains largely underexplored
in the field of legged navigation. We propose two cascaded zonotope-based
neural networks: a Pedestrian Prediction Network (PPN) for pedestrians' future
trajectory prediction and an Ego-agent Social Network (ESN) for ego-agent
social path planning. Representing future paths as zonotopes allows for
efficient reachability-based planning and collision checking. The ESN is then
integrated with a Model Predictive Controller (ESN-MPC) for footstep planning
for our bipedal robot Digit designed by Agility Robotics. ESN-MPC solves for a
collision-free optimal trajectory by optimizing through the gradients of ESN.
ESN-MPC optimal trajectory is sent to the low-level controller for full-order
simulation of Digit. The overall proposed framework is validated with extensive
simulations on randomly generated initial settings with varying human crowd
densities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Cooperative Maneuver Planning in Mixed Traffic at Urban
  Intersections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marvin Klimke, Max Bastian Mertens, Benjamin Völz, Michael Buchholz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Connected automated driving promises a significant improvement of traffic
efficiency and safety on highways and in urban areas. Apart from sharing of
awareness and perception information over wireless communication links,
cooperative maneuver planning may facilitate active guidance of connected
automated vehicles at urban intersections. Research in automatic intersection
management put forth a large body of works that mostly employ rule-based or
optimization-based approaches primarily in fully automated simulated
environments. In this work, we present two cooperative planning approaches that
are capable of handling mixed traffic, i.e., the road being shared by automated
vehicles and regular vehicles driven by humans. Firstly, we propose an
optimization-based planner trained on real driving data that cyclically selects
the most efficient out of multiple predicted coordinated maneuvers.
Additionally, we present a cooperative planning approach based on graph-based
reinforcement learning, which conquers the lack of ground truth data for
cooperative maneuvers. We present evaluation results of both cooperative
planners in high-fidelity simulation and real-world traffic. Simulative
experiments in fully automated traffic and mixed traffic show that cooperative
maneuver planning leads to less delay due to interaction and a reduced number
of stops. In real-world experiments with three prototype connected automated
vehicles in public traffic, both planners demonstrate their ability to perform
efficient cooperative maneuvers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>M. Klimke and M. Mertens are both first authors with equal
  contribution. 11 pages, 10 figures, 2 tables, submitted to IEEE Transactions
  on Intelligent Vehicles</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Producing and Leveraging Online Map Uncertainty in Trajectory Prediction <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xunjiang Gu, Guanyu Song, Igor Gilitschenski, Marco Pavone, Boris Ivanovic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-definition (HD) maps have played an integral role in the development of
modern autonomous vehicle (AV) stacks, albeit with high associated labeling and
maintenance costs. As a result, many recent works have proposed methods for
estimating HD maps online from sensor data, enabling AVs to operate outside of
previously-mapped regions. However, current online map estimation approaches
are developed in isolation of their downstream tasks, complicating their
integration in AV stacks. In particular, they do not produce uncertainty or
confidence estimates. In this work, we extend multiple state-of-the-art online
map estimation methods to additionally estimate uncertainty and show how this
enables more tightly integrating online mapping with trajectory forecasting. In
doing so, we find that incorporating uncertainty yields up to 50% faster
training convergence and up to 15% better prediction performance on the
real-world nuScenes driving dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 14 figures, 6 tables. CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AeroBridge: Autonomous Drone Handoff System for Emergency Battery
  Service 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avishkar Seth, Alice James, Endrowednes Kuantama, Richard Han, Subhas Mukhopadhyay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an Emergency Battery Service (EBS) for drones in which an
EBS drone flies to a drone in the field with a depleted battery and transfers a
fresh battery to the exhausted drone. The authors present a unique battery
transfer mechanism and drone localization that uses the Cross Marker Position
(CMP) method. The main challenges include a stable and balanced transfer that
precisely localizes the receiver drone. The proposed EBS drone mitigates the
effects of downwash due to the vertical proximity between the drones by
implementing diagonal alignment with the receiver, reducing the distance to 0.5
m between the two drones. CFD analysis shows that diagonal instead of
perpendicular alignment minimizes turbulence, and the authors verify the actual
system for change in output airflow and thrust measurements. The CMP
marker-based localization method enables position lock for the EBS drone with
up to 0.9 cm accuracy. The performance of the transfer mechanism is validated
experimentally by successful mid-air transfer in 5 seconds, where the EBS drone
is within 0.5 m vertical distance from the receiver drone, wherein 4m/s
turbulence does not affect the transfer process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Visual Place Recognition via Fast and Slow Adaptive Biasing in
  Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gokul B. Nair, Michael Milford, Tobias Fischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras are increasingly popular in robotics due to their beneficial
features, such as low latency, energy efficiency, and high dynamic range.
Nevertheless, their downstream task performance is greatly influenced by the
optimization of bias parameters. These parameters, for instance, regulate the
necessary change in light intensity to trigger an event, which in turn depends
on factors such as the environment lighting and camera motion. This paper
introduces feedback control algorithms that automatically tune the bias
parameters through two interacting methods: 1) An immediate, on-the-fly fast
adaptation of the refractory period, which sets the minimum interval between
consecutive events, and 2) if the event rate exceeds the specified bounds even
after changing the refractory period repeatedly, the controller adapts the
pixel bandwidth and event thresholds, which stabilizes after a short period of
noise events across all pixels (slow adaptation). Our evaluation focuses on the
visual place recognition task, where incoming query images are compared to a
given reference database. We conducted comprehensive evaluations of our
algorithms' adaptive feedback control in real-time. To do so, we collected the
QCR-Fast-and-Slow dataset that contains DAVIS346 event camera streams from 366
repeated traversals of a Scout Mini robot navigating through a 100 meter long
indoor lab setting (totaling over 35km distance traveled) in varying brightness
conditions with ground truth location information. Our proposed feedback
controllers result in superior performance when compared to the standard bias
settings and prior feedback control methods. Our findings also detail the
impact of bias adjustments on task performance and feature ablation studies on
the fast and slow adaptation mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures, paper under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Terrain-Attentive Learning for Efficient 6-DoF Kinodynamic Modeling on
  Vertically Challenging Terrain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniket Datar, Chenhui Pan, Mohammad Nazeri, Anuj Pokhrel, Xuesu Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wheeled robots have recently demonstrated superior mechanical capability to
traverse vertically challenging terrain (e.g., extremely rugged boulders
comparable in size to the vehicles themselves). Negotiating such terrain
introduces significant variations of vehicle pose in all six Degrees-of-Freedom
(DoFs), leading to imbalanced contact forces, varying momentum, and chassis
deformation due to non-rigid tires and suspensions. To autonomously navigate on
vertically challenging terrain, all these factors need to be efficiently
reasoned within limited onboard computation and strict real-time constraints.
In this paper, we propose a 6-DoF kinodynamics learning approach that is
attentive only to the specific underlying terrain critical to the current
vehicle-terrain interaction, so that it can be efficiently queried in real-time
motion planners onboard small robots. Physical experiment results show our
Terrain-Attentive Learning demonstrates on average 51.1% reduction in model
prediction error among all 6 DoFs compared to a state-of-the-art model for
vertically challenging terrain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ASDF: Assembly State Detection Utilizing Late Fusion by Integrating 6D
  Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah Schieber, Shiyu Li, Niklas Corell, Philipp Beckerle, Julian Kreimeier, Daniel Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In medical and industrial domains, providing guidance for assembly processes
is critical to ensure efficiency and safety. Errors in assembly can lead to
significant consequences such as extended surgery times, and prolonged
manufacturing or maintenance times in industry. Assembly scenarios can benefit
from in-situ AR visualization to provide guidance, reduce assembly times and
minimize errors. To enable in-situ visualization 6D pose estimation can be
leveraged. Existing 6D pose estimation techniques primarily focus on individual
objects and static captures. However, assembly scenarios have various dynamics
including occlusion during assembly and dynamics in the assembly objects
appearance. Existing work, combining object detection/6D pose estimation and
assembly state detection focuses either on pure deep learning-based approaches,
or limit the assembly state detection to building blocks. To address the
challenges of 6D pose estimation in combination with assembly state detection,
our approach ASDF builds upon the strengths of YOLOv8, a real-time capable
object detection framework. We extend this framework, refine the object pose
and fuse pose knowledge with network-detected pose information. Utilizing our
late fusion in our Pose2State module results in refined 6D pose estimation and
assembly state detection. By combining both pose and state information, our
Pose2State module predicts the final assembly state with precision. Our
evaluation on our ASDF dataset shows that our Pose2State module leads to an
improved assembly state detection and that the improvement of the assembly
state further leads to a more robust 6D pose estimation. Moreover, on the GBOT
dataset, we outperform the pure deep learning-based network, and even
outperform the hybrid and pure tracking-based approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SE(3) Linear Parameter Varying Dynamical Systems for Globally
  Asymptotically Stable End-Effector Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunan Sun, Nadia Figueroa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linear Parameter Varying Dynamical Systems (LPV-DS) encode trajectories into
an autonomous first-order DS that enables reactive responses to perturbations,
while ensuring globally asymptotic stability at the target. However, the
current LPV-DS framework is established on Euclidean data only and has not been
applicable to broader robotic applications requiring pose control. In this
paper we present an extension to the current LPV-DS framework, named
Quaternion-DS, which efficiently learns a DS-based motion policy for
orientation. Leveraging techniques from differential geometry and Riemannian
statistics, our approach properly handles the non-Euclidean orientation data in
quaternion space, enabling the integration with positional control, namely
SE(3) LPV-DS, so that the synergistic behaviour within the full SE(3) pose is
preserved. Through simulation and real robot experiments, we validate our
method, demonstrating its ability to efficiently and accurately reproduce the
original SE(3) trajectory while exhibiting strong robustness to perturbations
in task space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bipedal Safe Navigation over Uncertain Rough Terrain: Unifying Terrain
  <span class="highlight-title">Mapping</span> and Locomotion Stability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kasidit Muenprasitivej, Jesse Jiang, Abdulaziz Shamsah, Samuel Coogan, Ye Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of bipedal robot navigation in complex environments with
uncertain and rough terrain. In particular, we consider a scenario in which the
robot is expected to reach a desired goal location by traversing an environment
with uncertain terrain elevation. Such terrain uncertainties induce not only
untraversable regions but also robot motion perturbations. Thus, the problems
of terrain mapping and locomotion stability are intertwined. We evaluate three
different kernels for Gaussian process (GP) regression to learn the terrain
elevation. We also learn the motion deviation resulting from both the terrain
as well as the discrepancy between the reduced-order Prismatic Inverted
Pendulum Model used for planning and the full-order locomotion dynamics. We
propose a hierarchical locomotion-dynamics-aware sampling-based navigation
planner. The global navigation planner plans a series of local waypoints to
reach the desired goal locations while respecting locomotion stability
constraints. Then, a local navigation planner is used to generate a sequence of
dynamically feasible footsteps to reach local waypoints. We develop a novel
trajectory evaluation metric to minimize motion deviation and maximize
information gain of the terrain elevation map. We evaluate the efficacy of our
planning framework on Digit bipedal robot simulation in MuJoCo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Stress Response and Perceived Safety during Encounters with
  Quadruped Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Gupta, Hyonyoung Shin, Emily Norman, Keri K. Stephens, Nanshu Lu, Luis Sentis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the rise of mobile robot deployments in home and work settings,
perceived safety of users and bystanders is understudied in the human-robot
interaction (HRI) literature. To address this, we present a study designed to
identify elements of a human-robot encounter that correlate with observed
stress response. Stress is a key component of perceived safety and is strongly
associated with human physiological response. In this study a Boston Dynamics
Spot and a Unitree Go1 navigate autonomously through a shared environment
occupied by human participants wearing multimodal physiological sensors to
track their electrocardiography (ECG) and electrodermal activity (EDA). The
encounters are varied through several trials and participants self-rate their
stress levels after each encounter. The study resulted in a multidimensional
dataset archiving various objective and subjective aspects of a human-robot
encounter, containing insights for understanding perceived safety in such
encounters. To this end, acute stress responses were decoded from the human
participants' ECG and EDA and compared across different human-robot encounter
conditions. Statistical analysis of data indicate that on average (1)
participants feel more stress during encounters compared to baselines, (2)
participants feel more stress encountering multiple robots compared to a single
robot and (3) participants stress increases during navigation behavior compared
with search behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figs, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring CausalWorld: Enhancing robotic manipulation via knowledge
  transfer and curriculum learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinrui Wang, Yan Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores a learning-based tri-finger robotic arm manipulating
task, which requires complex movements and coordination among the fingers. By
employing reinforcement learning, we train an agent to acquire the necessary
skills for proficient manipulation. To enhance the efficiency and effectiveness
of the learning process, two knowledge transfer strategies, fine-tuning and
curriculum learning, were utilized within the soft actor-critic architecture.
Fine-tuning allows the agent to leverage pre-trained knowledge and adapt it to
new tasks. Several variations like model transfer, policy transfer, and
across-task transfer were implemented and evaluated. To eliminate the need for
pretraining, curriculum learning decomposes the advanced task into simpler,
progressive stages, mirroring how humans learn. The number of learning stages,
the context of the sub-tasks, and the transition timing were found to be the
critical design parameters. The key factors of two learning strategies and
corresponding effects were explored in context-aware and context-unaware
scenarios, enabling us to identify the scenarios where the methods demonstrate
optimal performance, derive conclusive insights, and contribute to a broader
range of learning-based engineering applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact-Aware Bimanual Catching of Large-Momentum Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Yan, Theodoros Stouraitis, João Moura, Wenfu Xu, Michael Gienger, Sethu Vijayakumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates one of the most challenging tasks in dynamic
manipulation -- catching large-momentum moving objects. Beyond the realm of
quasi-static manipulation, dealing with highly dynamic objects can
significantly improve the robot's capability of interacting with its
surrounding environment. Yet, the inevitable motion mismatch between the fast
moving object and the approaching robot will result in large impulsive forces,
which lead to the unstable contacts and irreversible damage to both the object
and the robot. To address the above problems, we propose an online optimization
framework to: 1) estimate and predict the linear and angular motion of the
object; 2) search and select the optimal contact locations across every surface
of the object to mitigate impact through sequential quadratic programming
(SQP); 3) simultaneously optimize the end-effector motion, stiffness, and
contact force for both robots using multi-mode trajectory optimization (MMTO);
and 4) realise the impact-aware catching motion on the compliant robotic system
based on indirect force controller. We validate the impulse distribution,
contact selection, and impact-aware MMTO algorithms in simulation and
demonstrate the benefits of the proposed framework in real-world experiments
including catching large-momentum moving objects with well-defined motion,
constrained motion and free-flying motion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DASA: Delay-Adaptive Multi-Agent Stochastic Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolo Dal Fabbro, Arman Adibi, H. Vincent Poor, Sanjeev R. Kulkarni, Aritra Mitra, George J. Pappas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a setting in which $N$ agents aim to speedup a common Stochastic
Approximation (SA) problem by acting in parallel and communicating with a
central server. We assume that the up-link transmissions to the server are
subject to asynchronous and potentially unbounded time-varying delays. To
mitigate the effect of delays and stragglers while reaping the benefits of
distributed computation, we propose \texttt{DASA}, a Delay-Adaptive algorithm
for multi-agent Stochastic Approximation. We provide a finite-time analysis of
\texttt{DASA} assuming that the agents' stochastic observation processes are
independent Markov chains. Significantly advancing existing results,
\texttt{DASA} is the first algorithm whose convergence rate depends only on the
mixing time $\tmix$ and on the average delay $\tau_{avg}$ while jointly
achieving an $N$-fold convergence speedup under Markovian sampling. Our work is
relevant for various SA applications, including multi-agent and distributed
temporal difference (TD) learning, Q-learning and stochastic optimization with
correlated data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TwoStep: Multi-agent Task Planning using Classical Planners and Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishika Singh, David Traum, Jesse Thomason
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classical planning formulations like the Planning Domain Definition Language
(PDDL) admit action sequences guaranteed to achieve a goal state given an
initial state if any are possible. However, reasoning problems defined in PDDL
do not capture temporal aspects of action taking, for example that two agents
in the domain can execute an action simultaneously if postconditions of each do
not interfere with preconditions of the other. A human expert can decompose a
goal into largely independent constituent parts and assign each agent to one of
these subgoals to take advantage of simultaneous actions for faster execution
of plan steps, each using only single agent planning. By contrast, large
language models (LLMs) used for directly inferring plan steps do not guarantee
execution success, but do leverage commonsense reasoning to assemble action
sequences. We combine the strengths of classical planning and LLMs by
approximating human intuitions for two-agent planning goal decomposition. We
demonstrate that LLM-based goal decomposition leads to faster planning times
than solving multi-agent PDDL problems directly while simultaneously achieving
fewer plan execution steps than a single agent plan alone and preserving
execution success. Additionally, we find that LLM-based approximations of
subgoals can achieve similar multi-agent execution steps than those specified
by human experts. Website and resources at https://glamor-usc.github.io/twostep
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal and Semantic Evaluation Metrics for Foundation Models in
  Post-Hoc Analysis of Robotic Sub-tasks <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Salfity, Selma Wanna, Minkyu Choi, Mitch Pryor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works in Task and Motion Planning (TAMP) show that training control
policies on language-supervised robot trajectories with quality labeled data
markedly improves agent task success rates. However, the scarcity of such data
presents a significant hurdle to extending these methods to general use cases.
To address this concern, we present an automated framework to decompose
trajectory data into temporally bounded and natural language-based descriptive
sub-tasks by leveraging recent prompting strategies for Foundation Models (FMs)
including both Large Language Models (LLMs) and Vision Language Models (VLMs).
Our framework provides both time-based and language-based descriptions for
lower-level sub-tasks that comprise full trajectories. To rigorously evaluate
the quality of our automatic labeling framework, we contribute an algorithm
SIMILARITY to produce two novel metrics, temporal similarity and semantic
similarity. The metrics measure the temporal alignment and semantic fidelity of
language descriptions between two sub-task decompositions, namely an FM
sub-task decomposition prediction and a ground-truth sub-task decomposition. We
present scores for temporal similarity and semantic similarity above 90%,
compared to 30% of a randomized baseline, for multiple robotic environments,
demonstrating the effectiveness of our proposed framework. Our results enable
building diverse, large-scale, language-supervised datasets for improved
robotic TAMP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures. IROS 2024 Submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speeding Up Path Planning via Reinforcement Learning in MCTS for
  Automated Parking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinlong Zheng, Xiaozhou Zhang, Donghao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address a method that integrates reinforcement learning
into the Monte Carlo tree search to boost online path planning under fully
observable environments for automated parking tasks. Sampling-based planning
methods under high-dimensional space can be computationally expensive and
time-consuming. State evaluation methods are useful by leveraging the prior
knowledge into the search steps, making the process faster in a real-time
system. Given the fact that automated parking tasks are often executed under
complex environments, a solid but lightweight heuristic guidance is challenging
to compose in a traditional analytical way. To overcome this limitation, we
propose a reinforcement learning pipeline with a Monte Carlo tree search under
the path planning framework. By iteratively learning the value of a state and
the best action among samples from its previous cycle's outcomes, we are able
to model a value estimator and a policy generator for given states. By doing
that, we build up a balancing mechanism between exploration and exploitation,
speeding up the path planning process while maintaining its quality without
using human expert driver data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PROSPECT: Precision Robot Spectroscopy Exploration and Characterization
  Tool 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathaniel Hanson, Gary Lvov, Vedant Rautela, Samuel Hibbard, Ethan Holand, Charles DiMarzio, Taşkın Padır
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Near Infrared (NIR) spectroscopy is widely used in industrial quality control
and automation to test the purity and material quality of items. In this
research, we propose a novel sensorized end effector and acquisition strategy
to capture spectral signatures from objects and register them with a 3D point
cloud. Our methodology first takes a 3D scan of an object generated by a
time-of-flight depth camera and decomposes the object into a series of planned
viewpoints covering the surface. We generate motion plans for a robot
manipulator and end-effector to visit these viewpoints while maintaining a
fixed distance and surface normal to ensure maximal spectral signal quality
enabled by the spherical motion of the end-effector. By continuously acquiring
surface reflectance values as the end-effector scans the target object, the
autonomous system develops a four-dimensional model of the target object:
position in an R^3 coordinate frame, and a wavelength vector denoting the
associated spectral signature. We demonstrate this system in building
spectral-spatial object profiles of increasingly complex geometries. As a point
of comparison, we show our proposed system and spectral acquisition planning
yields more consistent signal signals than naive point scanning strategies for
capturing spectral information over complex surface geometries. Our work
represents a significant step towards high-resolution spectral-spatial sensor
fusion for automated quality assessment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dyna-LfLH: Learning Agile Navigation in Dynamic Environments from
  Learned Hallucination <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saad Abdul Ghani, Zizhao Wang, Peter Stone, Xuesu Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a self-supervised learning method to safely learn a
motion planner for ground robots to navigate environments with dense and
dynamic obstacles. When facing highly-cluttered, fast-moving, hard-to-predict
obstacles, classical motion planners may not be able to keep up with limited
onboard computation. For learning-based planners, high-quality demonstrations
are difficult to acquire for imitation learning while reinforcement learning
becomes inefficient due to the high probability of collision during
exploration. To safely and efficiently provide training data, the Learning from
Hallucination (LfH) approaches synthesize difficult navigation environments
based on past successful navigation experiences in relatively easy or
completely open ones, but unfortunately cannot address dynamic obstacles. In
our new Dynamic Learning from Learned Hallucination (Dyna-LfLH), we design and
learn a novel latent distribution and sample dynamic obstacles from it, so the
generated training data can be used to learn a motion planner to navigate in
dynamic environments. Dyna-LfLH is evaluated on a ground robot in both
simulated and physical environments and achieves up to 25% better success rate
compared to baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to International Conference on Intelligent Robots and
  Systems (IROS) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Contact Inertial Estimation and Localization in Legged Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergi Martinez, Robert Griffin, Carlos Mastalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal estimation is a promising tool for multi-contact inertial estimation
and localization. To harness its advantages in robotics, it is crucial to solve
these large and challenging optimization problems efficiently. To tackle this,
we (i) develop a multiple-shooting solver that exploits both temporal and
parametric structures through a parametrized Riccati recursion. Additionally,
we (ii) propose an inertial local manifold that ensures its full physical
consistency. It also enhances convergence compared to the singularity-free
log-Cholesky approach. To handle its singularities, we (iii) introduce a
nullspace approach in our optimal estimation solver. We (iv) finally develop
the analytical derivatives of contact dynamics for both inertial
parametrizations. Our framework can successfully solve estimation problems for
complex maneuvers such as brachiation in humanoids. We demonstrate its
numerical capabilities across various robotics tasks and its benefits in
experimental trials with the Go1 robot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hearing the shape of an arena with spectral swarm robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leo Cazenille, Nicolas Lobato-Dauzier, Alessia Loi, Mika Ito, Olivier Marchal, Nathanael Aubert-Kato, Nicolas Bredeche, Anthony J. Genot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Swarm robotics promises adaptability to unknown situations and robustness
against failures. However, it still struggles with global tasks that require
understanding the broader context in which the robots operate, such as
identifying the shape of the arena in which the robots are embedded. Biological
swarms, such as shoals of fish, flocks of birds, and colonies of insects,
routinely solve global geometrical problems through the diffusion of local
cues. This paradigm can be explicitly described by mathematical models that
could be directly computed and exploited by a robotic swarm. Diffusion over a
domain is mathematically encapsulated by the Laplacian, a linear operator that
measures the local curvature of a function. Crucially the geometry of a domain
can generally be reconstructed from the eigenspectrum of its Laplacian. Here we
introduce spectral swarm robotics where robots diffuse information to their
neighbors to emulate the Laplacian operator - enabling them to "hear" the
spectrum of their arena. We reveal a universal scaling that links the optimal
number of robots (a global parameter) with their optimal radius of interaction
(a local parameter). We validate experimentally spectral swarm robotics under
challenging conditions with the one-shot classification of arena shapes using a
sparse swarm of Kilobots. Spectral methods can assist with challenging tasks
where robots need to build an emergent consensus on their environment, such as
adaptation to unknown terrains, division of labor, or quorum sensing. Spectral
methods may extend beyond robotics to analyze and coordinate swarms of agents
of various natures, such as traffic or crowds, and to better understand the
long-range dynamics of natural systems emerging from short-range interactions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Step Duration for Precise Foot Placement: Achieving Robust
  Bipedal Locomotion on Terrains with Restricted Footholds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Xiang, Victor Paredes, Ayonga Hereid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel multi-step preview foot placement planning
algorithm designed to enhance the robustness of bipedal robotic walking across
challenging terrains with restricted footholds. Traditional one-step preview
planning struggles to maintain stability when stepping areas are severely
limited, such as with random stepping stones. In this work, we developed a
discrete-time Model Predictive Control (MPC) based on the step-to-step discrete
evolution of the Divergent Component of Motion (DCM) of bipedal locomotion.
This approach adaptively changes the step duration for optimal foot placement
under constraints, thereby ensuring the robot's operational viability over
multiple future steps and significantly improving its ability to navigate
through environments with tight constraints on possible footholds. The
effectiveness of this planning algorithm is demonstrated through simulations
that include a variety of complex stepping-stone configurations and external
perturbations. These tests underscore the algorithm's improved performance for
navigating foothold-restricted environments, even with the presence of external
disturbances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures, submitted to CDC 2024, for associated simulation
  video, see https://youtu.be/2jhikPlZmbE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounding Language Plans in Demonstrations Through Counterfactual
  Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanwei Wang, Tsun-Hsuan Wang, Jiayuan Mao, Michael Hagenow, Julie Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grounding the common-sense reasoning of Large Language Models in physical
domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior
works have focused on leveraging LLMs directly for planning in symbolic spaces,
this work uses LLMs to guide the search of task structures and constraints
implicit in multi-step demonstrations. Specifically, we borrow from
manipulation planning literature the concept of mode families, which group
robot configurations by specific motion constraints, to serve as an abstraction
layer between the high-level language representations of an LLM and the
low-level physical trajectories of a robot. By replaying a few human
demonstrations with synthetic perturbations, we generate coverage over the
demonstrations' state space with additional successful executions as well as
counterfactuals that fail the task. Our explanation-based learning framework
trains an end-to-end differentiable neural network to predict successful
trajectories from failures and as a by-product learns classifiers that ground
low-level states and images in mode families without dense labeling. The
learned grounding classifiers can further be used to translate language plans
into reactive policies in the physical domain in an interpretable manner. We
show our approach improves the interpretability and reactivity of imitation
learning through 2D navigation and simulated and real robot manipulation tasks.
Website: https://sites.google.com/view/grounding-plans
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision-Based Dexterous Motion Planning by Dynamic Movement Primitives
  with Human Hand Demonstration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuo Chen, Ya-Jun Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a vision-based framework for a 7-degree-of-freedom
robotic manipulator, with the primary objective of facilitating its capacity to
acquire information from human hand demonstrations for the execution of
dexterous pick-and-place tasks. Most existing works only focus on the position
demonstration without considering the orientations. In this paper, by employing
a single depth camera, MediaPipe is applied to generate the three-dimensional
coordinates of a human hand, thereby comprehensively recording the hand's
motion, encompassing the trajectory of the wrist, orientation of the hand, and
the grasp motion. A mean filter is applied during data pre-processing to smooth
the raw data. The demonstration is designed to pick up an object at a specific
angle, navigate around obstacles in its path and subsequently, deposit it
within a sloped container. The robotic system demonstrates its learning
capabilities, facilitated by the implementation of Dynamic Movement Primitives,
enabling the assimilation of user actions into its trajectories with different
start and end poi
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Berry Twist: a Twisting-Tube Soft Robotic Gripper for Blackberry
  Harvesting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes F. Elfferich, Ebrahim Shahabi, Cosimo Della Santina, Dimitra Dodou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As global demand for fruits and vegetables continues to rise, the
agricultural industry faces challenges in securing adequate labor. Robotic
harvesting devices offer a promising solution to solve this issue. However,
harvesting delicate fruits, notably blackberries, poses unique challenges due
to their fragility. This study introduces and evaluates a prototype robotic
gripper specifically designed for blackberry harvesting. The gripper features
an innovative fabric tube mechanism employing motorized twisting action to
gently envelop the fruit, ensuring uniform pressure application and minimizing
damage. Three types of tubes were developed, varying in elasticity and
compressibility using foam padding, spandex, and food-safe cotton cheesecloth.
Performance testing focused on assessing each gripper's ability to detach and
release blackberries, with emphasis on quantifying damage rates. Results
indicate the proposed gripper achieved an 82% success rate in detaching
blackberries and a 95% success rate in releasing them, showcasing the promised
potential for robotic harvesting applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Analysis of Visual <span class="highlight-title">Odometry</span> in Virtual and Real-World
  Railways Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianluca D'Amico, Mauro Marinoni, Giorgio Buttazzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perception tasks play a crucial role in the development of automated
operations and systems across multiple application fields. In the railway
transportation domain, these tasks can improve the safety, reliability, and
efficiency of various perations, including train localization, signal
recognition, and track discrimination. However, collecting considerable and
precisely labeled datasets for testing such novel algorithms poses extreme
challenges in the railway environment due to the severe restrictions in
accessing the infrastructures and the practical difficulties associated with
properly equipping trains with the required sensors, such as cameras and
LiDARs. The remarkable innovations of graphic engine tools offer new solutions
to craft realistic synthetic datasets. To illustrate the advantages of
employing graphic simulation for early-stage testing of perception tasks in the
railway domain, this paper presents a comparative analysis of the performance
of a SLAM algorithm applied both in a virtual synthetic environment and a
real-world scenario. The analysis leverages virtual railway environments
created with the latest version of Unreal Engine, facilitating data collection
and allowing the examination of challenging scenarios, including
low-visibility, dangerous operational modes, and complex environments. The
results highlight the feasibility and potentiality of graphic simulation to
advance perception tasks in the railway domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trajectory Optimization with Global Yaw Parameterization for
  Field-of-View Constrained Autonomous Flight 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Wu, Yuezhan Tao, Igor Spasojevic, Vijay Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory generation for quadrotors with limited field-of-view sensors has
numerous applications such as aerial exploration, coverage, inspection,
videography, and target tracking. Most previous works simplify the task of
optimizing yaw trajectories by either aligning the heading of the robot with
its velocity, or potentially restricting the feasible space of candidate
trajectories by using a limited yaw domain to circumvent angular singularities.
In this paper, we propose a novel \textit{global} yaw parameterization method
for trajectory optimization that allows a 360-degree yaw variation as demanded
by the underlying algorithm. This approach effectively bypasses inherent
singularities by including supplementary quadratic constraints and transforming
the final decision variables into the desired state representation. This method
significantly reduces the needed control effort, and improves optimization
feasibility. Furthermore, we apply the method to several examples of different
applications that require jointly optimizing over both the yaw and position
trajectories. Ultimately, we present a comprehensive numerical analysis and
evaluation of our proposed method in both simulation and real-world
experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Calib3D: Calibrating Model Preferences for Reliable 3D Scene
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingdong Kong, Xiang Xu, Jun Cen, Wenwei Zhang, Liang Pan, Kai Chen, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety-critical 3D scene understanding tasks necessitate not only accurate
but also confident predictions from 3D perception models. This study introduces
Calib3D, a pioneering effort to benchmark and scrutinize the reliability of 3D
scene understanding models from an uncertainty estimation viewpoint. We
comprehensively evaluate 28 state-of-the-art models across 10 diverse 3D
datasets, uncovering insightful phenomena that cope with both the aleatoric and
epistemic uncertainties in 3D scene understanding. We discover that despite
achieving impressive levels of accuracy, existing models frequently fail to
provide reliable uncertainty estimates -- a pitfall that critically undermines
their applicability in safety-sensitive contexts. Through extensive analysis of
key factors such as network capacity, LiDAR representations, rasterization
resolutions, and 3D data augmentation techniques, we correlate these aspects
directly with the model calibration efficacy. Furthermore, we introduce DeptS,
a novel depth-aware scaling approach aimed at enhancing 3D model calibration.
Extensive experiments across a wide range of configurations validate the
superiority of our method. We hope this work could serve as a cornerstone for
fostering reliable 3D scene understanding. Code and benchmark toolkits are
publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint; 37 pages, 8 figures, 11 tables; Code at
  https://github.com/ldkong1205/Calib3D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing <span class="highlight-title">LiDAR</span> Placements for Robust Driving Perception in Adverse
  Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Li, Lingdong Kong, Hanjiang Hu, Xiaohao Xu, Xiaonan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The robustness of driving perception systems under unprecedented conditions
is crucial for safety-critical usages. Latest advancements have prompted
increasing interests towards multi-LiDAR perception. However, prevailing
driving datasets predominantly utilize single-LiDAR systems and collect data
devoid of adverse conditions, failing to capture the complexities of real-world
environments accurately. Addressing these gaps, we proposed Place3D, a
full-cycle pipeline that encompasses LiDAR placement optimization, data
generation, and downstream evaluations. Our framework makes three appealing
contributions. 1) To identify the most effective configurations for multi-LiDAR
systems, we introduce a Surrogate Metric of the Semantic Occupancy Grids
(M-SOG) to evaluate LiDAR placement quality. 2) Leveraging the M-SOG metric, we
propose a novel optimization strategy to refine multi-LiDAR placements. 3)
Centered around the theme of multi-condition multi-LiDAR perception, we collect
a 364,000-frame dataset from both clean and adverse conditions. Extensive
experiments demonstrate that LiDAR placements optimized using our approach
outperform various baselines. We showcase exceptional robustness in both 3D
object detection and LiDAR semantic segmentation tasks, under diverse adverse
weather and sensor failure conditions. Code and benchmark toolkit are publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint; 40 pages, 11 figures, 15 tables; Code at
  https://github.com/ywyeli/Place3D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DriveCoT: Integrating Chain-of-Thought Reasoning with End-to-End Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianqi Wang, Enze Xie, Ruihang Chu, Zhenguo Li, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end driving has made significant progress in recent years,
demonstrating benefits such as system simplicity and competitive driving
performance under both open-loop and closed-loop settings. Nevertheless, the
lack of interpretability and controllability in its driving decisions hinders
real-world deployment for end-to-end driving systems. In this paper, we collect
a comprehensive end-to-end driving dataset named DriveCoT, leveraging the CARLA
simulator. It contains sensor data, control decisions, and chain-of-thought
labels to indicate the reasoning process. We utilize the challenging driving
scenarios from the CARLA leaderboard 2.0, which involve high-speed driving and
lane-changing, and propose a rule-based expert policy to control the vehicle
and generate ground truth labels for its reasoning process across different
driving aspects and the final decisions. This dataset can serve as an open-loop
end-to-end driving benchmark, enabling the evaluation of accuracy in various
chain-of-thought aspects and the final decision. In addition, we propose a
baseline model called DriveCoT-Agent, trained on our dataset, to generate
chain-of-thought predictions and final decisions. The trained model exhibits
strong performance in both open-loop and closed-loop evaluations, demonstrating
the effectiveness of our proposed dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Whole-Body Control for Legged Loco-Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghuan Liu, Zixuan Chen, Xuxin Cheng, Yandong Ji, Ruihan Yang, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of mobile manipulation using legged robots equipped with
an arm, namely legged loco-manipulation. The robot legs, while usually utilized
for mobility, offer an opportunity to amplify the manipulation capabilities by
conducting whole-body control. That is, the robot can control the legs and the
arm at the same time to extend its workspace. We propose a framework that can
conduct the whole-body control autonomously with visual observations. Our
approach, namely \ourFull~(\our), is composed of a low-level policy using all
degrees of freedom to track the end-effector manipulator position and a
high-level policy proposing the end-effector position based on visual inputs.
We train both levels of policies in simulation and perform Sim2Real transfer
for real robot deployment. We perform extensive experiments and show
significant improvements over baselines in picking up diverse objects in
different configurations (heights, locations, orientations) and environments.
Project page: https://wholebody-b1.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contribute equally. Project page:
  https://wholebody-b1.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Methods for Trust in Collaborative Multi-Agent Autonomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        R. Spencer Hallyburton, Miroslav Pajic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent, collaborative sensor fusion is a vital component of a
multi-national intelligence toolkit. In safety-critical and/or contested
environments, adversaries may infiltrate and compromise a number of agents. We
analyze state of the art multi-target tracking algorithms under this
compromised agent threat model. We prove that the track existence probability
test ("track score") is significantly vulnerable to even small numbers of
adversaries. To add security awareness, we design a trust estimation framework
using hierarchical Bayesian updating. Our framework builds beliefs of trust on
tracks and agents by mapping sensor measurements to trust pseudomeasurements
(PSMs) and incorporating prior trust beliefs in a Bayesian context. In case
studies, our trust estimation algorithm accurately estimates the
trustworthiness of tracks/agents, subject to observability limitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Symbolic and Subsymbolic Temporal Task Constraints from
  Bimanual Human Demonstrations <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Dreher, Tamim Asfour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning task models of bimanual manipulation from human demonstration and
their execution on a robot should take temporal constraints between actions
into account. This includes constraints on (i) the symbolic level such as
precedence relations or temporal overlap in the execution, and (ii) the
subsymbolic level such as the duration of different actions, or their starting
and end points in time. Such temporal constraints are crucial for temporal
planning, reasoning, and the exact timing for the execution of bimanual actions
on a bimanual robot. In our previous work, we addressed the learning of
temporal task constraints on the symbolic level and demonstrated how a robot
can leverage this knowledge to respond to failures during execution. In this
work, we propose a novel model-driven approach for the combined learning of
symbolic and subsymbolic temporal task constraints from multiple bimanual human
demonstrations. Our main contributions are a subsymbolic foundation of a
temporal task model that describes temporal nexuses of actions in the task
based on distributions of temporal differences between semantic action
keypoints, as well as a method based on fuzzy logic to derive symbolic temporal
task constraints from this representation. This complements our previous work
on learning comprehensive temporal task models by integrating symbolic and
subsymbolic information based on a subsymbolic foundation, while still
maintaining the symbolic expressiveness of our previous approach. We compare
our proposed approach with our previous pure-symbolic approach and show that we
can reproduce and even outperform it. Additionally, we show how the subsymbolic
temporal task constraints can synchronize otherwise unimanual movement
primitives for bimanual behavior on a humanoid robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DHP-<span class="highlight-title">Mapping</span>: A Dense Panoptic <span class="highlight-title">Mapping</span> System with Hierarchical World
  Representation and Label Optimization Techniques <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianshuai Hu, Jianhao Jiao, Yucheng Xu, Hongji Liu, Sheng Wang, Ming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Maps provide robots with crucial environmental knowledge, thereby enabling
them to perform interactive tasks effectively. Easily accessing accurate
abstract-to-detailed geometric and semantic concepts from maps is crucial for
robots to make informed and efficient decisions. To comprehensively model the
environment and effectively manage the map data structure, we propose
DHP-Mapping, a dense mapping system that utilizes multiple Truncated Signed
Distance Field (TSDF) submaps and panoptic labels to hierarchically model the
environment. The output map is able to maintain both voxel- and submap-level
metric and semantic information. Two modules are presented to enhance the
mapping efficiency and label consistency: (1) an inter-submaps label fusion
strategy to eliminate duplicate points across submaps and (2) a conditional
random field (CRF) based approach to enhance panoptic labels through object
label comprehension and contextual information. We conducted experiments with
two public datasets including indoor and outdoor scenarios. Our system performs
comparably to state-of-the-art (SOTA) methods across geometry and label
accuracy evaluation metrics. The experiment results highlight the effectiveness
and scalability of our system, as it is capable of constructing precise
geometry and maintaining consistent panoptic labels. Our code is publicly
available at https://github.com/hutslib/DHP-Mapping.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submit to IROS 2024. Project website
  https://github.com/hutslib/DHP-Mapping</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proprioception Is All You Need: Terrain Classification for Boreal
  Forests <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Damien LaRocque, William Guimont-Martin, David-Alexandre Duclos, Philippe Giguère, François Pomerleau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works in field robotics highlighted the importance of resiliency
against different types of terrains. Boreal forests, in particular, are home to
many mobility-impeding terrains that should be considered for off-road
autonomous navigation. Also, being one of the largest land biomes on Earth,
boreal forests are an area where autonomous vehicles are expected to become
increasingly common. In this paper, we address this issue by introducing
BorealTC, a publicly available dataset for proprioceptive-based terrain
classification (TC). Recorded with a Husky A200, our dataset contains 116 min
of Inertial Measurement Unit (IMU), motor current, and wheel odometry data,
focusing on typical boreal forest terrains, notably snow, ice, and silty loam.
Combining our dataset with another dataset from the state-of-the-art, we
evaluate both a Convolutional Neural Network (CNN) and the novel state space
model (SSM)-based Mamba architecture on a TC task. Interestingly, we show that
while CNN outperforms Mamba on each separate dataset, Mamba achieves greater
accuracy when trained on a combination of both. In addition, we demonstrate
that Mamba's learning capacity is greater than a CNN for increasing amounts of
data. We show that the combination of two TC datasets yields a latent space
that can be interpreted with the properties of the terrains. We also discuss
the implications of merging datasets on classification. Our source code and
dataset are publicly available online:
https://github.com/norlab-ulaval/BorealTC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the 2024 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TAIL: A Terrain-Aware Multi-Modal <span class="highlight-title">SLAM</span> <span class="highlight-title">Dataset</span> for Robot Locomotion in
  Deformable Granular Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16875v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16875v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Yao, Yangtao Ge, Guowei Shi, Zirui Wang, Ningbo Yang, Zheng Zhu, Hexiang Wei, Yuntian Zhao, Jing Wu, Zhenzhong Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Terrain-aware perception holds the potential to improve the robustness and
accuracy of autonomous robot navigation in the wilds, thereby facilitating
effective off-road traversals. However, the lack of multi-modal perception
across various motion patterns hinders the solutions of Simultaneous
Localization And Mapping (SLAM), especially when confronting non-geometric
hazards in demanding landscapes. In this paper, we first propose a
Terrain-Aware multI-modaL (TAIL) dataset tailored to deformable and sandy
terrains. It incorporates various types of robotic proprioception and distinct
ground interactions for the unique challenges and benchmark of multi-sensor
fusion SLAM. The versatile sensor suite comprises stereo frame cameras,
multiple ground-pointing RGB-D cameras, a rotating 3D LiDAR, an IMU, and an RTK
device. This ensemble is hardware-synchronized, well-calibrated, and
self-contained. Utilizing both wheeled and quadrupedal locomotion, we
efficiently collect comprehensive sequences to capture rich unstructured
scenarios. It spans the spectrum of scope, terrain interactions, scene changes,
ground-level properties, and dynamic robot characteristics. We benchmark
several state-of-the-art SLAM methods against ground truth and provide
performance validations. Corresponding challenges and limitations are also
reported. All associated resources are accessible upon request at
\url{https://tailrobot.github.io/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Robotics and Automation Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Semi-Lagrangian Approach for Time and Energy Path Planning
  Optimization in Static Flow Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Víctor C. da S. Campos, Armando A. Neto, Douglas G. Macharet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient path planning for autonomous mobile robots is a critical problem
across numerous domains, where optimizing both time and energy consumption is
paramount. This paper introduces a novel methodology that considers the dynamic
influence of an environmental flow field and considers geometric constraints,
including obstacles and forbidden zones, enriching the complexity of the
planning problem. We formulate it as a multi-objective optimal control problem,
propose a novel transformation called Harmonic Transformation, and apply a
semi-Lagrangian scheme to solve it. The set of Pareto efficient solutions is
obtained considering two distinct approaches: a deterministic method and an
evolutionary-based one, both of which are designed to make use of the proposed
Harmonic Transformation. Through an extensive analysis of these approaches, we
demonstrate their efficacy in finding optimized paths.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, initial paper submission; Preprint submitted to the IEEE
  Transactions on Intelligent Transportation Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProIn: Learning to Predict Trajectory Based on Progressive Interactions
  for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinke Dong, Haifeng Yuan, Hongkun Liu, Wei Jing, Fangzhen Li, Hongmin Liu, Bin Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate motion prediction of pedestrians, cyclists, and other surrounding
vehicles (all called agents) is very important for autonomous driving. Most
existing works capture map information through an one-stage interaction with
map by vector-based attention, to provide map constraints for social
interaction and multi-modal differentiation. However, these methods have to
encode all required map rules into the focal agent's feature, so as to retain
all possible intentions' paths while at the meantime to adapt to potential
social interaction. In this work, a progressive interaction network is proposed
to enable the agent's feature to progressively focus on relevant maps, in order
to better learn agents' feature representation capturing the relevant map
constraints. The network progressively encode the complex influence of map
constraints into the agent's feature through graph convolutions at the
following three stages: after historical trajectory encoder, after social
interaction, and after multi-modal differentiation. In addition, a weight
allocation mechanism is proposed for multi-modal training, so that each mode
can obtain learning opportunities from a single-mode ground truth. Experiments
have validated the superiority of progressive interactions to the existing
one-stage interaction, and demonstrate the effectiveness of each component.
Encouraging results were obtained in the challenging benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Model Predictive Control for Legged Robots through
  Distributed Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11742v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11742v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Amatucci, Giulio Turrisi, Angelo Bratta, Victor Barasuol, Claudio Semini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to enhance Model Predictive Control
(MPC) for legged robots through Distributed Optimization. Our method focuses on
decomposing the robot dynamics into smaller, parallelizable subsystems, and
utilizing the Alternating Direction Method of Multipliers (ADMM) to ensure
consensus among them. Each subsystem is managed by its own Optimal Control
Problem, with ADMM facilitating consistency between their optimizations. This
approach not only decreases the computational time but also allows for
effective scaling with more complex robot configurations, facilitating the
integration of additional subsystems such as articulated arms on a quadruped
robot. We demonstrate, through numerical evaluations, the convergence of our
approach on two systems with increasing complexity. In addition, we showcase
that our approach converges towards the same solution when compared to a
state-of-the-art centralized whole-body MPC implementation. Moreover, we
quantitatively compare the computational efficiency of our method to the
centralized approach, revealing up to a 75\% reduction in computational time.
Overall, our approach offers a promising avenue for accelerating MPC solutions
for legged robots, paving the way for more effective utilization of the
computational performance of modern hardware.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Precise 3D Human Pose Estimation with Multi-Perspective
  Spatial-Temporal Relational Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16700v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16700v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianbin Jiao, Xina Cheng, Weijie Chen, Xiaoting Yin, Hao Shi, Kailun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D human pose estimation captures the human joint points in three-dimensional
space while keeping the depth information and physical structure. That is
essential for applications that require precise pose information, such as
human-computer interaction, scene understanding, and rehabilitation training.
Due to the challenges in data collection, mainstream datasets of 3D human pose
estimation are primarily composed of multi-view video data collected in
laboratory environments, which contains rich spatial-temporal correlation
information besides the image frame content. Given the remarkable
self-attention mechanism of transformers, capable of capturing the
spatial-temporal correlation from multi-view video datasets, we propose a
multi-stage framework for 3D sequence-to-sequence (seq2seq) human pose
detection. Firstly, the spatial module represents the human pose feature by
intra-image content, while the frame-image relation module extracts temporal
relationships and 3D spatial positional relationship features between the
multi-perspective images. Secondly, the self-attention mechanism is adopted to
eliminate the interference from non-human body parts and reduce computing
resources. Our method is evaluated on Human3.6M, a popular 3D human pose
detection dataset. Experimental results demonstrate that our approach achieves
state-of-the-art performance on this dataset. The source code will be available
at https://github.com/WUJINHUAN/3D-human-pose.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IJCNN 2024. The source code will be available at
  https://github.com/WUJINHUAN/3D-human-pose</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement
  Learning with Diverse Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02423v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02423v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifu Yuan, Jianye Hao, Yi Ma, Zibin Dong, Hebin Liang, Jinyi Liu, Zhixin Feng, Kai Zhao, Yan Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning with Human Feedback (RLHF) has received significant
attention for performing tasks without the need for costly manual reward design
by aligning human preferences. It is crucial to consider diverse human feedback
types and various learning methods in different environments. However,
quantifying progress in RLHF with diverse feedback is challenging due to the
lack of standardized annotation platforms and widely used unified benchmarks.
To bridge this gap, we introduce Uni-RLHF, a comprehensive system
implementation tailored for RLHF. It aims to provide a complete workflow from
real human feedback, fostering progress in the development of practical
problems. Uni-RLHF contains three packages: 1) a universal multi-feedback
annotation platform, 2) large-scale crowdsourced feedback datasets, and 3)
modular offline RLHF baseline implementations. Uni-RLHF develops a
user-friendly annotation interface tailored to various feedback types,
compatible with a wide range of mainstream RL environments. We then establish a
systematic pipeline of crowdsourced annotations, resulting in large-scale
annotated datasets comprising more than 15 million steps across 30+ popular
tasks. Through extensive experiments, the results in the collected datasets
demonstrate competitive performance compared to those from well-designed manual
rewards. We evaluate various design choices and offer insights into their
strengths and potential areas of improvement. We wish to build valuable
open-source platforms, datasets, and baselines to facilitate the development of
more robust and reliable RLHF solutions based on realistic human feedback. The
website is available at https://uni-rlhf.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2024. The website is
  available at https://uni-rlhf.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Modular Pneumatic Soft Gripper Design for Aerial Grasping and Landing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00390v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00390v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiu Ching Cheung, Ching-Wei Chang, Bailun Jiang, Chih-Yung Wen, Henry K. Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aerial robots have garnered significant attention due to their potential
applications in various industries, such as inspection, search and rescue, and
drone delivery. Successful missions often depend on the ability of these robots
to grasp and land effectively. This paper presents a novel modular soft gripper
design tailored explicitly for aerial grasping and landing operations. The
proposed modular pneumatic soft gripper incorporates a feed-forward
proportional controller to regulate pressure, enabling compliant gripping
capabilities. The modular connectors of the soft fingers offer two
configurations for the 4-tip soft gripper, H-base (cylindrical) and X-base
(spherical), allowing adaptability to different target objects. Additionally,
the gripper can serve as a soft landing gear when deflated, eliminating the
need for an extra landing gear. This design reduces weight, simplifies aerial
manipulation control, and enhances flight efficiency. We demonstrate the
efficacy of indoor aerial grasping and achieve a maximum payload of 217 g using
the proposed soft aerial vehicle and its H-base pneumatic soft gripper (808 g).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 13 figures, accepted by IEEE RoboSoft 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Integral Consensus Control of Multi-Agent Networks Perturbed by
  Matched and Unmatched Disturbances: The Case of Directed Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jose Guadalupe Romero, David Navarro-Alarcon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a new method to design consensus controllers for perturbed
double integrator systems whose interconnection is described by a directed
graph containing a rooted spanning tree. We propose new robust controllers to
solve the consensus and synchronization problems when the systems are under the
effects of matched and unmatched disturbances. In both problems, we present
simple continuous controllers, whose integral actions allow us to handle the
disturbances. A rigorous stability analysis based on Lyapunov's direct method
for unperturbed networked systems is presented. To assess the performance of
our result, a representative simulation study is presented.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Group Theoretic Metric for Robot State Estimation Leveraging Chebyshev
  Interpolation <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Varun Agrawal, Frank Dellaert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new metric for robot state estimation based on the recently
introduced $\text{SE}_2(3)$ Lie group definition. Our metric is related to
prior metrics for SLAM but explicitly takes into account the linear velocity of
the state estimate, improving over current pose-based trajectory analysis. This
has the benefit of providing a single, quantitative metric to evaluate state
estimation algorithms against, while being compatible with existing tools and
libraries. Since ground truth data generally consists of pose data from motion
capture systems, we also propose an approach to compute the ground truth linear
velocity based on polynomial interpolation. Using Chebyshev interpolation and a
pseudospectral parameterization, we can accurately estimate the ground truth
linear velocity of the trajectory in an optimal fashion with best approximation
error. We demonstrate how this approach performs on multiple robotic platforms
where accurate state estimation is vital, and compare it to alternative
approaches such as finite differences. The pseudospectral parameterization also
provides a means of trajectory data compression as an additional benefit.
Experimental results show our method provides a valid and accurate means of
comparing state estimation systems, which is also easy to interpret and report.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I-PHYRE: Interactive Physical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiqian Li, Kewen Wu, Chi Zhang, Yixin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current evaluation protocols predominantly assess physical reasoning in
stationary scenes, creating a gap in evaluating agents' abilities to interact
with dynamic events. While contemporary methods allow agents to modify initial
scene configurations and observe consequences, they lack the capability to
interact with events in real time. To address this, we introduce I-PHYRE, a
framework that challenges agents to simultaneously exhibit intuitive physical
reasoning, multi-step planning, and in-situ intervention. Here, intuitive
physical reasoning refers to a quick, approximate understanding of physics to
address complex problems; multi-step denotes the need for extensive sequence
planning in I-PHYRE, considering each intervention can significantly alter
subsequent choices; and in-situ implies the necessity for timely object
manipulation within a scene, where minor timing deviations can result in task
failure. We formulate four game splits to scrutinize agents' learning and
generalization of essential principles of interactive physical reasoning,
fostering learning through interaction with representative scenarios. Our
exploration involves three planning strategies and examines several supervised
and reinforcement agents' zero-shot generalization proficiency on I-PHYRE. The
outcomes highlight a notable gap between existing learning algorithms and human
performance, emphasizing the imperative for more research in enhancing agents
with interactive physical reasoning capabilities. The environment and baselines
will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via
  Vision-Language Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Kuang, Hai Lin, Meng Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object navigation (ObjectNav) requires an agent to navigate through unseen
environments to find queried objects. Many previous methods attempted to solve
this task by relying on supervised or reinforcement learning, where they are
trained on limited household datasets with close-set objects. However, two key
challenges are unsolved: understanding free-form natural language instructions
that demand open-set objects, and generalizing to new environments in a
zero-shot manner. Aiming to solve the two challenges, in this paper, we propose
OpenFMNav, an Open-set Foundation Model based framework for zero-shot object
Navigation. We first unleash the reasoning abilities of large language models
(LLMs) to extract proposed objects from natural language instructions that meet
the user's demand. We then leverage the generalizability of large vision
language models (VLMs) to actively discover and detect candidate objects from
the scene, building a Versatile Semantic Score Map (VSSM). Then, by conducting
common sense reasoning on VSSM, our method can perform effective
language-guided exploration and exploitation of the scene and finally reach the
goal. By leveraging the reasoning and generalizing abilities of foundation
models, our method can understand free-form human instructions and perform
effective open-set zero-shot navigation in diverse environments. Extensive
experiments on the HM3D ObjectNav benchmark show that our method surpasses all
the strong baselines on all metrics, proving our method's effectiveness.
Furthermore, we perform real robot demonstrations to validate our method's
open-set-ness and generalizability to real-world environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Massive Interaction with Generalist Robotics: A Systematic
  <span class="highlight-title">Review</span> of XR-enabled Remote Human-Robot Interaction Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11384v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11384v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xian Wang, Luyao Shen, Lik-Hang Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey provides an exhaustive review of the applications of extended
reality (XR) technologies in the field of remote human-computer interaction
(HRI). We developed a systematic search strategy based on the PRISMA
methodology. From the initial 2,561 articles selected, 100 research papers that
met our inclusion criteria were included. We categorized and summarized the
domain in detail, delving into XR technologies, including augmented reality
(AR), virtual reality (VR), and mixed reality (MR), and their applications in
facilitating intuitive and effective remote control and interaction with
robotic systems.The survey highlights existing articles on the application of
XR technologies, user experience enhancement, and various interaction designs
for XR in remote HRI, providing insights into current trends and future
directions. We also identified potential gaps and opportunities for future
research to improve remote HRI systems through XR technology to guide and
inform future XR and robotics research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Directionality-Aware Mixture Model Parallel Sampling for Efficient
  Linear Parameter Varying Dynamical System Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02609v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02609v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunan Sun, Haihui Gao, Tianyu Li, Nadia Figueroa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Linear Parameter Varying Dynamical System (LPV-DS) is an effective
approach that learns stable, time-invariant motion policies using statistical
modeling and semi-definite optimization to encode complex motions for reactive
robot control. Despite its strengths, the LPV-DS learning approach faces
challenges in achieving a high model accuracy without compromising the
computational efficiency. To address this, we introduce the
Directionality-Aware Mixture Model (DAMM), a novel statistical model that
applies the Riemannian metric on the n-sphere $\mathbb{S}^n$ to efficiently
blend non-Euclidean directional data with $\mathbb{R}^m$ Euclidean states.
Additionally, we develop a hybrid Markov chain Monte Carlo technique that
combines Gibbs Sampling with Split/Merge Proposal, allowing for parallel
computation to drastically speed up inference. Our extensive empirical tests
demonstrate that LPV-DS integrated with DAMM achieves higher reproduction
accuracy, better model efficiency, and near real-time/online learning compared
to standard estimation methods on various datasets. Lastly, we demonstrate its
suitability for incrementally learning multi-behavior policies in real-world
robot experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DTG : Diffusion-based Trajectory Generation for Mapless Global
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09900v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09900v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Liang, Amirreza Payandeh, Daeun Song, Xuesu Xiao, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel end-to-end diffusion-based trajectory generation method,
DTG, for mapless global navigation in challenging outdoor scenarios with
occlusions and unstructured off-road features like grass, buildings, bushes,
etc. Given a distant goal, our approach computes a trajectory that satisfies
the following goals: (1) minimize the travel distance to the goal; (2) maximize
the traversability by choosing paths that do not lie in undesirable areas.
Specifically, we present a novel Conditional RNN(CRNN) for diffusion models to
efficiently generate trajectories. Furthermore, we propose an adaptive training
method that ensures that the diffusion model generates more traversable
trajectories. We evaluate our methods in various outdoor scenes and compare the
performance with other global navigation algorithms on a Husky robot. In
practice, we observe at least a 15% improvement in traveling distance and
around a 7% improvement in traversability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast <span class="highlight-title">LiDAR</span> Informed Visual Search in Unseen Indoor Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14150v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14150v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Gupta, Kyle Morgenstein, Steven Ortega, Luis Sentis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the problem of planning for visual search without prior
map information. We leverage the pixel-wise environment perception problem
where one is given wide Field of View 2D scan data and must perform LiDAR
segmentation to contextually label points in the surroundings. These pixel
classifications provide an informed prior on which to plan next best viewpoints
during visual search tasks. We present LIVES: LiDAR Informed Visual Search, a
method aimed at finding objects of interest in unknown indoor environments. A
robust map-free classifier is trained from expert data collected using a simple
cart platform equipped with a map-based classifier. An autonomous exploration
planner takes the contextual data from scans and uses that prior to plan
viewpoints more likely to yield detection of the search target. We propose a
utility function that accounts for traditional metrics like information gain
and path cost and for the contextual information. LIVES is baselined against
several existing exploration methods in simulation to verify its performance.
It is validated in real-world experiments with single and multiple search
objects with a Spot robot in two unseen environments. Videos of experiments,
implementation details and open source code can be found at
https://sites.google.com/view/lives-2024/home.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages + references. 6 figures. 1 algorithm. 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diagrammatic Instructions to Specify Spatial Objectives and Constraints
  with Applications to Mobile Base Placement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12465v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12465v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qilin Sun, Weiming Zhi, Tianyi Zhang, Matthew Johnson-Roberson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Spatial Diagrammatic Instructions (SDIs), an approach
for human operators to specify objectives and constraints that are related to
spatial regions in the working environment. Human operators are enabled to
sketch out regions directly on camera images that correspond to the objectives
and constraints. These sketches are projected to 3D spatial coordinates, and
continuous Spatial Instruction Maps (SIMs) are learned upon them. These maps
can then be integrated into optimization problems for tasks of robots. In
particular, we demonstrate how Spatial Diagrammatic Instructions can be applied
to solve the Base Placement Problem of mobile manipulators, which concerns the
best place to put the manipulator to facilitate a certain task. Human operators
can specify, via sketch, spatial regions of interest for a manipulation task
and permissible regions for the mobile manipulator to be at. Then, an
optimization problem that maximizes the manipulator's reachability, or
coverage, over the designated regions of interest while remaining in the
permissible regions is solved. We provide extensive empirical evaluations, and
show that our formulation of Spatial Instruction Maps provides accurate
representations of user-specified diagrammatic instructions. Furthermore, we
demonstrate that our diagrammatic approach to the Mobile Base Placement Problem
enables higher quality solutions and faster run-time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Greedy Perspectives: Multi-Drone View Planning for Collaborative
  Perception in Cluttered Environments <span class="chip">IROS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10863v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10863v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krishna Suresh, Aditya Rauniyar, Micah Corah, Sebastian Scherer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deployment of teams of aerial robots could enable large-scale filming of
dynamic groups of people (actors) in complex environments for applications in
areas such as team sports and cinematography. Toward this end, methods for
submodular maximization via sequential greedy planning can be used for scalable
optimization of camera views across teams of robots but face challenges with
efficient coordination in cluttered environments. Obstacles can produce
occlusions and increase chances of inter-robot collision which can violate
requirements for near-optimality guarantees. To coordinate teams of aerial
robots in filming groups of people in dense environments, a more general
view-planning approach is required. We explore how collision and occlusion
impact performance in filming applications through the development of a
multi-robot multi-actor view planner with an occlusion-aware objective for
filming groups of people and compare with a formation planner and a greedy
planner that ignores inter-robot collisions. We evaluate our approach based on
five test environments and complex multi-actor behaviors. Compared with a
formation planner, our sequential planner generates 14% greater view reward
over the actors for three scenarios and comparable performance to formation
planning on two others. We also observe near identical view rewards for
sequential planning both with and without inter-robot collision constraints
which indicates that robots are able to avoid collisions without impairing
performance in the perception task. Overall, we demonstrate effective
coordination of teams of aerial robots for filming groups that may split,
merge, or spread apart and in environments cluttered with obstacles that may
cause collisions or occlusions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS'24; 8 pages, 8 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Crowd-Aware Multi-Agent Path Finding through Local
  Broadcasting with Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10275v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10275v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phu Pham, Aniket Bera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Agent Path Finding (MAPF) in crowded environments presents a
challenging problem in motion planning, aiming to find collision-free paths for
all agents in the system. MAPF finds a wide range of applications in various
domains, including aerial swarms, autonomous warehouse robotics, and
self-driving vehicles. Current approaches to MAPF generally fall into two main
categories: centralized and decentralized planning. Centralized planning
suffers from the curse of dimensionality when the number of agents or states
increases and thus does not scale well in large and complex environments. On
the other hand, decentralized planning enables agents to engage in real-time
path planning within a partially observable environment, demonstrating implicit
coordination. However, they suffer from slow convergence and performance
degradation in dense environments. In this paper, we introduce CRAMP, a novel
crowd-aware decentralized reinforcement learning approach to address this
problem by enabling efficient local communication among agents via Graph Neural
Networks (GNNs), facilitating situational awareness and decision-making
capabilities in congested environments. We test CRAMP on simulated environments
and demonstrate that our method outperforms the state-of-the-art decentralized
methods for MAPF on various metrics. CRAMP improves the solution quality up to
59% measured in makespan and collision count, and up to 35% improvement in
success rate in comparison to previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SO(2)-Equivariant Downwash Models for Close Proximity Flight 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18983v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18983v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H. Smith, A. Shankar, J. Gielis, J. Blumenkamp, A. Prorok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multirotors flying in close proximity induce aerodynamic wake effects on each
other through propeller downwash. Conventional methods have fallen short of
providing adequate 3D force-based models that can be incorporated into robust
control paradigms for deploying dense formations. Thus, learning a model for
these downwash patterns presents an attractive solution. In this paper, we
present a novel learning-based approach for modelling the downwash forces that
exploits the latent geometries (i.e. symmetries) present in the problem. We
demonstrate that when trained with only 5 minutes of real-world flight data,
our geometry-aware model outperforms state-of-the-art baseline models trained
with more than 15 minutes of data. In dense real-world flights with two
vehicles, deploying our model online improves 3D trajectory tracking by nearly
36% on average (and vertical tracking by 56%).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decision-Oriented Learning Using Differentiable Submodular Maximization
  for Multi-Robot Coordination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01519v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01519v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyao Shi, Chak Lam Shek, Nare Karapetyan, Pratap Tokekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a differentiable, decision-oriented learning framework for cost
prediction in a class of multi-robot decision-making problems, in which the
robots need to trade off the task performance with the costs of taking actions
when they select actions to take. Specifically, we consider the cases where the
task performance is measured by a known monotone submodular function (e.g.,
coverage, mutual information), and the cost of actions depends on the context
(e.g., wind and terrain conditions). We need to learn a function that maps the
context to the costs. Classically, we treat such a learning problem and the
downstream decision-making problem as two decoupled problems, i.e., we first
learn to predict the cost function without considering the downstream
decision-making problem, and then use the learned function for predicting the
cost and using it in the decision-making problem. However, the loss function
used in learning a prediction function may not be aligned with the downstream
decision-making. We propose a decision-oriented learning framework that
incorporates the downstream task performance in the prediction phase via a
differentiable optimization layer. The main computational challenge in such a
framework is to make the combinatorial optimization, i.e., non-monotone
submodular maximization, differentiable. This function is not naturally
differentiable. We propose the Differentiable Cost Scaled Greedy algorithm
(D-CSG), which is a continuous and differentiable relaxation of CSG. We
demonstrate the efficacy of the proposed framework through numerical
simulations. The results show that the proposed framework can result in better
performance than the traditional two-stage approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2303.01543</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pre-Trained Masked Image Model for Mobile Robot Navigation <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07021v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07021v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishnu Dutt Sharma, Anukriti Singh, Pratap Tokekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  2D top-down maps are commonly used for the navigation and exploration of
mobile robots through unknown areas. Typically, the robot builds the navigation
maps incrementally from local observations using onboard sensors. Recent works
have shown that predicting the structural patterns in the environment through
learning-based approaches can greatly enhance task efficiency. While many such
works build task-specific networks using limited datasets, we show that the
existing foundational vision networks can accomplish the same without any
fine-tuning. Specifically, we use Masked Autoencoders, pre-trained on street
images, to present novel applications for field-of-view expansion, single-agent
topological exploration, and multi-agent exploration for indoor mapping, across
different input modalities. Our work motivates the use of foundational vision
models for generalized structure prediction-driven applications, especially in
the dearth of training data. For more qualitative results see
https://raaslab.org/projects/MIM4Robots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GelLink: A Compact Multi-phalanx Finger with Vision-based Tactile
  Sensing and Proprioception <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14887v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14887v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Ma, Jialiang Zhao, Edward Adelson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared to fully-actuated robotic end-effectors, underactuated ones are
generally more adaptive, robust, and cost-effective. However, state estimation
for underactuated hands is usually more challenging. Vision-based tactile
sensors, like Gelsight, can mitigate this issue by providing high-resolution
tactile sensing and accurate proprioceptive sensing. As such, we present
GelLink, a compact, underactuated, linkage-driven robotic finger with low-cost,
high-resolution vision-based tactile sensing and proprioceptive sensing
capabilities. In order to reduce the amount of embedded hardware, i.e. the
cameras and motors, we optimize the linkage transmission with a planar linkage
mechanism simulator and develop a planar reflection simulator to simplify the
tactile sensing hardware. As a result, GelLink only requires one motor to
actuate the three phalanges, and one camera to capture tactile signals along
the entire finger. Overall, GelLink is a compact robotic finger that shows
adaptability and robustness when performing grasping tasks. The integration of
vision-based tactile sensors can significantly enhance the capabilities of
underactuated fingers and potentially broaden their future usage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Supplement video: https://www.youtube.com/watch?v=hZwUpAig5C0 . 7
  pages, 9 figures. ICRA 2024 (IEEE International Conference on Robotics and
  Automation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ C3D: Cascade Control with Change Point Detection and Deep Koopman
  Learning for Autonomous Surface Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05972v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05972v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianwen Li, Hyunsang Park, Wenjian Hao, Lei Xin, Jalil Chavez-Galaviz, Ajinkya Chaudhary, Meredith Bloss, Kyle Pattison, Christopher Vo, Devesh Upadhyay, Shreyas Sundaram, Shaoshuai Mou, Nina Mahmoudian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we discuss the development and deployment of a robust
autonomous system capable of performing various tasks in the maritime domain
under unknown dynamic conditions. We investigate a data-driven approach based
on modular design for ease of transfer of autonomy across different maritime
surface vessel platforms. The data-driven approach alleviates issues related to
a priori identification of system models that may become deficient under
evolving system behaviors or shifting, unanticipated, environmental influences.
Our proposed learning-based platform comprises a deep Koopman system model and
a change point detector that provides guidance on domain shifts prompting
relearning under severe exogenous and endogenous perturbations. Motion control
of the autonomous system is achieved via an optimal controller design. The
Koopman linearized model naturally lends itself to a linear-quadratic regulator
(LQR) control design. We propose the C3D control architecture Cascade Control
with Change Point Detection and Deep Koopman Learning. The framework is
verified in station keeping task on an ASV in both simulation and real
experiments. The approach achieved at least 13.9 percent improvement in mean
distance error in all test cases compared to the methods that do not consider
system changes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Feedback Law in Stochastic Optimal Nonlinear Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2004.01041v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2004.01041v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Naveed Gul Mohamed, Suman Chakravorty, Raman Goyal, Ran Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of nonlinear stochastic optimal control. This problem
is thought to be fundamentally intractable owing to Bellman's ``curse of
dimensionality". We present a result that shows that repeatedly solving an
open-loop deterministic problem from the current state with progressively
shorter horizons, similar to Model Predictive Control (MPC), results in a
feedback policy that is $O(\epsilon^4)$ near to the true global stochastic
optimal policy, \nxx{where $\epsilon$ is a perturbation parameter modulating
the noise.} We show that the optimal deterministic feedback problem has a
perturbation structure in that higher-order terms of the feedback law do not
affect lower-order terms, and that this structure is lost in the optimal
stochastic feedback problem. Consequently, solving the Stochastic Dynamic
Programming problem is highly susceptible to noise, even when tractable, and in
practice, the MPC-type feedback law offers superior performance even for
stochastic systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2002.10505,
  arXiv:2002.09478</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Body-mounted MR-conditional Robot for Minimally Invasive Liver
  Intervention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07822v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07822v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhefeng Huang, Anthony L. Gunderman, Samuel E. Wilcox, Saikat Sengupta, Jay Shah, Aiming Lu, David Woodrum, Yue Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MR-guided microwave ablation (MWA) has proven effective in treating
hepatocellular carcinoma (HCC) with small-sized tumors, but the
state-of-the-art technique suffers from sub-optimal workflow due to speed and
accuracy of needle placement. This paper presents a compact body-mounted
MR-conditional robot that can operate in closed-bore MR scanners for accurate
needle guidance. The robotic platform consists of two stacked Cartesian XY
stages, each with two degrees of freedom, that facilitate needle guidance. The
robot is actuated using 3D-printed pneumatic turbines with MR-conditional bevel
gear transmission systems. Pneumatic valves and control mechatronics are
located inside the MRI control room and are connected to the robot with
pneumatic transmission lines and optical fibers. Free space experiments
indicated robot-assisted needle insertion error of 2.6$\pm$1.3 mm at an
insertion depth of 80 mm. The MR-guided phantom studies were conducted to
verify the MR-conditionality and targeting performance of the robot. Future
work will focus on the system optimization and validations in animal trials.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mind Meets Robots: A <span class="highlight-title">Review</span> of EEG-Based Brain-Robot Interaction Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06186v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06186v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchong Zhang, Nona Rajabi, Farzaneh Taleb, Andrii Matviienko, Yong Ma, Mårten Björkman, Danica Kragic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain-robot interaction (BRI) empowers individuals to control
(semi-)automated machines through their brain activity, either passively or
actively. In the past decade, BRI systems have achieved remarkable success,
predominantly harnessing electroencephalogram (EEG) signals as the central
component. This paper offers an up-to-date and exhaustive examination of 87
curated studies published during the last five years (2018-2023), focusing on
identifying the research landscape of EEG-based BRI systems. This review aims
to consolidate and underscore methodologies, interaction modes, application
contexts, system evaluation, existing challenges, and potential avenues for
future investigations in this domain. Based on our analysis, we present a BRI
system model with three entities: Brain, Robot, and Interaction, depicting the
internal relationships of a BRI system. We especially investigate the essence
and principles on interaction modes between human brains and robots, a domain
that has not yet been identified anywhere. We then discuss these entities with
different dimensions encompassed. Within this model, we scrutinize and classify
current research, reveal insights, specify challenges, and provide
recommendations for future research trajectories in this field. Meanwhile, we
envision our findings offer a design space for future human-robot interaction
(HRI) research, informing the creation of efficient BRI frameworks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning with Options and State Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10855v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10855v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayoub Ghriss, Masashi Sugiyama, Alessandro Lazaric
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current thesis aims to explore the reinforcement learning field and build
on existing methods to produce improved ones to tackle the problem of learning
in high-dimensional and complex environments. It addresses such goals by
decomposing learning tasks in a hierarchical fashion known as Hierarchical
Reinforcement Learning.
  We start in the first chapter by getting familiar with the Markov Decision
Process framework and presenting some of its recent techniques that the
following chapters use. We then proceed to build our Hierarchical Policy
learning as an answer to the limitations of a single primitive policy. The
hierarchy is composed of a manager agent at the top and employee agents at the
lower level.
  In the last chapter, which is the core of this thesis, we attempt to learn
lower-level elements of the hierarchy independently of the manager level in
what is known as the "Eigenoption". Based on the graph structure of the
environment, Eigenoptions allow us to build agents that are aware of the
geometric and dynamic properties of the environment. Their decision-making has
a special property: it is invariant to symmetric transformations of the
environment, allowing as a consequence to greatly reduce the complexity of the
learning task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Master Thesis 2018, MVA ENS Paris-Saclay, Tokyo RIKEN AIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-BEV: Zero-shot Projection of Any First-Person Modality to BEV Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianluca Monaci, Leonid Antsfeld, Boris Chidlovskii, Christian Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bird's-eye view (BEV) maps are an important geometrically structured
representation widely used in robotics, in particular self-driving vehicles and
terrestrial robots. Existing algorithms either require depth information for
the geometric projection, which is not always reliably available, or are
trained end-to-end in a fully supervised way to map visual first-person
observations to BEV representation, and are therefore restricted to the output
modality they have been trained for. In contrast, we propose a new model
capable of performing zero-shot projections of any modality available in a
first person view to the corresponding BEV map. This is achieved by
disentangling the geometric inverse perspective projection from the modality
transformation, eg. RGB to occupancy. The method is general and we showcase
experiments projecting to BEV three different modalities: semantic
segmentation, motion vectors and object bounding boxes detected in first
person. We experimentally show that the model outperforms competing methods, in
particular the widely used baseline resorting to monocular depth estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vehicle Trajectory Tracking Through Magnetic Sensors: A Case Study of
  Two-lane Road 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.09020v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.09020v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojiang Ren, Yan Wang, Yingfan Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent Transportation Systems (ITS) have a pressing need for efficient
and reliable traffic surveillance solutions. This paper for the first time
proposes a surveillance system that utilizes low-cost magnetic sensors for
detecting and tracking vehicles continuously along the road. The system uses
multiple sensors mounted along the roadside and lane boundaries to capture the
movement of vehicles. Real-time measurement data is collected by base stations
and processed to produce vehicle trajectories that include position, timestamp,
and speed. To address the challenge of tracking vehicles continuously on a road
network using a large amount of unlabeled magnetic sensor measurements, we
first define a vehicle trajectory tracking problem. We then propose a
graph-based data association algorithm to track each detected vehicle, and
design a related online algorithm framework respectively. We finally validate
the performance via both experimental simulation and real-world road
deployment. The experimental results demonstrate that the proposed solution
provides a cost-effective solution to capture the driving status of vehicles
and on that basis form various traffic safety and efficiency applications.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">127</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Priors from 3D Diffusion Models for RGB-Based One-Shot View
  Planning <span class="chip">IROS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicong Pan, Liren Jin, Xuying Huang, Cyrill Stachniss, Marija Popović, Maren Bennewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object reconstruction is relevant for many autonomous robotic tasks that
require interaction with the environment. A key challenge in such scenarios is
planning view configurations to collect informative measurements for
reconstructing an initially unknown object. One-shot view planning enables
efficient data collection by predicting view configurations and planning the
globally shortest path connecting all views at once. However, geometric priors
about the object are required to conduct one-shot view planning. In this work,
we propose a novel one-shot view planning approach that utilizes the powerful
3D generation capabilities of diffusion models as priors. By incorporating such
geometric priors into our pipeline, we achieve effective one-shot view planning
starting with only a single RGB image of the object to be reconstructed. Our
planning experiments in simulation and real-world setups indicate that our
approach balances well between object reconstruction quality and movement cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Sicong Pan and Liren Jin have equal contribution. Submitted to IROS
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CurbNet: Curb Detection Framework Based on <span class="highlight-title">LiDAR</span> Point Cloud
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoyang Zhao, Fulong Ma, Yuxuan Liu, Weiqing Qi, Ming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Curb detection is an important function in intelligent driving and can be
used to determine drivable areas of the road. However, curbs are difficult to
detect due to the complex road environment. This paper introduces CurbNet, a
novel framework for curb detection, leveraging point cloud segmentation.
Addressing the dearth of comprehensive curb datasets and the absence of 3D
annotations, we have developed the 3D-Curb dataset, encompassing 7,100 frames,
which represents the largest and most categorically diverse collection of curb
point clouds currently available. Recognizing that curbs are primarily
characterized by height variations, our approach harnesses spatially-rich 3D
point clouds for training. To tackle the challenges presented by the uneven
distribution of curb features on the xy-plane and their reliance on z-axis
high-frequency features, we introduce the multi-scale and channel attention
(MSCA) module, a bespoke solution designed to optimize detection performance.
Moreover, we propose an adaptive weighted loss function group, specifically
formulated to counteract the imbalance in the distribution of curb point clouds
relative to other categories. Our extensive experimentation on 2 major datasets
has yielded results that surpass existing benchmarks set by leading curb
detection and point cloud segmentation models. By integrating multi-clustering
and curve fitting techniques in our post-processing stage, we have
substantially reduced noise in curb detection, thereby enhancing precision to
0.8744. Notably, CurbNet has achieved an exceptional average metrics of over
0.95 at a tolerance of just 0.15m, thereby establishing a new benchmark.
Furthermore, corroborative real-world experiments and dataset analyzes mutually
validate each other, solidifying CurbNet's superior detection proficiency and
its robust generalizability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HPL-ESS: Hybrid Pseudo-Labeling for Unsupervised Event-based Semantic
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linglin Jing, Yiming Ding, Yunpeng Gao, Zhigang Wang, Xu Yan, Dong Wang, Gerald Schaefer, Hui Fang, Bin Zhao, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event-based semantic segmentation has gained popularity due to its capability
to deal with scenarios under high-speed motion and extreme lighting conditions,
which cannot be addressed by conventional RGB cameras. Since it is hard to
annotate event data, previous approaches rely on event-to-image reconstruction
to obtain pseudo labels for training. However, this will inevitably introduce
noise, and learning from noisy pseudo labels, especially when generated from a
single source, may reinforce the errors. This drawback is also called
confirmation bias in pseudo-labeling. In this paper, we propose a novel hybrid
pseudo-labeling framework for unsupervised event-based semantic segmentation,
HPL-ESS, to alleviate the influence of noisy pseudo labels. In particular, we
first employ a plain unsupervised domain adaptation framework as our baseline,
which can generate a set of pseudo labels through self-training. Then, we
incorporate offline event-to-image reconstruction into the framework, and
obtain another set of pseudo labels by predicting segmentation maps on the
reconstructed images. A noisy label learning strategy is designed to mix the
two sets of pseudo labels and enhance the quality. Moreover, we propose a soft
prototypical alignment module to further improve the consistency of target
domain features. Extensive experiments show that our proposed method
outperforms existing state-of-the-art methods by a large margin on the
DSEC-Semantic dataset (+5.88% accuracy, +10.32% mIoU), which even surpasses
several supervised methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Anatomy of Adversarial Attacks: Concept-based XAI Dissection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgii Mikriukov, Gesina Schwalbe, Franz Motzkus, Korinna Bade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks (AAs) pose a significant threat to the reliability and
robustness of deep neural networks. While the impact of these attacks on model
predictions has been extensively studied, their effect on the learned
representations and concepts within these models remains largely unexplored. In
this work, we perform an in-depth analysis of the influence of AAs on the
concepts learned by convolutional neural networks (CNNs) using eXplainable
artificial intelligence (XAI) techniques. Through an extensive set of
experiments across various network architectures and targeted AA techniques, we
unveil several key findings. First, AAs induce substantial alterations in the
concept composition within the feature space, introducing new concepts or
modifying existing ones. Second, the adversarial perturbation itself can be
linearly decomposed into a set of latent vector components, with a subset of
these being responsible for the attack's success. Notably, we discover that
these components are target-specific, i.e., are similar for a given target
class throughout different AA techniques and starting classes. Our findings
provide valuable insights into the nature of AAs and their impact on learned
representations, paving the way for the development of more robust and
interpretable deep learning models, as well as effective defenses against
adversarial threats.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sophie Starck, Vasiliki Sideri-Lampretsa, Bernhard Kainz, Martin Menten, Tamara Mueller, Daniel Rueckert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anatomical atlases are widely used for population analysis. Conditional
atlases target a particular sub-population defined via certain conditions (e.g.
demographics or pathologies) and allow for the investigation of fine-grained
anatomical differences - such as morphological changes correlated with age.
Existing approaches use either registration-based methods that are unable to
handle large anatomical variations or generative models, which can suffer from
training instabilities and hallucinations. To overcome these limitations, we
use latent diffusion models to generate deformation fields, which transform a
general population atlas into one representing a specific sub-population. By
generating a deformation field and registering the conditional atlas to a
neighbourhood of images, we ensure structural plausibility and avoid
hallucinations, which can occur during direct image synthesis. We compare our
method to several state-of-the-art atlas generation methods in experiments
using 5000 brain as well as whole-body MR images from UK Biobank. Our method
generates highly realistic atlases with smooth transformations and high
anatomical fidelity, outperforming the baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Creating a Digital Twin of Spinal Surgery: A Proof of Concept 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Hein, Frederic Giraud, Lilian Calvet, Alexander Schwarz, Nicola Alessandro Cavalcanti, Sergey Prokudin, Mazda Farshad, Siyu Tang, Marc Pollefeys, Fabio Carrillo, Philipp Fürnstahl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgery digitalization is the process of creating a virtual replica of
real-world surgery, also referred to as a surgical digital twin (SDT). It has
significant applications in various fields such as education and training,
surgical planning, and automation of surgical tasks. Given their detailed
representations of surgical procedures, SDTs are an ideal foundation for
machine learning methods, enabling automatic generation of training data. In
robotic surgery, SDTs can provide realistic virtual environments in which
robots may learn through trial and error. In this paper, we present a proof of
concept (PoC) for surgery digitalization that is applied to an ex-vivo spinal
surgery performed in realistic conditions. The proposed digitalization focuses
on the acquisition and modelling of the geometry and appearance of the entire
surgical scene. We employ five RGB-D cameras for dynamic 3D reconstruction of
the surgeon, a high-end camera for 3D reconstruction of the anatomy, an
infrared stereo camera for surgical instrument tracking, and a laser scanner
for 3D reconstruction of the operating room and data fusion. We justify the
proposed methodology, discuss the challenges faced and further extensions of
our prototype. While our PoC partially relies on manual data curation, its high
quality and great potential motivate the development of automated methods for
the creation of SDTs. The quality of our SDT can be assessed in a rendered
video available at https://youtu.be/LqVaWGgaTMY .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DPStyler: Dynamic PromptStyler for Source-Free Domain Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunlong Tang, Yuxuan Wan, Lei Qi, Xin Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Source-Free Domain Generalization (SFDG) aims to develop a model that works
for unseen target domains without relying on any source domain. Recent work,
PromptStyler, employs text prompts to simulate different distribution shifts in
the joint vision-language space, allowing the model to generalize effectively
to unseen domains without using any images. However, 1) PromptStyler's style
generation strategy has limitations, as all style patterns are fixed after the
first training phase. This leads to the training set in the second training
phase being restricted to a limited set of styles. Additionally, 2) the frozen
text encoder in PromptStyler result in the encoder's output varying with the
style of the input text prompts, making it difficult for the model to learn
domain-invariant features. In this paper, we introduce Dynamic PromptStyler
(DPStyler), comprising Style Generation and Style Removal modules to address
these issues. The Style Generation module refreshes all styles at every
training epoch, while the Style Removal module eliminates variations in the
encoder's output features caused by input styles. Moreover, since the Style
Generation module, responsible for generating style word vectors using random
sampling or style mixing, makes the model sensitive to input text prompts, we
introduce a model ensemble method to mitigate this sensitivity. Extensive
experiments demonstrate that our framework outperforms state-of-the-art methods
on benchmark datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing the Performance of Deep Learning for Automated Gleason Grading
  in Prostate Cancer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Müller, Philip Meyer, Lukas Rentschler, Robin Manz, Daniel Hieber, Jonas Bäcker, Samantha Cramer, Christoph Wengenmayr, Bruno Märkl, Ralf Huss, Frank Kramer, Iñaki Soto-Rey, Johannes Raffler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prostate cancer is a dominant health concern calling for advanced diagnostic
tools. Utilizing digital pathology and artificial intelligence, this study
explores the potential of 11 deep neural network architectures for automated
Gleason grading in prostate carcinoma focusing on comparing traditional and
recent architectures. A standardized image classification pipeline, based on
the AUCMEDI framework, facilitated robust evaluation using an in-house dataset
consisting of 34,264 annotated tissue tiles. The results indicated varying
sensitivity across architectures, with ConvNeXt demonstrating the strongest
performance. Notably, newer architectures achieved superior performance, even
though with challenges in differentiating closely related Gleason grades. The
ConvNeXt model was capable of learning a balance between complexity and
generalizability. Overall, this study lays the groundwork for enhanced Gleason
grading systems, potentially improving diagnostic efficiency for prostate
cancer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synapse: Learning Preferential Concepts from Visual Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadanand Modak, Noah Patton, Isil Dillig, Joydeep Biswas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of preference learning, which aims to learn
user-specific preferences (e.g., "good parking spot", "convenient drop-off
location") from visual input. Despite its similarity to learning factual
concepts (e.g., "red cube"), preference learning is a fundamentally harder
problem due to its subjective nature and the paucity of person-specific
training data. We address this problem using a new framework called Synapse,
which is a neuro-symbolic approach designed to efficiently learn preferential
concepts from limited demonstrations. Synapse represents preferences as
neuro-symbolic programs in a domain-specific language (DSL) that operates over
images, and leverages a novel combination of visual parsing, large language
models, and program synthesis to learn programs representing individual
preferences. We evaluate Synapse through extensive experimentation including a
user case study focusing on mobility-related concepts in mobile robotics and
autonomous driving. Our evaluation demonstrates that Synapse significantly
outperforms existing baselines as well as its own ablations. The code and other
details can be found on the project website https://amrl.cs.utexas.edu/synapse .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 7 figures; Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepGleason: a System for Automated Gleason Grading of Prostate Cancer
  using Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Müller, Philip Meyer, Lukas Rentschler, Robin Manz, Jonas Bäcker, Samantha Cramer, Christoph Wengenmayr, Bruno Märkl, Ralf Huss, Iñaki Soto-Rey, Johannes Raffler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in digital pathology and artificial intelligence (AI) offer
promising opportunities for clinical decision support and enhancing diagnostic
workflows. Previous studies already demonstrated AI's potential for automated
Gleason grading, but lack state-of-the-art methodology and model reusability.
To address this issue, we propose DeepGleason: an open-source deep neural
network based image classification system for automated Gleason grading using
whole-slide histopathology images from prostate tissue sections. Implemented
with the standardized AUCMEDI framework, our tool employs a tile-wise
classification approach utilizing fine-tuned image preprocessing techniques in
combination with a ConvNeXt architecture which was compared to various
state-of-the-art architectures. The neural network model was trained and
validated on an in-house dataset of 34,264 annotated tiles from 369 prostate
carcinoma slides. We demonstrated that DeepGleason is capable of highly
accurate and reliable Gleason grading with a macro-averaged F1-score of 0.806,
AUC of 0.991, and Accuracy of 0.974. The internal architecture comparison
revealed that the ConvNeXt model was superior performance-wise on our dataset
to established and other modern architectures like transformers. Furthermore,
we were able to outperform the current state-of-the-art in tile-wise
fine-classification with a sensitivity and specificity of 0.94 and 0.98 for
benign vs malignant detection as well as of 0.91 and 0.75 for Gleason 3 vs
Gleason 4 & 5 classification, respectively. Our tool contributes to the wider
adoption of AI-based Gleason grading within the research community and paves
the way for broader clinical application of deep learning models in digital
pathology. DeepGleason is open-source and publicly available for research
application in the following Git repository:
https://github.com/frankkramer-lab/DeepGleason.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FOOL: Addressing the Downlink Bottleneck in Satellite Computing with
  Neural Feature Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Furutanpey, Qiyang Zhang, Philipp Raith, Tobias Pfandzelter, Shangguang Wang, Schahram Dustdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nanosatellite constellations equipped with sensors capturing large geographic
regions provide unprecedented opportunities for Earth observation. As
constellation sizes increase, network contention poses a downlink bottleneck.
Orbital Edge Computing (OEC) leverages limited onboard compute resources to
reduce transfer costs by processing the raw captures at the source. However,
current solutions have limited practicability due to reliance on crude
filtering methods or over-prioritizing particular downstream tasks.
  This work presents FOOL, an OEC-native and task-agnostic feature compression
method that preserves prediction performance. FOOL partitions high-resolution
satellite imagery to maximize throughput. Further, it embeds context and
leverages inter-tile dependencies to lower transfer costs with negligible
overhead. While FOOL is a feature compressor, it can recover images with
competitive scores on perceptual quality measures at lower bitrates. We
extensively evaluate transfer cost reduction by including the peculiarity of
intermittently available network connections in low earth orbit. Lastly, we
test the feasibility of our system for standardized nanosatellite form factors.
We demonstrate that FOOL permits downlinking over 100x the data volume without
relying on prior information on the downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, double column, 19 figures, 7 tables, Initial Submission to
  IEEE Transactions on Mobile Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Adaptive Detection of MAVs: A Benchmark and Noise Suppression
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yin Zhang, Jinhong Deng, Peidong Liu, Wen Li, Shiyu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual detection of Micro Air Vehicles (MAVs) has attracted increasing
attention in recent years due to its important application in various tasks.
The existing methods for MAV detection assume that the training set and testing
set have the same distribution. As a result, when deployed in new domains, the
detectors would have a significant performance degradation due to domain
discrepancy. In this paper, we study the problem of cross-domain MAV detection.
The contributions of this paper are threefold. 1) We propose a
Multi-MAV-Multi-Domain (M3D) dataset consisting of both simulation and
realistic images. Compared to other existing datasets, the proposed one is more
comprehensive in the sense that it covers rich scenes, diverse MAV types, and
various viewing angles. A new benchmark for cross-domain MAV detection is
proposed based on the proposed dataset. 2) We propose a Noise Suppression
Network (NSN) based on the framework of pseudo-labeling and a large-to-small
training procedure. To reduce the challenging pseudo-label noises, two novel
modules are designed in this network. The first is a prior-based curriculum
learning module for allocating adaptive thresholds for pseudo labels with
different difficulties. The second is a masked copy-paste augmentation module
for pasting truly-labeled MAVs on unlabeled target images and thus decreasing
pseudo-label noises. 3) Extensive experimental results verify the superior
performance of the proposed method compared to the state-of-the-art ones. In
particular, it achieves mAP of 46.9%(+5.8%), 50.5%(+3.7%), and 61.5%(+11.3%) on
the tasks of simulation-to-real adaptation, cross-scene adaptation, and
cross-camera adaptation, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 11 figures. Accepted by IEEE Transactions on Automation
  Science and Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clustering Propagation for Universal Medical Image Segmentation <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Ding, Liulei Li, Wenguan Wang, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prominent solutions for medical image segmentation are typically tailored for
automatic or interactive setups, posing challenges in facilitating progress
achieved in one task to another.$_{\!}$ This$_{\!}$ also$_{\!}$
necessitates$_{\!}$ separate$_{\!}$ models for each task, duplicating both
training time and parameters.$_{\!}$ To$_{\!}$ address$_{\!}$ above$_{\!}$
issues,$_{\!}$ we$_{\!}$ introduce$_{\!}$ S2VNet,$_{\!}$ a$_{\!}$
universal$_{\!}$ framework$_{\!}$ that$_{\!}$ leverages$_{\!}$
Slice-to-Volume$_{\!}$ propagation$_{\!}$ to$_{\!}$ unify automatic/interactive
segmentation within a single model and one training session. Inspired by
clustering-based segmentation techniques, S2VNet makes full use of the
slice-wise structure of volumetric data by initializing cluster centers from
the cluster$_{\!}$ results$_{\!}$ of$_{\!}$ previous$_{\!}$ slice.$_{\!}$ This
enables knowledge acquired from prior slices to assist in the segmentation of
the current slice, further efficiently bridging the communication between
remote slices using mere 2D networks. Moreover, such a framework readily
accommodates interactive segmentation with no architectural change, simply by
initializing centroids from user inputs. S2VNet distinguishes itself by swift
inference speeds and reduced memory consumption compared to prevailing 3D
solutions. It can also handle multi-class interactions with each of them
serving to initialize different centroids. Experiments on three benchmarks
demonstrate S2VNet surpasses task-specified solutions on both
automatic/interactive setups.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Adaptive Reality-Guided Diffusion for Artifact-Free
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingping Zheng, Ling Zheng, Yuanfan Guo, Ying Li, Songcen Xu, Jiankang Deng, Hang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artifact-free super-resolution (SR) aims to translate low-resolution images
into their high-resolution counterparts with a strict integrity of the original
content, eliminating any distortions or synthetic details. While traditional
diffusion-based SR techniques have demonstrated remarkable abilities to enhance
image detail, they are prone to artifact introduction during iterative
procedures. Such artifacts, ranging from trivial noise to unauthentic textures,
deviate from the true structure of the source image, thus challenging the
integrity of the super-resolution process. In this work, we propose
Self-Adaptive Reality-Guided Diffusion (SARGD), a training-free method that
delves into the latent space to effectively identify and mitigate the
propagation of artifacts. Our SARGD begins by using an artifact detector to
identify implausible pixels, creating a binary mask that highlights artifacts.
Following this, the Reality Guidance Refinement (RGR) process refines artifacts
by integrating this mask with realistic latent representations, improving
alignment with the original image. Nonetheless, initial realistic-latent
representations from lower-quality images result in over-smoothing in the final
output. To address this, we introduce a Self-Adaptive Guidance (SAG) mechanism.
It dynamically computes a reality score, enhancing the sharpness of the
realistic latent. These alternating mechanisms collectively achieve
artifact-free super-resolution. Extensive experiments demonstrate the
superiority of our method, delivering detailed artifact-free high-resolution
images while reducing sampling steps by 2X. We release our code at
https://github.com/ProAirVerse/Self-Adaptive-Guidance-Diffusion.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Scale Texture Loss for CT denoising with GANs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Di Feola, Lorenzo Tronchin, Valerio Guarrasi, Paolo Soda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Adversarial Networks (GANs) have proved as a powerful framework
for denoising applications in medical imaging. However, GAN-based denoising
algorithms still suffer from limitations in capturing complex relationships
within the images. In this regard, the loss function plays a crucial role in
guiding the image generation process, encompassing how much a synthetic image
differs from a real image. To grasp highly complex and non-linear textural
relationships in the training process, this work presents a loss function that
leverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrence
Matrix (GLCM). Although the recent advances in deep learning have demonstrated
superior performance in classification and detection tasks, we hypothesize that
its information content can be valuable when integrated into GANs' training. To
this end, we propose a differentiable implementation of the GLCM suited for
gradient-based optimization. Our approach also introduces a self-attention
layer that dynamically aggregates the multi-scale texture information extracted
from the images. We validate our approach by carrying out extensive experiments
in the context of low-dose CT denoising, a challenging application that aims to
enhance the quality of noisy CT scans. We utilize three publicly available
datasets, including one simulated and two real datasets. The results are
promising as compared to other well-established loss functions, being also
consistent across three different GAN architectures. The code is available at:
https://github.com/FrancescoDiFeola/DenoTextureLoss
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-Generated Video Detection via Spatio-Temporal Anomaly Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianfa Bai, Man Lin, Gang Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of generation models has led to the emergence of highly
realistic artificial intelligence (AI)-generated videos. Malicious users can
easily create non-existent videos to spread false information. This letter
proposes an effective AI-generated video detection (AIGVDet) scheme by
capturing the forensic traces with a two-branch spatio-temporal convolutional
neural network (CNN). Specifically, two ResNet sub-detectors are learned
separately for identifying the anomalies in spatical and optical flow domains,
respectively. Results of such sub-detectors are fused to further enhance the
discrimination ability. A large-scale generated video dataset (GVD) is
constructed as a benchmark for model training and evaluation. Extensive
experimental results verify the high generalization and robustness of our
AIGVDet scheme. Code and dataset will be available at
https://github.com/multimediaFor/AIGVDet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ V2X-PC: Vehicle-to-everything Collaborative Perception via Point Cluster 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Si Liu, Zihan Ding, Jiahui Fu, Hongyu Li, Siheng Chen, Shifeng Zhang, Xu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective of the collaborative vehicle-to-everything perception task is
to enhance the individual vehicle's perception capability through message
communication among neighboring traffic agents. Previous methods focus on
achieving optimal performance within bandwidth limitations and typically adopt
BEV maps as the basic collaborative message units. However, we demonstrate that
collaboration with dense representations is plagued by object feature
destruction during message packing, inefficient message aggregation for
long-range collaboration, and implicit structure representation communication.
To tackle these issues, we introduce a brand new message unit, namely point
cluster, designed to represent the scene sparsely with a combination of
low-level structure information and high-level semantic information. The point
cluster inherently preserves object information while packing messages, with
weak relevance to the collaboration range, and supports explicit structure
modeling. Building upon this representation, we propose a novel framework
V2X-PC for collaborative perception. This framework includes a Point Cluster
Packing (PCP) module to keep object feature and manage bandwidth through the
manipulation of cluster point numbers. As for effective message aggregation, we
propose a Point Cluster Aggregation (PCA) module to match and merge point
clusters associated with the same object. To further handle time latency and
pose errors encountered in real-world scenarios, we propose parameter-free
solutions that can adapt to different noisy levels without finetuning.
Experiments on two widely recognized collaborative perception benchmarks
showcase the superior performance of our method compared to the previous
state-of-the-art approaches relying on BEV maps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuda Song, Zehao Sun, Xuanwu Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in diffusion models have positioned them at the forefront
of image generation. Despite their superior performance, diffusion models are
not without drawbacks; they are characterized by complex architectures and
substantial computational demands, resulting in significant latency due to
their iterative sampling process. To mitigate these limitations, we introduce a
dual approach involving model miniaturization and a reduction in sampling
steps, aimed at significantly decreasing model latency. Our methodology
leverages knowledge distillation to streamline the U-Net and image decoder
architectures, and introduces an innovative one-step DM training technique that
utilizes feature matching and score distillation. We present two models,
SDXS-512 and SDXS-1024, achieving inference speeds of approximately 100 FPS
(30x faster than SD v1.5) and 30 FP (60x faster than SDXL) on a single GPU,
respectively. Moreover, our training approach offers promising applications in
image-conditioned control, facilitating efficient image-to-image translation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Busra Asan, Abdullah Akgul, Alper Unal, Melih Kandemir, Gozde Unal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Seasonal forecasting is a crucial task when it comes to detecting the extreme
heat and colds that occur due to climate change. Confidence in the predictions
should be reliable since a small increase in the temperatures in a year has a
big impact on the world. Calibration of the neural networks provides a way to
ensure our confidence in the predictions. However, calibrating regression
models is an under-researched topic, especially in forecasters. We calibrate a
UNet++ based architecture, which was shown to outperform physics-based models
in temperature anomalies. We show that with a slight trade-off between
prediction error and calibration error, it is possible to get more reliable and
sharper forecasts. We believe that calibration should be an important part of
safety-critical machine learning applications such as weather forecasters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a workshop paper at "ICLR 2024 Tackling Climate Change
  with Machine Learning"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Industrial Transfer Learning with Style Filter: Cost Reduction
  and Defect-Focus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Li, Ruijie Ma, Xiang Qian, Xiaohao Wang, Xinghui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing the challenge of data scarcity in industrial domains, transfer
learning emerges as a pivotal paradigm. This work introduces Style Filter, a
tailored methodology for industrial contexts. By selectively filtering source
domain data before knowledge transfer, Style Filter reduces the quantity of
data while maintaining or even enhancing the performance of transfer learning
strategy. Offering label-free operation, minimal reliance on prior knowledge,
independence from specific models, and re-utilization, Style Filter is
evaluated on authentic industrial datasets, highlighting its effectiveness when
employed before conventional transfer strategies in the deep learning domain.
The results underscore the effectiveness of Style Filter in real-world
industrial applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 11 figures,4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SatSynth: Augmenting Image-Mask Pairs through Diffusion Models for
  Aerial Semantic Segmentation <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aysim Toker, Marvin Eisenberger, Daniel Cremers, Laura Leal-Taixé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, semantic segmentation has become a pivotal tool in
processing and interpreting satellite imagery. Yet, a prevalent limitation of
supervised learning techniques remains the need for extensive manual
annotations by experts. In this work, we explore the potential of generative
image diffusion to address the scarcity of annotated data in earth observation
tasks. The main idea is to learn the joint data manifold of images and labels,
leveraging recent advancements in denoising diffusion probabilistic models. To
the best of our knowledge, we are the first to generate both images and
corresponding masks for satellite segmentation. We find that the obtained pairs
not only display high quality in fine-scale features but also ensure a wide
sampling diversity. Both aspects are crucial for earth observation data, where
semantic classes can vary severely in scale and occurrence frequency. We employ
the novel data instances for downstream segmentation, as a form of data
augmentation. In our experiments, we provide comparisons to prior works based
on discriminative diffusion models or GANs. We demonstrate that integrating
generated samples yields significant quantitative improvements for satellite
semantic segmentation -- both compared to baselines and when training only on
the original data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EDUE: Expert Disagreement-Guided One-Pass Uncertainty Estimation for
  Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kudaibergen Abutalip, Numan Saeed, Ikboljon Sobirov, Vincent Andrearczyk, Adrien Depeursinge, Mohammad Yaqub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying deep learning (DL) models in medical applications relies on
predictive performance and other critical factors, such as conveying
trustworthy predictive uncertainty. Uncertainty estimation (UE) methods provide
potential solutions for evaluating prediction reliability and improving the
model confidence calibration. Despite increasing interest in UE, challenges
persist, such as the need for explicit methods to capture aleatoric uncertainty
and align uncertainty estimates with real-life disagreements among domain
experts. This paper proposes an Expert Disagreement-Guided Uncertainty
Estimation (EDUE) for medical image segmentation. By leveraging variability in
ground-truth annotations from multiple raters, we guide the model during
training and incorporate random sampling-based strategies to enhance
calibration confidence. Our method achieves 55% and 23% improvement in
correlation on average with expert disagreements at the image and pixel levels,
respectively, better calibration, and competitive segmentation performance
compared to the state-of-the-art deep ensembles, requiring only a single
forward pass.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In the Search for Optimal Multi-view Learning Models for Crop
  Classification with Global Remote Sensing Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco Mena, Diego Arenas, Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crop classification is of critical importance due to its role in studying
crop pattern changes, resource management, and carbon sequestration. When
employing data-driven techniques for its prediction, utilizing various temporal
data sources is necessary. Deep learning models have proven to be effective for
this task by mapping time series data to high-level representation for
prediction. However, they face substantial challenges when dealing with
multiple input patterns. The literature offers limited guidance for Multi-View
Learning (MVL) scenarios, as it has primarily focused on exploring fusion
strategies with specific encoders and validating them in local regions. In
contrast, we investigate the impact of simultaneous selection of the fusion
strategy and the encoder architecture evaluated on a global-scale cropland and
crop-type classifications. We use a range of five fusion strategies (Input,
Feature, Decision, Ensemble, Hybrid) and five temporal encoder architectures
(LSTM, GRU, TempCNN, TAE, L-TAE) as possible MVL model configurations. The
validation is on the CropHarvest dataset that provides optical, radar, and
weather time series, and topographic information as input data. We found that
in scenarios with a limited number of labeled samples, a unique configuration
is insufficient for all the cases. Instead, a specialized combination,
including encoder and fusion strategy, should be meticulously sought. To
streamline this search process, we suggest initially identifying the optimal
encoder architecture tailored for a particular fusion strategy, and then
determining the most suitable fusion strategy for the classification task. We
provide a technical framework for researchers exploring crop classification or
related tasks through a MVL approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SegICL: A Universal In-context Learning Framework for Enhanced
  Segmentation in Medical Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingdong Shen, Fangxin Shang, Yehui Yang, Xiaoshuang Huang, Shining Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation models adapting to new tasks in a training-free
manner through in-context learning is an exciting advancement. Universal
segmentation models aim to generalize across the diverse modality of medical
images, yet their effectiveness often diminishes when applied to
out-of-distribution (OOD) data modalities and tasks, requiring intricate
fine-tuning of model for optimal performance. For addressing this challenge, we
introduce SegICL, a novel approach leveraging In-Context Learning (ICL) for
image segmentation. Unlike existing methods, SegICL has the capability to
employ text-guided segmentation and conduct in-context learning with a small
set of image-mask pairs, eliminating the need for training the model from
scratch or fine-tuning for OOD tasks (including OOD modality and dataset).
Extensive experimental validation of SegICL demonstrates a positive correlation
between the number of prompt samples and segmentation performance on OOD
modalities and tasks. This indicates that SegICL effectively address new
segmentation tasks based on contextual information. Additionally, SegICL also
exhibits comparable segmentation performance to mainstream models on OOD and
in-distribution tasks. Our code will be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revealing Vulnerabilities of Neural Networks in Parameter Learning and
  Defense Against Explanation-Aware Backdoors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Abdul Kadir, GowthamKrishna Addluri, Daniel Sonntag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable Artificial Intelligence (XAI) strategies play a crucial part in
increasing the understanding and trustworthiness of neural networks.
Nonetheless, these techniques could potentially generate misleading
explanations. Blinding attacks can drastically alter a machine learning
algorithm's prediction and explanation, providing misleading information by
adding visually unnoticeable artifacts into the input, while maintaining the
model's accuracy. It poses a serious challenge in ensuring the reliability of
XAI methods. To ensure the reliability of XAI methods poses a real challenge,
we leverage statistical analysis to highlight the changes in CNN weights within
a CNN following blinding attacks. We introduce a method specifically designed
to limit the effectiveness of such attacks during the evaluation phase,
avoiding the need for extra training. The method we suggest defences against
most modern explanation-aware adversarial attacks, achieving an approximate
decrease of ~99\% in the Attack Success Rate (ASR) and a ~91\% reduction in the
Mean Square Error (MSE) between the original explanation and the defended
(post-attack) explanation across three unique types of attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Elysium: Exploring Object-level Perception in Videos via MLLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Wang, Yanjie Wang, Yongjie Ye, Yuxiang Nie, Can Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal Large Language Models (MLLMs) have demonstrated their ability to
perceive objects in still images, but their application in video-related tasks,
such as object tracking, remains understudied. This lack of exploration is
primarily due to two key challenges. Firstly, extensive pretraining on
large-scale video datasets is required to equip MLLMs with the capability to
perceive objects across multiple frames and understand inter-frame
relationships. Secondly, processing a large number of frames within the context
window of Large Language Models (LLMs) can impose a significant computational
burden. To address the first challenge, we introduce ElysiumTrack-1M, a
large-scale video dataset paired with novel tasks: Referring Single Object
Tracking (RSOT) and Video Referring Expression Generation (Video-REG).
ElysiumTrack-1M contains 1.27 million annotated video frames with corresponding
object boxes and descriptions. Leveraging this dataset, we conduct training of
MLLMs and propose a token-compression model T-Selector to tackle the second
challenge. Our proposed approach, Elysium: Exploring Object-level Perception in
Videos via MLLM, is an end-to-end trainable MLLM that makes the first attempt
to conduct object-level tasks in videos without requiring any additional
plug-in or expert models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QKFormer: Hierarchical Spiking Transformer using Q-K Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenlin Zhou, Han Zhang, Zhaokun Zhou, Liutao Yu, Liwei Huang, Xiaopeng Fan, Li Yuan, Zhengyu Ma, Huihui Zhou, Yonghong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with
Transformer architectures, have attracted significant attention due to their
potential for energy efficiency and high performance. However, existing models
in this domain still suffer from suboptimal performance. We introduce several
innovations to improve the performance: i) We propose a novel spike-form Q-K
attention mechanism, tailored for SNNs, which efficiently models the importance
of token or channel dimensions through binary vectors with linear complexity.
ii) We incorporate the hierarchical structure, which significantly benefits the
performance of both the brain and artificial neural networks, into spiking
transformers to obtain multi-scale spiking representation. iii) We design a
versatile and powerful patch embedding module with a deformed shortcut
specifically for spiking transformers. Together, we develop QKFormer, a
hierarchical spiking transformer based on Q-K attention with direct training.
QKFormer shows significantly superior performance over existing
state-of-the-art SNN models on various mainstream datasets. Notably, with
comparable size to Spikformer (66.34 M, 74.81%), QKFormer (64.96 M) achieves a
groundbreaking top-1 accuracy of 85.65% on ImageNet-1k, substantially
outperforming Spikformer by 10.84%. To our best knowledge, this is the first
time that directly training SNNs have exceeded 85% accuracy on ImageNet-1K. The
code and models are publicly available at
https://github.com/zhouchenlin2096/QKFormer
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, code: https://github.com/zhouchenlin2096/QKFormer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DOrA: 3D Visual Grounding with Order-Aware Referring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tung-Yu Wu, Sheng-Yu Huang, Yu-Chiang Frank Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D visual grounding aims to identify the target object within a 3D point
cloud scene referred to by a natural language description. While previous works
attempt to exploit the verbo-visual relation with proposed cross-modal
transformers, unstructured natural utterances and scattered objects might lead
to undesirable performances. In this paper, we introduce DOrA, a novel 3D
visual grounding framework with Order-Aware referring. DOrA is designed to
leverage Large Language Models (LLMs) to parse language description, suggesting
a referential order of anchor objects. Such ordered anchor objects allow DOrA
to update visual features and locate the target object during the grounding
process. Experimental results on the NR3D and ScanRefer datasets demonstrate
our superiority in both low-resource and full-data scenarios. In particular,
DOrA surpasses current state-of-the-art frameworks by 9.3% and 7.8% grounding
accuracy under 1% data and 10% data settings, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VMRNN: Integrating Vision Mamba and LSTM for Efficient and Accurate
  Spatiotemporal Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujin Tang, Peijie Dong, Zhenheng Tang, Xiaowen Chu, Junwei Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining CNNs or ViTs, with RNNs for spatiotemporal forecasting, has yielded
unparalleled results in predicting temporal and spatial dynamics. However,
modeling extensive global information remains a formidable challenge; CNNs are
limited by their narrow receptive fields, and ViTs struggle with the intensive
computational demands of their attention mechanisms. The emergence of recent
Mamba-based architectures has been met with enthusiasm for their exceptional
long-sequence modeling capabilities, surpassing established vision models in
efficiency and accuracy, which motivates us to develop an innovative
architecture tailored for spatiotemporal forecasting. In this paper, we propose
the VMRNN cell, a new recurrent unit that integrates the strengths of Vision
Mamba blocks with LSTM. We construct a network centered on VMRNN cells to
tackle spatiotemporal prediction tasks effectively. Our extensive evaluations
show that our proposed approach secures competitive results on a variety of
tasks while maintaining a smaller model size. Our code is available at
https://github.com/yyyujintang/VMRNN-PyTorch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures. arXiv admin note: text overlap with
  arXiv:2308.09891 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Intermediate Fusion ViT Enables Efficient Text-Image Alignment in
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zizhao Hu, Shaochong Jia, Mohammad Rostami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have been widely used for conditional data cross-modal
generation tasks such as text-to-image and text-to-video. However,
state-of-the-art models still fail to align the generated visual concepts with
high-level semantics in a language such as object count, spatial relationship,
etc. We approach this problem from a multimodal data fusion perspective and
investigate how different fusion strategies can affect vision-language
alignment. We discover that compared to the widely used early fusion of
conditioning text in a pretrained image feature space, a specially designed
intermediate fusion can: (i) boost text-to-image alignment with improved
generation quality and (ii) improve training and inference efficiency by
reducing low-rank text-to-image attention calculations. We perform experiments
using a text-to-image generation task on the MS-COCO dataset. We compare our
intermediate fusion mechanism with the classic early fusion mechanism on two
common conditioning methods on a U-shaped ViT backbone. Our intermediate fusion
model achieves a higher CLIP Score and lower FID, with 20% reduced FLOPs, and
50% increased training speed compared to a strong U-ViT baseline with an early
fusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-Set Recognition in the Age of Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimity Miller, Niko Sünderhauf, Alex Kenna, Keita Mason
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Are vision-language models (VLMs) open-set models because they are trained on
internet-scale datasets? We answer this question with a clear no - VLMs
introduce closed-set assumptions via their finite query set, making them
vulnerable to open-set conditions. We systematically evaluate VLMs for open-set
recognition and find they frequently misclassify objects not contained in their
query set, leading to alarmingly low precision when tuned for high recall and
vice versa. We show that naively increasing the size of the query set to
contain more and more classes does not mitigate this problem, but instead
causes diminishing task performance and open-set performance. We establish a
revised definition of the open-set problem for the age of VLMs, define a new
benchmark and evaluation protocol to facilitate standardised evaluation and
research in this important area, and evaluate promising baseline approaches
based on predictive uncertainty and dedicated negative embeddings on a range of
VLM classifiers and object detectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ModeTv2: GPU-accelerated Motion Decomposition Transformer for Pairwise
  Optimization in Medical Image Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiqiao Wang, Zhuoyuan Wang, Dong Ni, Yi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deformable image registration plays a crucial role in medical imaging, aiding
in disease diagnosis and image-guided interventions. Traditional iterative
methods are slow, while deep learning (DL) accelerates solutions but faces
usability and precision challenges. This study introduces a pyramid network
with the enhanced motion decomposition Transformer (ModeTv2) operator,
showcasing superior pairwise optimization (PO) akin to traditional methods. We
re-implement ModeT operator with CUDA extensions to enhance its computational
efficiency. We further propose RegHead module which refines deformation fields,
improves the realism of deformation and reduces parameters. By adopting the PO,
the proposed network balances accuracy, efficiency, and generalizability.
Extensive experiments on two public brain MRI datasets and one abdominal CT
dataset demonstrate the network's suitability for PO, providing a DL model with
enhanced usability and interpretability. The code is publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CMViM: Contrastive Masked Vim Autoencoder for 3D Multi-modal
  Representation Learning for AD classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangqian Yang, Kangrui Du, Zhihan Yang, Ye Du, Yongping Zheng, Shujun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alzheimer's disease (AD) is an incurable neurodegenerative condition leading
to cognitive and functional deterioration. Given the lack of a cure, prompt and
precise AD diagnosis is vital, a complex process dependent on multiple factors
and multi-modal data. While successful efforts have been made to integrate
multi-modal representation learning into medical datasets, scant attention has
been given to 3D medical images. In this paper, we propose Contrastive Masked
Vim Autoencoder (CMViM), the first efficient representation learning method
tailored for 3D multi-modal data. Our proposed framework is built on a masked
Vim autoencoder to learn a unified multi-modal representation and
long-dependencies contained in 3D medical images. We also introduce an
intra-modal contrastive learning module to enhance the capability of the
multi-modal Vim encoder for modeling the discriminative features in the same
modality, and an inter-modal contrastive learning module to alleviate
misaligned representation among modalities. Our framework consists of two main
steps: 1) incorporate the Vision Mamba (Vim) into the mask autoencoder to
reconstruct 3D masked multi-modal data efficiently. 2) align the multi-modal
representations with contrastive learning mechanisms from both intra-modal and
inter-modal aspects. Our framework is pre-trained and validated ADNI2 dataset
and validated on the downstream task for AD classification. The proposed CMViM
yields 2.7\% AUC performance improvement compared with other state-of-the-art
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visually Guided Generative Text-Layout Pre-training for Document
  Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiming Mao, Haoli Bai, Lu Hou, Jiansheng Wei, Xin Jiang, Qun Liu, Kam-Fai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior study shows that pre-training techniques can boost the performance of
visual document understanding (VDU), which typically requires models to gain
abilities to perceive and reason both document texts and layouts (e.g.,
locations of texts and table-cells). To this end, we propose visually guided
generative text-layout pre-training, named ViTLP. Given a document image, the
model optimizes hierarchical language and layout modeling objectives to
generate the interleaved text and layout sequence. In addition, to address the
limitation of processing long documents by Transformers, we introduce a
straightforward yet effective multi-segment generative pre-training scheme,
facilitating ViTLP to process word-intensive documents of any length. ViTLP can
function as a native OCR model to localize and recognize texts of document
images. Besides, ViTLP can be effectively applied to various downstream VDU
tasks. Extensive experiments show that ViTLP achieves competitive performance
over existing baselines on benchmark VDU tasks, including information
extraction, document classification, and document question answering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024 main conference. The first version of this
  paper was submitted to OpenReview
  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Let Real Images be as a Judger, Spotting Fake Images Synthesized with
  Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyou Liang, Run Wang, Weifeng Liu, Yuyang Zhang, Wenyuan Yang, Lina Wang, Xingkai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last few years, generative models have shown their powerful
capabilities in synthesizing realistic images in both quality and diversity
(i.e., facial images, and natural subjects). Unfortunately, the artifact
patterns in fake images synthesized by different generative models are
inconsistent, leading to the failure of previous research that relied on
spotting subtle differences between real and fake. In our preliminary
experiments, we find that the artifacts in fake images always change with the
development of the generative model, while natural images exhibit stable
statistical properties. In this paper, we employ natural traces shared only by
real images as an additional predictive target in the detector. Specifically,
the natural traces are learned from the wild real images and we introduce
extended supervised contrastive learning to bring them closer to real images
and further away from fake ones. This motivates the detector to make decisions
based on the proximity of images to the natural traces. To conduct a
comprehensive experiment, we built a high-quality and diverse dataset that
includes generative models comprising 6 GAN and 6 diffusion models, to evaluate
the effectiveness in generalizing unknown forgery techniques and robustness in
surviving different transformations. Experimental results show that our
proposed method gives 96.1% mAP significantly outperforms the baselines.
Extensive experiments conducted on the widely recognized platform Midjourney
reveal that our proposed method achieves an accuracy exceeding 78.4%,
underscoring its practicality for real-world application deployment. The source
code and partial self-built dataset are available in supplementary material.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyao Huang, Fan Tang, Yong Zhang, Xiaodong Cun, Juan Cao, Jintao Li, Tong-Yee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable process of talking-head-based avatar-creating
solutions, directly generating anchor-style videos with full-body motions
remains challenging. In this study, we propose Make-Your-Anchor, a novel system
necessitating only a one-minute video clip of an individual for training,
subsequently enabling the automatic generation of anchor-style videos with
precise torso and hand movements. Specifically, we finetune a proposed
structure-guided diffusion model on input video to render 3D mesh conditions
into human appearances. We adopt a two-stage training strategy for the
diffusion model, effectively binding movements with specific appearances. To
produce arbitrary long temporal video, we extend the 2D U-Net in the frame-wise
diffusion model to a 3D style without additional training cost, and a simple
yet effective batch-overlapped temporal denoising module is proposed to bypass
the constraints on video length during inference. Finally, a novel
identity-specific face enhancement module is introduced to improve the visual
quality of facial regions in the output videos. Comparative experiments
demonstrate the effectiveness and superiority of the system in terms of visual
quality, temporal coherence, and identity preservation, outperforming SOTA
diffusion/non-diffusion methods. Project page:
\url{https://github.com/ICTMCG/Make-Your-Anchor}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Medical Image Registration and Its Application in Retinal Images: A
  <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiushi Nie, Xiaoqing Zhang, Yan Hu, Mingdao Gong, Jiang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image registration is vital for disease diagnosis and treatment with
its ability to merge diverse information of images, which may be captured under
different times, angles, or modalities. Although several surveys have reviewed
the development of medical image registration, these surveys have not
systematically summarized methodologies of existing medical image registration
methods. To this end, we provide a comprehensive review of these methods from
traditional and deep learning-based directions, aiming to help audiences
understand the development of medical image registration quickly. In
particular, we review recent advances in retinal image registration at the end
of each section, which has not attracted much attention. Additionally, we also
discuss the current challenges of retinal image registration and provide
insights and prospects for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Supervised Learning for Medical Image Data with Anatomy-Oriented
  Imaging Planes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianwei Zhang, Dong Wei, Mengmeng Zhua, Shi Gu, Yefeng Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning has emerged as a powerful tool for pretraining deep
networks on unlabeled data, prior to transfer learning of target tasks with
limited annotation. The relevance between the pretraining pretext and target
tasks is crucial to the success of transfer learning. Various pretext tasks
have been proposed to utilize properties of medical image data (e.g., three
dimensionality), which are more relevant to medical image analysis than generic
ones for natural images. However, previous work rarely paid attention to data
with anatomy-oriented imaging planes, e.g., standard cardiac magnetic resonance
imaging views. As these imaging planes are defined according to the anatomy of
the imaged organ, pretext tasks effectively exploiting this information can
pretrain the networks to gain knowledge on the organ of interest. In this work,
we propose two complementary pretext tasks for this group of medical image data
based on the spatial relationship of the imaging planes. The first is to learn
the relative orientation between the imaging planes and implemented as
regressing their intersecting lines. The second exploits parallel imaging
planes to regress their relative slice locations within a stack. Both pretext
tasks are conceptually straightforward and easy to implement, and can be
combined in multitask learning for better representation learning. Thorough
experiments on two anatomical structures (heart and knee) and representative
target tasks (semantic segmentation and classification) demonstrate that the
proposed pretext tasks are effective in pretraining deep networks for
remarkably boosted performance on the target tasks, and superior to other
recent approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Medical Image Analysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PathoTune: Adapting Visual Foundation Model to Pathological Specialists 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxuan Lu, Fang Yan, Xiaofan Zhang, Yue Gao, Shaoting Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As natural image understanding moves towards the pretrain-finetune era,
research in pathology imaging is concurrently evolving. Despite the predominant
focus on pretraining pathological foundation models, how to adapt foundation
models to downstream tasks is little explored. For downstream adaptation, we
propose the existence of two domain gaps, i.e., the Foundation-Task Gap and the
Task-Instance Gap. To mitigate these gaps, we introduce PathoTune, a framework
designed to efficiently adapt pathological or even visual foundation models to
pathology-specific tasks via multi-modal prompt tuning. The proposed framework
leverages Task-specific Visual Prompts and Task-specific Textual Prompts to
identify task-relevant features, along with Instance-specific Visual Prompts
for encoding single pathological image features. Results across multiple
datasets at both patch-level and WSI-level demonstrate its superior performance
over single-modality prompt tuning approaches. Significantly, PathoTune
facilitates the direct adaptation of natural visual foundation models to
pathological tasks, drastically outperforming pathological foundation models
with simple linear probing. The code will be available upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> CT-Bound: Fast Boundary Estimation From Noisy Images Via Hybrid
  Convolution and Transformer Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author">Wei Xu</span>, Junjie Luo, Qi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present CT-Bound, a fast boundary estimation method for noisy images using
a hybrid Convolution and Transformer neural network. The proposed architecture
decomposes boundary estimation into two tasks: local detection and global
regularization of image boundaries. It first estimates a parametric
representation of boundary structures only using the input image within a small
receptive field and then refines the boundary structure in the parameter domain
without accessing the input image. Because of this, a part of the network can
be easily trained using naive, synthetic images and still generalized to real
images, and the entire architecture is computationally efficient as the
boundary refinement is non-iterative and not in the image domain. Compared with
the previous highest accuracy methods, our experiment shows that CT-Bound is
100 times faster, producing comparably accurate, high-quality boundary and
color maps. We also demonstrate that CT-Bound can produce boundary and color
maps on real captured images without extra fine-tuning and real-time boundary
map and color map videos at ten frames per second.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REFRAME: Reflective Surface Real-Time Rendering for Mobile Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaojie Ji, Yufeng Li, Yiyi Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work tackles the challenging task of achieving real-time novel view
synthesis on various scenes, including highly reflective objects and unbounded
outdoor scenes. Existing real-time rendering methods, especially those based on
meshes, often have subpar performance in modeling surfaces with rich
view-dependent appearances. Our key idea lies in leveraging meshes for
rendering acceleration while incorporating a novel approach to parameterize
view-dependent information. We decompose the color into diffuse and specular,
and model the specular color in the reflected direction based on a neural
environment map. Our experiments demonstrate that our method achieves
comparable reconstruction quality for highly reflective surfaces compared to
state-of-the-art offline methods, while also efficiently enabling real-time
rendering on edge devices such as smartphones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page:https://xdimlab.github.io/REFRAME/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Camera-aware Label Refinement for Unsupervised Person Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengna Li, Kangyi Wu, Wenli Huang, Sanping Zhou, Jinjun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised person re-identification aims to retrieve images of a specified
person without identity labels. Many recent unsupervised Re-ID approaches adopt
clustering-based methods to measure cross-camera feature similarity to roughly
divide images into clusters. They ignore the feature distribution discrepancy
induced by camera domain gap, resulting in the unavoidable performance
degradation. Camera information is usually available, and the feature
distribution in the single camera usually focuses more on the appearance of the
individual and has less intra-identity variance. Inspired by the observation,
we introduce a \textbf{C}amera-\textbf{A}ware \textbf{L}abel
\textbf{R}efinement~(CALR) framework that reduces camera discrepancy by
clustering intra-camera similarity. Specifically, we employ intra-camera
training to obtain reliable local pseudo labels within each camera, and then
refine global labels generated by inter-camera clustering and train the
discriminative model using more reliable global pseudo labels in a self-paced
manner. Meanwhile, we develop a camera-alignment module to align feature
distributions under different cameras, which could help deal with the camera
variance further. Extensive experiments validate the superiority of our
proposed method over state-of-the-art approaches. The code is accessible at
https://github.com/leeBooMla/CALR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE TMM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ If CLIP Could Talk: Understanding Vision-Language Model Representations
  Through Their Preferred Concept Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Esfandiarpoor, Cristina Menghini, Stephen H. Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works often assume that Vision-Language Model (VLM) representations
are based on visual attributes like shape. However, it is unclear to what
extent VLMs prioritize this information to represent concepts. We propose
Extract and Explore (EX2), a novel approach to characterize important textual
features for VLMs. EX2 uses reinforcement learning to align a large language
model with VLM preferences and generates descriptions that incorporate the
important features for the VLM. Then, we inspect the descriptions to identify
the features that contribute to VLM representations. We find that spurious
descriptions have a major role in VLM representations despite providing no
helpful information, e.g., Click to enlarge photo of CONCEPT. More importantly,
among informative descriptions, VLMs rely significantly on non-visual
attributes like habitat to represent visual concepts. Also, our analysis
reveals that different VLMs prioritize different attributes in their
representations. Overall, we show that VLMs do not simply match images to scene
descriptions and that non-visual or even spurious descriptions significantly
influence their representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/BatsResearch/ex2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RCBEVDet: Radar-camera Fusion in Bird's Eye View for 3D Object Detection <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Lin, Zhe Liu, Zhongyu Xia, Xinhao Wang, Yongtao Wang, Shengxiang Qi, Yang Dong, Nan Dong, Le Zhang, Ce Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Three-dimensional object detection is one of the key tasks in autonomous
driving. To reduce costs in practice, low-cost multi-view cameras for 3D object
detection are proposed to replace the expansive LiDAR sensors. However, relying
solely on cameras is difficult to achieve highly accurate and robust 3D object
detection. An effective solution to this issue is combining multi-view cameras
with the economical millimeter-wave radar sensor to achieve more reliable
multi-modal 3D object detection. In this paper, we introduce RCBEVDet, a
radar-camera fusion 3D object detection method in the bird's eye view (BEV).
Specifically, we first design RadarBEVNet for radar BEV feature extraction.
RadarBEVNet consists of a dual-stream radar backbone and a Radar Cross-Section
(RCS) aware BEV encoder. In the dual-stream radar backbone, a point-based
encoder and a transformer-based encoder are proposed to extract radar features,
with an injection and extraction module to facilitate communication between the
two encoders. The RCS-aware BEV encoder takes RCS as the object size prior to
scattering the point feature in BEV. Besides, we present the Cross-Attention
Multi-layer Fusion module to automatically align the multi-modal BEV feature
from radar and camera with the deformable attention mechanism, and then fuse
the feature with channel and spatial fusion layers. Experimental results show
that RCBEVDet achieves new state-of-the-art radar-camera fusion results on
nuScenes and view-of-delft (VoD) 3D object detection benchmarks. Furthermore,
RCBEVDet achieves better 3D detection results than all real-time camera-only
and radar-camera 3D object detectors with a faster inference speed at 21~28
FPS. The source code will be released at https://github.com/VDIGPKU/RCBEVDet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Producing and Leveraging Online Map Uncertainty in Trajectory Prediction <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xunjiang Gu, Guanyu Song, Igor Gilitschenski, Marco Pavone, Boris Ivanovic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-definition (HD) maps have played an integral role in the development of
modern autonomous vehicle (AV) stacks, albeit with high associated labeling and
maintenance costs. As a result, many recent works have proposed methods for
estimating HD maps online from sensor data, enabling AVs to operate outside of
previously-mapped regions. However, current online map estimation approaches
are developed in isolation of their downstream tasks, complicating their
integration in AV stacks. In particular, they do not produce uncertainty or
confidence estimates. In this work, we extend multiple state-of-the-art online
map estimation methods to additionally estimate uncertainty and show how this
enables more tightly integrating online mapping with trajectory forecasting. In
doing so, we find that incorporating uncertainty yields up to 50% faster
training convergence and up to 15% better prediction performance on the
real-world nuScenes driving dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 14 figures, 6 tables. CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Neuron Segmentation for Voltage Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yosuke Bando, Ramdas Pillai, Atsushi Kajita, Farhan Abdul Hakeem, Yves Quemener, Hua-an Tseng, Kiryl D. Piatkevich, Changyang Linghu, Xue Han, Edward S. Boyden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In voltage imaging, where the membrane potentials of individual neurons are
recorded at from hundreds to thousand frames per second using fluorescence
microscopy, data processing presents a challenge. Even a fraction of a minute
of recording with a limited image size yields gigabytes of video data
consisting of tens of thousands of frames, which can be time-consuming to
process. Moreover, millisecond-level short exposures lead to noisy video
frames, obscuring neuron footprints especially in deep-brain samples where
noisy signals are buried in background fluorescence. To address this challenge,
we propose a fast neuron segmentation method able to detect multiple,
potentially overlapping, spiking neurons from noisy video frames, and implement
a data processing pipeline incorporating the proposed segmentation method along
with GPU-accelerated motion correction. By testing on existing datasets as well
as on new datasets we introduce, we show that our pipeline extracts neuron
footprints that agree well with human annotation even from cluttered datasets,
and demonstrate real-time processing of voltage imaging data on a single
desktop computer for the first time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DOCTR: Disentangled Object-Centric Transformer for Point Scene
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxuan Yu, Hao Wang, Weiming Li, Qiang Wang, Soonyong Cho, Younghun Sung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point scene understanding is a challenging task to process real-world scene
point cloud, which aims at segmenting each object, estimating its pose, and
reconstructing its mesh simultaneously. Recent state-of-the-art method first
segments each object and then processes them independently with multiple stages
for the different sub-tasks. This leads to a complex pipeline to optimize and
makes it hard to leverage the relationship constraints between multiple
objects. In this work, we propose a novel Disentangled Object-Centric
TRansformer (DOCTR) that explores object-centric representation to facilitate
learning with multiple objects for the multiple sub-tasks in a unified manner.
Each object is represented as a query, and a Transformer decoder is adapted to
iteratively optimize all the queries involving their relationship. In
particular, we introduce a semantic-geometry disentangled query (SGDQ) design
that enables the query features to attend separately to semantic information
and geometric information relevant to the corresponding sub-tasks. A hybrid
bipartite matching module is employed to well use the supervisions from all the
sub-tasks during training. Qualitative and quantitative experimental results
demonstrate that our method achieves state-of-the-art performance on the
challenging ScanNet dataset. Code is available at
https://github.com/SAITPublic/DOCTR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarks and Challenges in Pose Estimation for Egocentric Hand
  Interactions with Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zicong Fan, Takehiko Ohkawa, Linlin Yang, Nie Lin, Zhishan Zhou, Shihao Zhou, Jiajun Liang, Zhong Gao, Xuanyang Zhang, Xue Zhang, Fei Li, Liu Zheng, Feng Lu, Karim Abou Zeid, Bastian Leibe, Jeongwan On, Seungryul Baek, Aditya Prakash, Saurabh Gupta, Kun He, Yoichi Sato, Otmar Hilliges, Hyung Jin Chang, Angela Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We interact with the world with our hands and see it through our own
(egocentric) perspective. A holistic 3D understanding of such interactions from
egocentric views is important for tasks in robotics, AR/VR, action recognition
and motion generation. Accurately reconstructing such interactions in 3D is
challenging due to heavy occlusion, viewpoint bias, camera distortion, and
motion blur from the head movement. To this end, we designed the HANDS23
challenge based on the AssemblyHands and ARCTIC datasets with carefully
designed training and testing splits. Based on the results of the top submitted
methods and more recent baselines on the leaderboards, we perform a thorough
analysis on 3D hand(-object) reconstruction tasks. Our analysis demonstrates
the effectiveness of addressing distortion specific to egocentric cameras,
adopting high-capacity transformers to learn complex hand-object interactions,
and fusing predictions from different views. Our study further reveals
challenging scenarios intractable with state-of-the-art methods, such as fast
hand motion, object reconstruction from narrow egocentric views, and close
contact between two hands and objects. Our efforts will enrich the community's
knowledge foundation and facilitate future hand studies on egocentric
hand-object interactions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Visual Place Recognition via Fast and Slow Adaptive Biasing in
  Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gokul B. Nair, Michael Milford, Tobias Fischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras are increasingly popular in robotics due to their beneficial
features, such as low latency, energy efficiency, and high dynamic range.
Nevertheless, their downstream task performance is greatly influenced by the
optimization of bias parameters. These parameters, for instance, regulate the
necessary change in light intensity to trigger an event, which in turn depends
on factors such as the environment lighting and camera motion. This paper
introduces feedback control algorithms that automatically tune the bias
parameters through two interacting methods: 1) An immediate, on-the-fly fast
adaptation of the refractory period, which sets the minimum interval between
consecutive events, and 2) if the event rate exceeds the specified bounds even
after changing the refractory period repeatedly, the controller adapts the
pixel bandwidth and event thresholds, which stabilizes after a short period of
noise events across all pixels (slow adaptation). Our evaluation focuses on the
visual place recognition task, where incoming query images are compared to a
given reference database. We conducted comprehensive evaluations of our
algorithms' adaptive feedback control in real-time. To do so, we collected the
QCR-Fast-and-Slow dataset that contains DAVIS346 event camera streams from 366
repeated traversals of a Scout Mini robot navigating through a 100 meter long
indoor lab setting (totaling over 35km distance traveled) in varying brightness
conditions with ground truth location information. Our proposed feedback
controllers result in superior performance when compared to the standard bias
settings and prior feedback control methods. Our findings also detail the
impact of bias adjustments on task performance and feature ablation studies on
the fast and slow adaptation mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures, paper under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Refining Text-to-Image Generation: Towards Accurate Training-Free
  Glyph-Enhanced Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanyam Lakhanpal, Shivang Chopra, Vinija Jain, Aman Chadha, Man Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past few years, Text-to-Image (T2I) generation approaches based on
diffusion models have gained significant attention. However, vanilla diffusion
models often suffer from spelling inaccuracies in the text displayed within the
generated images. The capability to generate visual text is crucial, offering
both academic interest and a wide range of practical applications. To produce
accurate visual text images, state-of-the-art techniques adopt a
glyph-controlled image generation approach, consisting of a text layout
generator followed by an image generator that is conditioned on the generated
text layout. Nevertheless, our study reveals that these models still face three
primary challenges, prompting us to develop a testbed to facilitate future
research. We introduce a benchmark, LenCom-Eval, specifically designed for
testing models' capability in generating images with Lengthy and Complex visual
text. Subsequently, we introduce a training-free framework to enhance the
two-stage generation approaches. We examine the effectiveness of our approach
on both LenCom-Eval and MARIO-Eval benchmarks and demonstrate notable
improvements across a range of evaluation metrics, including CLIPScore, OCR
precision, recall, F1 score, accuracy, and edit distance scores. For instance,
our proposed framework improves the backbone model, TextDiffuser, by more than
23\% and 13.5\% in terms of OCR word F1 on LenCom-Eval and MARIO-Eval,
respectively. Our work makes a unique contribution to the field by focusing on
generating images with long and rare text sequences, a niche previously
unexplored by existing literature
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Template-assisted Point Cloud Shape Correspondence Network <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Deng, Jiahao Lu, Tianzhu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised point cloud shape correspondence aims to establish point-wise
correspondences between source and target point clouds. Existing methods obtain
correspondences directly by computing point-wise feature similarity between
point clouds. However, non-rigid objects possess strong deformability and
unusual shapes, making it a longstanding challenge to directly establish
correspondences between point clouds with unconventional shapes. To address
this challenge, we propose an unsupervised Template-Assisted point cloud shape
correspondence Network, termed TANet, including a template generation module
and a template assistance module. The proposed TANet enjoys several merits.
Firstly, the template generation module establishes a set of learnable
templates with explicit structures. Secondly, we introduce a template
assistance module that extensively leverages the generated templates to
establish more accurate shape correspondences from multiple perspectives.
Extensive experiments on four human and animal datasets demonstrate that TANet
achieves favorable performance against state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spike-NeRF: Neural Radiance Field Based On Spike Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijia Guo, Yuanxi Bai, Liwen Hu, Mianzhi Liu, Ziyi Guo, Lei Ma, Tiejun Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a neuromorphic sensor with high temporal resolution, spike cameras offer
notable advantages over traditional cameras in high-speed vision applications
such as high-speed optical estimation, depth estimation, and object tracking.
Inspired by the success of the spike camera, we proposed Spike-NeRF, the first
Neural Radiance Field derived from spike data, to achieve 3D reconstruction and
novel viewpoint synthesis of high-speed scenes. Instead of the multi-view
images at the same time of NeRF, the inputs of Spike-NeRF are continuous spike
streams captured by a moving spike camera in a very short time. To reconstruct
a correct and stable 3D scene from high-frequency but unstable spike data, we
devised spike masks along with a distinctive loss function. We evaluate our
method qualitatively and numerically on several challenging synthetic scenes
generated by blender with the spike camera simulator. Our results demonstrate
that Spike-NeRF produces more visually appealing results than the existing
methods and the baseline we proposed in high-speed scenes. Our code and data
will be released soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by ICME2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Long Video Generation: Challenges, Methods, and Prospects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengxuan Li, Di Huang, Zeyu Lu, Yang Xiao, Qingqi Pei, Lei Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video generation is a rapidly advancing research area, garnering significant
attention due to its broad range of applications. One critical aspect of this
field is the generation of long-duration videos, which presents unique
challenges and opportunities. This paper presents the first survey of recent
advancements in long video generation and summarises them into two key
paradigms: divide and conquer temporal autoregressive.
  We delve into the common models employed in each paradigm, including aspects
of network design and conditioning techniques. Furthermore, we offer a
comprehensive overview and classification of the datasets and evaluation
metrics which are crucial for advancing long video generation research.
Concluding with a summary of existing studies, we also discuss the emerging
challenges and future directions in this dynamic field. We hope that this
survey will serve as an essential reference for researchers and practitioners
in the realm of long video generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ensemble Adversarial Defense via Integration of Multiple Dispersed Low
  Curvature Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaikang Zhao, Xi Chen, Wei Huang, Liuxin Ding, Xianglong Kong, Fan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of an ensemble of deep learning models has been extensively
explored to enhance defense against adversarial attacks. The diversity among
sub-models increases the attack cost required to deceive the majority of the
ensemble, thereby improving the adversarial robustness. While existing
approaches mainly center on increasing diversity in feature representations or
dispersion of first-order gradients with respect to input, the limited
correlation between these diversity metrics and adversarial robustness
constrains the performance of ensemble adversarial defense. In this work, we
aim to enhance ensemble diversity by reducing attack transferability. We
identify second-order gradients, which depict the loss curvature, as a key
factor in adversarial robustness. Computing the Hessian matrix involved in
second-order gradients is computationally expensive. To address this, we
approximate the Hessian-vector product using differential approximation. Given
that low curvature provides better robustness, our ensemble model was designed
to consider the influence of curvature among different sub-models. We introduce
a novel regularizer to train multiple more-diverse low-curvature network
models. Extensive experiments across various datasets demonstrate that our
ensemble model exhibits superior robustness against a range of attacks,
underscoring the effectiveness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to The 2024 International Joint Conference on Neural
  Networks (IJCNN)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ASDF: Assembly State Detection Utilizing Late Fusion by Integrating 6D
  Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah Schieber, Shiyu Li, Niklas Corell, Philipp Beckerle, Julian Kreimeier, Daniel Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In medical and industrial domains, providing guidance for assembly processes
is critical to ensure efficiency and safety. Errors in assembly can lead to
significant consequences such as extended surgery times, and prolonged
manufacturing or maintenance times in industry. Assembly scenarios can benefit
from in-situ AR visualization to provide guidance, reduce assembly times and
minimize errors. To enable in-situ visualization 6D pose estimation can be
leveraged. Existing 6D pose estimation techniques primarily focus on individual
objects and static captures. However, assembly scenarios have various dynamics
including occlusion during assembly and dynamics in the assembly objects
appearance. Existing work, combining object detection/6D pose estimation and
assembly state detection focuses either on pure deep learning-based approaches,
or limit the assembly state detection to building blocks. To address the
challenges of 6D pose estimation in combination with assembly state detection,
our approach ASDF builds upon the strengths of YOLOv8, a real-time capable
object detection framework. We extend this framework, refine the object pose
and fuse pose knowledge with network-detected pose information. Utilizing our
late fusion in our Pose2State module results in refined 6D pose estimation and
assembly state detection. By combining both pose and state information, our
Pose2State module predicts the final assembly state with precision. Our
evaluation on our ASDF dataset shows that our Pose2State module leads to an
improved assembly state detection and that the improvement of the assembly
state further leads to a more robust 6D pose estimation. Moreover, on the GBOT
dataset, we outperform the pure deep learning-based network, and even
outperform the hybrid and pure tracking-based approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-attention Associate Prediction Network for Visual Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinglong Sun, Haijiang Sun, Shan Jiang, Jiacheng Wang, Xilai Wei, Zhonghe Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification-regression prediction networks have realized impressive
success in several modern deep trackers. However, there is an inherent
difference between classification and regression tasks, so they have diverse
even opposite demands for feature matching. Existed models always ignore the
key issue and only employ a unified matching block in two task branches,
decaying the decision quality. Besides, these models also struggle with
decision misalignment situation. In this paper, we propose a multi-attention
associate prediction network (MAPNet) to tackle the above problems. Concretely,
two novel matchers, i.e., category-aware matcher and spatial-aware matcher, are
first designed for feature comparison by integrating self, cross, channel or
spatial attentions organically. They are capable of fully capturing the
category-related semantics for classification and the local spatial contexts
for regression, respectively. Then, we present a dual alignment module to
enhance the correspondences between two branches, which is useful to find the
optimal tracking solution. Finally, we describe a Siamese tracker built upon
the proposed prediction network, which achieves the leading performance on five
tracking benchmarks, consisting of LaSOT, TrackingNet, GOT-10k, TNL2k and
UAV123, and surpasses other state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-IF: Leveraging Semantic Text Guidance for Degradation-Aware and
  Interactive Image Fusion <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xunpeng Yi, Han Xu, Hao Zhang, Linfeng Tang, Jiayi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image fusion aims to combine information from different source images to
create a comprehensively representative image. Existing fusion methods are
typically helpless in dealing with degradations in low-quality source images
and non-interactive to multiple subjective and objective needs. To solve them,
we introduce a novel approach that leverages semantic text guidance image
fusion model for degradation-aware and interactive image fusion task, termed as
Text-IF. It innovatively extends the classical image fusion to the text guided
image fusion along with the ability to harmoniously address the degradation and
interaction issues during fusion. Through the text semantic encoder and
semantic interaction fusion decoder, Text-IF is accessible to the all-in-one
infrared and visible image degradation-aware processing and the interactive
flexible fusion outcomes. In this way, Text-IF achieves not only multi-modal
image fusion, but also multi-modal information fusion. Extensive experiments
prove that our proposed text guided image fusion strategy has obvious
advantages over SOTA methods in the image fusion performance and degradation
treatment. The code is available at https://github.com/XunpengYi/Text-IF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dia-LLaMA: Towards Large Language Model-driven CT Report Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16386v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16386v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixuan Chen, Luyang Luo, Yequan Bie, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical report generation has achieved remarkable advancements yet has still
been faced with several challenges. First, the inherent imbalance in the
distribution of normal and abnormal cases may lead models to exhibit a biased
focus on normal samples, resulting in unreliable diagnoses. Second, the
frequent occurrence of common template sentences in the reports may overwhelm
the critical abnormal information. Moreover, existing works focus on 2D chest
X-rays, leaving CT report generation underexplored due to the high-dimensional
nature of CT images and the limited availability of CT-report pairs. Recently,
LLM has shown a great ability to generate reliable answers with appropriate
prompts, which shed light on addressing the aforementioned challenges. In this
paper, we propose Dia-LLaMA, a framework to adapt the LLaMA2-7B for CT report
generation by incorporating diagnostic information as guidance prompts.
Considering the high dimension of CT, we leverage a pre-trained ViT3D with
perceiver to extract the visual information. To tailor the LLM for report
generation and emphasize abnormality, we extract additional diagnostic
information by referring to a disease prototype memory bank, which is updated
during training to capture common disease representations. Furthermore, we
introduce disease-aware attention to enable the model to adjust attention for
different diseases. Experiments on the chest CT dataset demonstrated that our
proposed method outperformed previous methods and achieved state-of-the-art on
both clinical efficacy performance and natural language generation metrics. The
code will be made publically available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators
  for Reasoning-Based Chart VQA <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Zhuowan, Jasani Bhavan, Tang Peng, Ghadar Shabnam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding data visualizations like charts and plots requires reasoning
about both visual elements and numerics. Although strong in extractive
questions, current chart visual question answering (chart VQA) models suffer on
complex reasoning questions. In this work, we address the lack of reasoning
ability by data augmentation. We leverage Large Language Models (LLMs), which
have shown to have strong reasoning ability, as an automatic data annotator
that generates question-answer annotations for chart images. The key innovation
in our method lies in the Synthesize Step-by-Step strategy: our LLM-based data
generator learns to decompose the complex question into step-by-step
sub-questions (rationales), which are then used to derive the final answer
using external tools, i.e. Python. This step-wise generation procedure is
trained on synthetic data generated using a template-based QA generation
pipeline. Experimental results highlight the significance of the proposed
step-by-step generation. By training with the LLM-augmented data (LAMENDA), we
significantly enhance the chart VQA models, achieving the state-of-the-art
accuracy on the ChartQA and PlotQA datasets. In particular, our approach
improves the accuracy of the previous state-of-the-art approach from 38% to 54%
on the human-written questions in the ChartQA dataset, which needs strong
reasoning. We hope our work underscores the potential of synthetic data and
encourages further exploration of data augmentation using LLMs for
reasoning-heavy tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Residual Dense Swin Transformer for Continuous Depth-Independent
  Ultrasound Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintong Hu, Hui Che, Zishuo Li, Wenming Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultrasound imaging is crucial for evaluating organ morphology and function,
yet depth adjustment can degrade image quality and field-of-view, presenting a
depth-dependent dilemma. Traditional interpolation-based zoom-in techniques
often sacrifice detail and introduce artifacts. Motivated by the potential of
arbitrary-scale super-resolution to naturally address these inherent
challenges, we present the Residual Dense Swin Transformer Network (RDSTN),
designed to capture the non-local characteristics and long-range dependencies
intrinsic to ultrasound images. It comprises a linear embedding module for
feature enhancement, an encoder with shifted-window attention for modeling
non-locality, and an MLP decoder for continuous detail reconstruction. This
strategy streamlines balancing image quality and field-of-view, which offers
superior textures over traditional methods. Experimentally, RDSTN outperforms
existing approaches while requiring fewer parameters. In conclusion, RDSTN
shows promising potential for ultrasound image enhancement by overcoming the
limitations of conventional interpolation-based methods and achieving
depth-independent imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP2024, https://ieeexplore.ieee.org/document/10447712</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlashEval: Towards Fast and Accurate Evaluation of Text-to-image
  Diffusion Generative Models <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Zhao, Tianchen Zhao, Zinan Lin, Xuefei Ning, Guohao Dai, Huazhong Yang, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been significant progress in the development of
text-to-image generative models. Evaluating the quality of the generative
models is one essential step in the development process. Unfortunately, the
evaluation process could consume a significant amount of computational
resources, making the required periodic evaluation of model performance (e.g.,
monitoring training progress) impractical. Therefore, we seek to improve the
evaluation efficiency by selecting the representative subset of the text-image
dataset. We systematically investigate the design choices, including the
selection criteria (textural features or image-based metrics) and the selection
granularity (prompt-level or set-level). We find that the insights from prior
work on subset selection for training data do not generalize to this problem,
and we propose FlashEval, an iterative search algorithm tailored to evaluation
data selection. We demonstrate the effectiveness of FlashEval on ranking
diffusion models with various configurations, including architectures,
quantization levels, and sampler schedules on COCO and DiffusionDB datasets.
Our searched 50-item subset could achieve comparable evaluation quality to the
randomly sampled 500-item subset for COCO annotations on unseen models,
achieving a 10x evaluation speedup. We release the condensed subset of these
commonly used datasets to help facilitate diffusion algorithm design and
evaluation, and open-source FlashEval as a tool for condensing future datasets,
accessible at https://github.com/thu-nics/FlashEval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Elite360D: Towards Efficient 360 Depth Estimation via Semantic- and
  Distance-Aware Bi-Projection Fusion <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Ai, Lin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  360 depth estimation has recently received great attention for 3D
reconstruction owing to its omnidirectional field of view (FoV). Recent
approaches are predominantly focused on cross-projection fusion with
geometry-based re-projection: they fuse 360 images with equirectangular
projection (ERP) and another projection type, e.g., cubemap projection to
estimate depth with the ERP format. However, these methods suffer from 1)
limited local receptive fields, making it hardly possible to capture large FoV
scenes, and 2) prohibitive computational cost, caused by the complex
cross-projection fusion module design. In this paper, we propose Elite360D, a
novel framework that inputs the ERP image and icosahedron projection (ICOSAP)
point set, which is undistorted and spatially continuous. Elite360D is superior
in its capacity in learning a representation from a local-with-global
perspective. With a flexible ERP image encoder, it includes an ICOSAP point
encoder, and a Bi-projection Bi-attention Fusion (B2F) module (totally ~1M
parameters). Specifically, the ERP image encoder can take various perspective
image-trained backbones (e.g., ResNet, Transformer) to extract local features.
The point encoder extracts the global features from the ICOSAP. Then, the B2F
module captures the semantic- and distance-aware dependencies between each
pixel of the ERP feature and the entire ICOSAP feature set. Without specific
backbone design and obvious computational cost increase, Elite360D outperforms
the prior arts on several benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GoodSAM: Bridging Domain and Capacity Gaps via Segment Anything Model
  for Distortion-aware Panoramic Semantic Segmentation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiming Zhang, Yexin Liu, Xu Zheng, Lin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper tackles a novel yet challenging problem: how to transfer knowledge
from the emerging Segment Anything Model (SAM) -- which reveals impressive
zero-shot instance segmentation capacity -- to learn a compact panoramic
semantic segmentation model, i.e., student, without requiring any labeled data.
This poses considerable challenges due to SAM's inability to provide semantic
labels and the large capacity gap between SAM and the student. To this end, we
propose a novel framework, called GoodSAM, that introduces a teacher assistant
(TA) to provide semantic information, integrated with SAM to generate ensemble
logits to achieve knowledge transfer. Specifically, we propose a
Distortion-Aware Rectification (DAR) module that first addresses the distortion
problem of panoramic images by imposing prediction-level consistency and
boundary enhancement. This subtly enhances TA's prediction capacity on
panoramic images. DAR then incorporates a cross-task complementary fusion block
to adaptively merge the predictions of SAM and TA to obtain more reliable
ensemble logits. Moreover, we introduce a Multi-level Knowledge Adaptation
(MKA) module to efficiently transfer the multi-level feature knowledge from TA
and ensemble logits to learn a compact student model. Extensive experiments on
two benchmarks show that our GoodSAM achieves a remarkable +3.75\% mIoU
improvement over the state-of-the-art (SOTA) domain adaptation methods. Also,
our most lightweight model achieves comparable performance to the SOTA methods
with only 3.7M parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distilling Semantic Priors from SAM to Efficient Image Restoration
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Zhang, Xiaoyu Liu, Wei Li, Hanting Chen, Junchao Liu, Jie Hu, Zhiwei Xiong, Chun Yuan, Yunhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In image restoration (IR), leveraging semantic priors from segmentation
models has been a common approach to improve performance. The recent segment
anything model (SAM) has emerged as a powerful tool for extracting advanced
semantic priors to enhance IR tasks. However, the computational cost of SAM is
prohibitive for IR, compared to existing smaller IR models. The incorporation
of SAM for extracting semantic priors considerably hampers the model inference
efficiency. To address this issue, we propose a general framework to distill
SAM's semantic knowledge to boost exiting IR models without interfering with
their inference process. Specifically, our proposed framework consists of the
semantic priors fusion (SPF) scheme and the semantic priors distillation (SPD)
scheme. SPF fuses two kinds of information between the restored image predicted
by the original IR model and the semantic mask predicted by SAM for the refined
restored image. SPD leverages a self-distillation manner to distill the fused
semantic priors to boost the performance of original IR models. Additionally,
we design a semantic-guided relation (SGR) module for SPD, which ensures
semantic feature representation space consistency to fully distill the priors.
We demonstrate the effectiveness of our framework across multiple IR models and
tasks, including deraining, deblurring, and denoising.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Potent Poisons and Backdoors from Scratch with Guided
  Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein Souri, Arpit Bansal, Hamid Kazemi, Liam Fowl, Aniruddha Saha, Jonas Geiping, Andrew Gordon Wilson, Rama Chellappa, Tom Goldstein, Micah Goldblum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern neural networks are often trained on massive datasets that are web
scraped with minimal human inspection. As a result of this insecure curation
pipeline, an adversary can poison or backdoor the resulting model by uploading
malicious data to the internet and waiting for a victim to scrape and train on
it. Existing approaches for creating poisons and backdoors start with randomly
sampled clean data, called base samples, and then modify those samples to craft
poisons. However, some base samples may be significantly more amenable to
poisoning than others. As a result, we may be able to craft more potent poisons
by carefully choosing the base samples. In this work, we use guided diffusion
to synthesize base samples from scratch that lead to significantly more potent
poisons and backdoors than previous state-of-the-art attacks. Our Guided
Diffusion Poisoning (GDP) base samples can be combined with any downstream
poisoning or backdoor attack to boost its effectiveness. Our implementation
code is publicly available at: https://github.com/hsouri/GDP .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RSTAR: Rotational Streak Artifact Reduction in 4D CBCT using Separable
  and Circular Convolutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziheng Deng, Hua Chen, Haibo Hu, Zhiyong Xu, Tianling Lyu, Yan Xi, Yang Chen, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Four-dimensional cone-beam computed tomography (4D CBCT) provides
respiration-resolved images and can be used for image-guided radiation therapy.
However, the ability to reveal respiratory motion comes at the cost of image
artifacts. As raw projection data are sorted into multiple respiratory phases,
there is a limited number of cone-beam projections available for image
reconstruction. Consequently, the 4D CBCT images are covered by severe streak
artifacts. Although several deep learning-based methods have been proposed to
address this issue, most algorithms employ ordinary network models, neglecting
the intrinsic structural prior within 4D CBCT images. In this paper, we first
explore the origin and appearance of streak artifacts in 4D CBCT
images.Specifically, we find that streak artifacts exhibit a periodic
rotational motion along with the patient's respiration. This unique motion
pattern inspires us to distinguish the artifacts from the desired anatomical
structures in the spatiotemporal domain. Thereafter, we propose a
spatiotemporal neural network named RSTAR-Net with separable and circular
convolutions for Rotational Streak Artifact Reduction. The specially designed
model effectively encodes dynamic image features, facilitating the recovery of
4D CBCT images. Moreover, RSTAR-Net is also lightweight and computationally
efficient. Extensive experiments substantiate the effectiveness of our proposed
method, and RSTAR-Net shows superior performance to comparison methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChebMixer: Efficient Graph Representation Learning with MLP Mixer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyan Kui, Haonan Yan, Qinsong Li, Liming Chen, Beiji Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks have achieved remarkable success in learning graph
representations, especially graph Transformer, which has recently shown
superior performance on various graph mining tasks. However, graph Transformer
generally treats nodes as tokens, which results in quadratic complexity
regarding the number of nodes during self-attention computation. The graph MLP
Mixer addresses this challenge by using the efficient MLP Mixer technique from
computer vision. However, the time-consuming process of extracting graph tokens
limits its performance. In this paper, we present a novel architecture named
ChebMixer, a newly graph MLP Mixer that uses fast Chebyshev polynomials-based
spectral filtering to extract a sequence of tokens. Firstly, we produce
multiscale representations of graph nodes via fast Chebyshev polynomial-based
spectral filtering. Next, we consider each node's multiscale representations as
a sequence of tokens and refine the node representation with an effective MLP
Mixer. Finally, we aggregate the multiscale representations of nodes through
Chebyshev interpolation. Owing to the powerful representation capabilities and
fast computational properties of MLP Mixer, we can quickly extract more
informative node representations to improve the performance of downstream
tasks. The experimental results prove our significant improvements in a variety
of scenarios ranging from graph node classification to medical image
segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D-EffiViTCaps: 3D Efficient Vision Transformer with Capsule for Medical
  Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongwei Gan, Ming Chang, Juan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation (MIS) aims to finely segment various organs. It
requires grasping global information from both parts and the entire image for
better segmenting, and clinically there are often certain requirements for
segmentation efficiency. Convolutional neural networks (CNNs) have made
considerable achievements in MIS. However, they are difficult to fully collect
global context information and their pooling layer may cause information loss.
Capsule networks, which combine the benefits of CNNs while taking into account
additional information such as relative location that CNNs do not, have lately
demonstrated some advantages in MIS. Vision Transformer (ViT) employs
transformers in visual tasks. Transformer based on attention mechanism has
excellent global inductive modeling capabilities and is expected to capture
longrange information. Moreover, there have been resent studies on making ViT
more lightweight to minimize model complexity and increase efficiency. In this
paper, we propose a U-shaped 3D encoder-decoder network named 3D-EffiViTCaps,
which combines 3D capsule blocks with 3D EfficientViT blocks for MIS. Our
encoder uses capsule blocks and EfficientViT blocks to jointly capture local
and global semantic information more effectively and efficiently with less
information loss, while the decoder employs CNN blocks and EfficientViT blocks
to catch ffner details for segmentation. We conduct experiments on various
datasets, including iSeg-2017, Hippocampus and Cardiac to verify the
performance and efficiency of 3D-EffiViTCaps, which performs better than
previous 3D CNN-based, 3D Capsule-based and 3D Transformer-based models. We
further implement a series of ablation experiments on the main blocks. Our code
is available at: https://github.com/HidNeuron/3D-EffiViTCaps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures, submitted to ICPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of Video Compression Artifacts on Fisheye Camera Visual
  Perception Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Madhumitha Sakthi, Louis Kerofsky, Varun Ravi Kumar, Senthil Yogamani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving systems require extensive data collection schemes to cover
the diverse scenarios needed for building a robust and safe system. The data
volumes are in the order of Exabytes and have to be stored for a long period of
time (i.e., more than 10 years of the vehicle's life cycle). Lossless
compression doesn't provide sufficient compression ratios, hence, lossy video
compression has been explored. It is essential to prove that lossy video
compression artifacts do not impact the performance of the perception
algorithms. However, there is limited work in this area to provide a solid
conclusion. In particular, there is no such work for fisheye cameras, which
have high radial distortion and where compression may have higher artifacts.
Fisheye cameras are commonly used in automotive systems for 3D object detection
task. In this work, we provide the first analysis of the impact of standard
video compression codecs on wide FOV fisheye camera images. We demonstrate that
the achievable compression with negligible impact depends on the dataset and
temporal prediction of the video codec. We propose a radial distortion-aware
zonal metric to evaluate the performance of artifacts in fisheye images. In
addition, we present a novel method for estimating affine mode parameters of
the latest VVC codec, and suggest some areas for improvement in video codecs
for the application to fisheye imagery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MEDDAP: Medical <span class="highlight-title">Dataset</span> Enhancement via Diversified Augmentation
  Pipeline 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasamin Medghalchi, Niloufar Zakariaei, Arman Rahmim, Ilker Hacihaliloglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The effectiveness of Deep Neural Networks (DNNs) heavily relies on the
abundance and accuracy of available training data. However, collecting and
annotating data on a large scale is often both costly and time-intensive,
particularly in medical cases where practitioners are already occupied with
their duties. Moreover, ensuring that the model remains robust across various
scenarios of image capture is crucial in medical domains, especially when
dealing with ultrasound images that vary based on the settings of different
devices and the manual operation of the transducer. To address this challenge,
we introduce a novel pipeline called MEDDAP, which leverages Stable Diffusion
(SD) models to augment existing small datasets by automatically generating new
informative labeled samples. Pretrained checkpoints for SD are typically based
on natural images, and training them for medical images requires significant
GPU resources due to their heavy parameters. To overcome this challenge, we
introduce USLoRA (Ultrasound Low-Rank Adaptation), a novel fine-tuning method
tailored specifically for ultrasound applications. USLoRA allows for selective
fine-tuning of weights within SD, requiring fewer than 0.1\% of parameters
compared to fully fine-tuning only the UNet portion of SD. To enhance dataset
diversity, we incorporate different adjectives into the generation process
prompts, thereby desensitizing the classifiers to intensity changes across
different images. This approach is inspired by clinicians' decision-making
processes regarding breast tumors, where tumor shape often plays a more crucial
role than intensity. In conclusion, our pipeline not only outperforms
classifiers trained on the original dataset but also demonstrates superior
performance when encountering unseen datasets. The source code is available at
https://github.com/yasamin-med/MEDDAP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to miccai 2024 submitted to miccai 2024 Submitted to
  MICCAI-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HAIFIT: Human-Centered AI for Fashion Image Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08651v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08651v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianan Jiang, Xinglin Li, Weiren Yu, Di Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of fashion design, sketches serve as the canvas for expressing
an artist's distinctive drawing style and creative vision, capturing intricate
details like stroke variations and texture nuances. The advent of
sketch-to-image cross-modal translation technology has notably aided designers.
However, existing methods often compromise these sketch details during image
generation, resulting in images that deviate from the designer's intended
concept. This limitation hampers the ability to offer designers a precise
preview of the final output. To overcome this challenge, we introduce HAIFIT, a
novel approach that transforms sketches into high-fidelity, lifelike clothing
images by integrating multi-scale features and capturing extensive feature map
dependencies from diverse perspectives. Through extensive qualitative and
quantitative evaluations conducted on our self-collected dataset, our method
demonstrates superior performance compared to existing methods in generating
photorealistic clothing images. Our method excels in preserving the distinctive
style and intricate details essential for fashion design applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages,8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeMoLi: What Moves Together Belongs Together <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jenny Seidenschwarz, Aljoša Ošep, Francesco Ferroni, Simon Lucey, Laura Leal-Taixé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle semi-supervised object detection based on motion cues. Recent
results suggest that heuristic-based clustering methods in conjunction with
object trackers can be used to pseudo-label instances of moving objects and use
these as supervisory signals to train 3D object detectors in Lidar data without
manual supervision. We re-think this approach and suggest that both, object
detection, as well as motion-inspired pseudo-labeling, can be tackled in a
data-driven manner. We leverage recent advances in scene flow estimation to
obtain point trajectories from which we extract long-term, class-agnostic
motion patterns. Revisiting correlation clustering in the context of message
passing networks, we learn to group those motion patterns to cluster points to
object instances. By estimating the full extent of the objects, we obtain
per-scan 3D bounding boxes that we use to supervise a Lidar object detection
network. Our method not only outperforms prior heuristic-based approaches (57.5
AP, +14 improvement over prior work), more importantly, we show we can
pseudo-label and train object detectors across datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geometric Generative Models based on Morphological Equivariant PDEs and
  GANs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        El Hadji S. Diop, Thierno Fall, Alioune Mbengue, Mohamed Daoudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Content and image generation consist in creating or generating data from
noisy information by extracting specific features such as texture, edges, and
other thin image structures. We are interested here in generative models, and
two main problems are addressed. Firstly, the improvements of specific feature
extraction while accounting at multiscale levels intrinsic geometric features;
and secondly, the equivariance of the network to reduce its complexity and
provide a geometric interpretability. To proceed, we propose a geometric
generative model based on an equivariant partial differential equation (PDE)
for group convolution neural networks (G-CNNs), so called PDE-G-CNNs, built on
morphology operators and generative adversarial networks (GANs). Equivariant
morphological PDE layers are composed of multiscale dilations and erosions
formulated in Riemannian manifolds, while group symmetries are defined on a Lie
group. We take advantage of the Lie group structure to properly integrate the
equivariance in layers, and are able to use the Riemannian metric to solve the
multiscale morphological operations. Each point of the Lie group is associated
with a unique point in the manifold, which helps us derive a metric on the
Riemannian manifold from a tensor field invariant under the Lie group so that
the induced metric has the same symmetries. The proposed geometric
morphological GAN (GM-GAN) is obtained by using the proposed morphological
equivariant convolutions in PDE-G-CNNs to bring nonlinearity in classical CNNs.
GM-GAN is evaluated on MNIST data and compared with GANs. Preliminary results
show that GM-GAN model outperforms classical GAN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Precise 3D Human Pose Estimation with Multi-Perspective
  Spatial-Temporal Relational Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16700v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16700v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianbin Jiao, Xina Cheng, Weijie Chen, Xiaoting Yin, Hao Shi, Kailun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D human pose estimation captures the human joint points in three-dimensional
space while keeping the depth information and physical structure. That is
essential for applications that require precise pose information, such as
human-computer interaction, scene understanding, and rehabilitation training.
Due to the challenges in data collection, mainstream datasets of 3D human pose
estimation are primarily composed of multi-view video data collected in
laboratory environments, which contains rich spatial-temporal correlation
information besides the image frame content. Given the remarkable
self-attention mechanism of transformers, capable of capturing the
spatial-temporal correlation from multi-view video datasets, we propose a
multi-stage framework for 3D sequence-to-sequence (seq2seq) human pose
detection. Firstly, the spatial module represents the human pose feature by
intra-image content, while the frame-image relation module extracts temporal
relationships and 3D spatial positional relationship features between the
multi-perspective images. Secondly, the self-attention mechanism is adopted to
eliminate the interference from non-human body parts and reduce computing
resources. Our method is evaluated on Human3.6M, a popular 3D human pose
detection dataset. Experimental results demonstrate that our approach achieves
state-of-the-art performance on this dataset. The source code will be available
at https://github.com/WUJINHUAN/3D-human-pose.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IJCNN 2024. The source code will be available at
  https://github.com/WUJINHUAN/3D-human-pose</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via
  Temporal-Viewpoint Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04599v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04599v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Wang, Jun Liu, Liang Zheng, Tom Gedeon, Piotr Koniusz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video sequences exhibit significant nuisance variations (undesired effects)
of speed of actions, temporal locations, and subjects' poses, leading to
temporal-viewpoint misalignment when comparing two sets of frames or evaluating
the similarity of two sequences. Thus, we propose Joint tEmporal and cAmera
viewpoiNt alIgnmEnt (JEANIE) for sequence pairs. In particular, we focus on 3D
skeleton sequences whose camera and subjects' poses can be easily manipulated
in 3D. We evaluate JEANIE on skeletal Few-shot Action Recognition (FSAR), where
matching well temporal blocks (temporal chunks that make up a sequence) of
support-query sequence pairs (by factoring out nuisance variations) is
essential due to limited samples of novel classes. Given a query sequence, we
create its several views by simulating several camera locations. For a support
sequence, we match it with view-simulated query sequences, as in the popular
Dynamic Time Warping (DTW). Specifically, each support temporal block can be
matched to the query temporal block with the same or adjacent (next) temporal
index, and adjacent camera views to achieve joint local temporal-viewpoint
warping. JEANIE selects the smallest distance among matching paths with
different temporal-viewpoint warping patterns, an advantage over DTW which only
performs temporal alignment. We also propose an unsupervised FSAR akin to
clustering of sequences with JEANIE as a distance measure. JEANIE achieves
state-of-the-art results on NTU-60, NTU-120, Kinetics-skeleton and UWA3D
Multiview Activity II on supervised and unsupervised FSAR, and their
meta-learning inspired fusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the International Journal of Computer Vision (IJCV). An
  extension of our ACCV'22 paper [arXiv:arXiv:2210.16820] which was
  distinguished by the Sang Uk Lee Best Student Paper Award</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference
  Acceleration for Large Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06764v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06764v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, Baobao Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we identify the inefficient attention phenomena in Large
Vision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5,
QwenVL-Chat and Video-LLaVA. We find out that the attention computation over
visual tokens is of extreme inefficiency in the deep layers of popular LVLMs,
suggesting a need for a sparser approach compared to textual data handling. To
this end, we introduce FastV, a versatile plug-and-play method designed to
optimize computational efficiency by learning adaptive attention patterns in
early layers and pruning visual tokens in subsequent ones. Our evaluations
demonstrate FastV's ability to dramatically reduce computational costs (e.g., a
45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a
wide range of image and video understanding tasks. The computational efficiency
and performance trade-off of FastV are highly customizable and
pareto-efficient. It can compress the FLOPs of a 13B-parameter model to achieve
a lower budget than that of a 7B-parameter model, while still maintaining
superior performance. We believe FastV has practical values for deployment of
LVLMs in edge devices and commercial models. Code is released at
https://github.com/pkunlp-icler/FastV.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 papes, 8 figures, code is released at
  https://github.com/pkunlp-icler/FastV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MambaIR: A Simple Baseline for Image Restoration with State-Space Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15648v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15648v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Guo, Jinmin Li, Tao Dai, Zhihao Ouyang, Xudong Ren, Shu-Tao Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have seen significant advancements in image restoration, largely
attributed to the development of modern deep neural networks, such as CNNs and
Transformers. However, existing restoration backbones often face the dilemma
between global receptive fields and efficient computation, hindering their
application in practice. Recently, the Selective Structured State Space Model,
especially the improved version Mamba, has shown great potential for long-range
dependency modeling with linear complexity, which offers a way to resolve the
above dilemma. However, the standard Mamba still faces certain challenges in
low-level vision such as local pixel forgetting and channel redundancy. In this
work, we introduce a simple but effective baseline, named MambaIR, which
introduces both local enhancement and channel attention to improve the vanilla
Mamba. In this way, our MambaIR takes advantage of the local pixel similarity
and reduces the channel redundancy. Extensive experiments demonstrate the
superiority of our method, for example, MambaIR outperforms SwinIR by up to
0.45dB on image SR, using similar computational cost but with a global
receptive field. Code is available at \url{https://github.com/csguoh/MambaIR}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-Conditioned Resampler For Long Form Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Korbar, Yongqin Xian, Alessio Tonioni, Andrew Zisserman, Federico Tombari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present a text-conditioned video resampler (TCR) module that
uses a pre-trained and frozen visual encoder and large language model (LLM) to
process long video sequences for a task. TCR localises relevant visual features
from the video given a text condition and provides them to a LLM to generate a
text response. Due to its lightweight design and use of cross-attention, TCR
can process more than 100 frames at a time with plain attention and without
optimised implementations. We make the following contributions: (i) we design a
transformer-based sampling architecture that can process long videos
conditioned on a task, together with a training method that enables it to
bridge pre-trained visual and language models; (ii) we identify tasks that
could benefit from longer video perception; and (iii) we empirically validate
its efficacy on a wide variety of evaluation tasks including NextQA, EgoSchema,
and the EGO4D-LTA challenge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via
  Expressive Masked Audio Gesture Modeling <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00374v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00374v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyang Liu, Zihao Zhu, Giorgio Becherini, Yichen Peng, Mingyang Su, You Zhou, Xuefei Zhe, Naoya Iwamoto, Bo Zheng, Michael J. Black
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose EMAGE, a framework to generate full-body human gestures from audio
and masked gestures, encompassing facial, local body, hands, and global
movements. To achieve this, we first introduce BEAT2 (BEAT-SMPLX-FLAME), a new
mesh-level holistic co-speech dataset. BEAT2 combines MoShed SMPLX body with
FLAME head parameters and further refines the modeling of head, neck, and
finger movements, offering a community-standardized, high-quality 3D motion
captured dataset. EMAGE leverages masked body gesture priors during training to
boost inference performance. It involves a Masked Audio Gesture Transformer,
facilitating joint training on audio-to-gesture generation and masked gesture
reconstruction to effectively encode audio and body gesture hints. Encoded body
hints from masked gestures are then separately employed to generate facial and
body movements. Moreover, EMAGE adaptively merges speech features from the
audio's rhythm and content and utilizes four compositional VQ-VAEs to enhance
the results' fidelity and diversity. Experiments demonstrate that EMAGE
generates holistic gestures with state-of-the-art performance and is flexible
in accepting predefined spatial-temporal gesture inputs, generating complete,
audio-synchronized results. Our code and dataset are available at
https://pantomatrix.github.io/EMAGE/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR Camera Ready; Project Page: https://pantomatrix.github.io/EMAGE/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BioNeRF: Biologically Plausible Neural Radiance Fields for View
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07310v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07310v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leandro A. Passos, Douglas Rodrigues, Danilo Jodas, Kelton A. P. Costa, Ahsan Adeel, João Paulo Papa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents BioNeRF, a biologically plausible architecture that
models scenes in a 3D representation and synthesizes new views through radiance
fields. Since NeRF relies on the network weights to store the scene's
3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism
that fuses inputs from multiple sources into a memory-like structure, improving
the storing capacity and extracting more intrinsic and correlated information.
BioNeRF also mimics a behavior observed in pyramidal cells concerning
contextual information, in which the memory is provided as the context and
combined with the inputs of two subsequent neural models, one responsible for
producing the volumetric densities and the other the colors used to render the
scene. Experimental results show that BioNeRF outperforms state-of-the-art
results concerning a quality measure that encodes human perception in two
datasets: real-world images and synthetic data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junwen He, Yifan Wang, Lijun Wang, Huchuan Lu, Jun-Yan He, Jin-Peng Lan, Bin Luo, Xuansong Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Model (MLLMs) leverages Large Language Models as a
cognitive framework for diverse visual-language tasks. Recent efforts have been
made to equip MLLMs with visual perceiving and grounding capabilities. However,
there still remains a gap in providing fine-grained pixel-level perceptions and
extending interactions beyond text-specific inputs. In this work, we propose
{\bf{AnyRef}}, a general MLLM model that can generate pixel-wise object
perceptions and natural language descriptions from multi-modality references,
such as texts, boxes, images, or audio. This innovation empowers users with
greater flexibility to engage with the model beyond textual and regional
prompts, without modality-specific designs. Through our proposed refocusing
mechanism, the generated grounding output is guided to better focus on the
referenced object, implicitly incorporating additional pixel-level supervision.
This simple modification utilizes attention scores generated during the
inference of LLM, eliminating the need for extra computations while exhibiting
performance enhancements in both grounding masks and referring expressions.
With only publicly available training data, our model achieves state-of-the-art
results across multiple benchmarks, including diverse modality referring
segmentation and region-level referring expression generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Adversarial Transferability by Block Shuffle and Rotation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10299v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10299v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunyu Wang, Xuanran He, Wenxuan Wang, Xiaosen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial examples mislead deep neural networks with imperceptible
perturbations and have brought significant threats to deep learning. An
important aspect is their transferability, which refers to their ability to
deceive other models, thus enabling attacks in the black-box setting. Though
various methods have been proposed to boost transferability, the performance
still falls short compared with white-box attacks. In this work, we observe
that existing input transformation based attacks, one of the mainstream
transfer-based attacks, result in different attention heatmaps on various
models, which might limit the transferability. We also find that breaking the
intrinsic relation of the image can disrupt the attention heatmap of the
original image. Based on this finding, we propose a novel input transformation
based attack called block shuffle and rotation (BSR). Specifically, BSR splits
the input image into several blocks, then randomly shuffles and rotates these
blocks to construct a set of new images for gradient calculation. Empirical
evaluations on the ImageNet dataset demonstrate that BSR could achieve
significantly better transferability than the existing input transformation
based methods under single-model and ensemble-model settings. Combining BSR
with the current input transformation method can further improve the
transferability, which significantly outperforms the state-of-the-art methods.
Code is available at https://github.com/Trustworthy-AI-Group/BSR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Word4Per: Zero-shot Composed Person Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16515v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16515v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delong Liu, Haiwen Li, Zhicheng Zhao, Fei Su, Yuan Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Searching for specific person has great social benefits and security value,
and it often involves a combination of visual and textual information.
Conventional person retrieval methods, whether image-based or text-based,
usually fall short in effectively harnessing both types of information, leading
to the loss of accuracy. In this paper, a whole new task called Composed Person
Retrieval (CPR) is proposed to jointly utilize both image and text information
for target person retrieval. However, the supervised CPR requires very costly
manual annotation dataset, while there are currently no available resources. To
mitigate this issue, we firstly introduce the Zero-shot Composed Person
Retrieval (ZS-CPR), which leverages existing domain-related data to resolve the
CPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we
propose a two-stage learning framework, Word4Per, where a lightweight Textual
Inversion Network (TINet) and a text-based person retrieval model based on
fine-tuned Contrastive Language-Image Pre-training (CLIP) network are learned
without utilizing any CPR data. Thirdly, a finely annotated Image-Text Composed
Person Retrieval (ITCPR) dataset is built as the benchmark to assess the
performance of the proposed Word4Per framework. Extensive experiments under
both Rank-1 and mAP demonstrate the effectiveness of Word4Per for the ZS-CPR
task, surpassing the comparative methods by over 10\%. The code and ITCPR
dataset will be publicly available at
https://github.com/Delong-liu-bupt/Word4Per.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Distillation for Road Detection based on cross-model
  Semi-Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05305v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05305v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanli Ma, Oktay Karakus, Paul L. Rosin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of knowledge distillation has played a crucial role in
enabling the transfer of knowledge from larger teacher models to smaller and
more efficient student models, and is particularly beneficial for online and
resource-constrained applications. The effectiveness of the student model
heavily relies on the quality of the distilled knowledge received from the
teacher. Given the accessibility of unlabelled remote sensing data,
semi-supervised learning has become a prevalent strategy for enhancing model
performance. However, relying solely on semi-supervised learning with smaller
models may be insufficient due to their limited capacity for feature
extraction. This limitation restricts their ability to exploit training data.
To address this issue, we propose an integrated approach that combines
knowledge distillation and semi-supervised learning methods. This hybrid
approach leverages the robust capabilities of large models to effectively
utilise large unlabelled data whilst subsequently providing the small student
model with rich and informative features for enhancement. The proposed
semi-supervised learning-based knowledge distillation (SSLKD) approach
demonstrates a notable improvement in the performance of the student model, in
the application of road segmentation, surpassing the effectiveness of
traditional semi-supervised learning methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Blind Spots: A Critical Examination of Fairness in
  Autonomous Driving Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.02935v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.02935v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyue Li, Zhenpeng Chen, Jie M. Zhang, Federica Sarro, Ying Zhang, Xuanzhe Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving systems have extended the spectrum of Web of Things for
intelligent vehicles and have become an important component of the Web
ecosystem. Similar to traditional Web-based applications, fairness is an
essential aspect for ensuring the high quality of autonomous driving systems,
particularly in the context of pedestrian detectors within them. However, there
is an absence in the literature of a comprehensive assessment of the fairness
of current Deep Learning (DL)-based pedestrian detectors. To fill the gap, we
evaluate eight widely-explored DL-based pedestrian detectors across demographic
groups on large-scale real-world datasets. To enable a thorough fairness
evaluation, we provide extensive annotations for the datasets, resulting in
8,311 images with 16,070 gender labels, 20,115 age labels, and 3,513 skin tone
labels. Our findings reveal significant fairness issues related to age. The
undetected proportions for adults are 20.14% lower compared to children.
Furthermore, we explore how various driving scenarios affect the fairness of
pedestrian detectors. We find that the bias may exacerbate for children and
females towards low brightness and low contrast.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update the models evaluated and the experimental results</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HiFi-123: Towards High-fidelity One Image to 3D Content Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wangbo Yu, Li Yuan, Yan-Pei Cao, Xiangjun Gao, Xiaoyu Li, Wenbo Hu, Long Quan, Ying Shan, Yonghong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in diffusion models have enabled 3D generation from a single
image. However, current methods often produce suboptimal results for novel
views, with blurred textures and deviations from the reference image, limiting
their practical applications. In this paper, we introduce HiFi-123, a method
designed for high-fidelity and multi-view consistent 3D generation. Our
contributions are twofold: First, we propose a Reference-Guided Novel View
Enhancement (RGNV) technique that significantly improves the fidelity of
diffusion-based zero-shot novel view synthesis methods. Second, capitalizing on
the RGNV, we present a novel Reference-Guided State Distillation (RGSD) loss.
When incorporated into the optimization-based image-to-3D pipeline, our method
significantly improves 3D generation quality, achieving state-of-the-art
performance. Comprehensive evaluations demonstrate the effectiveness of our
approach over existing methods, both qualitatively and quantitatively. Video
results are available on the project page.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://drexubery.github.io/HiFi-123/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SVGDreamer: Text Guided SVG Generation with Diffusion Model <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16476v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16476v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ximing Xing, Haitao Zhou, Chuang Wang, Jing Zhang, Dong Xu, Qian Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, text-guided scalable vector graphics (SVGs) synthesis has shown
promise in domains such as iconography and sketch. However, existing
text-to-SVG generation methods lack editability and struggle with visual
quality and result diversity. To address these limitations, we propose a novel
text-guided vector graphics synthesis method called SVGDreamer. SVGDreamer
incorporates a semantic-driven image vectorization (SIVE) process that enables
the decomposition of synthesis into foreground objects and background, thereby
enhancing editability. Specifically, the SIVE process introduce attention-based
primitive control and an attention-mask loss function for effective control and
manipulation of individual elements. Additionally, we propose a Vectorized
Particle-based Score Distillation (VPSD) approach to tackle the challenges of
shape over-smoothing, color over-saturation, limited diversity in results, and
slow convergence in existing text-to-SVG generation methods. VPSD models SVGs
as distributions of control points and colors to counteract over-smoothing and
over-saturation. Furthermore, VPSD leverages a reward model to reweight vector
particles, which improves aesthetic appeal and accelerates convergence.
Extensive experiments have been conducted to validate the effectiveness of
SVGDreamer, demonstrating its superiority over baseline methods in terms of
editability, visual quality, and diversity. The code and demo of SVGDreamer can
be found at https://ximinng.github.io/SVGDreamer-project/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024. project link:
  https://ximinng.github.io/SVGDreamer-project/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational Bayes image restoration with compressive autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maud Biquard, Marie Chabert, Thomas Oberlin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regularization of inverse problems is of paramount importance in
computational imaging. The ability of neural networks to learn efficient image
representations has been recently exploited to design powerful data-driven
regularizers. While state-of-the-art plug-and-play methods rely on an implicit
regularization provided by neural denoisers, alternative Bayesian approaches
consider Maximum A Posteriori (MAP) estimation in the latent space of a
generative model, thus with an explicit regularization. However,
state-of-the-art deep generative models require a huge amount of training data
compared to denoisers. Besides, their complexity hampers the optimization
involved in latent MAP derivation. In this work, we first propose to use
compressive autoencoders instead. These networks, which can be seen as
variational autoencoders with a flexible latent prior, are smaller and easier
to train than state-of-the-art generative models. As a second contribution, we
introduce the Variational Bayes Latent Estimation (VBLE) algorithm, which
performs latent estimation within the framework of variational inference.
Thanks to a simple yet efficient parameterization of the variational posterior,
VBLE allows for fast and easy (approximate) posterior sampling. Experimental
results on image datasets BSD and FFHQ demonstrate that VBLE reaches similar
performance than state-of-the-art plug-and-play methods, while being able to
quantify uncertainties faster than other existing posterior sampling
techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mask Grounding for Referring Image Segmentation <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12198v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12198v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong Xien Chng, Henry Zheng, Yizeng Han, Xuchong Qiu, Gao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring Image Segmentation (RIS) is a challenging task that requires an
algorithm to segment objects referred by free-form language expressions.
Despite significant progress in recent years, most state-of-the-art (SOTA)
methods still suffer from considerable language-image modality gap at the pixel
and word level. These methods generally 1) rely on sentence-level language
features for language-image alignment and 2) lack explicit training supervision
for fine-grained visual grounding. Consequently, they exhibit weak object-level
correspondence between visual and language features. Without well-grounded
features, prior methods struggle to understand complex expressions that require
strong reasoning over relationships among multiple objects, especially when
dealing with rarely used or ambiguous clauses. To tackle this challenge, we
introduce a novel Mask Grounding auxiliary task that significantly improves
visual grounding within language features, by explicitly teaching the model to
learn fine-grained correspondence between masked textual tokens and their
matching visual objects. Mask Grounding can be directly used on prior RIS
methods and consistently bring improvements. Furthermore, to holistically
address the modality gap, we also design a cross-modal alignment loss and an
accompanying alignment module. These additions work synergistically with Mask
Grounding. With all these techniques, our comprehensive approach culminates in
MagNet (Mask-grounded Network), an architecture that significantly outperforms
prior arts on three key benchmarks (RefCOCO, RefCOCO+ and G-Ref), demonstrating
our method's effectiveness in addressing current limitations of RIS algorithms.
Our code and pre-trained weights will be released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024; Project page:
  https://yxchng.github.io/projects/mask-grounding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14828v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14828v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto Baldrati, Davide Morelli, Marcella Cornia, Marco Bertini, Rita Cucchiara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fashion illustration is a crucial medium for designers to convey their
creative vision and transform design concepts into tangible representations
that showcase the interplay between clothing and the human body. In the context
of fashion design, computer vision techniques have the potential to enhance and
streamline the design process. Departing from prior research primarily focused
on virtual try-on, this paper tackles the task of multimodal-conditioned
fashion image editing. Our approach aims to generate human-centric fashion
images guided by multimodal prompts, including text, human body poses, garment
sketches, and fabric textures. To address this problem, we propose extending
latent diffusion models to incorporate these multiple modalities and modifying
the structure of the denoising network, taking multimodal prompts as input. To
condition the proposed architecture on fabric textures, we employ textual
inversion techniques and let diverse cross-attention layers of the denoising
network attend to textual and texture information, thus incorporating different
granularity conditioning details. Given the lack of datasets for the task, we
extend two existing fashion datasets, Dress Code and VITON-HD, with multimodal
annotations. Experimental evaluations demonstrate the effectiveness of our
proposed approach in terms of realism and coherence concerning the provided
multimodal inputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LightIt: Illumination Modeling and Control for Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10615v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10615v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Kocsis, Julien Philip, Kalyan Sunkavalli, Matthias Nießner, Yannick Hold-Geoffroy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LightIt, a method for explicit illumination control for image
generation. Recent generative methods lack lighting control, which is crucial
to numerous artistic aspects of image generation such as setting the overall
mood or cinematic appearance. To overcome these limitations, we propose to
condition the generation on shading and normal maps. We model the lighting with
single bounce shading, which includes cast shadows. We first train a shading
estimation module to generate a dataset of real-world images and shading pairs.
Then, we train a control network using the estimated shading and normals as
input. Our method demonstrates high-quality image generation and lighting
control in numerous scenes. Additionally, we use our generated dataset to train
an identity-preserving relighting model, conditioned on an image and a target
shading. Our method is the first that enables the generation of images with
controllable, consistent lighting and performs on par with specialized
relighting state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://peter-kocsis.github.io/LightIt/ Video:
  https://youtu.be/cCfSBD5aPLI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fully automated workflow for the design of patient-specific orthopaedic
  implants: application to total knee arthroplasty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15353v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15353v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aziliz Guezou-Philippe, Arnaud Clavé, Ehouarn Maguet, Ludivine Maintier, Charles Garraud, Jean-Rassaire Fouefack, Valérie Burdin, Eric Stindel, Guillaume Dardenne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Arthroplasty is commonly performed to treat joint osteoarthritis, reducing
pain and improving mobility. While arthroplasty has known several technical
improvements, a significant share of patients are still unsatisfied with their
surgery. Personalised arthroplasty improves surgical outcomes however current
solutions require delays, making it difficult to integrate in clinical routine.
We propose a fully automated workflow to design patient-specific implants,
presented for total knee arthroplasty, the most widely performed arthroplasty
in the world nowadays.
  The proposed pipeline first uses artificial neural networks to segment the
proximal and distal extremities of the femur and tibia. Then the full bones are
reconstructed using augmented statistical shape models, combining shape and
landmarks information. Finally, 77 morphological parameters are computed to
design patient-specific implants. The developed workflow has been trained using
91 CT scans of lower limb and evaluated on 41 CT scans manually segmented, in
terms of accuracy and execution time.
  The workflow accuracy was $0.4\pm0.2mm$ for the segmentation, $1.2\pm0.4mm$
for the full bones reconstruction, and $2.8\pm2.2mm$ for the anatomical
landmarks determination. The custom implants fitted the patients' anatomy with
$0.6\pm0.2mm$ accuracy. The whole process from segmentation to implants' design
lasted about 5 minutes.
  The proposed workflow allows for a fast and reliable personalisation of knee
implants, directly from the patient CT image without requiring any manual
intervention. It establishes a patient-specific pre-operative planning for TKA
in a very short time making it easily available for all patients. Combined with
efficient implant manufacturing techniques, this solution could help answer the
growing number of arthroplasties while reducing complications and improving the
patients' satisfaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ denoiSplit: a method for joint image splitting and unsupervised
  denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11854v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11854v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashesh Ashesh, Florian Jug
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we present denoiSplit, a method to tackle a new analysis task,
i.e. the challenge of joint semantic image splitting and unsupervised
denoising. This dual approach has important applications in fluorescence
microscopy, where semantic image splitting has important applications but noise
does generally hinder the downstream analysis of image content. Image splitting
involves dissecting an image into its distinguishable semantic structures. We
show that the current state-of-the-art method for this task struggles in the
presence of image noise, inadvertently also distributing the noise across the
predicted outputs. The method we present here can deal with image noise by
integrating an unsupervised denoising sub-task. This integration results in
improved semantic image unmixing, even in the presence of notable and realistic
levels of imaging noise. A key innovation in denoiSplit is the use of
specifically formulated noise models and the suitable adjustment of
KL-divergence loss for the high-dimensional hierarchical latent space we are
training. We showcase the performance of denoiSplit across 4 tasks on
real-world microscopy images. Additionally, we perform qualitative and
quantitative evaluations and compare results to existing benchmarks,
demonstrating the effectiveness of using denoiSplit: a single Variational
Splitting Encoder-Decoder (VSE) Network using two suitable noise models to
jointly perform semantic splitting and denoising.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unraveling Instance Associations: A Closer Look for Audio-Visual
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02970v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02970v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhong Chen, Yuyuan Liu, Hu Wang, Fengbei Liu, Chong Wang, Helen Frazer, Gustavo Carneiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual segmentation (AVS) is a challenging task that involves
accurately segmenting sounding objects based on audio-visual cues. The
effectiveness of audio-visual learning critically depends on achieving accurate
cross-modal alignment between sound and visual objects. Successful audio-visual
learning requires two essential components: 1) a challenging dataset with
high-quality pixel-level multi-class annotated images associated with audio
files, and 2) a model that can establish strong links between audio information
and its corresponding visual object. However, these requirements are only
partially addressed by current methods, with training sets containing biased
audio-visual data, and models that generalise poorly beyond this biased
training set. In this work, we propose a new cost-effective strategy to build
challenging and relatively unbiased high-quality audio-visual segmentation
benchmarks. We also propose a new informative sample mining method for
audio-visual supervised contrastive learning to leverage discriminative
contrastive samples to enforce cross-modal understanding. We show empirical
results that demonstrate the effectiveness of our benchmark. Furthermore,
experiments conducted on existing AVS datasets and on our new benchmark show
that our method achieves state-of-the-art (SOTA) segmentation accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/cyh-0/CAVP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FocusCLIP: Multimodal Subject-Level Guidance for Zero-Shot Transfer in
  Human-Centric Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06904v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06904v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Saif Ullah Khan, Muhammad Ferjad Naeem, Federico Tombari, Luc Van Gool, Didier Stricker, Muhammad Zeshan Afzal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose FocusCLIP, integrating subject-level guidance--a specialized
mechanism for target-specific supervision--into the CLIP framework for improved
zero-shot transfer on human-centric tasks. Our novel contributions enhance CLIP
on both the vision and text sides. On the vision side, we incorporate ROI
heatmaps emulating human visual attention mechanisms to emphasize
subject-relevant image regions. On the text side, we introduce human pose
descriptions to provide rich contextual information. For human-centric tasks,
FocusCLIP is trained with images from the MPII Human Pose dataset. The proposed
approach surpassed CLIP by an average of 8.61% across five previously unseen
datasets covering three human-centric tasks. FocusCLIP achieved an average
accuracy of 33.65% compared to 25.04% by CLIP. We observed a 3.98% improvement
in activity recognition, a 14.78% improvement in age classification, and a
7.06% improvement in emotion recognition. Moreover, using our proposed
single-shot LLM prompting strategy, we release a high-quality MPII Pose
Descriptions dataset to encourage further research in multimodal learning for
human-centric tasks. Furthermore, we also demonstrate the effectiveness of our
subject-level supervision on non-human-centric tasks. FocusCLIP shows a 2.47%
improvement over CLIP in zero-shot bird classification using the CUB dataset.
Our findings emphasize the potential of integrating subject-level guidance with
general pretraining methods for enhanced downstream performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unleashing the Power of Self-Supervised Image Denoising: A Comprehensive
  <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.00247v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.00247v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dan Zhang, Fangfang Zhou, Felix Albu, Yuanzhou Wei, Xiao Yang, Yuan Gu, Qiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of deep learning has brought a revolutionary transformation to
image denoising techniques. However, the persistent challenge of acquiring
noise-clean pairs for supervised methods in real-world scenarios remains
formidable, necessitating the exploration of more practical self-supervised
image denoising. This paper focuses on self-supervised image denoising methods
that offer effective solutions to address this challenge. Our comprehensive
review thoroughly analyzes the latest advancements in self-supervised image
denoising approaches, categorizing them into three distinct classes: General
methods, Blind Spot Network (BSN)-based methods, and Transformer-based methods.
For each class, we provide a concise theoretical analysis along with their
practical applications. To assess the effectiveness of these methods, we
present both quantitative and qualitative experimental results on various
datasets, utilizing classical algorithms as benchmarks. Additionally, we
critically discuss the current limitations of these methods and propose
promising directions for future research. By offering a detailed overview of
recent developments in self-supervised image denoising, this review serves as
an invaluable resource for researchers and practitioners in the field,
facilitating a deeper understanding of this emerging domain and inspiring
further advancements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands
  from a Single Image <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08262v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08262v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minje Kim, Tae-Kyun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating personalized hand avatars is important to offer a realistic
experience to users on AR / VR platforms. While most prior studies focused on
reconstructing 3D hand shapes, some recent work has tackled the reconstruction
of hand textures on top of shapes. However, these methods are often limited to
capturing pixels on the visible side of a hand, requiring diverse views of the
hand in a video or multiple images as input. In this paper, we propose a novel
method, BiTT(Bi-directional Texture reconstruction of Two hands), which is the
first end-to-end trainable method for relightable, pose-free texture
reconstruction of two interacting hands taking only a single RGB image, by
three novel components: 1) bi-directional (left $\leftrightarrow$ right)
texture reconstruction using the texture symmetry of left / right hands, 2)
utilizing a texture parametric model for hand texture recovery, and 3) the
overall coarse-to-fine stage pipeline for reconstructing personalized texture
of two interacting hands. BiTT first estimates the scene light condition and
albedo image from an input image, then reconstructs the texture of both hands
through the texture parametric model and bi-directional texture reconstructor.
In experiments using InterHand2.6M and RGB2Hands datasets, our method
significantly outperforms state-of-the-art hand texture reconstruction methods
quantitatively and qualitatively. The code is available at
https://github.com/yunminjin2/BiTT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024, Project Page:
  https://yunminjin2.github.io/projects/bitt/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toulouse Hyperspectral Data Set: a benchmark data set to assess
  semi-supervised spectral representation learning and pixel-wise
  classification techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08863v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08863v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romain Thoreau, Laurent Risser, Véronique Achard, Béatrice Berthelot, Xavier Briottet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Airborne hyperspectral images can be used to map the land cover in large
urban areas, thanks to their very high spatial and spectral resolutions on a
wide spectral domain. While the spectral dimension of hyperspectral images is
highly informative of the chemical composition of the land surface, the use of
state-of-the-art machine learning algorithms to map the land cover has been
dramatically limited by the availability of training data. To cope with the
scarcity of annotations, semi-supervised and self-supervised techniques have
lately raised a lot of interest in the community. Yet, the publicly available
hyperspectral data sets commonly used to benchmark machine learning models are
not totally suited to evaluate their generalization performances due to one or
several of the following properties: a limited geographical coverage (which
does not reflect the spectral diversity in metropolitan areas), a small number
of land cover classes and a lack of appropriate standard train / test splits
for semi-supervised and self-supervised learning. Therefore, we release in this
paper the Toulouse Hyperspectral Data Set that stands out from other data sets
in the above-mentioned respects in order to meet key issues in spectral
representation learning and classification over large-scale hyperspectral
images with very few labeled pixels. Besides, we discuss and experiment
self-supervised techniques for spectral representation learning, including the
Masked Autoencoder, and establish a baseline for pixel-wise classification
achieving 85% overall accuracy and 77% F1 score. The Toulouse Hyperspectral
Data Set and our code are publicly available at
https://www.toulouse-hyperspectral-data-set.com and
https://www.github.com/Romain3Ch216/tlse-experiments, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geometric Prior Based Deep Human Point Cloud Geometry Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.01309v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.01309v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinju Wu, Pingping Zhang, Meng Wang, Peilin Chen, Shiqi Wang, Sam Kwong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of digital avatars has raised an exponential increase in the
demand for human point clouds with realistic and intricate details. The
compression of such data becomes challenging with overwhelming data amounts
comprising millions of points. Herein, we leverage the human geometric prior in
geometry redundancy removal of point clouds, greatly promoting the compression
performance. More specifically, the prior provides topological constraints as
geometry initialization, allowing adaptive adjustments with a compact parameter
set that could be represented with only a few bits. Therefore, we can envisage
high-resolution human point clouds as a combination of geometric priors and
structural deviations. The priors could first be derived with an aligned point
cloud, and subsequently the difference of features is compressed into a compact
latent code. The proposed framework can operate in a play-and-plug fashion with
existing learning based point cloud compression methods. Extensive experimental
results show that our approach significantly improves the compression
performance without deteriorating the quality, demonstrating its promise in a
variety of applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TCSVT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explaining CLIP's performance disparities on data from blind/low vision
  users <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17315v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17315v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniela Massiceti, Camilla Longden, Agnieszka Słowik, Samuel Wills, Martin Grayson, Cecily Morrison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large multi-modal models (LMMs) hold the potential to usher in a new era of
automated visual assistance for people who are blind or low vision (BLV). Yet,
these models have not been systematically evaluated on data captured by BLV
users. We address this by empirically assessing CLIP, a widely-used LMM likely
to underpin many assistive technologies. Testing 25 CLIP variants in a
zero-shot classification task, we find that their accuracy is 15 percentage
points lower on average for images captured by BLV users than web-crawled
images. This disparity stems from CLIP's sensitivities to 1) image content
(e.g. not recognizing disability objects as well as other objects); 2) image
quality (e.g. not being robust to lighting variation); and 3) text content
(e.g. not recognizing objects described by tactile adjectives as well as visual
ones). We delve deeper with a textual analysis of three common pre-training
datasets: LAION-400M, LAION-2B and DataComp-1B, showing that disability content
is rarely mentioned. We then provide three examples that illustrate how the
performance disparities extend to three downstream models underpinned by CLIP:
OWL-ViT, CLIPSeg and DALL-E2. We find that few-shot learning with as few as 5
images can mitigate CLIP's quality-of-service disparities for BLV users in some
scenarios, which we discuss alongside a set of other possible mitigations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2024 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributionally Generative Augmentation for Fair Facial Attribute
  Classification <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengda Zhang, Qianpei He, Kun Kuang, Jiashuo Liu, Long Chen, Chao Wu, Jun Xiao, Hanwang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial Attribute Classification (FAC) holds substantial promise in widespread
applications. However, FAC models trained by traditional methodologies can be
unfair by exhibiting accuracy inconsistencies across varied data
subpopulations. This unfairness is largely attributed to bias in data, where
some spurious attributes (e.g., Male) statistically correlate with the target
attribute (e.g., Smiling). Most of existing fairness-aware methods rely on the
labels of spurious attributes, which may be unavailable in practice. This work
proposes a novel, generation-based two-stage framework to train a fair FAC
model on biased data without additional annotation. Initially, we identify the
potential spurious attributes based on generative models. Notably, it enhances
interpretability by explicitly showing the spurious attributes in image space.
Following this, for each image, we first edit the spurious attributes with a
random degree sampled from a uniform distribution, while keeping target
attribute unchanged. Then we train a fair FAC model by fostering model
invariance to these augmentation. Extensive experiments on three common
datasets demonstrate the effectiveness of our method in promoting fairness in
FAC without compromising accuracy. Codes are in
https://github.com/heqianpei/DiGA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Pre-Training with Multi-View Fusion for No-Reference Point
  Cloud Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10066v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10066v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Shan, Yujie Zhang, Qi Yang, Haichen Yang, Yiling Xu, Jenq-Neng Hwang, Xiaozhong Xu, Shan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  No-reference point cloud quality assessment (NR-PCQA) aims to automatically
evaluate the perceptual quality of distorted point clouds without available
reference, which have achieved tremendous improvements due to the utilization
of deep neural networks. However, learning-based NR-PCQA methods suffer from
the scarcity of labeled data and usually perform suboptimally in terms of
generalization. To solve the problem, we propose a novel contrastive
pre-training framework tailored for PCQA (CoPA), which enables the pre-trained
model to learn quality-aware representations from unlabeled data. To obtain
anchors in the representation space, we project point clouds with different
distortions into images and randomly mix their local patches to form mixed
images with multiple distortions. Utilizing the generated anchors, we constrain
the pre-training process via a quality-aware contrastive loss following the
philosophy that perceptual quality is closely related to both content and
distortion. Furthermore, in the model fine-tuning stage, we propose a
semantic-guided multi-view fusion module to effectively integrate the features
of projected images from multiple perspectives. Extensive experiments show that
our method outperforms the state-of-the-art PCQA methods on popular benchmarks.
Further investigations demonstrate that CoPA can also benefit existing
learning-based PCQA models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentiable Point-based Inverse Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02480v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02480v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoon-Gyu Chung, Seokjun Choi, Seung-Hwan Baek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present differentiable point-based inverse rendering, DPIR, an
analysis-by-synthesis method that processes images captured under diverse
illuminations to estimate shape and spatially-varying BRDF. To this end, we
adopt point-based rendering, eliminating the need for multiple samplings per
ray, typical of volumetric rendering, thus significantly enhancing the speed of
inverse rendering. To realize this idea, we devise a hybrid point-volumetric
representation for geometry and a regularized basis-BRDF representation for
reflectance. The hybrid geometric representation enables fast rendering through
point-based splatting while retaining the geometric details and stability
inherent to SDF-based representations. The regularized basis-BRDF mitigates the
ill-posedness of inverse rendering stemming from limited light-view angular
samples. We also propose an efficient shadow detection method using point-based
shadow map rendering. Our extensive evaluations demonstrate that DPIR
outperforms prior works in terms of reconstruction accuracy, computational
efficiency, and memory footprint. Furthermore, our explicit point-based
representation and rendering enables intuitive geometry and reflectance
editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HallusionBench: An Advanced Diagnostic Suite for Entangled Language
  Hallucination and Visual Illusion in Large Vision-Language Models <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14566v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14566v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce HallusionBench, a comprehensive benchmark designed for the
evaluation of image-context reasoning. This benchmark presents significant
challenges to advanced large visual-language models (LVLMs), such as
GPT-4V(Vision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing
nuanced understanding and interpretation of visual data. The benchmark
comprises 346 images paired with 1129 questions, all meticulously crafted by
human experts. We introduce a novel structure for these visual questions
designed to establish control groups. This structure enables us to conduct a
quantitative analysis of the models' response tendencies, logical consistency,
and various failure modes. In our evaluation on HallusionBench, we benchmarked
15 different models, highlighting a 31.42% question-pair accuracy achieved by
the state-of-the-art GPT-4V. Notably, all other evaluated models achieve
accuracy below 16%. Moreover, our analysis not only highlights the observed
failure modes, including language hallucination and visual illusion, but also
deepens an understanding of these pitfalls. Our comprehensive case studies
within HallusionBench shed light on the challenges of hallucination and
illusion in LVLMs. Based on these insights, we suggest potential pathways for
their future improvement. The benchmark and codebase can be accessed at
https://github.com/tianyi-lab/HallusionBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time-Efficient and Identity-Consistent Virtual Try-On Using A Variant of
  Altered Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07371v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07371v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phuong Dam, Jihoon Jeong, Anh Tran, Daeyoung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study discusses the critical issues of Virtual Try-On in contemporary
e-commerce and the prospective metaverse, emphasizing the challenges of
preserving intricate texture details and distinctive features of the target
person and the clothes in various scenarios, such as clothing texture and
identity characteristics like tattoos or accessories. In addition to the
fidelity of the synthesized images, the efficiency of the synthesis process
presents a significant hurdle. Various existing approaches are explored,
highlighting the limitations and unresolved aspects, e.g., identity information
omission, uncontrollable artifacts, and low synthesis speed. It then proposes a
novel diffusion-based solution that addresses garment texture preservation and
user identity retention during virtual try-on. The proposed network comprises
two primary modules - a warping module aligning clothing with individual
features and a try-on module refining the attire and generating missing parts
integrated with a mask-aware post-processing technique ensuring the integrity
of the individual's identity. It demonstrates impressive results, surpassing
the state-of-the-art in speed by nearly 20 times during inference, with
superior fidelity in qualitative assessments. Quantitative evaluations confirm
comparable performance with the recent SOTA method on the VITON-HD and
Dresscode datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06199v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06199v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjie Zhu, Yichen Zhu, Xin Liu, Ning Liu, Zhiyuan Xu, Chaomin Shen, Yaxin Peng, Zhicai Ou, Feifei Feng, Jian Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have showcased impressive skills in
tasks related to visual understanding and reasoning. Yet, their widespread
application faces obstacles due to the high computational demands during both
the training and inference phases, restricting their use to a limited audience
within the research and user communities. In this paper, we investigate the
design aspects of Multimodal Small Language Models (MSLMs) and propose an
efficient multimodal assistant named Mipha, which is designed to create synergy
among various aspects: visual representation, language models, and optimization
strategies. We show that without increasing the volume of training data, our
Mipha-3B outperforms the state-of-the-art large MLLMs, especially
LLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide
insights and guidelines for developing strong MSLMs that rival the capabilities
of MLLMs. Our code is available at https://github.com/zhuyiche/llava-phi.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dispersed Structured Light for Hyperspectral 3D Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18287v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18287v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suhyun Shin, Seokjun Choi, Felix Heide, Seung-Hwan Baek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral 3D imaging aims to acquire both depth and spectral information
of a scene. However, existing methods are either prohibitively expensive and
bulky or compromise on spectral and depth accuracy. In this work, we present
Dispersed Structured Light (DSL), a cost-effective and compact method for
accurate hyperspectral 3D imaging. DSL modifies a traditional projector-camera
system by placing a sub-millimeter thick diffraction grating film front of the
projector. The grating disperses structured light based on light wavelength. To
utilize the dispersed structured light, we devise a model for dispersive
projection image formation and a per-pixel hyperspectral 3D reconstruction
method. We validate DSL by instantiating a compact experimental prototype. DSL
achieves spectral accuracy of 18.8nm full-width half-maximum (FWHM) and depth
error of 1mm. We demonstrate that DSL outperforms prior work on practical
hyperspectral 3D imaging. DSL promises accurate and practical hyperspectral 3D
imaging for diverse application domains, including computer vision and
graphics, cultural heritage, geology, and biology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PIA: Your Personalized Image Animator via Plug-and-Play Modules in
  Text-to-Image Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.13964v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.13964v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing Fang, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in personalized text-to-image (T2I) models have
revolutionized content creation, empowering non-experts to generate stunning
images with unique styles. While promising, adding realistic motions into these
personalized images by text poses significant challenges in preserving distinct
styles, high-fidelity details, and achieving motion controllability by text. In
this paper, we present PIA, a Personalized Image Animator that excels in
aligning with condition images, achieving motion controllability by text, and
the compatibility with various personalized T2I models without specific tuning.
To achieve these goals, PIA builds upon a base T2I model with well-trained
temporal alignment layers, allowing for the seamless transformation of any
personalized T2I model into an image animation model. A key component of PIA is
the introduction of the condition module, which utilizes the condition frame
and inter-frame affinity as input to transfer appearance information guided by
the affinity hint for individual frame synthesis in the latent space. This
design mitigates the challenges of appearance-related image alignment within
and allows for a stronger focus on aligning with motion-related guidance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://pi-animator.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I-PHYRE: Interactive Physical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiqian Li, Kewen Wu, Chi Zhang, Yixin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current evaluation protocols predominantly assess physical reasoning in
stationary scenes, creating a gap in evaluating agents' abilities to interact
with dynamic events. While contemporary methods allow agents to modify initial
scene configurations and observe consequences, they lack the capability to
interact with events in real time. To address this, we introduce I-PHYRE, a
framework that challenges agents to simultaneously exhibit intuitive physical
reasoning, multi-step planning, and in-situ intervention. Here, intuitive
physical reasoning refers to a quick, approximate understanding of physics to
address complex problems; multi-step denotes the need for extensive sequence
planning in I-PHYRE, considering each intervention can significantly alter
subsequent choices; and in-situ implies the necessity for timely object
manipulation within a scene, where minor timing deviations can result in task
failure. We formulate four game splits to scrutinize agents' learning and
generalization of essential principles of interactive physical reasoning,
fostering learning through interaction with representative scenarios. Our
exploration involves three planning strategies and examines several supervised
and reinforcement agents' zero-shot generalization proficiency on I-PHYRE. The
outcomes highlight a notable gap between existing learning algorithms and human
performance, emphasizing the imperative for more research in enhancing agents
with interactive physical reasoning capabilities. The environment and baselines
will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving the bongard-logo problem by modeling a probabilistic model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03173v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03173v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhuo Song, Beiming Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstract reasoning problems challenge the perceptual and cognitive abilities
of AI algorithms, demanding deeper pattern discernment and inductive reasoning
beyond explicit image features. This study introduces PMoC, a tailored
probability model for the Bongard-Logo problem, achieving high reasoning
accuracy by constructing independent probability models. Additionally, we
present Pose-Transformer, an enhanced Transformer-Encoder designed for complex
abstract reasoning tasks, including Bongard-Logo, RAVEN, I-RAVEN, and PGM.
Pose-Transformer incorporates positional information learning, inspired by
capsule networks' pose matrices, enhancing its focus on local positional
relationships in image data processing. When integrated with PMoC, it further
improves reasoning accuracy. Our approach effectively addresses reasoning
difficulties associated with abstract entities' positional changes,
outperforming previous models on the OIG, D3$\times$3 subsets of RAVEN, and PGM
databases. This research contributes to advancing AI's capabilities in abstract
reasoning and cognitive pattern recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 11 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Triple-CFN: Restructuring Conceptual Spaces for Enhancing Abstract
  Reasoning process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03190v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03190v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhuo Song, Beiming Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstract reasoning problems pose significant challenges to artificial
intelligence algorithms, demanding cognitive capabilities beyond those required
for perception tasks. This study introduces the Triple-CFN approach to tackle
the Bongard-Logo problem, achieving notable reasoning accuracy by implicitly
reorganizing the concept space of conflicting instances. Additionally, the
Triple-CFN paradigm proves effective for the RPM problem with necessary
modifications, yielding competitive results. To further enhance performance on
the RPM issue, we develop the Meta Triple-CFN network, which explicitly
structures the problem space while maintaining interpretability on progressive
patterns. The success of Meta Triple-CFN is attributed to its paradigm of
modeling the conceptual space, equivalent to normalizing reasoning information.
Based on this ideology, we introduce the Re-space layer, enhancing the
performance of both Meta Triple-CFN and Triple-CFN. This paper aims to
contribute to advancements in machine intelligence by exploring innovative
network designs for addressing abstract reasoning problems, paving the way for
further breakthroughs in this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 14 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ D4C glove-train: solving the RPM and Bongard-logo problem by
  distributing and Circumscribing concepts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03452v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03452v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhuo Song, Beiming Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper achieves noteworthy progress in the realm of abstract reasoning,
particularly in addressing Raven's Progressive Matrices (RPM) and Bongard-Logo
challenges. Initially, we introduce Lico-Net, a novel baseline model that
resolves RPM problems with remarkable accuracy. Leveraging this foundation, we
advance with the D3C approach, which advocates representing the underlying
concepts in abstract reasoning problems through distributions. This perspective
enhances the performance of both Lico-Net and a baseline model excelling in
Bongard-Logo tasks. To bolster the computational efficiency of D3C, we present
the D3C-cos variant, offering a streamlined yet precise solution. Furthermore,
we propose the D2C method, redefining conceptual boundaries within these
domains and bridging the divide between high-level abstractions and their
lower-dimensional counterparts. Finally, we extend our methodology to D4C,
employing adversarial techniques to refine conceptual boundaries further and
demonstrate substantial improvements in both RPM and Bongard-Logo challenges.
Overall, our contributions present a fresh outlook and practical advancements
in the field of abstract reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 19 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CiPR: An Efficient Framework with Cross-instance Positive Relations for
  Generalized Category Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06928v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06928v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaozhe Hao, Kai Han, Kwan-Yee K. Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the issue of generalized category discovery (GCD). GCD considers
the open-world problem of automatically clustering a partially labelled
dataset, in which the unlabelled data may contain instances from both novel
categories and labelled classes. In this paper, we address the GCD problem with
an unknown category number for the unlabelled data. We propose a framework,
named CiPR, to bootstrap the representation by exploiting Cross-instance
Positive Relations in the partially labelled data for contrastive learning,
which have been neglected in existing methods. To obtain reliable
cross-instance relations to facilitate representation learning, we introduce a
semi-supervised hierarchical clustering algorithm, named selective neighbor
clustering (SNC), which can produce a clustering hierarchy directly from the
connected components of a graph constructed from selective neighbors. We
further present a method to estimate the unknown class number using SNC with a
joint reference score that considers clustering indexes of both labelled and
unlabelled data, and extend SNC to allow label assignment for the unlabelled
instances with a given class number. We thoroughly evaluate our framework on
public generic image recognition datasets and challenging fine-grained
datasets, and establish a new state-of-the-art. Code:
https://github.com/haoosz/CiPR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TMLR. Code: https://github.com/haoosz/CiPR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction
  Data <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13614v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13614v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng Qin, Siliang Tang, Qi Tian, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal Large Language Models (MLLMs) tuned on machine-generated
instruction-following data have demonstrated remarkable performance in various
multi-modal understanding and generation tasks. However, the hallucinations
inherent in machine-generated data, which could lead to hallucinatory outputs
in MLLMs, remain under-explored. This work aims to investigate various
hallucinations (i.e., object, relation, attribute hallucinations) and mitigate
those hallucinatory toxicities in large-scale machine-generated visual
instruction datasets. Drawing on the human ability to identify factual errors,
we present a novel hallucination detection and elimination framework,
HalluciDoctor, based on the cross-checking paradigm. We use our framework to
identify and eliminate hallucinations in the training data automatically.
Interestingly, HalluciDoctor also indicates that spurious correlations arising
from long-tail object co-occurrences contribute to hallucinations. Based on
that, we execute counterfactual visual instruction expansion to balance data
distribution, thereby enhancing MLLMs' resistance to hallucinations.
Comprehensive experiments on hallucination evaluation benchmarks show that our
method successfully mitigates 44.6% hallucinations relatively and maintains
competitive performance compared to LLaVA. The data and code for this paper are
publicly available. \url{https://github.com/Yuqifan1117/HalluciDoctor}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ W-HMR: Human Mesh Recovery in World Space with Weak-supervised Camera
  Calibration and Orientation Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17460v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17460v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Yao, Hongwen Zhang, Yunlian Sun, Jinhui Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For a long time, in reconstructing 3D human bodies from monocular images,
most methods opted to simplify the task by minimizing the influence of the
camera. Using a coarse focal length setting results in the reconstructed bodies
not aligning well with distorted images. Ignoring camera rotation leads to an
unrealistic reconstructed body pose in world space. Consequently, the
application scenarios of existing methods are confined to controlled
environments. When confronted with complex and diverse in-the-wild images, they
struggle to achieve accurate and reasonable reconstruction in world space. To
address the above issues, we propose W-HMR, which decouples global body
recovery into camera calibration, local body recovery, and global body
orientation correction. We design the first weak-supervised camera calibration
method for body distortion, eliminating dependence on focal length labels and
achieving finer mesh-image alignment. We propose a novel orientation correction
module to allow the reconstructed human body to remain normal in world space.
Decoupling body orientation and body pose enables our model to consider the
accuracy in camera coordinate and the reasonableness in world coordinate
simultaneously, expanding the range of applications. As a result, W-HMR
achieves high-quality reconstruction in dual coordinate systems, particularly
in challenging scenes. Codes and demos have been released on the project page
https://yw0208.github.io/w-hmr/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://yw0208.github.io/w-hmr/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Semantic Segmentation Meets Frequency Aliasing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09065v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09065v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linwei Chen, Lin Gu, Ying Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advancements in semantic segmentation, where and what pixels
are hard to segment remains largely unexplored. Existing research only
separates an image into easy and hard regions and empirically observes the
latter are associated with object boundaries. In this paper, we conduct a
comprehensive analysis of hard pixel errors, categorizing them into three
types: false responses, merging mistakes, and displacements. Our findings
reveal a quantitative association between hard pixels and aliasing, which is
distortion caused by the overlapping of frequency components in the Fourier
domain during downsampling. To identify the frequencies responsible for
aliasing, we propose using the equivalent sampling rate to calculate the
Nyquist frequency, which marks the threshold for aliasing. Then, we introduce
the aliasing score as a metric to quantify the extent of aliasing. While
positively correlated with the proposed aliasing score, three types of hard
pixels exhibit different patterns. Here, we propose two novel de-aliasing
filter (DAF) and frequency mixing (FreqMix) modules to alleviate aliasing
degradation by accurately removing or adjusting frequencies higher than the
Nyquist frequency. The DAF precisely removes the frequencies responsible for
aliasing before downsampling, while the FreqMix dynamically selects
high-frequency components within the encoder block. Experimental results
demonstrate consistent improvements in semantic segmentation and low-light
instance segmentation tasks. The code is available at:
https://github.com/Linwei-Chen/Seg-Aliasing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cell Variational Information Bottleneck Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15082v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15082v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhonghua Zhai, Chen Ju, Jinsong Lan, Shuai Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose Cell Variational Information Bottleneck Network
(cellVIB), a convolutional neural network using information bottleneck
mechanism, which can be combined with the latest feedforward network
architecture in an end-to-end training method. Our Cell Variational Information
Bottleneck Network is constructed by stacking VIB cells, which generate feature
maps with uncertainty. As layers going deeper, the regularization effect will
gradually increase, instead of directly adding excessive regular constraints to
the output layer of the model as in Deep VIB. Under each VIB cell, the
feedforward process learns an independent mean term and an standard deviation
term, and predicts the Gaussian distribution based on them. The feedback
process is based on reparameterization trick for effective training. This work
performs an extensive analysis on MNIST dataset to verify the effectiveness of
each VIB cells, and provides an insightful analysis on how the VIB cells affect
mutual information. Experiments conducted on CIFAR-10 also prove that our
cellVIB is robust against noisy labels during training and against corrupted
images during testing. Then, we validate our method on PACS dataset, whose
results show that the VIB cells can significantly improve the generalization
performance of the basic model. Finally, in a more complex representation
learning task, face recognition, our network structure has also achieved very
competitive results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Don't Judge by the Look: Towards Motion Coherent Video Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09506v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09506v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yitian Zhang, Yue Bai, Huan Wang, Yizhou Wang, Yun Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current training pipelines in object recognition neglect Hue Jittering when
doing data augmentation as it not only brings appearance changes that are
detrimental to classification, but also the implementation is inefficient in
practice. In this study, we investigate the effect of hue variance in the
context of video understanding and find this variance to be beneficial since
static appearances are less important in videos that contain motion
information. Based on this observation, we propose a data augmentation method
for video understanding, named Motion Coherent Augmentation (MCA), that
introduces appearance variation in videos and implicitly encourages the model
to prioritize motion patterns, rather than static appearances. Concretely, we
propose an operation SwapMix to efficiently modify the appearance of video
samples, and introduce Variation Alignment (VA) to resolve the distribution
shift caused by SwapMix, enforcing the model to learn appearance invariant
representations. Comprehensive empirical evaluation across various
architectures and different datasets solidly validates the effectiveness and
generalization ability of MCA, and the application of VA in other augmentation
methods. Code is available at https://github.com/BeSpontaneous/MCA-pytorch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15048v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15048v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bumsoo Kim, Wonseop Shin, Kyuchul Lee, Sanghyun Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale Text-to-Image (TTI) models have become a common approach for
generating training data in various generative fields. However, visual
hallucinations, which contain perceptually critical defects, remain a concern,
especially in non-photorealistic styles like cartoon characters. We propose a
novel visual hallucination detection system for cartoon character images
generated by TTI models. Our approach leverages pose-aware in-context visual
learning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB
images and pose information. By incorporating pose guidance from a fine-tuned
pose estimator, we enable VLMs to make more accurate decisions. Experimental
results demonstrate significant improvements in identifying visual
hallucinations compared to baseline methods relying solely on RGB images. This
research advances TTI models by mitigating visual hallucinations, expanding
their potential in non-photorealistic domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 12 figures, 1 table, Project page:
  https://gh-bumsookim.github.io/Cartoon-Hallucinations-Detection/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMA-Diffusion: MultiModal Attack on Diffusion Models <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17516v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17516v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijun Yang, Ruiyuan Gao, Xiaosen Wang, Tsung-Yi Ho, Nan Xu, Qiang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Text-to-Image (T2I) models have seen remarkable
advancements, gaining widespread adoption. However, this progress has
inadvertently opened avenues for potential misuse, particularly in generating
inappropriate or Not-Safe-For-Work (NSFW) content. Our work introduces
MMA-Diffusion, a framework that presents a significant and realistic threat to
the security of T2I models by effectively circumventing current defensive
measures in both open-source models and commercial online services. Unlike
previous approaches, MMA-Diffusion leverages both textual and visual modalities
to bypass safeguards like prompt filters and post-hoc safety checkers, thus
exposing and highlighting the vulnerabilities in existing defense mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Code is available at
  https://github.com/yangyijune/MMA-Diffusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Noisy-Correspondence Learning for Text-to-Image Person Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.09911v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.09911v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Qin, Yingke Chen, Dezhong Peng, Xi Peng, Joey Tianyi Zhou, Peng Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image person re-identification (TIReID) is a compelling topic in the
cross-modal community, which aims to retrieve the target person based on a
textual query. Although numerous TIReID methods have been proposed and achieved
promising performance, they implicitly assume the training image-text pairs are
correctly aligned, which is not always the case in real-world scenarios. In
practice, the image-text pairs inevitably exist under-correlated or even
false-correlated, a.k.a noisy correspondence (NC), due to the low quality of
the images and annotation errors. To address this problem, we propose a novel
Robust Dual Embedding method (RDE) that can learn robust visual-semantic
associations even with NC. Specifically, RDE consists of two main components:
1) A Confident Consensus Division (CCD) module that leverages the dual-grained
decisions of dual embedding modules to obtain a consensus set of clean training
data, which enables the model to learn correct and reliable visual-semantic
associations. 2) A Triplet Alignment Loss (TAL) relaxes the conventional
Triplet Ranking loss with the hardest negative samples to a log-exponential
upper bound over all negative ones, thus preventing the model collapse under NC
and can also focus on hard-negative samples for promising performance. We
conduct extensive experiments on three public benchmarks, namely CUHK-PEDES,
ICFG-PEDES, and RSTPReID, to evaluate the performance and robustness of our
RDE. Our method achieves state-of-the-art results both with and without
synthetic noisy correspondences on all three datasets. Code is available at
https://github.com/QinYang79/RDE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CRS-Diff: Controllable Generative Remote Sensing Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11614v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11614v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Datao Tang, Xiangyong Cao, Xingsong Hou, Zhongyuan Jiang, Deyu Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of diffusion models has revolutionized the field of image
generation, providing new methods for creating high-quality, high-resolution
images across various applications. However, the potential of these models for
generating domain-specific images, particularly remote sensing (RS) images,
remains largely untapped. RS images that are notable for their high resolution,
extensive coverage, and rich information content, bring new challenges that
general diffusion models may not adequately address. This paper proposes
CRS-Diff, a pioneering diffusion modeling framework specifically tailored for
generating remote sensing imagery, leveraging the inherent advantages of
diffusion models while integrating advanced control mechanisms to ensure that
the imagery is not only visually clear but also enriched with geographic and
temporal information. The model integrates global and local control inputs,
enabling precise combinations of generation conditions to refine the generation
process. A comprehensive evaluation of CRS-Diff has demonstrated its superior
capability to generate RS imagery both in a single condition and multiple
conditions compared with previous methods in terms of image quality and
diversity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Telling Left from Right: Identifying Geometry-Aware Semantic
  Correspondence <span class="chip">CVPR 24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17034v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17034v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyi Zhang, Charles Herrmann, Junhwa Hur, Eric Chen, Varun Jampani, Deqing Sun, Ming-Hsuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While pre-trained large-scale vision models have shown significant promise
for semantic correspondence, their features often struggle to grasp the
geometry and orientation of instances. This paper identifies the importance of
being geometry-aware for semantic correspondence and reveals a limitation of
the features of current foundation models under simple post-processing. We show
that incorporating this information can markedly enhance semantic
correspondence performance with simple but effective solutions in both
zero-shot and supervised settings. We also construct a new challenging
benchmark for semantic correspondence built from an existing animal pose
estimation dataset, for both pre-training validating models. Our method
achieves a PCK@0.10 score of 65.4 (zero-shot) and 85.6 (supervised) on the
challenging SPair-71k dataset, outperforming the state of the art by 5.5p and
11.0p absolute gains, respectively. Our code and datasets are publicly
available at: https://telling-left-from-right.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 24, project page:
  https://telling-left-from-right.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VURF: A General-purpose Reasoning and Self-refinement Framework for
  Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14743v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14743v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Mahmood, Ashmal Vayani, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have demonstrated the effectiveness of Large Language Models
(LLMs) as reasoning modules that can deconstruct complex tasks into more
manageable sub-tasks, particularly when applied to visual reasoning tasks for
images. In contrast, this paper introduces a Video Understanding and Reasoning
Framework (VURF) based on the reasoning power of LLMs. Ours is a novel approach
to extend the utility of LLMs in the context of video tasks, leveraging their
capacity to generalize from minimal input and output demonstrations within a
contextual framework. By presenting LLMs with pairs of instructions and their
corresponding high-level programs, we harness their contextual learning
capabilities to generate executable visual programs for video understanding. To
enhance program's accuracy and robustness, we implement two important
strategies. Firstly, we employ a feedback-generation approach, powered by
GPT-3.5, to rectify errors in programs utilizing unsupported functions.
Secondly, taking motivation from recent works on self refinement of LLM
outputs, we introduce an iterative procedure for improving the quality of the
in-context examples by aligning the initial outputs to the outputs that would
have been generated had the LLM not been bound by the structure of the
in-context examples. Our results on several video-specific tasks, including
visual QA, video anticipation, pose estimation and multi-video QA illustrate
the efficacy of these enhancements in improving the performance of visual
programming approaches for video tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ URS-NeRF: Unordered Rolling Shutter Bundle Adjustment for Neural
  Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Xu, Ziao Liu, Mengqi Guo, Jiancheng Li, Gim Hee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel rolling shutter bundle adjustment method for neural
radiance fields (NeRF), which utilizes the unordered rolling shutter (RS)
images to obtain the implicit 3D representation. Existing NeRF methods suffer
from low-quality images and inaccurate initial camera poses due to the RS
effect in the image, whereas, the previous method that incorporates the RS into
NeRF requires strict sequential data input, limiting its widespread
applicability. In constant, our method recovers the physical formation of RS
images by estimating camera poses and velocities, thereby removing the input
constraints on sequential data. Moreover, we adopt a coarse-to-fine training
strategy, in which the RS epipolar constraints of the pairwise frames in the
scene graph are used to detect the camera poses that fall into local minima.
The poses detected as outliers are corrected by the interpolation method with
neighboring poses. The experimental results validate the effectiveness of our
method over state-of-the-art works and demonstrate that the reconstruction of
3D representations is not constrained by the requirement of video sequence
input.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving White-box Robustness of Pre-processing Defenses via Joint
  Adversarial Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.05453v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.05453v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dawei Zhou, Nannan Wang, Xinbo Gao, Bo Han, Jun Yu, Xiaoyu Wang, Tongliang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are vulnerable to adversarial noise. A range of
adversarial defense techniques have been proposed to mitigate the interference
of adversarial noise, among which the input pre-processing methods are scalable
and show great potential to safeguard DNNs. However, pre-processing methods may
suffer from the robustness degradation effect, in which the defense reduces
rather than improving the adversarial robustness of a target model in a
white-box setting. A potential cause of this negative effect is that
adversarial training examples are static and independent to the pre-processing
model. To solve this problem, we investigate the influence of full adversarial
examples which are crafted against the full model, and find they indeed have a
positive impact on the robustness of defenses. Furthermore, we find that simply
changing the adversarial training examples in pre-processing methods does not
completely alleviate the robustness degradation effect. This is due to the
adversarial risk of the pre-processed model being neglected, which is another
cause of the robustness degradation effect. Motivated by above analyses, we
propose a method called Joint Adversarial Training based Pre-processing (JATP)
defense. Specifically, we formulate a feature similarity based adversarial risk
for the pre-processing model by using full adversarial examples found in a
feature space. Unlike standard adversarial training, we only update the
pre-processing model, which prompts us to introduce a pixel-wise loss to
improve its cross-model transferability. We then conduct a joint adversarial
training on the pre-processing model to minimize this overall risk. Empirical
results show that our method could effectively mitigate the robustness
degradation effect across different target models in comparison to previous
state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Masked Vector Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06626v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06626v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David D. Nguyen, David Leibowitz, Surya Nepal, Salil S. Kanhere
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models with discrete latent representations have recently
demonstrated an impressive ability to learn complex high-dimensional data
distributions. However, their performance relies on a long sequence of tokens
per instance and a large number of codebook entries, resulting in long sampling
times and considerable computation to fit the categorical posterior. To address
these issues, we propose the Masked Vector Quantization (MVQ) framework which
increases the representational capacity of each code vector by learning mask
configurations via a stochastic winner-takes-all training regime called
Multiple Hypothese Dropout (MH-Dropout). On ImageNet 64$\times$64, MVQ reduces
FID in existing vector quantization architectures by up to $68\%$ at 2 tokens
per instance and $57\%$ at 5 tokens. These improvements widen as codebook
entries is reduced and allows for $7\textit{--}45\times$ speed-up in token
sampling during inference. As an additional benefit, we find that smaller
latent spaces lead to MVQ identifying transferable visual representations where
multiple can be smoothly combined.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A newer version of this manuscript was archived under 2312.11735</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-24T00:00:00Z">2024-03-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">25</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single-Motor Robotic Gripper with Multi-Surface Fingers for Variable
  Grasping Configurations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toshihiro Nishimura, Yosuke Suzuki, Tokuo Tsuj, Tetsuyou Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study proposes a novel robotic gripper with variable grasping
configurations for grasping various objects. The fingers of the developed
gripper incorporate multiple different surfaces. The gripper possesses the
function of altering the finger surfaces facing a target object by rotating the
fingers in its longitudinal direction. In the proposed design equipped with two
fingers, the two fingers incorporate three and four surfaces, respectively,
resulting in the nine available grasping configurations by the combination of
these finger surfaces. The developed gripper is equipped with the functions of
opening/closing its fingers for grasping and rotating its fingers to alter the
grasping configuration -all achieved with a single motor. To enable the two
motions using a single motor, this study introduces a self-motion switching
mechanism utilizing magnets. This mechanism automatically transitions between
gripper motions based on the direction of the motor rotation when the gripper
is fully opened. In this state, rotating the motor towards closing initiates
the finger closing action, while further opening the fingers from the fully
opened state activates the finger rotation. This letter presents the gripper
design, the mechanics of the self-motion switching mechanism, the control
method, and the grasping configuration selection strategy. The performance of
the gripper is experimentally demonstrated.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guessing human intentions to avoid dangerous situations in caregiving
  robots <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noé Zapata, Gerardo Pérez, Lucas Bonilla, Pedro Núñez, Pilar Bachiller, Pablo Bustos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For robots to interact socially, they must interpret human intentions and
anticipate their potential outcomes accurately. This is particularly important
for social robots designed for human care, which may face potentially dangerous
situations for people, such as unseen obstacles in their way, that should be
avoided. This paper explores the Artificial Theory of Mind (ATM) approach to
inferring and interpreting human intentions. We propose an algorithm that
detects risky situations for humans, selecting a robot action that removes the
danger in real time. We use the simulation-based approach to ATM and adopt the
'like-me' policy to assign intentions and actions to people. Using this
strategy, the robot can detect and act with a high rate of success under
time-constrained situations. The algorithm has been implemented as part of an
existing robotics cognitive architecture and tested in simulation scenarios.
Three experiments have been conducted to test the implementation's robustness,
precision and real-time response, including a simulated scenario, a
human-in-the-loop hybrid configuration and a real-world scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures. Submitted to IROS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combined Task and Motion Planning Via Sketch Decompositions (Extended
  Version with Supplementary Material) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Magí Dalmau-Moreno, Néstor García, Vicenç Gómez, Héctor Geffner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The challenge in combined task and motion planning (TAMP) is the effective
integration of a search over a combinatorial space, usually carried out by a
task planner, and a search over a continuous configuration space, carried out
by a motion planner. Using motion planners for testing the feasibility of task
plans and filling out the details is not effective because it makes the
geometrical constraints play a passive role. This work introduces a new
interleaved approach for integrating the two dimensions of TAMP that makes use
of sketches, a recent simple but powerful language for expressing the
decomposition of problems into subproblems. A sketch has width 1 if it
decomposes the problem into subproblems that can be solved greedily in linear
time. In the paper, a general sketch is introduced for several classes of TAMP
problems which has width 1 under suitable assumptions. While sketch
decompositions have been developed for classical planning, they offer two
important benefits in the context of TAMP. First, when a task plan is found to
be unfeasible due to the geometric constraints, the combinatorial search
resumes in a specific sub-problem. Second, the sampling of object
configurations is not done once, globally, at the start of the search, but
locally, at the start of each subproblem. Optimizations of this basic setting
are also considered and experimental results over existing and new
pick-and-place benchmarks are reported.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M^3RS: Multi-robot, Multi-objective, and Multi-mode Routing and
  Scheduling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishaan Mehta, Junseo Kim, Sharareh Taghipour, Sajad Saeedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a novel problem coined multi-robot,
multi-objective, and multi-mode routing and scheduling (M^3RS). The formulation
for M^3RS is introduced for time-bound multi-robot, multi-objective routing and
scheduling missions where each task has multiple execution modes. Different
execution modes have distinct resource consumption, associated execution time,
and quality. M^3RS assigns the optimal sequence of tasks and the execution
modes to each agent. The routes and associated modes depend on user preferences
for different objective criteria. The need for M^3RS comes from multi-robot
applications in which a trade-off between multiple criteria arises from
different task execution modes. We use M^3RS for the application of multi-robot
disinfection in public locations. The objectives considered for disinfection
application are disinfection quality and number of tasks completed. A
mixed-integer linear programming model is proposed for M^3RS. Then, a
time-efficient column generation scheme is presented to tackle the issue of
computation times for larger problem instances. The advantage of using multiple
modes over fixed execution mode is demonstrated using experiments on synthetic
data. The results suggest that M^3RS provides flexibility to the user in terms
of available solutions and performs well in joint performance metrics. The
application of the proposed problem is shown for a team of disinfection
robots.} The videos for the experiments are available on the project website:
https://sites.google.com/view/g-robot/m3rs/ .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HT-LIP Model based Robust Control of Quadrupedal Robot Locomotion under
  Unknown Vertical Ground Motion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Iqbal, Sushant Veer, Christopher Niezrecki, Yan Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a hierarchical control framework that enables robust
quadrupedal locomotion on a dynamic rigid surface (DRS) with general and
unknown vertical motions. The key novelty of the framework lies in its higher
layer, which is a discrete-time, provably stabilizing footstep controller. The
basis of the footstep controller is a new hybrid, time-varying, linear inverted
pendulum (HT-LIP) model that is low-dimensional and accurately captures the
essential robot dynamics during DRS locomotion. A new set of sufficient
stability conditions are then derived to directly guide the controller design
for ensuring the asymptotic stability of the HT-LIP model under general,
unknown, vertical DRS motions. Further, the footstep controller is cast as a
computationally efficient quadratic program that incorporates the proposed
HT-LIP model and stability conditions. The middle layer takes the desired
footstep locations generated by the higher layer as input to produce
kinematically feasible full-body reference trajectories, which are then
accurately tracked by a lower-layer torque controller. Hardware experiments on
a Unitree Go1 quadrupedal robot confirm the robustness of the proposed
framework under various unknown, aperiodic, vertical DRS motions and
uncertainties (e.g., slippery and uneven surfaces, solid and liquid loads, and
sudden pushes).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Legged Robot State Estimation within Non-inertial Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian He, Sangli Teng, Tzu-Yuan Lin, Maani Ghaffari, Yan Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the robot state estimation problem within a
non-inertial environment. The proposed state estimation approach relaxes the
common assumption of static ground in the system modeling. The process and
measurement models explicitly treat the movement of the non-inertial
environments without requiring knowledge of its motion in the inertial frame or
relying on GPS or sensing environmental landmarks. Further, the proposed state
estimator is formulated as an invariant extended Kalman filter (InEKF) with the
deterministic part of its process model obeying the group-affine property,
leading to log-linear error dynamics. The observability analysis of the filter
confirms that the robot's pose (i.e., position and orientation) and velocity
relative to the non-inertial environment are observable. Hardware experiments
on a humanoid robot moving on a rotating and translating treadmill demonstrate
the high convergence rate and accuracy of the proposed InEKF even under
significant treadmill pitch sway, as well as large estimation errors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KITchen: A Real-World Benchmark and <span class="highlight-title">Dataset</span> for 6D Object Pose
  Estimation in Kitchen Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Younes, Tamim Asfour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent progress on 6D object pose estimation methods for robotic
grasping, a substantial performance gap persists between the capabilities of
these methods on existing datasets and their efficacy in real-world mobile
manipulation tasks, particularly when robots rely solely on their monocular
egocentric field of view (FOV). Existing real-world datasets primarily focus on
table-top grasping scenarios, where a robotic arm is placed in a fixed position
and the objects are centralized within the FOV of fixed external camera(s).
Assessing performance on such datasets may not accurately reflect the
challenges encountered in everyday mobile manipulation tasks within kitchen
environments such as retrieving objects from higher shelves, sinks,
dishwashers, ovens, refrigerators, or microwaves. To address this gap, we
present Kitchen, a novel benchmark designed specifically for estimating the 6D
poses of objects located in diverse positions within kitchen settings. For this
purpose, we recorded a comprehensive dataset comprising around 205k real-world
RGBD images for 111 kitchen objects captured in two distinct kitchens,
utilizing one humanoid robot with its egocentric perspectives. Subsequently, we
developed a semi-automated annotation pipeline, to streamline the labeling
process of such datasets, resulting in the generation of 2D object labels, 2D
object segmentation masks, and 6D object poses with minimized human effort. The
benchmark, the dataset, and the annotation pipeline are available at
https://kitchen-dataset.github.io/KITchen.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixed-Initiative Human-Robot Teaming under Suboptimality with Online
  Bayesian Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manisha Natarajan, Chunyue Xue, Sanne van Waveren, Karen Feigh, Matthew Gombolay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For effective human-agent teaming, robots and other artificial intelligence
(AI) agents must infer their human partner's abilities and behavioral response
patterns and adapt accordingly. Most prior works make the unrealistic
assumption that one or more teammates can act near-optimally. In real-world
collaboration, humans and autonomous agents can be suboptimal, especially when
each only has partial domain knowledge. In this work, we develop computational
modeling and optimization techniques for enhancing the performance of
suboptimal human-agent teams, where the human and the agent have asymmetric
capabilities and act suboptimally due to incomplete environmental knowledge. We
adopt an online Bayesian approach that enables a robot to infer people's
willingness to comply with its assistance in a sequential decision-making game.
Our user studies show that user preferences and team performance indeed vary
with robot intervention styles, and our approach for mixed-initiative
collaborations enhances objective team performance ($p<.001$) and subjective
measures, such as user's trust ($p<.001$) and perceived likeability of the
robot ($p<.001$).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 pages for supplementary</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Realtime Robust Shape Estimation of Deformable Linear Object <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Zhang, Zhaomeng Zhang, Yihao Liu, Yaqian Chen, Amir Kheradmand, Mehran Armand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Realtime shape estimation of continuum objects and manipulators is essential
for developing accurate planning and control paradigms. The existing methods
that create dense point clouds from camera images, and/or use distinguishable
markers on a deformable body have limitations in realtime tracking of large
continuum objects/manipulators. The physical occlusion of markers can often
compromise accurate shape estimation. We propose a robust method to estimate
the shape of linear deformable objects in realtime using scattered and
unordered key points. By utilizing a robust probability-based labeling
algorithm, our approach identifies the true order of the detected key points
and then reconstructs the shape using piecewise spline interpolation. The
approach only relies on knowing the number of the key points and the interval
between two neighboring points. We demonstrate the robustness of the method
when key points are partially occluded. The proposed method is also integrated
into a simulation in Unity for tracking the shape of a cable with a length of
1m and a radius of 5mm. The simulation results show that our proposed approach
achieves an average length error of 1.07% over the continuum's centerline and
an average cross-section error of 2.11mm. The real-world experiments of
tracking and estimating a heavy-load cable prove that the proposed approach is
robust under occlusion and complex entanglement scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to IEEE ICRA 2024 as a contributed paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CG-<span class="highlight-title">SLAM</span>: Efficient Dense RGB-D <span class="highlight-title">SLAM</span> in a Consistent Uncertainty-aware 3D
  Gaussian Field 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Hu, Xianhao Chen, Boyin Feng, Guanglin Li, Liangjing Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently neural radiance fields (NeRF) have been widely exploited as 3D
representations for dense simultaneous localization and mapping (SLAM). Despite
their notable successes in surface modeling and novel view synthesis, existing
NeRF-based methods are hindered by their computationally intensive and
time-consuming volume rendering pipeline. This paper presents an efficient
dense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D
Gaussian field with high consistency and geometric stability. Through an
in-depth analysis of Gaussian Splatting, we propose several techniques to
construct a consistent and stable 3D Gaussian field suitable for tracking and
mapping. Additionally, a novel depth uncertainty model is proposed to ensure
the selection of valuable Gaussian primitives during optimization, thereby
improving tracking efficiency and accuracy. Experiments on various datasets
demonstrate that CG-SLAM achieves superior tracking and mapping performance
with a notable tracking speed of up to 15 Hz. We will make our source code
publicly available. Project page: https://zju3dv.github.io/cg-slam.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://zju3dv.github.io/cg-slam</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are NeRFs ready for autonomous driving? Towards closing the
  real-to-simulation gap 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carl Lindström, Georg Hess, Adam Lilja, Maryam Fatemi, Lars Hammarstrand, Christoffer Petersson, Lennart Svensson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing
autonomous driving (AD) research, offering scalable closed-loop simulation and
data augmentation capabilities. However, to trust the results achieved in
simulation, one needs to ensure that AD systems perceive real and rendered data
in the same way. Although the performance of rendering methods is increasing,
many scenarios will remain inherently challenging to reconstruct faithfully. To
this end, we propose a novel perspective for addressing the real-to-simulated
data gap. Rather than solely focusing on improving rendering fidelity, we
explore simple yet effective methods to enhance perception model robustness to
NeRF artifacts without compromising performance on real data. Moreover, we
conduct the first large-scale investigation into the real-to-simulated data gap
in an AD setting using a state-of-the-art neural rendering technique.
Specifically, we evaluate object detectors and an online mapping model on real
and simulated data, and study the effects of different pre-training strategies.
Our results show notable improvements in model robustness to simulated data,
even improving real-world performance in some cases. Last, we delve into the
correlation between the real-to-simulated gap and image reconstruction metrics,
identifying FID and LPIPS as strong indicators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RPMArt: Towards Robust Perception and Manipulation for Articulated
  Objects <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junbo Wang, Wenhai Liu, Qiaojun Yu, Yang You, Liu Liu, Weiming Wang, Cewu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Articulated objects are commonly found in daily life. It is essential that
robots can exhibit robust perception and manipulation skills for articulated
objects in real-world robotic applications. However, existing methods for
articulated objects insufficiently address noise in point clouds and struggle
to bridge the gap between simulation and reality, thus limiting the practical
deployment in real-world scenarios. To tackle these challenges, we propose a
framework towards Robust Perception and Manipulation for Articulated Objects
(RPMArt), which learns to estimate the articulation parameters and manipulate
the articulation part from the noisy point cloud. Our primary contribution is a
Robust Articulation Network (RoArtNet) that is able to predict both joint
parameters and affordable points robustly by local feature learning and point
tuple voting. Moreover, we introduce an articulation-aware classification
scheme to enhance its ability for sim-to-real transfer. Finally, with the
estimated affordable point and articulation joint constraint, the robot can
generate robust actions to manipulate articulated objects. After learning only
from synthetic data, RPMArt is able to transfer zero-shot to real-world
articulated objects. Experimental results confirm our approach's effectiveness,
with our framework achieving state-of-the-art performance in both noise-added
simulation and real-world environments. The code and data will be open-sourced
for reproduction. More results are published on the project website at
https://r-pmart.github.io .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, submitted to 2024 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2024), project website at
  https://r-pmart.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MQE: Unleashing the Power of Interaction with Multi-agent Quadruped
  Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyan Xiong, Bo Chen, Shiyu Huang, Wei-Wei Tu, Zhaofeng He, Yang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of deep reinforcement learning (DRL) has significantly advanced
the field of robotics, particularly in the control and coordination of
quadruped robots. However, the complexity of real-world tasks often
necessitates the deployment of multi-robot systems capable of sophisticated
interaction and collaboration. To address this need, we introduce the
Multi-agent Quadruped Environment (MQE), a novel platform designed to
facilitate the development and evaluation of multi-agent reinforcement learning
(MARL) algorithms in realistic and dynamic scenarios. MQE emphasizes complex
interactions between robots and objects, hierarchical policy structures, and
challenging evaluation scenarios that reflect real-world applications. We
present a series of collaborative and competitive tasks within MQE, ranging
from simple coordination to complex adversarial interactions, and benchmark
state-of-the-art MARL algorithms. Our findings indicate that hierarchical
reinforcement learning can simplify task learning, but also highlight the need
for advanced algorithms capable of handling the intricate dynamics of
multi-agent interactions. MQE serves as a stepping stone towards bridging the
gap between simulation and practical deployment, offering a rich environment
for future research in multi-agent systems and robot learning. For open-sourced
code and more details of MQE, please refer to
https://ziyanx02.github.io/multiagent-quadruped-environment/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Open-source code is available at
  https://github.com/ziyanx02/multiagent-quadruped-environment</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust-Locomotion-by-Logic: Perturbation-Resilient Bipedal Locomotion
  via Signal Temporal Logic Guided Model Predictive Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyuan Gu, Yuntian Zhao, Yipu Chen, Rongming Guo, Jennifer K. Leestma, Gregory S. Sawicki, Ye Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces a robust planning framework that utilizes a model
predictive control (MPC) approach, enhanced by incorporating signal temporal
logic (STL) specifications. This marks the first-ever study to apply STL-guided
trajectory optimization for bipedal locomotion, specifically designed to handle
both translational and orientational perturbations. Existing recovery
strategies often struggle with reasoning complex task logic and evaluating
locomotion robustness systematically, making them susceptible to failures
caused by inappropriate recovery strategies or lack of robustness. To address
these issues, we design an analytical robustness metric for bipedal locomotion
and quantify this metric using STL specifications, which guide the generation
of recovery trajectories to achieve maximum locomotion robustness. To enable
safe and computational-efficient crossed-leg maneuver, we design data-driven
self-leg-collision constraints that are $1000$ times faster than the
traditional inverse-kinematics-based approach. Our framework outperforms a
state-of-the-art locomotion controller, a standard MPC without STL, and a
linear-temporal-logic-based planner in a high-fidelity dynamic simulation,
especially in scenarios involving crossed-leg maneuvers. Additionally, the
Cassie bipedal robot achieves robust performance under horizontal and
orientational perturbations such as those observed in ship motions. These
environments are validated in simulations and deployed on hardware.
Furthermore, our proposed method demonstrates versatility on stepping stones
and terrain-agnostic features on inclined terrains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Make a Donut: Hierarchical EMD-Space Planning for Zero-Shot Deformable
  Manipulation with Tools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02787v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02787v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang You, Bokui Shen, Congyue Deng, Haoran Geng, Songlin Wei, He Wang, Leonidas Guibas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deformable object manipulation stands as one of the most captivating yet
formidable challenges in robotics. While previous techniques have predominantly
relied on learning latent dynamics through demonstrations, typically
represented as either particles or images, there exists a pertinent limitation:
acquiring suitable demonstrations, especially for long-horizon tasks, can be
elusive. Moreover, basing learning entirely on demonstrations can hamper the
model's ability to generalize beyond the demonstrated tasks. In this work, we
introduce a demonstration-free hierarchical planning approach capable of
tackling intricate long-horizon tasks without necessitating any training. We
employ large language models (LLMs) to articulate a high-level, stage-by-stage
plan corresponding to a specified task. For every individual stage, the LLM
provides both the tool's name and the Python code to craft intermediate subgoal
point clouds. With the tool and subgoal for a particular stage at our disposal,
we present a granular closed-loop model predictive control strategy. This
leverages Differentiable Physics with Point-to-Point correspondence
(DiffPhysics-P2P) loss in the earth mover distance (EMD) space, applied
iteratively. Experimental findings affirm that our technique surpasses multiple
benchmarks in dough manipulation, spanning both short and long horizons.
Remarkably, our model demonstrates robust generalization capabilities to novel
and previously unencountered complex tasks without any preliminary
demonstrations. We further substantiate our approach with experimental trials
on real-world robotic platforms. Our project page:
https://qq456cvb.github.io/projects/donut.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Development and Evaluation of a Learning-based Model for Real-time
  Haptic Texture Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.13332v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.13332v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Negin Heravi, Heather Culbertson, Allison M. Okamura, Jeannette Bohg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Virtual Reality (VR) environments lack the rich haptic signals that
humans experience during real-life interactions, such as the sensation of
texture during lateral movement on a surface. Adding realistic haptic textures
to VR environments requires a model that generalizes to variations of a user's
interaction and to the wide variety of existing textures in the world. Current
methodologies for haptic texture rendering exist, but they usually develop one
model per texture, resulting in low scalability. We present a deep
learning-based action-conditional model for haptic texture rendering and
evaluate its perceptual performance in rendering realistic texture vibrations
through a multi part human user study. This model is unified over all materials
and uses data from a vision-based tactile sensor (GelSight) to render the
appropriate surface conditioned on the user's action in real time. For
rendering texture, we use a high-bandwidth vibrotactile transducer attached to
a 3D Systems Touch device. The result of our user study shows that our
learning-based method creates high-frequency texture renderings with comparable
or better quality than state-of-the-art methods without the need for learning a
separate model per texture. Furthermore, we show that the method is capable of
rendering previously unseen textures using a single GelSight image of their
surface.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Transactions on Haptics 2024. 12
  pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Graphical Inverse Kinematics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.08812v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.08812v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Limoyo, Filip Marić, Matthew Giamou, Petra Alexson, Ivan Petrović, Jonathan Kelly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quickly and reliably finding accurate inverse kinematics (IK) solutions
remains a challenging problem for many robot manipulators. Existing numerical
solvers are broadly applicable but typically only produce a single solution and
rely on local search techniques to minimize nonconvex objective functions. More
recent learning-based approaches that approximate the entire feasible set of
solutions have shown promise as a means to generate multiple fast and accurate
IK results in parallel. However, existing learning-based techniques have a
significant drawback: each robot of interest requires a specialized model that
must be trained from scratch. To address this key shortcoming, we propose a
novel distance-geometric robot representation coupled with a graph structure
that allows us to leverage the sample efficiency of Euclidean equivariant
functions and the generalizability of graph neural networks (GNNs). Our
approach is generative graphical inverse kinematics (GGIK), the first learned
IK solver able to accurately and efficiently produce a large number of diverse
solutions in parallel while also displaying the ability to generalize -- a
single learned model can be used to produce IK solutions for a variety of
different robots. When compared to several other learned IK methods, GGIK
provides more accurate solutions with the same amount of data. GGIK can
generalize reasonably well to robot manipulators unseen during training.
Additionally, GGIK can learn a constrained distribution that encodes joint
limits and scales efficiently to larger robots and a high number of sampled
solutions. Finally, GGIK can be used to complement local IK solvers by
providing reliable initializations for a local optimization process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Robotics, June 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SG-Bot: Object Rearrangement via Coarse-to-Fine Robotic Imagination on
  Scene Graphs <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12188v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12188v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyao Zhai, Xiaoni Cai, Dianye Huang, Yan Di, Fabian Manhardt, Federico Tombari, Nassir Navab, Benjamin Busam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object rearrangement is pivotal in robotic-environment interactions,
representing a significant capability in embodied AI. In this paper, we present
SG-Bot, a novel rearrangement framework that utilizes a coarse-to-fine scheme
with a scene graph as the scene representation. Unlike previous methods that
rely on either known goal priors or zero-shot large models, SG-Bot exemplifies
lightweight, real-time, and user-controllable characteristics, seamlessly
blending the consideration of commonsense knowledge with automatic generation
capabilities. SG-Bot employs a three-fold procedure--observation, imagination,
and execution--to adeptly address the task. Initially, objects are discerned
and extracted from a cluttered scene during the observation. These objects are
first coarsely organized and depicted within a scene graph, guided by either
commonsense or user-defined criteria. Then, this scene graph subsequently
informs a generative model, which forms a fine-grained goal scene considering
the shape information from the initial scene and object semantics. Finally, for
execution, the initial and envisioned goal scenes are matched to formulate
robotic action policies. Experimental results demonstrate that SG-Bot
outperforms competitors by a large margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2024 accepted. Project website:
  https://sites.google.com/view/sg-bot</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EnduRL: Enhancing Safety, Stability, and Efficiency of Mixed Traffic
  Under Real-World Perturbations Via Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12261v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12261v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bibek Poudel, Weizi Li, Kevin Heaslip
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-driven vehicles (HVs) amplify naturally occurring perturbations in
traffic, leading to congestion--a major contributor to increased fuel
consumption, higher collision risks, and reduced road capacity utilization.
While previous research demonstrates that Robot Vehicles (RVs) can be leveraged
to mitigate these issues, most such studies rely on simulations with simplistic
models of human car-following behaviors. In this work, we analyze real-world
driving trajectories and extract a wide range of acceleration profiles. We then
incorporates these profiles into simulations for training RVs to mitigate
congestion. We evaluate the safety, efficiency, and stability of mixed traffic
via comprehensive experiments conducted in two mixed traffic environments (Ring
and Bottleneck) at various traffic densities, configurations, and RV
penetration rates. The results show that under real-world perturbations, prior
RV controllers experience performance degradation on all three objectives
(sometimes even lower than 100% HVs). To address this, we introduce a
reinforcement learning based RV that employs a congestion stage classifier to
optimize the safety, efficiency, and stability of mixed traffic. Our RVs
demonstrate significant improvements: safety by up to 66%, efficiency by up to
54%, and stability by up to 97%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Soft finger rotational stability for precision grasps <span class="chip">IROS24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04846v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04846v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hun Jang, Valentyn Petrichenko, Joonbum Bae, Kevin Haninger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soft robotic fingers can safely grasp fragile or variable form objects, but
their force capacity is limited, especially with less contact area: precision
grasps and when objects are smaller or not spherical. Current research is
improving force capacity through mechanical design by increasing contact area
or stiffness, typically without models which explain soft finger force
limitations. To address this, this paper considers two types of soft grip
failure, slip and dynamic rotational stability. For slip, the validity of a
Coulomb model investigated, identifying the effect of contact area, pressure,
and relative pose. For rotational stability, bulk linear stiffness of the
fingers is used to develop conditions for dynamic stability and identify when
rotation leads to slip. Together, these models suggest contact area improves
force capacity by increasing transverse stiffness and normal force. The models
are validated on pneumatic fingers, both custom PneuNets-based and commercially
available. The models are used to find grip parameters which increase force
capacity without failure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted IROS24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Combining Sampling- and Gradient-based Planning for Contact-rich
  Manipulation <span class="chip">ICRA24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04822v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04822v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filippo Rozzi, Loris Roveda, Kevin Haninger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planning over discontinuous dynamics is needed for robotics tasks like
contact-rich manipulation, which presents challenges in the numerical stability
and speed of planning methods when either neural network or analytical models
are used. On the one hand, sampling-based planners require higher sample
complexity in high-dimensional problems and cannot describe safety constraints
such as force limits. On the other hand, gradient-based solvers can suffer from
local optima and convergence issues when the Hessian is poorly conditioned. We
propose a planning method with both sampling- and gradient-based elements,
using the Cross-entropy Method to initialize a gradient-based solver, providing
better search over local minima and the ability to handle explicit constraints.
We show the approach allows smooth, stable contact-rich planning for an
impedance-controlled robot making contact with a stiff environment,
benchmarking against gradient-only MPC and CEM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted ICRA24. Video available at https://youtu.be/COqR90392Kw
  Code available at https://gitlab.cc-asp.fraunhofer.de/hanikevi/contact_mpc</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SwarmPRM: Probabilistic Roadmap Motion Planning for Large-Scale Swarm
  Robotic Systems <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16699v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16699v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunze Hu, Xuru Yang, Kangjie Zhou, Qinghang Liu, Kang Ding, Han Gao, Pingping Zhu, Chang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale swarm robotic systems consisting of numerous cooperative agents
show considerable promise for performing autonomous tasks across various
sectors. Nonetheless, traditional motion planning approaches often face a
trade-off between scalability and solution quality due to the exponential
growth of the joint state space of robots. In response, this work proposes
SwarmPRM, a hierarchical, scalable, computationally efficient, and risk-aware
sampling-based motion planning approach for large-scale swarm robots. SwarmPRM
utilizes a Gaussian Mixture Model (GMM) to represent the swarm's macroscopic
state and constructs a Probabilistic Roadmap in Gaussian space, referred to as
the Gaussian roadmap, to generate a transport trajectory of GMM. This
trajectory is then followed by each robot at the microscopic stage. To enhance
trajectory safety, SwarmPRM incorporates the conditional value-at-risk (CVaR)
in the collision checking process to impart the property of risk awareness to
the constructed Gaussian roadmap. SwarmPRM then crafts a linear programming
formulation to compute the optimal GMM transport trajectory within this
roadmap. Extensive simulations demonstrate that SwarmPRM outperforms
state-of-the-art methods in computational efficiency, scalability, and
trajectory quality while offering the capability to adjust the risk tolerance
of generated trajectories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DRL-Based Trajectory Tracking for Motion-Related Modules in Autonomous
  Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15991v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15991v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinda Xu, Lidong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving systems are always built on motion-related modules such as
the planner and the controller. An accurate and robust trajectory tracking
method is indispensable for these motion-related modules as a primitive
routine. Current methods often make strong assumptions about the model such as
the context and the dynamics, which are not robust enough to deal with the
changing scenarios in a real-world system. In this paper, we propose a Deep
Reinforcement Learning (DRL)-based trajectory tracking method for the
motion-related modules in autonomous driving systems. The representation
learning ability of DL and the exploration nature of RL bring strong robustness
and improve accuracy. Meanwhile, it enhances versatility by running the
trajectory tracking in a model-free and data-driven manner. Through extensive
experiments, we demonstrate both the efficiency and effectiveness of our method
compared to current methods. Code and documentation are released to facilitate
both further research and industrial deployment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report. Code:
  https://github.com/MARMOTatZJU/drl-based-trajectory-tracking Documentation:
  https://drl-based-trajectory-tracking.readthedocs.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Number Sense as an Emergent Property of the Manipulating Brain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.04132v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.04132v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neehar Kondapaneni, Pietro Perona
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to understand and manipulate numbers and quantities emerges
during childhood, but the mechanism through which humans acquire and develop
this ability is still poorly understood. We explore this question through a
model, assuming that the learner is able to pick up and place small objects
from, and to, locations of its choosing, and will spontaneously engage in such
undirected manipulation. We further assume that the learner's visual system
will monitor the changing arrangements of objects in the scene and will learn
to predict the effects of each action by comparing perception with a
supervisory signal from the motor system. We model perception using standard
deep networks for feature extraction and classification, and gradient descent
learning. Our main finding is that, from learning the task of action
prediction, an unexpected image representation emerges exhibiting regularities
that foreshadow the perception and representation of numbers and quantity.
These include distinct categories for zero and the first few natural numbers, a
strict ordering of the numbers, and a one-dimensional signal that correlates
with numerical quantity. As a result, our model acquires the ability to
estimate numerosity, i.e. the number of objects in the scene, as well as
subitization, i.e. the ability to recognize at a glance the exact number of
objects in small scenes. Remarkably, subitization and numerosity estimation
extrapolate to scenes containing many objects, far beyond the three objects
used during training. We conclude that important aspects of a facility with
numbers and quantities may be learned with supervision from a simple
pre-training task. Our observations suggest that cross-modal learning is a
powerful learning mechanism that may be harnessed in artificial intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures, 15 supplemental figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Effective Integration of Weighted Cost-to-go and Conflict Heuristic
  within Suboptimal CBS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.11624v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.11624v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishi Veerapaneni, Tushar Kusnur, Maxim Likhachev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conflict-Based Search (CBS) is a popular multi-agent path finding (MAPF)
solver that employs a low-level single agent planner and a high-level
constraint tree to resolve conflicts. The vast majority of modern MAPF solvers
focus on improving CBS by reducing the size of this tree through various
strategies with few methods modifying the low level planner. Typically low
level planners in existing CBS methods use an unweighted cost-to-go heuristic,
with suboptimal CBS methods also using a conflict heuristic to help the high
level search. In this paper, we show that, contrary to prevailing CBS beliefs,
a weighted cost-to-go heuristic can be used effectively alongside the conflict
heuristic in two possible variants. In particular, one of these variants can
obtain large speedups, 2-100x, across several scenarios and suboptimal CBS
methods. Importantly, we discover that performance is related not to the
weighted cost-to-go heuristic but rather to the relative conflict heuristic
weight's ability to effectively balance low-level and high-level work.
Additionally, to the best of our knowledge, we show the first theoretical
relation of prioritized planning and bounded suboptimal CBS and demonstrate
that our methods are their natural generalization. Update March 2024: We found
that the relative speedup decreases to around 1.2-10x depending on how the
conflict heuristic is computed (see appendix for more details).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">23</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoInst: Automatic Instance-Based Segmentation of <span class="highlight-title">LiDAR</span> 3D Scans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cedric Perauer, Laurenz Adrian Heidrich, Haifan Zhang, Matthias Nießner, Anastasiia Kornilova, Alexey Artemov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, progress in acquisition equipment such as LiDAR sensors has enabled
sensing increasingly spacious outdoor 3D environments. Making sense of such 3D
acquisitions requires fine-grained scene understanding, such as constructing
instance-based 3D scene segmentations. Commonly, a neural network is trained
for this task; however, this requires access to a large, densely annotated
dataset, which is widely known to be challenging to obtain. To address this
issue, in this work we propose to predict instance segmentations for 3D scenes
in an unsupervised way, without relying on ground-truth annotations. To this
end, we construct a learning framework consisting of two components: (1) a
pseudo-annotation scheme for generating initial unsupervised pseudo-labels; and
(2) a self-training algorithm for instance segmentation to fit robust, accurate
instances from initial noisy proposals. To enable generating 3D instance mask
proposals, we construct a weighted proxy-graph by connecting 3D points with
edges integrating multi-modal image- and point-based self-supervised features,
and perform graph-cuts to isolate individual pseudo-instances. We then build on
a state-of-the-art point-based architecture and train a 3D instance
segmentation model, resulting in significant refinement of initial proposals.
To scale to arbitrary complexity 3D scenes, we design our algorithm to operate
on local 3D point chunks and construct a merging step to generate scene-level
instance segmentations. Experiments on the challenging SemanticKITTI benchmark
demonstrate the potential of our approach, where it attains 13.3% higher
Average Precision and 9.1% higher F1 score compared to the best-performing
baseline. The code will be made publicly available at
https://github.com/artonson/autoinst.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ latentSplat: Autoencoding Variational Gaussians for Fast Generalizable
  3D Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Wewer, Kevin Raj, Eddy Ilg, Bernt Schiele, Jan Eric Lenssen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present latentSplat, a method to predict semantic Gaussians in a 3D latent
space that can be splatted and decoded by a light-weight generative 2D
architecture. Existing methods for generalizable 3D reconstruction either do
not enable fast inference of high resolution novel views due to slow volume
rendering, or are limited to interpolation of close input views, even in
simpler settings with a single central object, where 360-degree generalization
is possible. In this work, we combine a regression-based approach with a
generative model, moving towards both of these capabilities within the same
method, trained purely on readily available real video data. The core of our
method are variational 3D Gaussians, a representation that efficiently encodes
varying uncertainty within a latent space consisting of 3D feature Gaussians.
From these Gaussians, specific instances can be sampled and rendered via
efficient Gaussian splatting and a fast, generative decoder network. We show
that latentSplat outperforms previous works in reconstruction quality and
generalization, while being fast and scalable to high-resolution data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://geometric-rl.mpi-inf.mpg.de/latentsplat/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HemoSet: The First Blood Segmentation <span class="highlight-title">Dataset</span> for Automation of
  Hemostasis Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Albert J. Miao Shan Lin, Jingpei Lu, Florian Richter, Benjamin Ostrander, Emily K. Funk, Ryan K. Orosco, Michael C. Yip
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hemorrhaging occurs in surgeries of all types, forcing surgeons to quickly
adapt to the visual interference that results from blood rapidly filling the
surgical field. Introducing automation into the crucial surgical task of
hemostasis management would offload mental and physical tasks from the surgeon
and surgical assistants while simultaneously increasing the efficiency and
safety of the operation. The first step in automation of hemostasis management
is detection of blood in the surgical field. To propel the development of blood
detection algorithms in surgeries, we present HemoSet, the first blood
segmentation dataset based on bleeding during a live animal robotic surgery.
Our dataset features vessel hemorrhage scenarios where turbulent flow leads to
abnormal pooling geometries in surgical fields. These pools are formed in
conditions endemic to surgical procedures -- uneven heterogeneous tissue, under
glossy lighting conditions and rapid tool movement. We benchmark several
state-of-the-art segmentation models and provide insight into the difficulties
specific to blood detection. We intend for HemoSet to spur development of
autonomous blood suction tools by providing a platform for training and
refining blood segmentation models, addressing the precision needed for such
robotics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AVicuna: Audio-Visual LLM with Interleaver and Context-Boundary
  Alignment for Temporal Referential Dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunlong Tang, Daiki Shimada, Jing Bi, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In everyday communication, humans frequently use speech and gestures to refer
to specific areas or objects, a process known as Referential Dialogue (RD).
While prior studies have investigated RD through Large Language Models (LLMs)
or Large Multimodal Models (LMMs) in static contexts, the exploration of
Temporal Referential Dialogue (TRD) within audio-visual media remains limited.
Two primary challenges hinder progress in this field: (1) the absence of
comprehensive, untrimmed audio-visual video datasets with precise temporal
annotations, and (2) the need for methods to integrate complex temporal
auditory and visual cues effectively. To address these challenges, we introduce
a novel framework to generate PU-VALOR, an extensive audio-visual dataset
comprising over 114,000 untrimmed videos with accurate temporal demarcations.
We also present AVicuna, featuring an Audio-Visual Tokens Interleaver (AVTI)
that ensures the temporal alignment of audio-visual information. Additionally,
we develop the A5-222K dataset, encompassing more than 200,000 audio-text
pairings, to facilitate the audio and text alignments. Our experiments
demonstrate that AVicuna can effectively handle TRD in audio-visual videos and
achieve state-of-the-art performance on various audio-visual video
understanding tasks, particularly in untrimmed videos. We further investigate
the optimal audio-interleaving rate for interleaved audio-visual inputs, which
maximizes performance on the Audio-Visual Event Dense Localization task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ L-MAE: Longitudinal masked auto-encoder with time and severity-aware
  encoding for diabetic retinopathy progression prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachid Zeghlache, Pierre-Henri Conze, Mostafa El Habib Daho, Yihao Li, Alireza Rezaei, Hugo Le Boité, Ramin Tadayoni, Pascal Massin, Béatrice Cochener, Ikram Brahim, Gwenolé Quellec, Mathieu Lamard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training strategies based on self-supervised learning (SSL) have proven
to be effective pretext tasks for many downstream tasks in computer vision. Due
to the significant disparity between medical and natural images, the
application of typical SSL is not straightforward in medical imaging.
Additionally, those pretext tasks often lack context, which is critical for
computer-aided clinical decision support. In this paper, we developed a
longitudinal masked auto-encoder (MAE) based on the well-known
Transformer-based MAE. In particular, we explored the importance of time-aware
position embedding as well as disease progression-aware masking. Taking into
account the time between examinations instead of just scheduling them offers
the benefit of capturing temporal changes and trends. The masking strategy, for
its part, evolves during follow-up to better capture pathological changes,
ensuring a more accurate assessment of disease progression. Using OPHDIAT, a
large follow-up screening dataset targeting diabetic retinopathy (DR), we
evaluated the pre-trained weights on a longitudinal task, which is to predict
the severity label of the next visit within 3 years based on the past time
series examinations. Our results demonstrated the relevancy of both time-aware
position embedding and masking strategies based on disease progression
knowledge. Compared to popular baseline models and standard longitudinal
Transformers, these simple yet effective extensions significantly enhance the
predictive ability of deep classification models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object Detectors in the Open Environment:Challenges, Solutions, and
  Outlook 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Liang, Wei Wang, Ruoyu Chen, Aishan Liu, Boxi Wu, Ee-Chien Chang, Xiaochun Cao, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of foundation models, deep learning-based object detectors
have shown practical usability in closed set scenarios. However, for real-world
tasks, object detectors often operate in open environments, where crucial
factors (\eg, data distribution, objective) that influence model learning are
often changing. The dynamic and intricate nature of the open environment poses
novel and formidable challenges to object detectors. Unfortunately, current
research on object detectors in open environments lacks a comprehensive
analysis of their distinctive characteristics, challenges, and corresponding
solutions, which hinders their secure deployment in critical real-world
scenarios. This paper aims to bridge this gap by conducting a comprehensive
review and analysis of object detectors in open environments. We initially
identified limitations of key structural components within the existing
detection pipeline and propose the open environment object detector challenge
framework that includes four quadrants (\ie, out-of-domain, out-of-category,
robust learning, and incremental learning) based on the dimensions of the data
/ target changes. For each quadrant of challenges in the proposed framework, we
present a detailed description and systematic analysis of the overarching goals
and core difficulties, systematically review the corresponding solutions, and
benchmark their performance over multiple widely adopted datasets. In addition,
we engage in a discussion of open problems and potential avenues for future
research. This paper aims to provide a fresh, comprehensive, and systematic
understanding of the challenges and solutions associated with open-environment
object detectors, thus catalyzing the development of more solid applications in
real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constricting Normal Latent Space for Anomaly Detection with Normal-only
  Training Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcella Astrid, Muhammad Zaigham Zaheer, Seung-Ik Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to devise an anomaly detection model using only normal training
data, an autoencoder (AE) is typically trained to reconstruct the data. As a
result, the AE can extract normal representations in its latent space. During
test time, since AE is not trained using real anomalies, it is expected to
poorly reconstruct the anomalous data. However, several researchers have
observed that it is not the case. In this work, we propose to limit the
reconstruction capability of AE by introducing a novel latent constriction
loss, which is added to the existing reconstruction loss. By using our method,
no extra computational cost is added to the AE during test time. Evaluations
using three video anomaly detection benchmark datasets, i.e., Ped2, Avenue, and
ShanghaiTech, demonstrate the effectiveness of our method in limiting the
reconstruction capability of AE, which leads to a better anomaly detection
model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR Workshop 2024 (PML4LRS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotion Recognition from the perspective of Activity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Savinay Nagendra, Prapti Panigrahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applications of an efficient emotion recognition system can be found in
several domains such as medicine, driver fatigue surveillance, social robotics,
and human-computer interaction. Appraising human emotional states, behaviors,
and reactions displayed in real-world settings can be accomplished using latent
continuous dimensions. Continuous dimensional models of human affect, such as
those based on valence and arousal are more accurate in describing a broad
range of spontaneous everyday emotions than more traditional models of discrete
stereotypical emotion categories (e.g. happiness, surprise). Most of the prior
work on estimating valence and arousal considers laboratory settings and acted
data. But, for emotion recognition systems to be deployed and integrated into
real-world mobile and computing devices, we need to consider data collected in
the world. Action recognition is a domain of Computer Vision that involves
capturing complementary information on appearance from still frames and motion
between frames. In this paper, we treat emotion recognition from the
perspective of action recognition by exploring the application of deep learning
architectures specifically designed for action recognition, for continuous
affect recognition. We propose a novel three-stream end-to-end deep learning
regression pipeline with an attention mechanism, which is an ensemble design
based on sub-modules of multiple state-of-the-art action recognition systems.
The pipeline constitutes a novel data pre-processing approach with a spatial
self-attention mechanism to extract keyframes. The optical flow of
high-attention regions of the face is extracted to capture temporal context.
AFEW-VA in-the-wild dataset has been used to conduct comparative experiments.
Quantitative analysis shows that the proposed model outperforms multiple
standard baselines of both emotion recognition and action recognition models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhui Xu, Fuxun Yu, Zirui Xu, Nathan Inkawhich, Xiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research underscores the pivotal role of the Out-of-Distribution (OOD)
feature representation field scale in determining the efficacy of models in OOD
detection. Consequently, the adoption of model ensembles has emerged as a
prominent strategy to augment this feature representation field, capitalizing
on anticipated model diversity.
  However, our introduction of novel qualitative and quantitative model
ensemble evaluation methods, specifically Loss Basin/Barrier Visualization and
the Self-Coupling Index, reveals a critical drawback in existing ensemble
methods. We find that these methods incorporate weights that are
affine-transformable, exhibiting limited variability and thus failing to
achieve the desired diversity in feature representation.
  To address this limitation, we elevate the dimensions of traditional model
ensembles, incorporating various factors such as different weight
initializations, data holdout, etc., into distinct supervision tasks. This
innovative approach, termed Multi-Comprehension (MC) Ensemble, leverages
diverse training tasks to generate distinct comprehensions of the data and
labels, thereby extending the feature representation field.
  Our experimental results demonstrate the superior performance of the MC
Ensemble strategy in OOD detection compared to both the naive Deep Ensemble
method and a standalone model of comparable size. This underscores the
effectiveness of our proposed approach in enhancing the model's capability to
detect instances outside its training distribution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated
  Synthesis <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atefeh Khoshkhahtinat, Ali Zafari, Piyush M. Mehta, Nasser M. Nasrabadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While replacing Gaussian decoders with a conditional diffusion model enhances
the perceptual quality of reconstructions in neural image compression, their
lack of inductive bias for image data restricts their ability to achieve
state-of-the-art perceptual levels. To address this limitation, we adopt a
non-isotropic diffusion model at the decoder side. This model imposes an
inductive bias aimed at distinguishing between frequency contents, thereby
facilitating the generation of high-quality images. Moreover, our framework is
equipped with a novel entropy model that accurately models the probability
distribution of latent representation by exploiting spatio-channel correlations
in latent space, while accelerating the entropy decoding step. This
channel-wise entropy model leverages both local and global spatial contexts
within each channel chunk. The global spatial context is built upon the
Transformer, which is specifically designed for image compression tasks. The
designed Transformer employs a Laplacian-shaped positional encoding, the
learnable parameters of which are adaptively adjusted for each channel cluster.
Our experiments demonstrate that our proposed framework yields better
perceptual quality compared to cutting-edge generative-based codecs, and the
proposed entropy model contributes to notable bitrate savings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlearning Backdoor Threats: Enhancing Backdoor Defense in Multimodal
  Contrastive Learning via Local Token Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Liang, Kuanrong Liu, Jiajun Gong, Jiawei Liang, Yuan Xun, Ee-Chien Chang, Xiaochun Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal contrastive learning has emerged as a powerful paradigm for
building high-quality features using the complementary strengths of various
data modalities. However, the open nature of such systems inadvertently
increases the possibility of backdoor attacks. These attacks subtly embed
malicious behaviors within the model during training, which can be activated by
specific triggers in the inference phase, posing significant security risks.
Despite existing countermeasures through fine-tuning that reduce the adverse
impacts of such attacks, these defenses often degrade the clean accuracy and
necessitate the construction of extensive clean training pairs. In this paper,
we explore the possibility of a less-cost defense from the perspective of model
unlearning, that is, whether the model can be made to quickly \textbf{u}nlearn
\textbf{b}ackdoor \textbf{t}hreats (UBT) by constructing a small set of
poisoned samples. Specifically, we strengthen the backdoor shortcuts to
discover suspicious samples through overfitting training prioritized by weak
similarity samples. Building on the initial identification of suspicious
samples, we introduce an innovative token-based localized forgetting training
regime. This technique specifically targets the poisoned aspects of the model,
applying a focused effort to unlearn the backdoor associations and trying not
to damage the integrity of the overall model. Experimental results show that
our method not only ensures a minimal success rate for attacks, but also
preserves the model's high clean accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Partially Blinded Unlearning: Class Unlearning for Deep Networks a
  Bayesian Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhodip Panda, Shashwat Sourav, Prathosh A. P
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to adhere to regulatory standards governing individual data privacy
and safety, machine learning models must systematically eliminate information
derived from specific subsets of a user's training data that can no longer be
utilized. The emerging discipline of Machine Unlearning has arisen as a pivotal
area of research, facilitating the process of selectively discarding
information designated to specific sets or classes of data from a pre-trained
model, thereby eliminating the necessity for extensive retraining from scratch.
The principal aim of this study is to formulate a methodology tailored for the
purposeful elimination of information linked to a specific class of data from a
pre-trained classification network. This intentional removal is crafted to
degrade the model's performance specifically concerning the unlearned data
class while concurrently minimizing any detrimental impacts on the model's
performance in other classes. To achieve this goal, we frame the class
unlearning problem from a Bayesian perspective, which yields a loss function
that minimizes the log-likelihood associated with the unlearned data with a
stability regularization in parameter space. This stability regularization
incorporates Mohalanobis distance with respect to the Fisher Information matrix
and $l_2$ distance from the pre-trained model parameters. Our novel approach,
termed \textbf{Partially-Blinded Unlearning (PBU)}, surpasses existing
state-of-the-art class unlearning methods, demonstrating superior
effectiveness. Notably, PBU achieves this efficacy without requiring awareness
of the entire training dataset but only to the unlearned data points, marking a
distinctive feature of its performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Equivalency, Substitutability, and Flexibility of Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Che-Jui Chang, Danrui Li, Seonghyeon Moon, Mubbasir Kapadia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study, from an empirical standpoint, the efficacy of synthetic data in
real-world scenarios. Leveraging synthetic data for training perception models
has become a key strategy embraced by the community due to its efficiency,
scalability, perfect annotations, and low costs. Despite proven advantages, few
studies put their stress on how to efficiently generate synthetic datasets to
solve real-world problems and to what extent synthetic data can reduce the
effort for real-world data collection. To answer the questions, we
systematically investigate several interesting properties of synthetic data --
the equivalency of synthetic data to real-world data, the substitutability of
synthetic data for real data, and the flexibility of synthetic data generators
to close up domain gaps. Leveraging the M3Act synthetic data generator, we
conduct experiments on DanceTrack and MOT17. Our results suggest that synthetic
data not only enhances model performance but also demonstrates substitutability
for real data, with 60% to 80% replacement without performance loss. In
addition, our study of the impact of synthetic data distributions on downstream
performance reveals the importance of flexible data generators in narrowing
domain gaps for improved model adaptability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ghost on the Shell: An Expressive Representation of General 3D Shapes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15168v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15168v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Liu, Yao Feng, Yuliang Xiu, Weiyang Liu, Liam Paull, Michael J. Black, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The creation of photorealistic virtual worlds requires the accurate modeling
of 3D surface geometry for a wide range of objects. For this, meshes are
appealing since they 1) enable fast physics-based rendering with realistic
material and lighting, 2) support physical simulation, and 3) are
memory-efficient for modern graphics pipelines. Recent work on reconstructing
and statistically modeling 3D shape, however, has critiqued meshes as being
topologically inflexible. To capture a wide range of object shapes, any 3D
representation must be able to model solid, watertight, shapes as well as thin,
open, surfaces. Recent work has focused on the former, and methods for
reconstructing open surfaces do not support fast reconstruction with material
and lighting or unconditional generative modelling. Inspired by the observation
that open surfaces can be seen as islands floating on watertight surfaces, we
parameterize open surfaces by defining a manifold signed distance field on
watertight templates. With this parameterization, we further develop a
grid-based and differentiable representation that parameterizes both watertight
and non-watertight meshes of arbitrary topology. Our new representation, called
Ghost-on-the-Shell (G-Shell), enables two important applications:
differentiable rasterization-based reconstruction from multiview images and
generative modelling of non-watertight meshes. We empirically demonstrate that
G-Shell achieves state-of-the-art performance on non-watertight mesh
reconstruction and generation tasks, while also performing effectively for
watertight meshes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 Oral (v3: 30 pages, 19 figures, Project Page:
  https://gshell3d.github.io/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VQPy: An Object-Oriented Approach to Modern Video Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01623v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01623v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shan Yu, Zhenting Zhu, Yu Chen, Hanchen Xu, Pengzhan Zhao, Yang Wang, Arthi Padmanabhan, Hugo Latapie, Harry Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video analytics is widely used in contemporary systems and services. At the
forefront of video analytics are video queries that users develop to find
objects of particular interest. Building upon the insight that video objects
(e.g., human, animals, cars, etc.), the center of video analytics, are similar
in spirit to objects modeled by traditional object-oriented languages, we
propose to develop an object-oriented approach to video analytics. This
approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant
with constructs that make it easy for users to express video objects and their
interactions$\unicode{x2015}$as well as an extensible backend that can
automatically construct and optimize pipelines based on video objects. We have
implemented and open-sourced VQPy, which has been productized in Cisco as part
of its DeepVision framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MLSys'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent <span class="highlight-title">Dataset</span> Distillation with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03881v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03881v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian B. Moser, Federico Raue, Sebastian Palacio, Stanislav Frolov, Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The efficacy of machine learning has traditionally relied on the availability
of increasingly larger datasets. However, large datasets pose storage
challenges and contain non-influential samples, which could be ignored during
training without impacting the final accuracy of the model. In response to
these limitations, the concept of distilling the information on a dataset into
a condensed set of (synthetic) samples, namely a distilled dataset, emerged.
One crucial aspect is the selected architecture (usually ConvNet) for linking
the original and synthetic datasets. However, the final accuracy is lower if
the employed model architecture differs from the model used during
distillation. Another challenge is the generation of high-resolution images,
e.g., 128x128 and higher. In this paper, we propose Latent Dataset Distillation
with Diffusion Models (LD3M) that combine diffusion in latent space with
dataset distillation to tackle both challenges. LD3M incorporates a novel
diffusion process tailored for dataset distillation, which improves the
gradient norms for learning synthetic images. By adjusting the number of
diffusion steps, LD3M also offers a straightforward way of controlling the
trade-off between speed and accuracy. We evaluate our approach in several
ImageNet subsets and for high-resolution images (128x128 and 256x256). As a
result, LD3M consistently outperforms state-of-the-art distillation techniques
by up to 4.8 p.p. and 4.2 p.p. for 1 and 10 images per class, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BAGS: Blur Agnostic Gaussian Splatting through Multi-Scale Kernel
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04926v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04926v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Peng, Yutao Tang, Yifan Zhou, Nengyu Wang, Xijun Liu, Deming Li, Rama Chellappa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent efforts in using 3D Gaussians for scene reconstruction and novel view
synthesis can achieve impressive results on curated benchmarks; however, images
captured in real life are often blurry. In this work, we analyze the robustness
of Gaussian-Splatting-based methods against various image blur, such as motion
blur, defocus blur, downscaling blur, \etc. Under these degradations,
Gaussian-Splatting-based methods tend to overfit and produce worse results than
Neural-Radiance-Field-based methods. To address this issue, we propose Blur
Agnostic Gaussian Splatting (BAGS). BAGS introduces additional 2D modeling
capacities such that a 3D-consistent and high quality scene can be
reconstructed despite image-wise blur. Specifically, we model blur by
estimating per-pixel convolution kernels from a Blur Proposal Network (BPN).
BPN is designed to consider spatial, color, and depth variations of the scene
to maximize modeling capacity. Additionally, BPN also proposes a
quality-assessing mask, which indicates regions where blur occur. Finally, we
introduce a coarse-to-fine kernel optimization scheme; this optimization scheme
is fast and avoids sub-optimal solutions due to a sparse point cloud
initialization, which often occurs when we apply Structure-from-Motion on
blurry images. We demonstrate that BAGS achieves photorealistic renderings
under various challenging blur conditions and imaging geometry, while
significantly improving upon existing approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detection of diabetic retinopathy using longitudinal self-supervised
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.00915v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.00915v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachid Zeghlache, Pierre-Henri Conze, Mostafa El Habib Daho, Ramin Tadayoni, Pascal Massin, Béatrice Cochener, Gwenolé Quellec, Mathieu Lamard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Longitudinal imaging is able to capture both static anatomical structures and
dynamic changes in disease progression towards earlier and better
patient-specific pathology management. However, conventional approaches for
detecting diabetic retinopathy (DR) rarely take advantage of longitudinal
information to improve DR analysis. In this work, we investigate the benefit of
exploiting self-supervised learning with a longitudinal nature for DR diagnosis
purposes. We compare different longitudinal self-supervised learning (LSSL)
methods to model the disease progression from longitudinal retinal color fundus
photographs (CFP) to detect early DR severity changes using a pair of
consecutive exams. The experiments were conducted on a longitudinal DR
screening dataset with or without those trained encoders (LSSL) acting as a
longitudinal pretext task. Results achieve an AUC of 0.875 for the baseline
(model trained from scratch) and an AUC of 0.96 (95% CI: 0.9593-0.9655 DeLong
test) with a p-value < 2.2e-16 on early fusion using a simple ResNet alike
architecture with frozen LSSL weights, suggesting that the LSSL latent space
enables to encode the dynamic of DR progression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted preprint for presentation at MICCAI-OMIA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Influencer Backdoor Attack on Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12054v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12054v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoheng Lan, Jindong Gu, Philip Torr, Hengshuang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When a small number of poisoned samples are injected into the training
dataset of a deep neural network, the network can be induced to exhibit
malicious behavior during inferences, which poses potential threats to
real-world applications. While they have been intensively studied in
classification, backdoor attacks on semantic segmentation have been largely
overlooked. Unlike classification, semantic segmentation aims to classify every
pixel within a given image. In this work, we explore backdoor attacks on
segmentation models to misclassify all pixels of a victim class by injecting a
specific trigger on non-victim pixels during inferences, which is dubbed
Influencer Backdoor Attack (IBA). IBA is expected to maintain the
classification accuracy of non-victim pixels and mislead classifications of all
victim pixels in every single inference and could be easily applied to
real-world scenes. Based on the context aggregation ability of segmentation
models, we proposed a simple, yet effective, Nearest-Neighbor trigger injection
strategy. We also introduce an innovative Pixel Random Labeling strategy which
maintains optimal performance even when the trigger is placed far from the
victim pixels. Our extensive experiments reveal that current segmentation
models do suffer from backdoor attacks, demonstrate IBA real-world
applicability, and show that our proposed techniques can further increase
attack performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with
  Global-Local Depth Normalization <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06912v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06912v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, Lin Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiance fields have demonstrated impressive performance in synthesizing
novel views from sparse input views, yet prevailing methods suffer from high
training costs and slow inference speed. This paper introduces DNGaussian, a
depth-regularized framework based on 3D Gaussian radiance fields, offering
real-time and high-quality few-shot novel view synthesis at low costs. Our
motivation stems from the highly efficient representation and surprising
quality of the recent 3D Gaussian Splatting, despite it will encounter a
geometry degradation when input views decrease. In the Gaussian radiance
fields, we find this degradation in scene geometry primarily lined to the
positioning of Gaussian primitives and can be mitigated by depth constraint.
Consequently, we propose a Hard and Soft Depth Regularization to restore
accurate scene geometry under coarse monocular depth supervision while
maintaining a fine-grained color appearance. To further refine detailed
geometry reshaping, we introduce Global-Local Depth Normalization, enhancing
the focus on small local depth changes. Extensive experiments on LLFF, DTU, and
Blender datasets demonstrate that DNGaussian outperforms state-of-the-art
methods, achieving comparable or better results with significantly reduced
memory cost, a $25 \times$ reduction in training time, and over $3000 \times$
faster rendering speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024. Project page:
  https://fictionarry.github.io/DNGaussian/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DGC-GNN: Leveraging Geometry and Color Cues for Visual Descriptor-Free
  2D-3D Matching <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.12547v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.12547v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuzhe Wang, Juho Kannala, Daniel Barath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Matching 2D keypoints in an image to a sparse 3D point cloud of the scene
without requiring visual descriptors has garnered increased interest due to its
low memory requirements, inherent privacy preservation, and reduced need for
expensive 3D model maintenance compared to visual descriptor-based methods.
However, existing algorithms often compromise on performance, resulting in a
significant deterioration compared to their descriptor-based counterparts. In
this paper, we introduce DGC-GNN, a novel algorithm that employs a
global-to-local Graph Neural Network (GNN) that progressively exploits
geometric and color cues to represent keypoints, thereby improving matching
accuracy. Our procedure encodes both Euclidean and angular relations at a
coarse level, forming the geometric embedding to guide the point matching. We
evaluate DGC-GNN on both indoor and outdoor datasets, demonstrating that it not
only doubles the accuracy of the state-of-the-art visual descriptor-free
algorithm but also substantially narrows the performance gap between
descriptor-based and descriptor-free methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DemoCaricature: Democratising Caricature Generation with a Rough Sketch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04364v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04364v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dar-Yen Chen, Ayan Kumar Bhunia, Subhadeep Koley, Aneeshan Sain, Pinaki Nath Chowdhury, Yi-Zhe Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we democratise caricature generation, empowering individuals
to effortlessly craft personalised caricatures with just a photo and a
conceptual sketch. Our objective is to strike a delicate balance between
abstraction and identity, while preserving the creativity and subjectivity
inherent in a sketch. To achieve this, we present Explicit Rank-1 Model Editing
alongside single-image personalisation, selectively applying nuanced edits to
cross-attention layers for a seamless merge of identity and style.
Additionally, we propose Random Mask Reconstruction to enhance robustness,
directing the model to focus on distinctive identity and style features.
Crucially, our aim is not to replace artists but to eliminate accessibility
barriers, allowing enthusiasts to engage in the artistry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SG-Bot: Object Rearrangement via Coarse-to-Fine Robotic Imagination on
  Scene Graphs <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12188v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12188v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyao Zhai, Xiaoni Cai, Dianye Huang, Yan Di, Fabian Manhardt, Federico Tombari, Nassir Navab, Benjamin Busam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object rearrangement is pivotal in robotic-environment interactions,
representing a significant capability in embodied AI. In this paper, we present
SG-Bot, a novel rearrangement framework that utilizes a coarse-to-fine scheme
with a scene graph as the scene representation. Unlike previous methods that
rely on either known goal priors or zero-shot large models, SG-Bot exemplifies
lightweight, real-time, and user-controllable characteristics, seamlessly
blending the consideration of commonsense knowledge with automatic generation
capabilities. SG-Bot employs a three-fold procedure--observation, imagination,
and execution--to adeptly address the task. Initially, objects are discerned
and extracted from a cluttered scene during the observation. These objects are
first coarsely organized and depicted within a scene graph, guided by either
commonsense or user-defined criteria. Then, this scene graph subsequently
informs a generative model, which forms a fine-grained goal scene considering
the shape information from the initial scene and object semantics. Finally, for
execution, the initial and envisioned goal scenes are matched to formulate
robotic action policies. Experimental results demonstrate that SG-Bot
outperforms competitors by a large margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2024 accepted. Project website:
  https://sites.google.com/view/sg-bot</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-23T00:00:00Z">2024-03-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">26</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Risk-Calibrated Human-Robot Interaction via Set-Valued Intent Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justin Lidard, Hang Pham, Ariel Bachman, Bryan Boateng, Anirudha Majumdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tasks where robots must cooperate with humans, such as navigating around a
cluttered home or sorting everyday items, are challenging because they exhibit
a wide range of valid actions that lead to similar outcomes. Moreover,
zero-shot cooperation between human-robot partners is an especially challenging
problem because it requires the robot to infer and adapt on the fly to a latent
human intent, which could vary significantly from human to human. Recently,
deep learned motion prediction models have shown promising results in
predicting human intent but are prone to being confidently incorrect. In this
work, we present Risk-Calibrated Interactive Planning (RCIP), which is a
framework for measuring and calibrating risk associated with uncertain action
selection in human-robot cooperation, with the fundamental idea that the robot
should ask for human clarification when the risk associated with the
uncertainty in the human's intent cannot be controlled. RCIP builds on the
theory of set-valued risk calibration to provide a finite-sample statistical
guarantee on the cumulative loss incurred by the robot while minimizing the
cost of human clarification in complex multi-step settings. Our main insight is
to frame the risk control problem as a sequence-level multi-hypothesis testing
problem, allowing efficient calibration using a low-dimensional parameter that
controls a pre-trained risk-aware policy. Experiments across a variety of
simulated and real-world environments demonstrate RCIP's ability to predict and
adapt to a diverse set of dynamic human intents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website with additional information, videos, and code:
  https://risk-calibrated-planning.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explore until Confident: Efficient Exploration for Embodied Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allen Z. Ren, Jaden Clark, Anushri Dixit, Masha Itkina, Anirudha Majumdar, Dorsa Sadigh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of Embodied Question Answering (EQA), which refers to
settings where an embodied agent such as a robot needs to actively explore an
environment to gather information until it is confident about the answer to a
question. In this work, we leverage the strong semantic reasoning capabilities
of large vision-language models (VLMs) to efficiently explore and answer such
questions. However, there are two main challenges when using VLMs in EQA: they
do not have an internal memory for mapping the scene to be able to plan how to
explore over time, and their confidence can be miscalibrated and can cause the
robot to prematurely stop exploration or over-explore. We propose a method that
first builds a semantic map of the scene based on depth information and via
visual prompting of a VLM - leveraging its vast knowledge of relevant regions
of the scene for exploration. Next, we use conformal prediction to calibrate
the VLM's question answering confidence, allowing the robot to know when to
stop exploration - leading to a more calibrated and efficient exploration
strategy. To test our framework in simulation, we also contribute a new EQA
dataset with diverse, realistic human-robot scenarios and scenes built upon the
Habitat-Matterport 3D Research Dataset (HM3D). Both simulated and real robot
experiments show our proposed approach improves the performance and efficiency
over baselines that do no leverage VLM for exploration or do not calibrate its
confidence. Webpage with experiment videos and code:
https://explore-eqa.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ iA$^*$: Imperative Learning-based A$^*$ Search for Pathfinding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Chen, Fan Yang, Chen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pathfinding problem, which aims to identify a collision-free path between
two points, is crucial for many applications, such as robot navigation and
autonomous driving. Classic methods, such as A$^*$ search, perform well on
small-scale maps but face difficulties scaling up. Conversely, data-driven
approaches can improve pathfinding efficiency but require extensive data
labeling and lack theoretical guarantees, making it challenging for practical
applications. To combine the strengths of the two methods, we utilize the
imperative learning (IL) strategy and propose a novel self-supervised
pathfinding framework, termed imperative learning-based A$^*$ (iA$^*$).
Specifically, iA$^*$ is a bilevel optimization process where the lower-level
optimization is dedicated to finding the optimal path by a differentiable A$^*$
search module, and the upper-level optimization narrows down the search space
to improve efficiency via setting suitable initial values from a data-driven
model. Besides, the model within the upper-level optimization is a fully
convolutional network, trained by the calculated loss in the lower-level
optimization. Thus, the framework avoids extensive data labeling and can be
applied in diverse environments. Our comprehensive experiments demonstrate that
iA$^*$ surpasses both classical and data-driven methods in pathfinding
efficiency and shows superior robustness among different tasks, validated with
public datasets and simulation environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated System-level Testing of Unmanned Aerial Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15857v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15857v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hassan Sartaj, Asmar Muqeet, Muhammad Zohaib Iqbal, Muhammad Uzair Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned aerial systems (UAS) rely on various avionics systems that are
safety-critical and mission-critical. A major requirement of international
safety standards is to perform rigorous system-level testing of avionics
software systems. The current industrial practice is to manually create test
scenarios, manually/automatically execute these scenarios using simulators, and
manually evaluate outcomes. The test scenarios typically consist of setting
certain flight or environment conditions and testing the system under test in
these settings. The state-of-the-art approaches for this purpose also require
manual test scenario development and evaluation. In this paper, we propose a
novel approach to automate the system-level testing of the UAS. The proposed
approach (AITester) utilizes model-based testing and artificial intelligence
(AI) techniques to automatically generate, execute, and evaluate various test
scenarios. The test scenarios are generated on the fly, i.e., during test
execution based on the environmental context at runtime. The approach is
supported by a toolset. We empirically evaluate the proposed approach on two
core components of UAS, an autopilot system of an unmanned aerial vehicle (UAV)
and cockpit display systems (CDS) of the ground control station (GCS). The
results show that the AITester effectively generates test scenarios causing
deviations from the expected behavior of the UAV autopilot and reveals
potential flaws in the GCS-CDS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ARO: Large Language Model Supervised Robotics Text2Skill Autonomous
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Chen, Yuyao Ye, Ziyi Chen, Chuheng Zhang, Marcelo H. Ang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotics learning highly relies on human expertise and efforts, such as
demonstrations, design of reward functions in reinforcement learning,
performance evaluation using human feedback, etc. However, reliance on human
assistance can lead to expensive learning costs and make skill learning
difficult to scale. In this work, we introduce the Large Language Model
Supervised Robotics Text2Skill Autonomous Learning (ARO) framework, which aims
to replace human participation in the robot skill learning process with
large-scale language models that incorporate reward function design and
performance evaluation. We provide evidence that our approach enables fully
autonomous robot skill learning, capable of completing partial tasks without
human intervention. Furthermore, we also analyze the limitations of this
approach in task understanding and optimization stability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Learning based Policy Optimization for Temporal Tasks via
  Dropout 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navid Hashemi, Bardh Hoxha, Danil Prokhorov, Georgios Fainekos, Jyotirmoy Deshmukh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a model-based approach for training feedback
controllers for an autonomous agent operating in a highly nonlinear
environment. We desire the trained policy to ensure that the agent satisfies
specific task objectives, expressed in discrete-time Signal Temporal Logic
(DT-STL). One advantage for reformulation of a task via formal frameworks, like
DT-STL, is that it permits quantitative satisfaction semantics. In other words,
given a trajectory and a DT-STL formula, we can compute the robustness, which
can be interpreted as an approximate signed distance between the trajectory and
the set of trajectories satisfying the formula. We utilize feedback
controllers, and we assume a feed forward neural network for learning these
feedback controllers. We show how this learning problem is similar to training
recurrent neural networks (RNNs), where the number of recurrent units is
proportional to the temporal horizon of the agent's task objectives. This poses
a challenge: RNNs are susceptible to vanishing and exploding gradients, and
na\"{i}ve gradient descent-based strategies to solve long-horizon task
objectives thus suffer from the same problems. To tackle this challenge, we
introduce a novel gradient approximation algorithm based on the idea of dropout
or gradient sampling. We show that, the existing smooth semantics for
robustness are inefficient regarding gradient computation when the
specification becomes complex. To address this challenge, we propose a new
smooth semantics for DT-STL that under-approximates the robustness value and
scales well for backpropagation over a complex specification. We show that our
control synthesis methodology, can be quite helpful for stochastic gradient
descent to converge with less numerical issues, enabling scalable
backpropagation over long time horizons and trajectories over high dimensional
state spaces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Early Social Maneuvers for Enhanced Social Navigation <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15813v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15813v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yigit Yıldırim, Mehmet Suzer, Emre Ugur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Socially compliant navigation is an integral part of safety features in
Human-Robot Interaction. Traditional approaches to mobile navigation prioritize
physical aspects, such as efficiency, but social behaviors gain traction as
robots appear more in daily life. Recent techniques to improve the social
compliance of navigation often rely on predefined features or reward functions,
introducing assumptions about social human behavior. To address this
limitation, we propose a novel Learning from Demonstration (LfD) framework for
social navigation that exclusively utilizes raw sensory data. Additionally, the
proposed system contains mechanisms to consider the future paths of the
surrounding pedestrians, acknowledging the temporal aspect of the problem. The
final product is expected to reduce the anxiety of people sharing their
environment with a mobile robot, helping them trust that the robot is aware of
their presence and will not harm them. As the framework is currently being
developed, we outline its components, present experimental results, and discuss
future work towards realizing this framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the workshop of Robot Trust for Symbiotic Societies
  (RTSS) at ICRA 2024 on March 23, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impact of Evolutionary Computation on Robotic Design: A Case Study
  with an Underactuated Hand Exoskeleton <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baris Akbas, Huseyin Taner Yuksel, Aleyna Soylemez, Mazhar Eid Zyada, Mine Sarac, Fabio Stroppa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic exoskeletons can enhance human strength and aid people with physical
disabilities. However, designing them to ensure safety and optimal performance
presents significant challenges. Developing exoskeletons should incorporate
specific optimization algorithms to find the best design. This study
investigates the potential of Evolutionary Computation (EC) methods in robotic
design optimization, with an underactuated hand exoskeleton (U-HEx) used as a
case study. We propose improving the performance and usability of the U-HEx
design, which was initially optimized using a naive brute-force approach, by
integrating EC techniques such as Genetic Algorithm and Big Bang-Big Crunch
Algorithm. Comparative analysis revealed that EC methods consistently yield
more precise and optimal solutions than brute force in a significantly shorter
time. This allowed us to improve the optimization by increasing the number of
variables in the design, which was impossible with naive methods. The results
show significant improvements in terms of the torque magnitude the device
transfers to the user, enhancing its efficiency. These findings underline the
importance of performing proper optimization while designing exoskeletons, as
well as providing a significant improvement to this specific robotic design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages (+ref), 4 figures, IEEE International Conference on Robotics
  and Automation (ICRA) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AirCrab: A Hybrid Aerial-Ground Manipulator with An Active Wheel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muqing Cao, Jiayan Zhao, Xinhang Xu, Lihua Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the behavior of birds, we present AirCrab, a hybrid aerial ground
manipulator (HAGM) with a single active wheel and a 3-degree of freedom (3-DoF)
manipulator. AirCrab leverages a single point of contact with the ground to
reduce position drift and improve manipulation accuracy. The single active
wheel enables locomotion on narrow surfaces without adding significant weight
to the robot. To realize accurate attitude maintenance using propellers on the
ground, we design a control allocation method for AirCrab that prioritizes
attitude control and dynamically adjusts the thrust input to reduce energy
consumption. Experiments verify the effectiveness of the proposed control
method and the gain in manipulation accuracy with ground contact. A series of
operations to complete the letters 'NTU' demonstrates the capability of the
robot to perform challenging hybrid aerial-ground manipulation missions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vid2Real HRI: Align video-based HRI study designs with real-world
  settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elliott Hauser, Yao-Cheng Chan, Sadanand Modak, Joydeep Biswas, Justin Hart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  HRI research using autonomous robots in real-world settings can produce
results with the highest ecological validity of any study modality, but many
difficulties limit such studies' feasibility and effectiveness. We propose
Vid2Real HRI, a research framework to maximize real-world insights offered by
video-based studies. The Vid2Real HRI framework was used to design an online
study using first-person videos of robots as real-world encounter surrogates.
The online study ($n = 385$) distinguished the within-subjects effects of four
robot behavioral conditions on perceived social intelligence and human
willingness to help the robot enter an exterior door. A real-world,
between-subjects replication ($n = 26$) using two conditions confirmed the
validity of the online study's findings and the sufficiency of the participant
recruitment target ($22$) based on a power analysis of online study results.
The Vid2Real HRI framework offers HRI researchers a principled way to take
advantage of the efficiency of video-based study modalities while generating
directly transferable knowledge of real-world HRI. Code and data from the study
are provided at https://vid2real.github.io/vid2realHRI
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DriveEnv-NeRF: Exploration of A NeRF-Based Autonomous Driving
  Environment for Real-World Performance Validation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mu-Yi Shen, Chia-Chi Hsu, Hao-Yu Hou, Yu-Chen Huang, Wei-Fang Sun, Chia-Che Chang, Yu-Lun Liu, Chun-Yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we introduce the DriveEnv-NeRF framework, which leverages
Neural Radiance Fields (NeRF) to enable the validation and faithful forecasting
of the efficacy of autonomous driving agents in a targeted real-world scene.
Standard simulator-based rendering often fails to accurately reflect real-world
performance due to the sim-to-real gap, which represents the disparity between
virtual simulations and real-world conditions. To mitigate this gap, we propose
a workflow for building a high-fidelity simulation environment of the targeted
real-world scene using NeRF. This approach is capable of rendering realistic
images from novel viewpoints and constructing 3D meshes for emulating
collisions. The validation of these capabilities through the comparison of
success rates in both simulated and real environments demonstrates the benefits
of using DriveEnv-NeRF as a real-world performance indicator. Furthermore, the
DriveEnv-NeRF framework can serve as a training environment for autonomous
driving agents under various lighting conditions. This approach enhances the
robustness of the agents and reduces performance degradation when deployed to
the target real scene, compared to agents fully trained using the standard
simulator rendering pipeline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/muyishen2040/DriveEnvNeRF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RicMonk: A Three-Link Brachiation Robot with Passive Grippers for
  Energy-Efficient Brachiation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shourie S. Grama, Mahdi Javadi, Shivesh Kumar, Hossein Zamani Boroujeni, Frank Kirchner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the design, analysis, and performance evaluation of
RicMonk, a novel three-link brachiation robot equipped with passive hook-shaped
grippers. Brachiation, an agile and energy-efficient mode of locomotion
observed in primates, has inspired the development of RicMonk to explore
versatile locomotion and maneuvers on ladder-like structures. The robot's
anatomical resemblance to gibbons and the integration of a tail mechanism for
energy injection contribute to its unique capabilities. The paper discusses the
use of the Direct Collocation methodology for optimizing trajectories for the
robot's dynamic behaviors and stabilization of these trajectories using a
Time-varying Linear Quadratic Regulator. With RicMonk we demonstrate
bidirectional brachiation, and provide comparative analysis with its
predecessor, AcroMonk - a two-link brachiation robot, to demonstrate that the
presence of a passive tail helps improve energy efficiency. The system design,
controllers, and software implementation are publicly available on GitHub and
the video demonstration of the experiments can be viewed YouTube.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Open sourced system design, controllers, software implementation can
  be found at https://github.com/dfki-ric-underactuated-lab/ricmonk and a video
  demonstrating the experiments performed with RicMonk can be found at
  https://www.youtube.com/watch?v=hOuDQI7CD8w</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Robust Learning based Formation Control of Mobile Robots
  based on Bioinspired Neural Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15716v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15716v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Xu, Tao Yan, Simon X. Yang, S. Andrew Gadsden, Mohammad Biglarbegian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenges of distributed formation control in
multiple mobile robots, introducing a novel approach that enhances real-world
practicability. We first introduce a distributed estimator using a variable
structure and cascaded design technique, eliminating the need for derivative
information to improve the real time performance. Then, a kinematic tracking
control method is developed utilizing a bioinspired neural dynamic-based
approach aimed at providing smooth control inputs and effectively resolving the
speed jump issue. Furthermore, to address the challenges for robots operating
with completely unknown dynamics and disturbances, a learning-based robust
dynamic controller is developed. This controller provides real time parameter
estimates while maintaining its robustness against disturbances. The overall
stability of the proposed method is proved with rigorous mathematical analysis.
At last, multiple comprehensive simulation studies have shown the advantages
and effectiveness of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by IEEE Transactions on Intelligent Vehicles</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PNAS-MOT: Multi-Modal Object Tracking with Pareto Neural Architecture
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chensheng Peng, Zhaoyu Zeng, Jinling Gao, Jundong Zhou, Masayoshi Tomizuka, Xinbing Wang, Chenghu Zhou, Nanyang Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple object tracking is a critical task in autonomous driving. Existing
works primarily focus on the heuristic design of neural networks to obtain high
accuracy. As tracking accuracy improves, however, neural networks become
increasingly complex, posing challenges for their practical application in real
driving scenarios due to the high level of latency. In this paper, we explore
the use of the neural architecture search (NAS) methods to search for efficient
architectures for tracking, aiming for low real-time latency while maintaining
relatively high accuracy. Another challenge for object tracking is the
unreliability of a single sensor, therefore, we propose a multi-modal framework
to improve the robustness. Experiments demonstrate that our algorithm can run
on edge devices within lower latency constraints, thus greatly reducing the
computational requirements for multi-modal object tracking while keeping lower
latency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Robotics and Automation Letters 2024. Code is available at
  https://github.com/PholyPeng/PNAS-MOT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Driven Predictive Control for Robust Exoskeleton Locomotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kejun Li, Jeeseop Kim, Xiaobin Xiong, Kaveh Akbari Hamed, Yisong Yue, Aaron D. Ames
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exoskeleton locomotion must be robust while being adaptive to different users
with and without payloads. To address these challenges, this work introduces a
data-driven predictive control (DDPC) framework to synthesize walking gaits for
lower-body exoskeletons, employing Hankel matrices and a state transition
matrix for its data-driven model. The proposed approach leverages DDPC through
a multi-layer architecture. At the top layer, DDPC serves as a planner
employing Hankel matrices and a state transition matrix to generate a
data-driven model that can learn and adapt to varying users and payloads. At
the lower layer, our method incorporates inverse kinematics and passivity-based
control to map the planned trajectory from DDPC into the full-order states of
the lower-body exoskeleton. We validate the effectiveness of this approach
through numerical simulations and hardware experiments conducted on the
Atalante lower-body exoskeleton with different payloads. Moreover, we conducted
a comparative analysis against the model predictive control (MPC) framework
based on the reduced-order linear inverted pendulum (LIP) model. Through this
comparison, the paper demonstrates that DDPC enables robust bipedal walking at
various velocities while accounting for model uncertainties and unknown
perturbations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LONER: <span class="highlight-title">LiDAR</span> Only Neural Representations for Real-Time <span class="highlight-title">SLAM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04937v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04937v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seth Isaacson, Pou-Chun Kung, Mani Ramanagopal, Ram Vasudevan, Katherine A. Skinner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes LONER, the first real-time LiDAR SLAM algorithm that uses
a neural implicit scene representation. Existing implicit mapping methods for
LiDAR show promising results in large-scale reconstruction, but either require
groundtruth poses or run slower than real-time. In contrast, LONER uses LiDAR
data to train an MLP to estimate a dense map in real-time, while simultaneously
estimating the trajectory of the sensor. To achieve real-time performance, this
paper proposes a novel information-theoretic loss function that accounts for
the fact that different regions of the map may be learned to varying degrees
throughout online training. The proposed method is evaluated qualitatively and
quantitatively on two open-source datasets. This evaluation illustrates that
the proposed loss function converges faster and leads to more accurate geometry
reconstruction than other loss functions used in depth-supervised neural
implicit frameworks. Finally, this paper shows that LONER estimates
trajectories competitively with state-of-the-art LiDAR SLAM methods, while also
producing dense maps competitive with existing real-time implicit mapping
methods that use groundtruth poses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First two authors equally contributed. Webpage:
  https://umautobots.github.io/loner</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ARTEMIS: AI-driven Robotic Triage Labeling and Emergency Medical
  Information System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08865v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08865v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Revanth Krishna Senthilkumaran, Mridu Prashanth, Hrishikesh Viswanath, Sathvika Kotha, Kshitij Tiwari, Aniket Bera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mass casualty incidents (MCIs) pose a significant challenge to emergency
medical services by overwhelming available resources and personnel. Effective
victim assessment is the key to minimizing casualties during such a crisis. We
introduce ARTEMIS, an AI-driven Robotic Triage Labeling and Emergency Medical
Information System, to aid first responders in MCI events. It leverages speech
processing, natural language processing, and deep learning to help with acuity
classification. This is deployed on a quadruped that performs victim
localization and preliminary injury severity assessment. First responders
access victim information through a Graphical User Interface that is updated in
real-time. To validate our proposed algorithmic triage protocol, we used the
Unitree Go1 quadruped. The robot identifies humans, interacts with them, gets
vitals and information, and assigns an acuity label. Simulations of an MCI in
software and a controlled environment outdoors were conducted. The system
achieved a triage-level classification precision of over 74% on average and 99%
for the most critical victims, i.e. level 1 acuity, outperforming
state-of-the-art deep learning-based triage labeling systems. In this paper, we
showcase the potential of human-robot interaction in assisting medical
personnel in MCI events.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NaVid: Video-based VLM Plans the Next Step for Vision-and-Language
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15852v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15852v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, He Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-and-Language Navigation (VLN) stands as a key research problem of
Embodied AI, aiming at enabling agents to navigate in unseen environments
following linguistic instructions. In this field, generalization is a
long-standing challenge, either to out-of-distribution scenes or from Sim to
Real. In this paper, we propose NaVid, a video-based large vision language
model (VLM), to mitigate such a generalization gap. NaVid makes the first
endeavour to showcase the capability of VLMs to achieve state-of-the-art level
navigation performance without any maps, odometer and depth inputs. Following
human instruction, NaVid only requires an on-the-fly video stream from a
monocular RGB camera equipped on the robot to output the next-step action. Our
formulation mimics how humans navigate and naturally gets rid of the problems
introduced by odometer noises, and the Sim2Real gaps from map or depth inputs.
Moreover, our video-based approach can effectively encode the historical
observations of robots as spatio-temporal contexts for decision-making and
instruction following. We train NaVid with 550k navigation samples collected
from VLN-CE trajectories, including action-planning and instruction-reasoning
samples, along with 665k large-scale web data. Extensive experiments show that
NaVid achieves SOTA performance in simulation environments and the real world,
demonstrating superior cross-dataset and Sim2Real transfer. We thus believe our
proposed VLM approach plans the next step for not only the navigation agents
but also this research field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tactile Estimation of Extrinsic Contact Patch for Stable Placement <span class="chip">ICRA2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14552v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14552v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kei Ota, Devesh K. Jha, Krishna Murthy Jatavallabhula, Asako Kanezaki, Joshua B. Tenenbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise perception of contact interactions is essential for fine-grained
manipulation skills for robots. In this paper, we present the design of
feedback skills for robots that must learn to stack complex-shaped objects on
top of each other (see Fig.1). To design such a system, a robot should be able
to reason about the stability of placement from very gentle contact
interactions. Our results demonstrate that it is possible to infer the
stability of object placement based on tactile readings during contact
formation between the object and its environment. In particular, we estimate
the contact patch between a grasped object and its environment using force and
tactile observations to estimate the stability of the object during a contact
formation. The contact patch could be used to estimate the stability of the
object upon release of the grasp. The proposed method is demonstrated in
various pairs of objects that are used in a very popular board game.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICRA2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integration of Large Language Models within Cognitive Architectures for
  Autonomous Robots <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14945v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14945v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miguel Á. González-Santamarta, Francisco J. Rodríguez-Lera, Ángel Manuel Guerrero-Higueras, Vicente Matellán-Olivera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symbolic reasoning systems have been used in cognitive architectures to
provide inference and planning capabilities. However, defining domains and
problems has proven difficult and prone to errors. Moreover, Large Language
Models (LLMs) have emerged as tools to process natural language for different
tasks. In this paper, we propose the use of LLMs to tackle these problems. This
way, this paper proposes the integration of LLMs in the ROS 2-integrated
cognitive architecture MERLIN2 for autonomous robots. Specifically, we present
the design, development and deployment of how to leverage the reasoning
capabilities of LLMs inside the deliberative processes of MERLIN2. As a result,
the deliberative system is updated from a PDDL-based planner system to a
natural language planning system. This proposal is evaluated quantitatively and
qualitatively, measuring the impact of incorporating the LLMs in the cognitive
architecture. Results show that a classical approach achieves better
performance but the proposed solution provides an enhanced interaction through
natural language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 2 tables, Submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring 3D Human Pose Estimation and Forecasting from the Robot's
  Perspective: The HARPER <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14447v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14447v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Avogaro, Andrea Toaiari, Federico Cunico, Xiangmin Xu, Haralambos Dafas, Alessandro Vinciarelli, Emma Li, Marco Cristani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce HARPER, a novel dataset for 3D body pose estimation and forecast
in dyadic interactions between users and Spot, the quadruped robot manufactured
by Boston Dynamics. The key-novelty is the focus on the robot's perspective,
i.e., on the data captured by the robot's sensors. These make 3D body pose
analysis challenging because being close to the ground captures humans only
partially. The scenario underlying HARPER includes 15 actions, of which 10
involve physical contact between the robot and users. The Corpus contains not
only the recordings of the built-in stereo cameras of Spot, but also those of a
6-camera OptiTrack system (all recordings are synchronized). This leads to
ground-truth skeletal representations with a precision lower than a millimeter.
In addition, the Corpus includes reproducible benchmarks on 3D Human Pose
Estimation, Human Pose Forecasting, and Collision Prediction, all based on
publicly available baseline approaches. This enables future HARPER users to
rigorously compare their results with those we provide in this work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Space Filling Curves for Coverage Path Planning with Online Obstacle
  Avoidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.01426v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.01426v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashay Wakode, Arpita Sinha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper presents a strategy for robotic exploration problem using
Space-Filling curves (SFC). The strategy plans a path that avoids unknown
obstacles while ensuring complete coverage of the free space in region of
interest. The region of interest is first tessellated, and the tiles/cells are
connected using a SFC pattern. A robot follows the SFC to explore the entire
area. However, obstacles can block the systematic movement of the robot. We
overcome this problem by determining an alternate path online that avoids the
blocked cells while ensuring all the accessible cells are visited at least
once. The proposed strategy chooses next waypoint based on the graph
connectivity of the cells and the obstacle encountered so far. It is online,
exhaustive and works in situations demanding non-uniform coverage. The
completeness of the strategy is proved and its desirable properties are
discussed with examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fully Spiking Neural Network for Legged Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05022v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05022v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyang Jiang, Qiang Zhang, Jingkai Sun, Jiahang Cao, Jingtong Ma, Renjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, legged robots based on deep reinforcement learning have made
remarkable progress. Quadruped robots have demonstrated the ability to complete
challenging tasks in complex environments and have been deployed in real-world
scenarios to assist humans. Simultaneously, bipedal and humanoid robots have
achieved breakthroughs in various demanding tasks. Current reinforcement
learning methods can utilize diverse robot bodies and historical information to
perform actions. However, prior research has not emphasized the speed and
energy consumption of network inference, as well as the biological significance
of the neural networks themselves. Most of the networks employed are
traditional artificial neural networks that utilize multilayer perceptrons
(MLP). In this paper, we successfully apply a novel Spiking Neural Network
(SNN) to process legged robots, achieving outstanding results across a range of
simulated terrains. SNN holds a natural advantage over traditional neural
networks in terms of inference speed and energy consumption, and their
pulse-form processing of body perception signals offers improved biological
interpretability. Applying more biomimetic neural networks to legged robots can
further reduce the heat dissipation and structural burden caused by the high
power consumption of neural networks. To the best of our knowledge, this is the
first work to implement SNN in legged robots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language
  Models <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10062v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10062v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shyam Sundar Kannan, Vishnunandan L. N. Venkatesh, Byung-Cheol Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce SMART-LLM, an innovative framework designed for
embodied multi-robot task planning. SMART-LLM: Smart Multi-Agent Robot Task
Planning using Large Language Models (LLMs), harnesses the power of LLMs to
convert high-level task instructions provided as input into a multi-robot task
plan. It accomplishes this by executing a series of stages, including task
decomposition, coalition formation, and task allocation, all guided by
programmatic LLM prompts within the few-shot prompting paradigm. We create a
benchmark dataset designed for validating the multi-robot task planning
problem, encompassing four distinct categories of high-level instructions that
vary in task complexity. Our evaluation experiments span both simulation and
real-world scenarios, demonstrating that the proposed model can achieve
promising results for generating multi-robot task plans. The experimental
videos, code, and datasets from the work can be found at
https://sites.google.com/view/smart-llm/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SurgicalPart-SAM: Part-to-Whole Collaborative Prompting for Surgical
  Instrument Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14481v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14481v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxi Yue, Jing Zhang, Kun Hu, Qiuxia Wu, Zongyuan Ge, Yong Xia, Jiebo Luo, Zhiyong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Segment Anything Model (SAM) exhibits promise in generic object
segmentation and offers potential for various applications. Existing methods
have applied SAM to surgical instrument segmentation (SIS) by tuning SAM-based
frameworks with surgical data. However, they fall short in two crucial aspects:
(1) Straightforward model tuning with instrument masks treats each instrument
as a single entity, neglecting their complex structures and fine-grained
details; and (2) Instrument category-based prompts are not flexible and
informative enough to describe instrument structures. To address these
problems, in this paper, we investigate text promptable SIS and propose
SurgicalPart-SAM (SP-SAM), a novel SAM efficient-tuning approach that
explicitly integrates instrument structure knowledge with SAM's generic
knowledge, guided by expert knowledge on instrument part compositions.
Specifically, we achieve this by proposing (1) Collaborative Prompts that
describe instrument structures via collaborating category-level and part-level
texts; (2) Cross-Modal Prompt Encoder that encodes text prompts jointly with
visual embeddings into discriminative part-level representations; and (3)
Part-to-Whole Adaptive Fusion and Hierarchical Decoding that adaptively fuse
the part-level representations into a whole for accurate instrument
segmentation in surgical scenarios. Built upon them, SP-SAM acquires a better
capability to comprehend surgical instruments in terms of both overall
structure and part-level details. Extensive experiments on both the EndoVis2018
and EndoVis2017 datasets demonstrate SP-SAM's state-of-the-art performance with
minimal tunable parameters. The code will be available at
https://github.com/wenxi-yue/SurgicalPart-SAM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report. The source code will be released at
  https://github.com/wenxi-yue/SurgicalPart-SAM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bird's Eye View Based Pretrained World model for Visual Navigation <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18847v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18847v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiran Lekkala, Chen Liu, Laurent Itti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sim2Real transfer has gained popularity because it helps transfer from
inexpensive simulators to real world. This paper presents a novel system that
fuses components in a traditional World Model into a robust system, trained
entirely within a simulator, that Zero-Shot transfers to the real world. To
facilitate transfer, we use an intermediary representation that is based on
\textit{Bird's Eye View (BEV)} images. Thus, our robot learns to navigate in a
simulator by first learning to translate from complex \textit{First-Person View
(FPV)} based RGB images to BEV representations, then learning to navigate using
those representations. Later, when tested in the real world, the robot uses the
perception model that translates FPV-based RGB images to embeddings that were
learned by the FPV to BEV translator and that can be used by the downstream
policy. The incorporation of state-checking modules using \textit{Anchor
images} and Mixture Density LSTM not only interpolates uncertain and missing
observations but also enhances the robustness of the model in the real-world.
We trained the model using data from a Differential drive robot in the CARLA
simulator. Our methodology's effectiveness is shown through the deployment of
trained models onto a real-world Differential drive robot. Lastly we release a
comprehensive codebase, dataset and models for training and deployment
(\url{https://sites.google.com/view/value-explicit-pretraining}).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review at the IROS 2024; Accepted at NeurIPS 2023, Robot
  Learning Workshop</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-22T00:00:00Z">2024-03-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">47</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmented Reality based Simulated Data (ARSim) with multi-view
  consistency for AV perception networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aqeel Anwar, Tae Eun Choe, Zian Wang, Sanja Fidler, Minwoo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting a diverse range of objects under various driving scenarios is
essential for the effectiveness of autonomous driving systems. However, the
real-world data collected often lacks the necessary diversity presenting a
long-tail distribution. Although synthetic data has been utilized to overcome
this issue by generating virtual scenes, it faces hurdles such as a significant
domain gap and the substantial efforts required from 3D artists to create
realistic environments. To overcome these challenges, we present ARSim, a fully
automated, comprehensive, modular framework designed to enhance real multi-view
image data with 3D synthetic objects of interest. The proposed method
integrates domain adaptation and randomization strategies to address covariate
shift between real and simulated data by inferring essential domain attributes
from real data and employing simulation-based randomization for other
attributes. We construct a simplified virtual scene using real data and
strategically place 3D synthetic assets within it. Illumination is achieved by
estimating light distribution from multiple images capturing the surroundings
of the vehicle. Camera parameters from real data are employed to render
synthetic assets in each frame. The resulting augmented multi-view consistent
dataset is used to train a multi-camera perception network for autonomous
vehicles. Experimental results on various AV perception tasks demonstrate the
superior performance of networks trained on the augmented dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 15 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OceanPlan: Hierarchical Planning and Replanning for Natural Language AUV
  Piloting in Large-scale Unexplored Ocean Environments <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruochu Yang, Fumin Zhang, Mengxue Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a hierarchical LLM-task-motion planning and replanning framework
to efficiently ground an abstracted human command into tangible Autonomous
Underwater Vehicle (AUV) control through enhanced representations of the world.
We also incorporate a holistic replanner to provide real-world feedback with
all planners for robust AUV operation. While there has been extensive research
in bridging the gap between LLMs and robotic missions, they are unable to
guarantee success of AUV applications in the vast and unknown ocean
environment. To tackle specific challenges in marine robotics, we design a
hierarchical planner to compose executable motion plans, which achieves
planning efficiency and solution quality by decomposing long-horizon missions
into sub-tasks. At the same time, real-time data stream is obtained by a
replanner to address environmental uncertainties during plan execution.
Experiments validate that our proposed framework delivers successful AUV
performance of long-duration missions through natural language piloting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe and Stable Teleoperation of Quadrotor UAVs under Haptic Shared
  Autonomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dawei Zhang, Roberto Tron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach that aims to address both safety and stability of
a haptic teleoperation system within a framework of Haptic Shared Autonomy
(HSA). We use Control Barrier Functions (CBFs) to generate the control input
that follows the user's input as closely as possible while guaranteeing safety.
In the context of stability of the human-in-the-loop system, we limit the force
feedback perceived by the user via a small $L_2$-gain, which is achieved by
limiting the control and the force feedback via a differential constraint.
Specifically, with the property of HSA, we propose two pathways to design the
control and the force feedback: Sequential Control Force (SCF) and Joint
Control Force (JCF). Both designs can achieve safety and stability but with
different responses to the user's commands. We conducted experimental
simulations to evaluate and investigate the properties of the designed methods.
We also tested the proposed method on a physical quadrotor UAV and a haptic
interface.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gesture-Controlled Aerial Robot Formation for Human-Swarm Interaction in
  Safety Monitoring Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vít Krátký, Giuseppe Silano, Matouš Vrba, Christos Papaioannidis, Ioannis Mademlis, Robert Pěnička, Ioannis Pitas, Martin Saska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a formation control approach for contactless
gesture-based Human-Swarm Interaction (HSI) between a team of multi-rotor
Unmanned Aerial Vehicles (UAVs) and a human worker. The approach is intended
for monitoring the safety of human workers, especially those working at
heights. In the proposed dynamic formation scheme, one UAV acts as the leader
of the formation and is equipped with sensors for human worker detection and
gesture recognition. The follower UAVs maintain a predetermined formation
relative to the worker's position, thereby providing additional perspectives of
the monitored scene. Hand gestures allow the human worker to specify movements
and action commands for the UAV team and initiate other mission-related
commands without the need for an additional communication channel or specific
markers. Together with a novel unified human detection and tracking algorithm,
human pose estimation approach and gesture detection pipeline, the proposed
approach forms a first instance of an HSI system incorporating all these
modules onboard real-world UAVs. Simulations and field experiments with three
UAVs and a human worker in a mock-up scenario showcase the effectiveness and
responsiveness of the proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Introduction to Human-Robot Interaction: A Multi-Perspective
  Introductory Course 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper I describe the design of an introductory course in Human-Robot
Interaction. This project-driven course is designed to introduce undergraduate
and graduate engineering students, especially those enrolled in Computer
Science, Mechanical Engineering, and Robotics degree programs, to key theories
and methods used in the field of Human-Robot Interaction that they would
otherwise be unlikely to see in those degree programs. To achieve this aim, the
course takes students all the way from stakeholder analysis to empirical
evaluation, covering and integrating key Qualitative, Design, Computational,
and Quantitative methods along the way. I detail the goals, audience, and
format of the course, and provide a detailed walkthrough of the course
syllabus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the Designing an Intro to HRI Course Workshop at HRI
  2024 (arXiv:2403.05588)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HortiBot: An Adaptive Multi-Arm System for Robotic Horticulture of Sweet
  Peppers <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Lenz, Rohit Menon, Michael Schreiber, Melvin Paul Jacob, Sven Behnke, Maren Bennewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Horticultural tasks such as pruning and selective harvesting are labor
intensive and horticultural staff are hard to find. Automating these tasks is
challenging due to the semi-structured greenhouse workspaces, changing
environmental conditions such as lighting, dense plant growth with many
occlusions, and the need for gentle manipulation of non-rigid plant organs. In
this work, we present the three-armed system HortiBot, with two arms for
manipulation and a third arm as an articulated head for active perception using
stereo cameras. Its perception system detects not only peppers, but also
peduncles and stems in real time, and performs online data association to build
a world model of pepper plants. Collision-aware online trajectory generation
allows all three arms to safely track their respective targets for observation,
grasping, and cutting. We integrated perception and manipulation to perform
selective harvesting of peppers and evaluated the system in lab experiments.
Using active perception coupled with end-effector force torque sensing for
compliant manipulation, HortiBot achieves high success rates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to International Conference on Intelligent Robots and
  Systems (IROS) 2024. C. Lenz and R. Menon contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guided Decoding for Robot Motion Generation and Adaption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nutan Chen, Elie Aljalbout, Botond Cseke, Patrick van der Smagt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address motion generation for high-DoF robot arms in complex settings with
obstacles, via points, etc. A significant advancement in this domain is
achieved by integrating Learning from Demonstration (LfD) into the motion
generation process. This integration facilitates rapid adaptation to new tasks
and optimizes the utilization of accumulated expertise by allowing robots to
learn and generalize from demonstrated trajectories.
  We train a transformer architecture on a large dataset of simulated
trajectories. This architecture, based on a conditional variational autoencoder
transformer, learns essential motion generation skills and adapts these to meet
auxiliary tasks and constraints. Our auto-regressive approach enables real-time
integration of feedback from the physical system, enhancing the adaptability
and efficiency of motion generation. We show that our model can generate motion
from initial and target points, but also that it can adapt trajectories in
navigating complex tasks, including obstacle avoidance, via points, and meeting
velocity and acceleration constraints, across platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TriHelper: Zero-Shot Object Navigation with Dynamic Assistance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingfeng Zhang, Qiang Zhang, Hao Wang, Erjia Xiao, Zixuan Jiang, Honglei Chen, Renjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigating toward specific objects in unknown environments without additional
training, known as Zero-Shot object navigation, poses a significant challenge
in the field of robotics, which demands high levels of auxiliary information
and strategic planning. Traditional works have focused on holistic solutions,
overlooking the specific challenges agents encounter during navigation such as
collision, low exploration efficiency, and misidentification of targets. To
address these challenges, our work proposes TriHelper, a novel framework
designed to assist agents dynamically through three primary navigation
challenges: collision, exploration, and detection. Specifically, our framework
consists of three innovative components: (i) Collision Helper, (ii) Exploration
Helper, and (iii) Detection Helper. These components work collaboratively to
solve these challenges throughout the navigation process. Experiments on the
Habitat-Matterport 3D (HM3D) and Gibson datasets demonstrate that TriHelper
significantly outperforms all existing baseline methods in Zero-Shot object
navigation, showcasing superior success rates and exploration efficiency. Our
ablation studies further underscore the effectiveness of each helper in
addressing their respective challenges, notably enhancing the agent's
navigation capabilities. By proposing TriHelper, we offer a fresh perspective
on advancing the object navigation task, paving the way for future research in
the domain of Embodied AI and visual-based navigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DITTO: Demonstration Imitation by Trajectory Transformation <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Heppert, Max Argus, Tim Welschehold, Thomas Brox, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Teaching robots new skills quickly and conveniently is crucial for the
broader adoption of robotic systems. In this work, we address the problem of
one-shot imitation from a single human demonstration, given by an RGB-D video
recording through a two-stage process. In the first stage which is offline, we
extract the trajectory of the demonstration. This entails segmenting
manipulated objects and determining their relative motion in relation to
secondary objects such as containers. Subsequently, in the live online
trajectory generation stage, we first \mbox{re-detect} all objects, then we
warp the demonstration trajectory to the current scene, and finally, we trace
the trajectory with the robot. To complete these steps, our method makes
leverages several ancillary models, including those for segmentation, relative
object pose estimation, and grasp prediction. We systematically evaluate
different combinations of correspondence and re-detection methods to validate
our design decision across a diverse range of tasks. Specifically, we collect
demonstrations of ten different tasks including pick-and-place tasks as well as
articulated object manipulation. Finally, we perform extensive evaluations on a
real robot system to demonstrate the effectiveness and utility of our approach
in real-world scenarios. We make the code publicly available at
http://ditto.cs.uni-freiburg.de.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, 3 tables, submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRPlace: Camera-Radar Fusion with BEV Representation for Place
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaowei Fu, Yifan Duan, Yao Li, Chengzhen Meng, Yingjie Wang, Jianmin Ji, Yanyong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of complementary characteristics from camera and radar data
has emerged as an effective approach in 3D object detection. However, such
fusion-based methods remain unexplored for place recognition, an equally
important task for autonomous systems. Given that place recognition relies on
the similarity between a query scene and the corresponding candidate scene, the
stationary background of a scene is expected to play a crucial role in the
task. As such, current well-designed camera-radar fusion methods for 3D object
detection can hardly take effect in place recognition because they mainly focus
on dynamic foreground objects. In this paper, a background-attentive
camera-radar fusion-based method, named CRPlace, is proposed to generate
background-attentive global descriptors from multi-view images and radar point
clouds for accurate place recognition. To extract stationary background
features effectively, we design an adaptive module that generates the
background-attentive mask by utilizing the camera BEV feature and radar dynamic
points. With the guidance of a background mask, we devise a bidirectional
cross-attention-based spatial fusion strategy to facilitate comprehensive
spatial interaction between the background information of the camera BEV
feature and the radar BEV feature. As the first camera-radar fusion-based place
recognition network, CRPlace has been evaluated thoroughly on the nuScenes
dataset. The results show that our algorithm outperforms a variety of baseline
methods across a comprehensive set of metrics (recall@1 reaches 91.2%).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AV-Occupant Perceived Risk Model for Cut-In Scenarios with Empirical
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarah Barendswaard, Tong Duy Son
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in autonomous vehicle (AV) technologies necessitate precise
estimation of perceived risk to enhance user comfort, acceptance and trust.
This paper introduces a novel AV-Occupant Risk (AVOR) model designed for
perceived risk estimation during AV cut-in scenarios. An empirical study is
conducted with 18 participants with realistic cut-in scenarios. Two factors
were investigated: scenario risk and scene population. 76% of subjective risk
responses indicate an increase in perceived risk at cut-in initiation. The
existing perceived risk model did not capture this critical phenomenon. Our
AVOR model demonstrated a significant improvement in estimating perceived risk
during the early stages of cut-ins, especially for the high-risk scenario,
enhancing modelling accuracy by up to 54%. The concept of the AVOR model can
quantify perceived risk in other diverse driving contexts characterized by
dynamic uncertainties, enhancing the reliability and human-centred focus of AV
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Infrastructure-Assisted Collaborative Perception in Automated Valet
  Parking: A Safety Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukuan Jia, Jiawen Zhang, Shimeng Lu, Baokang Fan, Ruiqing Mao, Sheng Zhou, Zhisheng Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Environmental perception in Automated Valet Parking (AVP) has been a
challenging task due to severe occlusions in parking garages. Although
Collaborative Perception (CP) can be applied to broaden the field of view of
connected vehicles, the limited bandwidth of vehicular communications restricts
its application. In this work, we propose a BEV feature-based CP network
architecture for infrastructure-assisted AVP systems. The model takes the
roadside camera and LiDAR as optional inputs and adaptively fuses them with
onboard sensors in a unified BEV representation. Autoencoder and downsampling
are applied for channel-wise and spatial-wise dimension reduction, while
sparsification and quantization further compress the feature map with little
loss in data precision. Combining these techniques, the size of a BEV feature
map is effectively compressed to fit in the feasible data rate of the NR-V2X
network. With the synthetic AVP dataset, we observe that CP can effectively
increase perception performance, especially for pedestrians. Moreover, the
advantage of infrastructure-assisted CP is demonstrated in two typical
safety-critical scenarios in the AVP setting, increasing the maximum safe
cruising speed by up to 3m/s in both scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures, 4 tables, accepted by IEEE VTC2024-Spring</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RHINO-VR Experience: Teaching Mobile Robotics Concepts in an Interactive
  Museum Exhibit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Schlachhoff, Nils Dengler, Leif Van Holland, Patrick Stotko, Jorge de Heuvel, Reinhard Klein, Maren Bennewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In 1997, the very first tour guide robot RHINO was deployed in a museum in
Germany. With the ability to navigate autonomously through the environment, the
robot gave tours to over 2,000 visitors. Today, RHINO itself has become an
exhibit and is no longer operational. In this paper, we present RHINO-VR, an
interactive museum exhibit using virtual reality (VR) that allows museum
visitors to experience the historical robot RHINO in operation in a virtual
museum. RHINO-VR, unlike static exhibits, enables users to familiarize
themselves with basic mobile robotics concepts without the fear of damaging the
exhibit. In the virtual environment, the user is able to interact with RHINO in
VR by pointing to a location to which the robot should navigate and observing
the corresponding actions of the robot. To include other visitors who cannot
use the VR, we provide an external observation view to make RHINO visible to
them. We evaluated our system by measuring the frame rate of the VR simulation,
comparing the generated virtual 3D models with the originals, and conducting a
user study. The user-study showed that RHINO-VR improved the visitors'
understanding of the robot's functionality and that they would recommend
experiencing the VR exhibit to others.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE International Symposium on Robot and Human
  Interactive Communication (RO-MAN)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ALPINE: a climbing robot for operations in mountain environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michele Focchi, Andrea Del Prete, Daniele Fontanelli, Marco Frego, Angelika Peer, Luigi Palopoli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mountain slopes are perfect examples of harsh environments in which humans
are required to perform difficult and dangerous operations such as removing
unstable boulders, dangerous vegetation or deploying safety nets. A good
replacement for human intervention can be offered by climbing robots. The
different solutions existing in the literature are not up to the task for the
difficulty of the requirements (navigation, heavy payloads, flexibility in the
execution of the tasks). In this paper, we propose a robotic platform that can
fill this gap. Our solution is based on a robot that hangs on ropes, and uses a
retractable leg to jump away from the mountain walls. Our package of mechanical
solutions, along with the algorithms developed for motion planning and control,
delivers swift navigation on irregular and steep slopes, the possibility to
overcome or travel around significant natural barriers, and the ability to
carry heavy payloads and execute complex tasks. In the paper, we give a full
account of our main design and algorithmic choices and show the feasibility of
the solution through a large number of physically simulated scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collision Avoidance Safety Filter for an Autonomous E-Scooter using
  Ultrasonic Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin Strässer, Marc Seidel, Felix Brändle, David Meister, Raffaele Soloperto, David Hambach Ferrer, Frank Allgöwer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a collision avoidance safety filter for autonomous
electric scooters to enable safe operation of such vehicles in pedestrian
areas. In particular, we employ multiple low-cost ultrasonic sensors to detect
a wide range of possible obstacles in front of the e-scooter. Based on possibly
faulty distance measurements, we design a filter to mitigate measurement noise
and missing values as well as a gain-scheduled controller to limit the velocity
commanded to the e-scooter when required due to imminent collisions. The
proposed controller structure is able to prevent collisions with unknown
obstacles by deploying a reduced safe velocity ensuring a sufficiently large
safety distance. The collision avoidance approach is designed such that it may
be easily deployed in similar applications of general micromobility vehicles.
The effectiveness of our proposed safety filter is demonstrated in real-world
experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Set-membership target search and tracking within an unknown cluttered
  area using cooperating UAVs equipped with vision systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Zagar, Luc Meyer, Michel Kieffer, Hélène Piet-Lahanier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of target search and tracking using a fleet
of cooperating UAVs evolving in some unknown region of interest containing an a
priori unknown number of moving ground targets. Each drone is equipped with an
embedded Computer Vision System (CVS), providing an image with labeled pixels
and a depth map of the observed part of its environment. Moreover, a box
containing the corresponding pixels in the image frame is available when a UAV
identifies a target. Hypotheses regarding information provided by the pixel
classification, depth map construction, and target identification algorithms
are proposed to allow its exploitation by set-membership approaches. A
set-membership target location estimator is developed using the information
provided by the CVS. Each UAV evaluates sets guaranteed to contain the location
of the identified targets and a set possibly containing the locations of
targets still to be identified. Then, each UAV uses these sets to search and
track targets cooperatively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PseudoTouch: Efficiently Imaging the Surface Feel of Objects for Robotic
  Manipulation <span class="chip">IROS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Röfer, Nick Heppert, Abdallah Ayman, Eugenio Chisari, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans seemingly incorporate potential touch signals in their perception. Our
goal is to equip robots with a similar capability, which we term \ourmodel.
\ourmodel aims to predict the expected touch signal based on a visual patch
representing the touched area. We frame this problem as the task of learning a
low-dimensional visual-tactile embedding, wherein we encode a depth patch from
which we decode the tactile signal. To accomplish this task, we employ ReSkin,
an inexpensive and replaceable magnetic-based tactile sensor. Using ReSkin, we
collect and train PseudoTouch on a dataset comprising aligned tactile and
visual data pairs obtained through random touching of eight basic geometric
shapes. We demonstrate the efficacy of PseudoTouch through its application to
two downstream tasks: object recognition and grasp stability prediction. In the
object recognition task, we evaluate the learned embedding's performance on a
set of five basic geometric shapes and five household objects. Using
PseudoTouch, we achieve an object recognition accuracy 84% after just ten
touches, surpassing a proprioception baseline. For the grasp stability task, we
use ACRONYM labels to train and evaluate a grasp success predictor using
PseudoTouch's predictions derived from virtual depth information. Our approach
yields an impressive 32% absolute improvement in accuracy compared to the
baseline relying on partial point cloud data. We make the data, code, and
trained models publicly available at http://pseudotouch.cs.uni-freiburg.de.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, 2 tables, submitted to IROS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning from Visual Demonstrations through Differentiable Nonlinear MPC
  for Personalized Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Flavia Sofia Acerbo, Jan Swevers, Tinne Tuytelaars, Tong Duy Son
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-like autonomous driving controllers have the potential to enhance
passenger perception of autonomous vehicles. This paper proposes DriViDOC: a
model for Driving from Vision through Differentiable Optimal Control, and its
application to learn personalized autonomous driving controllers from human
demonstrations. DriViDOC combines the automatic inference of relevant features
from camera frames with the properties of nonlinear model predictive control
(NMPC), such as constraint satisfaction. Our approach leverages the
differentiability of parametric NMPC, allowing for end-to-end learning of the
driving model from images to control. The model is trained on an offline
dataset comprising various driving styles collected on a motion-base driving
simulator. During online testing, the model demonstrates successful imitation
of different driving styles, and the interpreted NMPC parameters provide
insights into the achievement of specific driving behaviors. Our experimental
results show that DriViDOC outperforms other methods involving NMPC and neural
networks, exhibiting an average improvement of 20% in imitation scores.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible. Accompanying video available at:
  https://youtu.be/WxWPuAtJ08E</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subequivariant Reinforcement Learning Framework for Coordinated Motion
  Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Wang, Xiaoyu Tan, Xihe Qiu, Chao Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective coordination is crucial for motion control with reinforcement
learning, especially as the complexity of agents and their motions increases.
However, many existing methods struggle to account for the intricate
dependencies between joints. We introduce CoordiGraph, a novel architecture
that leverages subequivariant principles from physics to enhance coordination
of motion control with reinforcement learning. This method embeds the
principles of equivariance as inherent patterns in the learning process under
gravity influence, which aids in modeling the nuanced relationships between
joints vital for motion control. Through extensive experimentation with
sophisticated agents in diverse environments, we highlight the merits of our
approach. Compared to current leading methods, CoordiGraph notably enhances
generalization and sample efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures, 2024 IEEE International Conference on Robotics
  and Automation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Feature Selection for Inverse Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daulet Baimukashev, Gokhan Alcan, Ville Kyrki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse reinforcement learning (IRL) is an imitation learning approach to
learning reward functions from expert demonstrations. Its use avoids the
difficult and tedious procedure of manual reward specification while retaining
the generalization power of reinforcement learning. In IRL, the reward is
usually represented as a linear combination of features. In continuous state
spaces, the state variables alone are not sufficiently rich to be used as
features, but which features are good is not known in general. To address this
issue, we propose a method that employs polynomial basis functions to form a
candidate set of features, which are shown to allow the matching of statistical
moments of state distributions. Feature selection is then performed for the
candidates by leveraging the correlation between trajectory probabilities and
feature expectations. We demonstrate the approach's effectiveness by recovering
reward functions that capture expert policies across non-linear control tasks
of increasing complexity. Code, data, and videos are available at
https://sites.google.com/view/feature4irl.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Twin Delayed Deep Deterministic Policy Gradient Algorithm for
  Autonomous Ground Vehicle Navigation via Digital Twin Perception Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kabirat Olayemi, Mien Van, Sean McLoone, Yuzhu Sun, Jack Close, Nguyen Minh Nhat, Stephen McIlvanna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous ground vehicle (UGV) navigation has the potential to revolutionize
the transportation system by increasing accessibility to disabled people,
ensure safety and convenience of use. However, UGV requires extensive and
efficient testing and evaluation to ensure its acceptance for public use. This
testing are mostly done in a simulator which result to sim2real transfer gap.
In this paper, we propose a digital twin perception awareness approach for the
control of robot navigation without prior creation of the virtual environment
(VT) environment state. To achieve this, we develop a twin delayed deep
deterministic policy gradient (TD3) algorithm that ensures collision avoidance
and goal-based path planning. We demonstrate the performance of our approach on
different environment dynamics. We show that our approach is capable of
efficiently avoiding collision with obstacles and navigating to its desired
destination, while at the same time safely avoids obstacles using the
information received from the LIDAR sensor mounted on the robot. Our approach
bridges the gap between sim-to-real transfer and contributes to the adoption of
UGVs in real world. We validate our approach in simulation and a real-world
application in an office space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking 6-Dof Grasp Detection: A Flexible Framework for High-Quality
  Grasping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Tang, Siang Chen, Pengwei Xie, Dingchang Hu, Wenming Yang, Guijin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic grasping is a primitive skill for complex tasks and is fundamental to
intelligence. For general 6-Dof grasping, most previous methods directly
extract scene-level semantic or geometric information, while few of them
consider the suitability for various downstream applications, such as
target-oriented grasping. Addressing this issue, we rethink 6-Dof grasp
detection from a grasp-centric view and propose a versatile grasp framework
capable of handling both scene-level and target-oriented grasping. Our
framework, FlexLoG, is composed of a Flexible Guidance Module and a Local Grasp
Model. Specifically, the Flexible Guidance Module is compatible with both
global (e.g., grasp heatmap) and local (e.g., visual grounding) guidance,
enabling the generation of high-quality grasps across various tasks. The Local
Grasp Model focuses on object-agnostic regional points and predicts grasps
locally and intently. Experiment results reveal that our framework achieves
over 18% and 23% improvement on unseen splits of the GraspNet-1Billion Dataset.
Furthermore, real-world robotic tests in three distinct settings yield a 95%
success rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linear Quadratic Guidance Law for Joint Motion Planning of a
  Pursuer-Turret Assembly 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhargav Jha, Shaunak Bopardikar, Alexander Von Moll, David Casbeer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents joint motion planning of a vehicle with an attached
rotating turret. The turret has a limited range as well as the field of view.
The objective is capture a maneuvering target such that at the terminal time it
is withing the field-of-view and range limits. Catering to it, we present a
minimum effort guidance law that commensurate for the turn rate abilities of
the vehicle and the turret. The guidance law is obtained using linearization
about the collision triangle and admits an analytical solution. Simulation
results are presented to exemplify the cooperation between the turret and the
vehicle.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boundary-Aware Value Function Generation for Safe Stochastic Motion
  Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhong Xu, Kai Yin, Jason M. Gregory, Kris Hauser, Lantao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigation safety is critical for many autonomous systems such as
self-driving vehicles in an urban environment. It requires an explicit
consideration of boundary constraints that describe the borders of any
infeasible, non-navigable, or unsafe regions. We propose a principled
boundary-aware safe stochastic planning framework with promising results. Our
method generates a value function that can strictly distinguish the state
values between free (safe) and non-navigable (boundary) spaces in the
continuous state, naturally leading to a safe boundary-aware policy. At the
core of our solution lies a seamless integration of finite elements and
kernel-based functions, where the finite elements allow us to characterize
safety-critical states' borders accurately, and the kernel-based function
speeds up computation for the non-safety-critical states. The proposed method
was evaluated through extensive simulations and demonstrated safe navigation
behaviors in mobile navigation tasks. Additionally, we demonstrate that our
approach can maneuver safely and efficiently in cluttered real-world
environments using a ground vehicle with strong external disturbances, such as
navigating on a slippery floor and against external human intervention.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by International Journal of Robotics Research</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SRLM: Human-in-Loop Interactive Social Robot Navigation with Large
  Language Model and Deep Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizheng Wang, Le Mao, Ruiqi Wang, Byung-Cheol Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An interactive social robotic assistant must provide services in complex and
crowded spaces while adapting its behavior based on real-time human language
commands or feedback. In this paper, we propose a novel hybrid approach called
Social Robot Planner (SRLM), which integrates Large Language Models (LLM) and
Deep Reinforcement Learning (DRL) to navigate through human-filled public
spaces and provide multiple social services. SRLM infers global planning from
human-in-loop commands in real-time, and encodes social information into a
LLM-based large navigation model (LNM) for low-level motion execution.
Moreover, a DRL-based planner is designed to maintain benchmarking performance,
which is blended with LNM by a large feedback model (LFM) to address the
instability of current text and LLM-driven LNM. Finally, SRLM demonstrates
outstanding performance in extensive experiments. More details about this work
are available at: https://sites.google.com/view/navi-srlm
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoNVOI: Context-aware Navigation using Vision Language Models in Outdoor
  and Indoor Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adarsh Jagan Sathyamoorthy, Kasun Weerakoon, Mohamed Elnoor, Anuj Zore, Brian Ichter, Fei Xia, Jie Tan, Wenhao Yu, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ConVOI, a novel method for autonomous robot navigation in
real-world indoor and outdoor environments using Vision Language Models (VLMs).
We employ VLMs in two ways: first, we leverage their zero-shot image
classification capability to identify the context or scenario (e.g., indoor
corridor, outdoor terrain, crosswalk, etc) of the robot's surroundings, and
formulate context-based navigation behaviors as simple text prompts (e.g.
``stay on the pavement"). Second, we utilize their state-of-the-art semantic
understanding and logical reasoning capabilities to compute a suitable
trajectory given the identified context. To this end, we propose a novel
multi-modal visual marking approach to annotate the obstacle-free regions in
the RGB image used as input to the VLM with numbers, by correlating it with a
local occupancy map of the environment. The marked numbers ground image
locations in the real-world, direct the VLM's attention solely to navigable
locations, and elucidate the spatial relationships between them and terrains
depicted in the image to the VLM. Next, we query the VLM to select numbers on
the marked image that satisfy the context-based behavior text prompt, and
construct a reference path using the selected numbers. Finally, we propose a
method to extrapolate the reference trajectory when the robot's environmental
context has not changed to prevent unnecessary VLM queries. We use the
reference trajectory to guide a motion planner, and demonstrate that it leads
to human-like behaviors (e.g. not cutting through a group of people, using
crosswalks, etc.) in various real-world indoor and outdoor scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global Games with Negative Feedback for Autonomous Colony Maintenance
  using Robot Teams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Logan E. Beaver
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article we address the colony maintenance problem, where a team of
robots are tasked with continuously maintaining the energy supply of an
autonomous colony. We model this as a global game, where robots measure the
energy level of a central nest to determine whether or not to forage for energy
sources. We design a mechanism that avoids the trivial equilibrium where all
robots always forage. Furthermore, we demonstrate that when the game is played
iteratively a negative feedback term stabilizes the number of foraging robots
at a non-trivial Nash equilibrium. We compare our approach qualitatively to
existing global games, where a positive positive feedback term admits
threshold-based decision making, and encourages many robots to forage
simultaneously. We discuss how positive feedback can lead to a cascading
failure in the presence of a human who recruits robots for external tasks, and
we demonstrate the performance of our approach in simulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autonomous Driving With Perception Uncertainties: Deep-Ensemble Based
  Adaptive Cruise Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Li, H. Eric Tseng, Anouck Girard, Ilya Kolmanovsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving depends on perception systems to understand the
environment and to inform downstream decision-making. While advanced perception
systems utilizing black-box Deep Neural Networks (DNNs) demonstrate human-like
comprehension, their unpredictable behavior and lack of interpretability may
hinder their deployment in safety critical scenarios. In this paper, we develop
an Ensemble of DNN regressors (Deep Ensemble) that generates predictions with
quantification of prediction uncertainties. In the scenario of Adaptive Cruise
Control (ACC), we employ the Deep Ensemble to estimate distance headway to the
lead vehicle from RGB images and enable the downstream controller to account
for the estimation uncertainty. We develop an adaptive cruise controller that
utilizes Stochastic Model Predictive Control (MPC) with chance constraints to
provide a probabilistic safety guarantee. We evaluate our ACC algorithm using a
high-fidelity traffic simulator and a real-world traffic dataset and
demonstrate the ability of the proposed approach to effect speed tracking and
car following while maintaining a safe distance headway. The
out-of-distribution scenarios are also examined.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Music to Dance as Language Translation using Sequence Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Correia, Luís A. Alexandre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthesising appropriate choreographies from music remains an open problem.
We introduce MDLT, a novel approach that frames the choreography generation
problem as a translation task. Our method leverages an existing data set to
learn to translate sequences of audio into corresponding dance poses. We
present two variants of MDLT: one utilising the Transformer architecture and
the other employing the Mamba architecture. We train our method on AIST++ and
PhantomDance data sets to teach a robotic arm to dance, but our method can be
applied to a full humanoid robot. Evaluation metrics, including Average Joint
Error and Frechet Inception Distance, consistently demonstrate that, when given
a piece of music, MDLT excels at producing realistic and high-quality
choreography. The code can be found at github.com/meowatthemoon/MDLT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaussian-<span class="highlight-title">SLAM</span>: Photo-realistic Dense <span class="highlight-title">SLAM</span> with Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10070v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10070v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladimir Yugay, Yue Li, Theo Gevers, Martin R. Oswald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a dense simultaneous localization and mapping (SLAM) method that
uses 3D Gaussians as a scene representation. Our approach enables
interactive-time reconstruction and photo-realistic rendering from real-world
single-camera RGBD videos. To this end, we propose a novel effective strategy
for seeding new Gaussians for newly explored areas and their effective online
optimization that is independent of the scene size and thus scalable to larger
scenes. This is achieved by organizing the scene into sub-maps which are
independently optimized and do not need to be kept in memory. We further
accomplish frame-to-model camera tracking by minimizing photometric and
geometric losses between the input and rendered frames. The Gaussian
representation allows for high-quality photo-realistic real-time rendering of
real-world scenes. Evaluation on synthetic and real-world datasets demonstrates
competitive or superior performance in mapping, tracking, and rendering
compared to existing neural dense SLAM methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Convex Formulation of Frictional Contact for the Material Point Method
  and Rigid Bodies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeshun Zong, Chenfanfu Jiang, Xuchen Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel convex formulation that seamlessly
integrates the Material Point Method (MPM) with articulated rigid body dynamics
in frictional contact scenarios. We extend the linear corotational hyperelastic
model into the realm of elastoplasticity and include an efficient return
mapping algorithm. This approach is particularly effective for MPM simulations
involving significant deformation and topology changes, while preserving the
convexity of the optimization problem. Our method ensures global convergence,
enabling the use of large simulation time steps without compromising
robustness. We have validated our approach through rigorous testing and
performance evaluations, highlighting its superior capabilities in managing
complex simulations relevant to robotics. Compared to previous MPM based
robotic simulators, our method significantly improves the stability of contact
resolution -- a critical factor in robot manipulation tasks. We make our method
available in the open-source robotics toolkit, Drake.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The supplemental video is available at https://youtu.be/5jrQtF5D0DA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning High-level Semantic-Relational Concepts for <span class="highlight-title">SLAM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jose Andres Millan-Romera, Hriday Bavle, Muhammad Shaheer, Martin R. Oswald, Holger Voos, Jose Luis Sanchez-Lopez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works on SLAM extend their pose graphs with higher-level semantic
concepts like Rooms exploiting relationships between them, to provide, not only
a richer representation of the situation/environment but also to improve the
accuracy of its estimation. Concretely, our previous work, Situational Graphs
(S-Graphs+), a pioneer in jointly leveraging semantic relationships in the
factor optimization process, relies on semantic entities such as Planes and
Rooms, whose relationship is mathematically defined. Nevertheless, there is no
unique approach to finding all the hidden patterns in lower-level factor-graphs
that correspond to high-level concepts of different natures. It is currently
tackled with ad-hoc algorithms, which limits its graph expressiveness.
  To overcome this limitation, in this work, we propose an algorithm based on
Graph Neural Networks for learning high-level semantic-relational concepts that
can be inferred from the low-level factor graph. Given a set of mapped Planes
our algorithm is capable of inferring Room entities relating to the Planes.
Additionally, to demonstrate the versatility of our method, our algorithm can
infer an additional semantic-relational concept, i.e. Wall, and its
relationship with its Planes. We validate our method in both simulated and real
datasets demonstrating improved performance over two baseline approaches.
Furthermore, we integrate our method into the S-Graphs+ algorithm providing
improved pose and map accuracy compared to the baseline while further enhancing
the scene representation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LaMI: Large Language Models for Multi-Modal Human-Robot Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15174v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15174v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Wang, Stephan Hasler, Daniel Tanneberg, Felix Ocker, Frank Joublin, Antonello Ceravola, Joerg Deigmoeller, Michael Gienger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an innovative large language model (LLM)-based robotic
system for enhancing multi-modal human-robot interaction (HRI). Traditional HRI
systems relied on complex designs for intent estimation, reasoning, and
behavior generation, which were resource-intensive. In contrast, our system
empowers researchers and practitioners to regulate robot behavior through three
key aspects: providing high-level linguistic guidance, creating "atomic
actions" and expressions the robot can use, and offering a set of examples.
Implemented on a physical robot, it demonstrates proficiency in adapting to
multi-modal inputs and determining the appropriate manner of action to assist
humans with its arms, following researchers' defined guidelines.
Simultaneously, it coordinates the robot's lid, neck, and ear movements with
speech output to produce dynamic, multi-modal expressions. This showcases the
system's potential to revolutionize HRI by shifting from conventional, manual
state-and-flow design methods to an intuitive, guidance-based, and
example-driven approach. Supplementary material can be found at
https://hri-eu.github.io/Lami/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bi-KVIL: Keypoints-based Visual Imitation Learning of Bimanual
  Manipulation Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianfeng Gao, Xiaoshu Jin, Franziska Krebs, Noémie Jaquier, Tamim Asfour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual imitation learning has achieved impressive progress in learning
unimanual manipulation tasks from a small set of visual observations, thanks to
the latest advances in computer vision. However, learning bimanual coordination
strategies and complex object relations from bimanual visual demonstrations, as
well as generalizing them to categorical objects in novel cluttered scenes
remain unsolved challenges. In this paper, we extend our previous work on
keypoints-based visual imitation learning (\mbox{K-VIL})~\cite{gao_kvil_2023}
to bimanual manipulation tasks. The proposed Bi-KVIL jointly extracts so-called
\emph{Hybrid Master-Slave Relationships} (HMSR) among objects and hands,
bimanual coordination strategies, and sub-symbolic task representations. Our
bimanual task representation is object-centric, embodiment-independent, and
viewpoint-invariant, thus generalizing well to categorical objects in novel
scenes. We evaluate our approach in various real-world applications, showcasing
its ability to learn fine-grained bimanual manipulation tasks from a small
number of human demonstration videos. Videos and source code are available at
https://sites.google.com/view/bi-kvil.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Event-based Simultaneous Localization and <span class="highlight-title">Mapping</span>: A Comprehensive
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.09793v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.09793v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunping Huang, Sen Zhang, Jing Zhang, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent decades, visual simultaneous localization and mapping (vSLAM) has
gained significant interest in both academia and industry. It estimates camera
motion and reconstructs the environment concurrently using visual sensors on a
moving robot. However, conventional cameras are limited by hardware, including
motion blur and low dynamic range, which can negatively impact performance in
challenging scenarios like high-speed motion and high dynamic range
illumination. Recent studies have demonstrated that event cameras, a new type
of bio-inspired visual sensor, offer advantages such as high temporal
resolution, dynamic range, low power consumption, and low latency. This paper
presents a timely and comprehensive review of event-based vSLAM algorithms that
exploit the benefits of asynchronous and irregular event streams for
localization and mapping tasks. The review covers the working principle of
event cameras and various event representations for preprocessing event data.
It also categorizes event-based vSLAM methods into four main categories:
feature-based, direct, motion-compensation, and deep learning methods, with
detailed discussions and practical guidance for each approach. Furthermore, the
paper evaluates the state-of-the-art methods on various benchmarks,
highlighting current challenges and future opportunities in this emerging
research area. A public repository will be maintained to keep track of the
rapid developments in this field at
{\url{https://github.com/kun150kun/ESLAM-survey}}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Direct Data-Driven Control for Probabilistic Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.16973v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.16973v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander von Rohr, Dmitrii Likhachev, Sebastian Trimpe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a data-driven control method for systems with aleatoric
uncertainty, for example, robot fleets with variations between agents. Our
method leverages shared trajectory data to increase the robustness of the
designed controller and thus facilitate transfer to new variations without the
need for prior parameter and uncertainty estimations. In contrast to existing
work on experience transfer for performance, our approach focuses on robustness
and uses data collected from multiple realizations to guarantee generalization
to unseen ones. Our method is based on scenario optimization combined with
recent formulations for direct data-driven control. We derive lower bounds on
the amount of data required to achieve quadratic stability for probabilistic
systems with aleatoric uncertainty and demonstrate the benefits of our
data-driven method through a numerical example. We find that the learned
controllers generalize well to high variations in the dynamics even when based
on only a few short open-loop trajectories. Robust experience transfer enables
the design of safe and robust controllers that work out of the box without any
additional learning during deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Wind-Aware Path Planning Method for UAV-Asisted Bridge Inspection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.10519v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.10519v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Xu, Hua Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In response to the gap in considering wind conditions in the bridge
inspection using unmanned aerial vehicle (UAV) , this paper proposes a path
planning method for UAVs that takes into account the influence of wind, based
on the simulated annealing algorithm. The algorithm considers the wind factors,
including the influence of different wind speeds and directions at the same
time on the path planning of the UAV. Firstly, An environment model is
constructed specifically for UAV bridge inspection, taking into account the
various objective functions and constraint conditions of UAVs. A more
sophisticated and precise mathematical model is then developed based on this
environmental model to enable efficient and effective UAV path planning.
Secondly, the bridge separation planning model is applied in a novel way, and a
series of parameters are simulated, including the adjustment of the initial
temperature value. The experimental results demonstrate that, compared with
traditional local search algorithms, the proposed method achieves a cost
reduction of 30.05\% and significantly improves effectiveness. Compared to path
planning methods that do not consider wind factors, the proposed approach
yields more realistic and practical results for UAV applications, as
demonstrated by its improved effectiveness in simulations. These findings
highlight the value of our method in facilitating more accurate and efficient
UAV path planning in wind-prone environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>After carefully analysis, there is a bit design flaws in Algorithm 1.
  The experimental work of the paper is not comprehensive,which lacks an
  evaluation of the algorithm's running time</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instance-aware Exploration-Verification-Exploitation for Instance
  ImageGoal Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17587v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17587v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Lei, Min Wang, Wengang Zhou, Li Li, Houqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a new embodied vision task, Instance ImageGoal Navigation (IIN) aims to
navigate to a specified object depicted by a goal image in an unexplored
environment.
  The main challenge of this task lies in identifying the target object from
different viewpoints while rejecting similar distractors.
  Existing ImageGoal Navigation methods usually adopt the simple
Exploration-Exploitation framework and ignore the identification of specific
instance during navigation.
  In this work, we propose to imitate the human behaviour of ``getting closer
to confirm" when distinguishing objects from a distance.
  Specifically, we design a new modular navigation framework named
Instance-aware Exploration-Verification-Exploitation (IEVE) for instance-level
image goal navigation.
  Our method allows for active switching among the exploration, verification,
and exploitation actions, thereby facilitating the agent in making reasonable
decisions under different situations.
  On the challenging HabitatMatterport 3D semantic (HM3D-SEM) dataset, our
method surpasses previous state-of-the-art work, with a classical segmentation
model (0.684 vs. 0.561 success) or a robust model (0.702 vs. 0.561 success)
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Global <span class="highlight-title">LiDAR</span> Localization: Challenges, Advances and Open
  Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.07433v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.07433v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huan Yin, Xuecheng Xu, Sha Lu, Xieyuanli Chen, Rong Xiong, Shaojie Shen, Cyrill Stachniss, Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge about the own pose is key for all mobile robot applications. Thus
pose estimation is part of the core functionalities of mobile robots. Over the
last two decades, LiDAR scanners have become the standard sensor for robot
localization and mapping. This article aims to provide an overview of recent
progress and advancements in LiDAR-based global localization. We begin by
formulating the problem and exploring the application scope. We then present a
review of the methodology, including recent advancements in several topics,
such as maps, descriptor extraction, and cross-robot localization. The contents
of the article are organized under three themes. The first theme concerns the
combination of global place retrieval and local pose estimation. The second
theme is upgrading single-shot measurements to sequential ones for sequential
global localization. Finally, the third theme focuses on extending single-robot
global localization to cross-robot localization in multi-robot systems. We
conclude the survey with a discussion of open challenges and promising
directions in global LiDAR localization. To our best knowledge, this is the
first comprehensive survey on global LiDAR localization for mobile robots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Publishe on International Journal of Computer Vision (IJCV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kinematic Modularity of Elementary Dynamic Actions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15271v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15271v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moses C. Nah, Johannes Lachner, Federico Tessari, Neville Hogan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a kinematically modular approach to robot control is
presented. The method involves structures called Elementary Dynamic Actions and
a network model combining these elements. With this control framework, a rich
repertoire of movements can be generated by combination of basic modules. The
problems of solving inverse kinematics, managing kinematic singularity and
kinematic redundancy are avoided. The modular approach is robust against
contact and physical interaction, which makes it particularly effective for
contact-rich manipulation. Each kinematic module can be learned by Imitation
Learning, thereby resulting in a modular learning strategy for robot control.
The theoretical foundations and their real robot implementation are presented.
Using a KUKA LBR iiwa14 robot, three tasks were considered: (1) generating a
sequence of discrete movements, (2) generating a combination of discrete and
rhythmic movements, and (3) a drawing and erasing task. The results obtained
indicate that this modular approach has the potential to simplify the
generation of a diverse range of robot actions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Hierarchical Control For Multi-Agent Capacity-Constrained
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14545v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14545v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charlott Vallon, Alessandro Pinto, Bartolomeo Stellato, Francesco Borrelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel data-driven hierarchical control scheme for
managing a fleet of nonlinear, capacity-constrained autonomous agents in an
iterative environment. We propose a control framework consisting of a
high-level dynamic task assignment and routing layer and low-level motion
planning and tracking layer. Each layer of the control hierarchy uses a
data-driven Model Predictive Control (MPC) policy, maintaining bounded
computational complexity at each calculation of a new task assignment or
actuation input. We utilize collected data to iteratively refine estimates of
agent capacity usage, and update MPC policy parameters accordingly. Our
approach leverages tools from iterative learning control to integrate learning
at both levels of the hierarchy, and coordinates learning between levels in
order to maintain closed-loop feasibility and performance improvement of the
connected architecture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ i<span class="highlight-title">SLAM</span>: Imperative <span class="highlight-title">SLAM</span> <span class="chip">RA-L</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07894v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07894v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taimeng Fu, Shaoshu Su, Yiren Lu, Chen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous Localization and Mapping (SLAM) stands as one of the critical
challenges in robot navigation. A SLAM system often consists of a front-end
component for motion estimation and a back-end system for eliminating
estimation drifts. Recent advancements suggest that data-driven methods are
highly effective for front-end tasks, while geometry-based methods continue to
be essential in the back-end processes. However, such a decoupled paradigm
between the data-driven front-end and geometry-based back-end can lead to
sub-optimal performance, consequently reducing the system's capabilities and
generalization potential. To solve this problem, we proposed a novel
self-supervised imperative learning framework, named imperative SLAM (iSLAM),
which fosters reciprocal correction between the front-end and back-end, thus
enhancing performance without necessitating any external supervision.
Specifically, we formulate the SLAM problem as a bilevel optimization so that
the front-end and back-end are bidirectionally connected. As a result, the
front-end model can learn global geometric knowledge obtained through pose
graph optimization by back-propagating the residuals from the back-end
component. We showcase the effectiveness of this new framework through an
application of stereo-inertial SLAM. The experiments show that the iSLAM
training strategy achieves an accuracy improvement of 22% on average over a
baseline model. To the best of our knowledge, iSLAM is the first SLAM system
showing that the front-end and back-end components can mutually correct each
other in a self-supervised manner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted by IEEE Robotics and Automation Letters
  (RA-L)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoTAMP: Autoregressive Task and Motion Planning with LLMs as
  Translators and Checkers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06531v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06531v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchao Chen, Jacob Arkin, Charles Dawson, Yang Zhang, Nicholas Roy, Chuchu Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For effective human-robot interaction, robots need to understand, plan, and
execute complex, long-horizon tasks described by natural language. Recent
advances in large language models (LLMs) have shown promise for translating
natural language into robot action sequences for complex tasks. However,
existing approaches either translate the natural language directly into robot
trajectories or factor the inference process by decomposing language into task
sub-goals and relying on a motion planner to execute each sub-goal. When
complex environmental and temporal constraints are involved, inference over
planning tasks must be performed jointly with motion plans using traditional
task-and-motion planning (TAMP) algorithms, making factorization into subgoals
untenable. Rather than using LLMs to directly plan task sub-goals, we instead
perform few-shot translation from natural language task descriptions to an
intermediate task representation that can then be consumed by a TAMP algorithm
to jointly solve the task and motion plan. To improve translation, we
automatically detect and correct both syntactic and semantic errors via
autoregressive re-prompting, resulting in significant improvements in task
completion. We show that our approach outperforms several methods using LLMs as
planners in complex task domains. See our project website
https://yongchao98.github.io/MIT-REALM-AutoTAMP/ for prompts, videos, and code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Multi-Robot Collaboration with Large Language Models:
  Centralized or Decentralized Systems? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchao Chen, Jacob Arkin, Yang Zhang, Nicholas Roy, Chuchu Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A flurry of recent work has demonstrated that pre-trained large language
models (LLMs) can be effective task planners for a variety of single-robot
tasks. The planning performance of LLMs is significantly improved via prompting
techniques, such as in-context learning or re-prompting with state feedback,
placing new importance on the token budget for the context window. An
under-explored but natural next direction is to investigate LLMs as multi-robot
task planners. However, long-horizon, heterogeneous multi-robot planning
introduces new challenges of coordination while also pushing up against the
limits of context window length. It is therefore critical to find
token-efficient LLM planning frameworks that are also able to reason about the
complexities of multi-robot coordination. In this work, we compare the task
success rate and token efficiency of four multi-agent communication frameworks
(centralized, decentralized, and two hybrid) as applied to four
coordination-dependent multi-agent 2D task scenarios for increasing numbers of
agents. We find that a hybrid framework achieves better task success rates
across all four tasks and scales better to more agents. We further demonstrate
the hybrid frameworks in 3D simulations where the vision-to-text problem and
dynamical errors are considered. See our project website
https://yongchao98.github.io/MIT-REALM-Multi-Robot/ for prompts, videos, and
code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Complex Motion Plans using Neural ODEs with Safety and
  Stability Guarantees <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.00186v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.00186v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farhad Nawaz, Tianyu Li, Nikolai Matni, Nadia Figueroa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a Dynamical System (DS) approach to learn complex, possibly
periodic motion plans from kinesthetic demonstrations using Neural Ordinary
Differential Equations (NODE). To ensure reactivity and robustness to
disturbances, we propose a novel approach that selects a target point at each
time step for the robot to follow, by combining tools from control theory and
the target trajectory generated by the learned NODE. A correction term to the
NODE model is computed online by solving a quadratic program that guarantees
stability and safety using control Lyapunov functions and control barrier
functions, respectively. Our approach outperforms baseline DS learning
techniques on the LASA handwriting dataset and complex periodic trajectories.
It is also validated on the Franka Emika robot arm to produce stable motions
for wiping and stirring tasks that do not have a single attractor, while being
robust to perturbations and safe around humans and obstacles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Traffic Management Framework for On-Demand Urban Air Mobility Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07139v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07139v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milad Pooladsanj, Ketan Savla, Petros A. Ioannou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urban Air Mobility (UAM) offers a solution to current traffic congestion by
providing on-demand air mobility in urban areas. Effective traffic management
is crucial for efficient operation of UAM systems, especially for high-demand
scenarios. In this paper, we present a centralized traffic management framework
for on-demand UAM systems. Specifically, we provide a scheduling policy, called
VertiSync, which schedules the aircraft for either servicing trip requests or
rebalancing in the system subject to aircraft safety margins and energy
requirements. We characterize the system-level throughput of VertiSync, which
determines the demand threshold at which passenger waiting times transition
from being stabilized to being increasing over time. We show that the proposed
policy is able to maximize throughput for sufficiently large fleet sizes. We
demonstrate the performance of VertiSync through a case study for the city of
Los Angeles, and show that it significantly reduces passenger waiting times
compared to a first-come first-serve scheduling policy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kinematics-aware Trajectory Generation and Prediction with Latent
  Stochastic Differential Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruochen Jiao, Yixuan Wang, Xiangguo Liu, Chao Huang, Qi Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory generation and trajectory prediction are two critical tasks in
autonomous driving, which generate various trajectories for testing during
development and predict the trajectories of surrounding vehicles during
operation, respectively. In recent years, emerging data-driven deep
learning-based methods have shown great promise for these two tasks in learning
various traffic scenarios and improving average performance without assuming
physical models. However, it remains a challenging problem for these methods to
ensure that the generated/predicted trajectories are physically realistic. This
challenge arises because learning-based approaches often function as opaque
black boxes and do not adhere to physical laws. Conversely, existing
model-based methods provide physically feasible results but are constrained by
predefined model structures, limiting their capabilities to address complex
scenarios. To address the limitations of these two types of approaches, we
propose a new method that integrates kinematic knowledge into neural stochastic
differential equations (SDE) and designs a variational autoencoder based on
this latent kinematics-aware SDE (LK-SDE) to generate vehicle motions.
Experimental results demonstrate that our method significantly outperforms both
model-based and learning-based baselines in producing physically realistic and
precisely controllable vehicle trajectories. Additionally, it performs well in
predicting unobservable physical variables in the latent space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, conference paper in motion generation</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from
  Partially Annotated Data <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanrong Ye, Dan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been an increased interest in the practical problem of
learning multiple dense scene understanding tasks from partially annotated
data, where each training sample is only labeled for a subset of the tasks. The
missing of task labels in training leads to low-quality and noisy predictions,
as can be observed from state-of-the-art methods. To tackle this issue, we
reformulate the partially-labeled multi-task dense prediction as a pixel-level
denoising problem, and propose a novel multi-task denoising diffusion framework
coined as DiffusionMTL. It designs a joint diffusion and denoising paradigm to
model a potential noisy distribution in the task prediction or feature maps and
generate rectified outputs for different tasks. To exploit multi-task
consistency in denoising, we further introduce a Multi-Task Conditioning
strategy, which can implicitly utilize the complementary nature of the tasks to
help learn the unlabeled tasks, leading to an improvement in the denoising
performance of the different tasks. Extensive quantitative and qualitative
experiments demonstrate that the proposed multi-task denoising diffusion model
can significantly improve multi-task prediction maps, and outperform the
state-of-the-art methods on three challenging multi-task benchmarks, under two
different partial-labeling evaluation settings. The code is available at
https://prismformore.github.io/diffusionmtl/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, Yan Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Multimodal Models (LMMs) have shown significant reasoning capabilities
by connecting a visual encoder and a large language model. LMMs typically use a
fixed amount of visual tokens, such as the penultimate layer features in the
CLIP visual encoder, as the prefix content. Recent LMMs incorporate more
complex visual inputs, such as high-resolution images and videos, which
increase the number of visual tokens significantly. However, due to the design
of the Transformer architecture, computational costs associated with these
models tend to increase quadratically with the number of input tokens. To
tackle this problem, we explore a token reduction mechanism and find, similar
to prior work, that many visual tokens are spatially redundant. Based on this,
we propose PruMerge, a novel adaptive visual token reduction approach, which
largely reduces the number of visual tokens while maintaining comparable model
performance. We first select the unpruned visual tokens based on their
similarity to class tokens and spatial tokens. We then cluster the pruned
tokens based on key similarity and merge the clustered tokens with the unpruned
tokens to supplement their information. Empirically, when applied to LLaVA-1.5,
our approach can compress the visual tokens by 14.4 times on average, and
achieve comparable performance across diverse visual question-answering and
reasoning tasks. Code and checkpoints are at https://llava-prumerge.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://llava-prumerge.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Xie, Jonathan Lorraine, Tianshi Cao, Jun Gao, James Lucas, Antonio Torralba, Sanja Fidler, Xiaohui Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent text-to-3D generation approaches produce impressive 3D results but
require time-consuming optimization that can take up to an hour per prompt.
Amortized methods like ATT3D optimize multiple prompts simultaneously to
improve efficiency, enabling fast text-to-3D synthesis. However, they cannot
capture high-frequency geometry and texture details and struggle to scale to
large prompt sets, so they generalize poorly. We introduce LATTE3D, addressing
these limitations to achieve fast, high-quality generation on a significantly
larger prompt set. Key to our method is 1) building a scalable architecture and
2) leveraging 3D data during optimization through 3D-aware diffusion priors,
shape regularization, and model initialization to achieve robustness to diverse
and complex training prompts. LATTE3D amortizes both neural field and textured
surface generation to produce highly detailed textured meshes in a single
forward pass. LATTE3D generates 3D objects in 400ms, and can be further
enhanced with fast test-time optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>See the project website at
  https://research.nvidia.com/labs/toronto-ai/LATTE3D/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenwei Wang, Tengfei Wang, Gerhard Hancke, Ziwei Liu, Rynson W. H. Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world applications often require a large gallery of 3D assets that share
a consistent theme. While remarkable advances have been made in general 3D
content creation from text or image, synthesizing customized 3D assets
following the shared theme of input 3D exemplars remains an open and
challenging problem. In this work, we present ThemeStation, a novel approach
for theme-aware 3D-to-3D generation. ThemeStation synthesizes customized 3D
assets based on given few exemplars with two goals: 1) unity for generating 3D
assets that thematically align with the given exemplars and 2) diversity for
generating 3D assets with a high degree of variations. To this end, we design a
two-stage framework that draws a concept image first, followed by a
reference-informed 3D modeling stage. We propose a novel dual score
distillation (DSD) loss to jointly leverage priors from both the input
exemplars and the synthesized concept image. Extensive experiments and user
studies confirm that ThemeStation surpasses prior works in producing diverse
theme-aware 3D models with impressive quality. ThemeStation also enables
various applications such as controllable 3D-to-3D generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://3dthemestation.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DragAPart: Learning a Part-Level Motion Prior for Articulated Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruining Li, Chuanxia Zheng, Christian Rupprecht, Andrea Vedaldi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce DragAPart, a method that, given an image and a set of drags as
input, can generate a new image of the same object in a new state, compatible
with the action of the drags. Differently from prior works that focused on
repositioning objects, DragAPart predicts part-level interactions, such as
opening and closing a drawer. We study this problem as a proxy for learning a
generalist motion model, not restricted to a specific kinematic structure or
object category. To this end, we start from a pre-trained image generator and
fine-tune it on a new synthetic dataset, Drag-a-Move, which we introduce.
Combined with a new encoding for the drags and dataset randomization, the new
model generalizes well to real images and different categories. Compared to
prior motion-controlled generators, we demonstrate much better part-level
motion understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://dragapart.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-CLIP: Unlocking the Long-Text Capability of CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Jiaqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Image Pre-training (CLIP) has been the cornerstone for
zero-shot classification, text-image retrieval, and text-image generation by
aligning image and text modalities. Despite its widespread adoption, a
significant limitation of CLIP lies in the inadequate length of text input. The
length of the text token is restricted to 77, and an empirical study shows the
actual effective length is even less than 20. This prevents CLIP from handling
detailed descriptions, limiting its applications for image retrieval and
text-to-image generation with extensive prerequisites. To this end, we propose
Long-CLIP as a plug-and-play alternative to CLIP that supports long-text input,
retains or even surpasses its zero-shot generalizability, and aligns the CLIP
latent space, making it readily replace CLIP without any further adaptation in
downstream frameworks. Nevertheless, achieving this goal is far from
straightforward, as simplistic fine-tuning can result in a significant
degradation of CLIP's performance. Moreover, substituting the text encoder with
a language model supporting longer contexts necessitates pretraining with vast
amounts of data, incurring significant expenses. Accordingly, Long-CLIP
introduces an efficient fine-tuning solution on CLIP with two novel strategies
designed to maintain the original capabilities, including (1) a
knowledge-preserved stretching of positional embedding and (2) a primary
component matching of CLIP features. With leveraging just one million extra
long text-image pairs, Long-CLIP has shown the superiority to CLIP for about
20% in long caption text-image retrieval and 6% in traditional text-image
retrieval tasks, e.g., COCO and Flickr30k. Furthermore, Long-CLIP offers
enhanced capabilities for generating images from detailed text descriptions by
replacing CLIP in a plug-and-play manner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>All codes and models are publicly available at
  https://github.com/beichenzbc/Long-CLIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InternVideo2: Scaling Video Foundation Models for Multimodal Video
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, Limin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce InternVideo2, a new video foundation model (ViFM) that achieves
the state-of-the-art performance in action recognition, video-text tasks, and
video-centric dialogue. Our approach employs a progressive training paradigm
that unifies the different self- or weakly-supervised learning frameworks of
masked video token reconstruction, cross-modal contrastive learning, and next
token prediction. Different training stages would guide our model to capture
different levels of structure and semantic information through different
pretext tasks. At the data level, we prioritize the spatiotemporal consistency
by semantically segmenting videos and generating video-audio-speech captions.
This improves the alignment between video and text. We scale both data and
model size for our InternVideo2. Through extensive experiments, we validate our
designs and demonstrate the state-of-the-art performance on over 60 video and
audio tasks. Notably, our model outperforms others on various video-related
captioning, dialogue, and long video understanding benchmarks, highlighting its
ability to reason and comprehend long temporal contexts. Code and models are
available at https://github.com/OpenGVLab/InternVideo2/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>a technical report about video understanding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmented Reality based Simulated Data (ARSim) with multi-view
  consistency for AV perception networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aqeel Anwar, Tae Eun Choe, Zian Wang, Sanja Fidler, Minwoo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting a diverse range of objects under various driving scenarios is
essential for the effectiveness of autonomous driving systems. However, the
real-world data collected often lacks the necessary diversity presenting a
long-tail distribution. Although synthetic data has been utilized to overcome
this issue by generating virtual scenes, it faces hurdles such as a significant
domain gap and the substantial efforts required from 3D artists to create
realistic environments. To overcome these challenges, we present ARSim, a fully
automated, comprehensive, modular framework designed to enhance real multi-view
image data with 3D synthetic objects of interest. The proposed method
integrates domain adaptation and randomization strategies to address covariate
shift between real and simulated data by inferring essential domain attributes
from real data and employing simulation-based randomization for other
attributes. We construct a simplified virtual scene using real data and
strategically place 3D synthetic assets within it. Illumination is achieved by
estimating light distribution from multiple images capturing the surroundings
of the vehicle. Camera parameters from real data are employed to render
synthetic assets in each frame. The resulting augmented multi-view consistent
dataset is used to train a multi-camera perception network for autonomous
vehicles. Experimental results on various AV perception tasks demonstrate the
superior performance of networks trained on the augmented dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 15 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Topological Representations for Deep Image Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoling Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many scenarios, especially biomedical applications, the correct
delineation of complex fine-scaled structures such as neurons, tissues, and
vessels is critical for downstream analysis. Despite the strong predictive
power of deep learning methods, they do not provide a satisfactory
representation of these structures, thus creating significant barriers in
scalable annotation and downstream analysis. In this dissertation, we tackle
such challenges by proposing novel representations of these topological
structures in a deep learning framework. We leverage the mathematical tools
from topological data analysis, i.e., persistent homology and discrete Morse
theory, to develop principled methods for better segmentation and uncertainty
estimation, which will become powerful tools for scalable annotation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ph.D. thesis from Stony Brook University. This thesis includes works
  arXiv:1906.05404, arXiv:2110.08335, arXiv:2112.07812, arXiv:2103.09992,
  arXiv:2206.01742</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate
  Time series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Badri N. Patro, Vijay S. Agneeswaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have widely adopted attention networks for sequence mixing and
MLPs for channel mixing, playing a pivotal role in achieving breakthroughs
across domains. However, recent literature highlights issues with attention
networks, including low inductive bias and quadratic complexity concerning
input sequence length. State Space Models (SSMs) like S4 and others (Hippo,
Global Convolutions, liquid S4, LRU, Mega, and Mamba), have emerged to address
the above issues to help handle longer sequence lengths. Mamba, while being the
state-of-the-art SSM, has a stability issue when scaled to large networks for
computer vision datasets. We propose SiMBA, a new architecture that introduces
Einstein FFT (EinFFT) for channel modeling by specific eigenvalue computations
and uses the Mamba block for sequence modeling. Extensive performance studies
across image and time-series benchmarks demonstrate that SiMBA outperforms
existing SSMs, bridging the performance gap with state-of-the-art transformers.
Notably, SiMBA establishes itself as the new state-of-the-art SSM on ImageNet
and transfer learning benchmarks such as Stanford Car and Flower as well as
task learning benchmarks as well as seven time series benchmark datasets. The
project page is available on this website
~\url{https://github.com/badripatro/Simba}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Plasticity-Inspired Foundation Model for Observing the Earth
  Crossing Modalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhitong Xiong, Yi Wang, Fahong Zhang, Adam J. Stewart, Joëlle Hanna, Damian Borth, Ioannis Papoutsis, Bertrand Le Saux, Gustau Camps-Valls, Xiao Xiang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of foundation models has revolutionized our ability to
interpret the Earth's surface using satellite observational data. Traditional
models have been siloed, tailored to specific sensors or data types like
optical, radar, and hyperspectral, each with its own unique characteristics.
This specialization hinders the potential for a holistic analysis that could
benefit from the combined strengths of these diverse data sources. Our novel
approach introduces the Dynamic One-For-All (DOFA) model, leveraging the
concept of neural plasticity in brain science to integrate various data
modalities into a single framework adaptively. This dynamic hypernetwork,
adjusting to different wavelengths, enables a single versatile Transformer
jointly trained on data from five sensors to excel across 12 distinct Earth
observation tasks, including sensors never seen during pretraining. DOFA's
innovative design offers a promising leap towards more accurate, efficient, and
unified Earth observation analysis, showcasing remarkable adaptability and
performance in harnessing the potential of multimodal Earth observation data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fully automated workflow for the design of patient-specific orthopaedic
  implants: application to total knee arthroplasty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aziliz Guezou-Philippe, Arnaud Clavé, Ehouarn Maguet, Ludivine Maintier, Charles Garraud, Jean-Rassaire Fouefack, Valérie Burdin, Eric Stindel, Guillaume Dardenne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Arthroplasty is commonly performed to treat joint osteoarthritis, reducing
pain and improving mobility. While arthroplasty has known several technical
improvements, a significant share of patients are still unsatisfied with their
surgery. Personalised arthroplasty improves surgical outcomes however current
solutions require delays, making it difficult to integrate in clinical routine.
We propose a fully automated workflow to design patient-specific implants,
presented for total knee arthroplasty, the most widely performed arthroplasty
in the world nowadays.
  The proposed pipeline first uses artificial neural networks to segment the
proximal and distal extremities of the femur and tibia. Then the full bones are
reconstructed using augmented statistical shape models, combining shape and
landmarks information. Finally, 77 morphological parameters are computed to
design patient-specific implants. The developed workflow has been trained using
91 CT scans of lower limb and evaluated on 41 CT scans manually segmented, in
terms of accuracy and execution time.
  The workflow accuracy was $0.4\pm0.2mm$ for the segmentation, $1.2\pm0.4mm$
for the full bones reconstruction, and $2.8\pm2.2mm$ for the anatomical
landmarks determination. The custom implants fitted the patients' anatomy with
$0.6\pm0.2mm$ accuracy. The whole process from segmentation to implants' design
lasted about 5 minutes.
  The proposed workflow allows for a fast and reliable personalisation of knee
implants, directly from the patient CT image without requiring any manual
intervention. It establishes a patient-specific pre-operative planning for TKA
in a very short time making it easily available for all patients. Combined with
efficient implant manufacturing techniques, this solution could help answer the
growing number of arthroplasties while reducing complications and improving the
patients' satisfaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selectively Informative Description can Reduce Undesired Embedding
  Entanglements in Text-to-Image Personalization <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jimyeong Kim, Jungwon Park, Wonjong Rhee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In text-to-image personalization, a timely and crucial challenge is the
tendency of generated images overfitting to the biases present in the reference
images. We initiate our study with a comprehensive categorization of the biases
into background, nearby-object, tied-object, substance (in style
re-contextualization), and pose biases. These biases manifest in the generated
images due to their entanglement into the subject embedding. This undesired
embedding entanglement not only results in the reflection of biases from the
reference images into the generated images but also notably diminishes the
alignment of the generated images with the given generation prompt. To address
this challenge, we propose SID~(Selectively Informative Description), a text
description strategy that deviates from the prevalent approach of only
characterizing the subject's class identification. SID is generated utilizing
multimodal GPT-4 and can be seamlessly integrated into optimization-based
models. We present comprehensive experimental results along with analyses of
cross-attention maps, subject-alignment, non-subject-disentanglement, and
text-alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for
  Weakly Semi-supervised 3D Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzhi Gao, Zheng Chen, Zehui Chen, Lin Chen, Jiaming Liu, Shanghang Zhang, Feng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training high-accuracy 3D detectors necessitates massive labeled 3D
annotations with 7 degree-of-freedom, which is laborious and time-consuming.
Therefore, the form of point annotations is proposed to offer significant
prospects for practical applications in 3D detection, which is not only more
accessible and less expensive but also provides strong spatial information for
object localization.In this paper, we empirically discover that it is
non-trivial to merely adapt Point-DETR to its 3D form, encountering two main
bottlenecks: 1) it fails to encode strong 3D prior into the model, and 2) it
generates low-quality pseudo labels in distant regions due to the extreme
sparsity of LiDAR points. To overcome these challenges, we introduce
Point-DETR3D, a teacher-student framework for weakly semi-supervised 3D
detection, designed to fully capitalize on point-wise supervision within a
constrained instance-wise annotation budget.Different from Point-DETR which
encodes 3D positional information solely through a point encoder, we propose an
explicit positional query initialization strategy to enhance the positional
prior. Considering the low quality of pseudo labels at distant regions produced
by the teacher model, we enhance the detector's perception by incorporating
dense imagery data through a novel Cross-Modal Deformable RoI Fusion
(D-RoI).Moreover, an innovative point-guided self-supervised learning technique
is proposed to allow for fully exploiting point priors, even in student
models.Extensive experiments on representative nuScenes dataset demonstrate our
Point-DETR3D obtains significant improvements compared to previous works.
Notably, with only 5% of labeled data, Point-DETR3D achieves over 90%
performance of its fully supervised counterpart.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ultrasound Imaging based on the Variance of a Diffusion Restoration
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Zhang, Clément Huneau, Jérôme Idier, Diana Mateus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite today's prevalence of ultrasound imaging in medicine, ultrasound
signal-to-noise ratio is still affected by several sources of noise and
artefacts. Moreover, enhancing ultrasound image quality involves balancing
concurrent factors like contrast, resolution, and speckle preservation.
Recently, there has been progress in both model-based and learning-based
approaches addressing the problem of ultrasound image reconstruction. Bringing
the best from both worlds, we propose a hybrid reconstruction method combining
an ultrasound linear direct model with a learning-based prior coming from a
generative Denoising Diffusion model. More specifically, we rely on the
unsupervised fine-tuning of a pre-trained Denoising Diffusion Restoration Model
(DDRM). Given the nature of multiplicative noise inherent to ultrasound, this
paper proposes an empirical model to characterize the stochasticity of
diffusion reconstruction of ultrasound images, and shows the interest of its
variance as an echogenicity map estimator. We conduct experiments on synthetic,
in-vitro, and in-vivo data, demonstrating the efficacy of our variance imaging
approach in achieving high-quality image reconstructions from single plane-wave
acquisitions and in comparison to state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages; submitted to EUSIPCO 2024. arXiv admin note: text overlap
  with arXiv:2310.20618</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global Control for Local SO(3)-Equivariant Scale-Invariant Vessel
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patryk Rygiel, Dieuwertje Alblas, Christoph Brune, Kak Khee Yeung, Jelmer M. Wolterink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized 3D vascular models can aid in a range of diagnostic, prognostic,
and treatment-planning tasks relevant to cardiovascular disease management.
Deep learning provides a means to automatically obtain such models. Ideally, a
user should have control over the exact region of interest (ROI) to be included
in a vascular model, and the model should be watertight and highly accurate. To
this end, we propose a combination of a global controller leveraging voxel mask
segmentations to provide boundary conditions for vessels of interest to a
local, iterative vessel segmentation model. We introduce the preservation of
scale- and rotational symmetries in the local segmentation model, leading to
generalisation to vessels of unseen sizes and orientations. Combined with the
global controller, this enables flexible 3D vascular model building, without
additional retraining. We demonstrate the potential of our method on a dataset
containing abdominal aortic aneurysms (AAAs). Our method performs on par with a
state-of-the-art segmentation model in the segmentation of AAAs, iliac arteries
and renal arteries, while providing a watertight, smooth surface segmentation.
Moreover, we demonstrate that by adapting the global controller, we can easily
extend vessel sections in the 3D model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Baumann, Michael Baumgartner, Edoardo Ghignone, Jonas Kühne, Tobias Fischer, Yung-Hsu Yang, Marc Pollefeys, Michele Magno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate detection and tracking of surrounding objects is essential to enable
self-driving vehicles. While Light Detection and Ranging (LiDAR) sensors have
set the benchmark for high performance, the appeal of camera-only solutions
lies in their cost-effectiveness. Notably, despite the prevalent use of Radio
Detection and Ranging (RADAR) sensors in automotive systems, their potential in
3D detection and tracking has been largely disregarded due to data sparsity and
measurement noise. As a recent development, the combination of RADARs and
cameras is emerging as a promising solution. This paper presents Camera-RADAR
3D Detection and Tracking (CR3DT), a camera-RADAR fusion model for 3D object
detection, and Multi-Object Tracking (MOT). Building upon the foundations of
the State-of-the-Art (SotA) camera-only BEVDet architecture, CR3DT demonstrates
substantial improvements in both detection and tracking capabilities, by
incorporating the spatial and velocity information of the RADAR sensor.
Experimental results demonstrate an absolute improvement in detection
performance of 5.3% in mean Average Precision (mAP) and a 14.9% increase in
Average Multi-Object Tracking Accuracy (AMOTA) on the nuScenes dataset when
leveraging both modalities. CR3DT bridges the gap between high-performance and
cost-effective perception systems in autonomous driving, by capitalizing on the
ubiquitous presence of RADAR in automotive applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controlled Training Data Generation with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teresa Yeo, Andrei Atanov, Harold Benoit, Aleksandr Alekseev, Ruchira Ray, Pooya Esmaeil Akhoondi, Amir Zamir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a method to control a text-to-image generative model
to produce training data specifically "useful" for supervised learning. Unlike
previous works that employ an open-loop approach and pre-define prompts to
generate new data using either a language model or human expertise, we develop
an automated closed-loop system which involves two feedback mechanisms. The
first mechanism uses feedback from a given supervised model and finds
adversarial prompts that result in image generations that maximize the model
loss. While these adversarial prompts result in diverse data informed by the
model, they are not informed of the target distribution, which can be
inefficient. Therefore, we introduce the second feedback mechanism that guides
the generation process towards a certain target distribution. We call the
method combining these two mechanisms Guided Adversarial Prompts. We perform
our evaluations on different tasks, datasets and architectures, with different
types of distribution shifts (spuriously correlated data, unseen domains) and
demonstrate the efficiency of the proposed feedback mechanisms compared to
open-loop approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page at https://adversarial-prompts.epfl.ch/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WSCLoc: Weakly-Supervised Sparse-View Camera Relocalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialu Wang, Kaichen Zhou, Andrew Markham, Niki Trigoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the advancements in deep learning for camera relocalization tasks,
obtaining ground truth pose labels required for the training process remains a
costly endeavor. While current weakly supervised methods excel in lightweight
label generation, their performance notably declines in scenarios with sparse
views. In response to this challenge, we introduce WSCLoc, a system capable of
being customized to various deep learning-based relocalization models to
enhance their performance under weakly-supervised and sparse view conditions.
This is realized with two stages. In the initial stage, WSCLoc employs a
multilayer perceptron-based structure called WFT-NeRF to co-optimize image
reconstruction quality and initial pose information. To ensure a stable
learning process, we incorporate temporal information as input. Furthermore,
instead of optimizing SE(3), we opt for $\mathfrak{sim}(3)$ optimization to
explicitly enforce a scale constraint. In the second stage, we co-optimize the
pre-trained WFT-NeRF and WFT-Pose. This optimization is enhanced by
Time-Encoding based Random View Synthesis and supervised by inter-frame
geometric constraints that consider pose, depth, and RGB information. We
validate our approaches on two publicly available datasets, one outdoor and one
indoor. Our experimental results demonstrate that our weakly-supervised
relocalization solutions achieve superior pose estimation accuracy in
sparse-view scenarios, comparable to state-of-the-art camera relocalization
methods. We will make our code publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyperbolic Metric Learning for Visual Outlier Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alvaro Gonzalez-Jimenez, Simone Lionetti, Dena Bazazian, Philippe Gottfrois, Fabian Gröger, Marc Pouly, Alexander Navarini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-Of-Distribution (OOD) detection is critical to deploy deep learning
models in safety-critical applications. However, the inherent hierarchical
concept structure of visual data, which is instrumental to OOD detection, is
often poorly captured by conventional methods based on Euclidean geometry. This
work proposes a metric framework that leverages the strengths of Hyperbolic
geometry for OOD detection. Inspired by previous works that refine the decision
boundary for OOD data with synthetic outliers, we extend this method to
Hyperbolic space. Interestingly, we find that synthetic outliers do not benefit
OOD detection in Hyperbolic space as they do in Euclidean space. Furthermore we
explore the relationship between OOD detection performance and Hyperbolic
embedding dimension, addressing practical concerns in resource-constrained
environments. Extensive experiments show that our framework improves the FPR95
for OOD detection from 22\% to 15\% and from 49% to 28% on CIFAR-10 and
CIFAR-100 respectively compared to Euclidean methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spectral Motion Alignment for Video Motion Transfer using Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geon Yeong Park, Hyeonho Jeong, Sang Wan Lee, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of diffusion models has greatly impacted video generation and
understanding. Particularly, text-to-video diffusion models (VDMs) have
significantly facilitated the customization of input video with target
appearance, motion, etc. Despite these advances, challenges persist in
accurately distilling motion information from video frames. While existing
works leverage the consecutive frame residual as the target motion vector, they
inherently lack global motion context and are vulnerable to frame-wise
distortions. To address this, we present Spectral Motion Alignment (SMA), a
novel framework that refines and aligns motion vectors using Fourier and
wavelet transforms. SMA learns motion patterns by incorporating
frequency-domain regularization, facilitating the learning of whole-frame
global motion dynamics, and mitigating spatial artifacts. Extensive experiments
demonstrate SMA's efficacy in improving motion transfer while maintaining
computational efficiency and compatibility across various video customization
frameworks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page:
  https://geonyeong-park.github.io/spectral-motion-alignment/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Supervised Backbone Framework for Diverse Agricultural Vision Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sudhir Sornapudi, Rajhans Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer vision in agriculture is game-changing with its ability to transform
farming into a data-driven, precise, and sustainable industry. Deep learning
has empowered agriculture vision to analyze vast, complex visual data, but
heavily rely on the availability of large annotated datasets. This remains a
bottleneck as manual labeling is error-prone, time-consuming, and expensive.
The lack of efficient labeling approaches inspired us to consider
self-supervised learning as a paradigm shift, learning meaningful feature
representations from raw agricultural image data. In this work, we explore how
self-supervised representation learning unlocks the potential applicability to
diverse agriculture vision tasks by eliminating the need for large-scale
annotated datasets. We propose a lightweight framework utilizing SimCLR, a
contrastive learning approach, to pre-train a ResNet-50 backbone on a large,
unannotated dataset of real-world agriculture field images. Our experimental
analysis and results indicate that the model learns robust features applicable
to a broad range of downstream agriculture tasks discussed in the paper.
Additionally, the reduced reliance on annotated data makes our approach more
cost-effective and accessible, paving the way for broader adoption of computer
vision in agriculture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasoning-Enhanced Object-Centric Learning for Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Li, Pu Ren, Yang Liu, Hao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-centric learning aims to break down complex visual scenes into more
manageable object representations, enhancing the understanding and reasoning
abilities of machine learning systems toward the physical world. Recently,
slot-based video models have demonstrated remarkable proficiency in segmenting
and tracking objects, but they overlook the importance of the effective
reasoning module. In the real world, reasoning and predictive abilities play a
crucial role in human perception and object tracking; in particular, these
abilities are closely related to human intuitive physics. Inspired by this, we
designed a novel reasoning module called the Slot-based Time-Space Transformer
with Memory buffer (STATM) to enhance the model's perception ability in complex
scenes. The memory buffer primarily serves as storage for slot information from
upstream modules, the Slot-based Time-Space Transformer makes predictions
through slot-based spatiotemporal attention computations and fusion. Our
experiment results on various datasets show that STATM can significantly
enhance object-centric learning capabilities of slot-based video models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IS-Fusion: Instance-Scene Collaborative Fusion for Multimodal 3D Object
  Detection <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junbo Yin, Jianbing Shen, Runnan Chen, Wei Li, Ruigang Yang, Pascal Frossard, Wenguan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bird's eye view (BEV) representation has emerged as a dominant solution for
describing 3D space in autonomous driving scenarios. However, objects in the
BEV representation typically exhibit small sizes, and the associated point
cloud context is inherently sparse, which leads to great challenges for
reliable 3D perception. In this paper, we propose IS-Fusion, an innovative
multimodal fusion framework that jointly captures the Instance- and Scene-level
contextual information. IS-Fusion essentially differs from existing approaches
that only focus on the BEV scene-level fusion by explicitly incorporating
instance-level multimodal information, thus facilitating the instance-centric
tasks like 3D object detection. It comprises a Hierarchical Scene Fusion (HSF)
module and an Instance-Guided Fusion (IGF) module. HSF applies Point-to-Grid
and Grid-to-Region transformers to capture the multimodal scene context at
different granularities. IGF mines instance candidates, explores their
relationships, and aggregates the local multimodal context for each instance.
These instances then serve as guidance to enhance the scene feature and yield
an instance-aware BEV representation. On the challenging nuScenes benchmark,
IS-Fusion outperforms all the published multimodal works to date. Code is
available at: https://github.com/yinjunbo/IS-Fusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024; Code: https://github.com/yinjunbo/IS-Fusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WEEP: A method for spatial interpretation of weakly supervised CNN
  models in computational pathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Sharma, Bojing Liu, Mattias Rantalainen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning enables the modelling of high-resolution histopathology
whole-slide images (WSI). Weakly supervised learning of tile-level data is
typically applied for tasks where labels only exist on the patient or WSI level
(e.g. patient outcomes or histological grading). In this context, there is a
need for improved spatial interpretability of predictions from such models. We
propose a novel method, Wsi rEgion sElection aPproach (WEEP), for model
interpretation. It provides a principled yet straightforward way to establish
the spatial area of WSI required for assigning a particular prediction label.
We demonstrate WEEP on a binary classification task in the area of breast
cancer computational pathology. WEEP is easy to implement, is directly
connected to the model-based decision process, and offers information relevant
to both research and diagnostic applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shadow Generation for Composite Image Using Diffusion model <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyang Liu, Junqi You, Jianting Wang, Xinhao Tao, Bo Zhang, Li Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of image composition, generating realistic shadow for the
inserted foreground remains a formidable challenge. Previous works have
developed image-to-image translation models which are trained on paired
training data. However, they are struggling to generate shadows with accurate
shapes and intensities, hindered by data scarcity and inherent task complexity.
In this paper, we resort to foundation model with rich prior knowledge of
natural shadow images. Specifically, we first adapt ControlNet to our task and
then propose intensity modulation modules to improve the shadow intensity.
Moreover, we extend the small-scale DESOBA dataset to DESOBAv2 using a novel
data acquisition pipeline. Experimental results on both DESOBA and DESOBAv2
datasets as well as real composite images demonstrate the superior capability
of our model for shadow generation task. The dataset, code, and model are
released at https://github.com/bcmi/Object-Shadow-Generation-Dataset-DESOBAv2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LeGO: Leveraging a Surface Deformation Network for Animatable Stylized
  Face Generation with One Example 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soyeon Yoon, Kwan Yun, Kwanggyoon Seo, Sihun Cha, Jung Eun Yoo, Junyong Noh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in 3D face stylization have made significant strides in few
to zero-shot settings. However, the degree of stylization achieved by existing
methods is often not sufficient for practical applications because they are
mostly based on statistical 3D Morphable Models (3DMM) with limited variations.
To this end, we propose a method that can produce a highly stylized 3D face
model with desired topology. Our methods train a surface deformation network
with 3DMM and translate its domain to the target style using a paired exemplar.
The network achieves stylization of the 3D face mesh by mimicking the style of
the target using a differentiable renderer and directional CLIP losses.
Additionally, during the inference process, we utilize a Mesh Agnostic Encoder
(MAGE) that takes deformation target, a mesh of diverse topologies as input to
the stylization process and encodes its shape into our latent space. The
resulting stylized face model can be animated by commonly used 3DMM blend
shapes. A set of quantitative and qualitative evaluations demonstrate that our
method can produce highly stylized face meshes according to a given style and
output them in a desired topology. We also demonstrate example applications of
our method including image-based stylized avatar generation, linear
interpolation of geometric styles, and facial animation of stylized avatars.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment
  Anything Model for Crowd-Sourcing Medical Image Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Kulkarni, Adway Kanhere, Dharmam Savani, Andrew Chan, Devina Chatterjee, Paul H. Yi, Vishwa S. Parekh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Curating annotations for medical image segmentation is a labor-intensive and
time-consuming task that requires domain expertise, resulting in "narrowly"
focused deep learning (DL) models with limited translational utility. Recently,
foundation models like the Segment Anything Model (SAM) have revolutionized
semantic segmentation with exceptional zero-shot generalizability across
various domains, including medical imaging, and hold a lot of promise for
streamlining the annotation process. However, SAM has yet to be evaluated in a
crowd-sourced setting to curate annotations for training 3D DL segmentation
models. In this work, we explore the potential of SAM for crowd-sourcing
"sparse" annotations from non-experts to generate "dense" segmentation masks
for training 3D nnU-Net models, a state-of-the-art DL segmentation model. Our
results indicate that while SAM-generated annotations exhibit high mean Dice
scores compared to ground-truth annotations, nnU-Net models trained on
SAM-generated annotations perform significantly worse than nnU-Net models
trained on ground-truth annotations ($p<0.001$, all).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GCN-DevLSTM: Path Development for Skeleton-Based Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Jiang, Weixin Yang, Xin Zhang, Hao Ni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skeleton-based action recognition (SAR) in videos is an important but
challenging task in computer vision. The recent state-of-the-art models for SAR
are primarily based on graph convolutional neural networks (GCNs), which are
powerful in extracting the spatial information of skeleton data. However, it is
yet clear that such GCN-based models can effectively capture the temporal
dynamics of human action sequences. To this end, we propose the DevLSTM module,
which exploits the path development -- a principled and parsimonious
representation for sequential data by leveraging the Lie group structure. The
path development, originated from Rough path theory, can effectively capture
the order of events in high-dimensional stream data with massive dimension
reduction and consequently enhance the LSTM module substantially. Our proposed
G-DevLSTM module can be conveniently plugged into the temporal graph,
complementing existing advanced GCN-based models. Our empirical studies on the
NTU60, NTU120 and Chalearn2013 datasets demonstrate that our proposed hybrid
model significantly outperforms the current best-performing methods in SAR
tasks. The code is available at https://github.com/DeepIntoStreams/GCN-DevLSTM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSCoTDet: Language-driven Multi-modal Fusion for Improved Multispectral
  Pedestrian Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taeheon Kim, Sangyun Chung, Damin Yeom, Youngjoon Yu, Hak Gu Kim, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multispectral pedestrian detection is attractive for around-the-clock
applications due to the complementary information between RGB and thermal
modalities. However, current models often fail to detect pedestrians in obvious
cases, especially due to the modality bias learned from statistically biased
datasets. From these problems, we anticipate that maybe understanding the
complementary information itself is difficult to achieve from vision-only
models. Accordingly, we propose a novel Multispectral Chain-of-Thought
Detection (MSCoTDet) framework, which incorporates Large Language Models (LLMs)
to understand the complementary information at the semantic level and further
enhance the fusion process. Specifically, we generate text descriptions of the
pedestrian in each RGB and thermal modality and design a Multispectral
Chain-of-Thought (MSCoT) prompting, which models a step-by-step process to
facilitate cross-modal reasoning at the semantic level and perform accurate
detection. Moreover, we design a Language-driven Multi-modal Fusion (LMF)
strategy that enables fusing vision-driven and language-driven detections.
Extensive experiments validate that MSCoTDet improves multispectral pedestrian
detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DITTO: Demonstration Imitation by Trajectory Transformation <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Heppert, Max Argus, Tim Welschehold, Thomas Brox, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Teaching robots new skills quickly and conveniently is crucial for the
broader adoption of robotic systems. In this work, we address the problem of
one-shot imitation from a single human demonstration, given by an RGB-D video
recording through a two-stage process. In the first stage which is offline, we
extract the trajectory of the demonstration. This entails segmenting
manipulated objects and determining their relative motion in relation to
secondary objects such as containers. Subsequently, in the live online
trajectory generation stage, we first \mbox{re-detect} all objects, then we
warp the demonstration trajectory to the current scene, and finally, we trace
the trajectory with the robot. To complete these steps, our method makes
leverages several ancillary models, including those for segmentation, relative
object pose estimation, and grasp prediction. We systematically evaluate
different combinations of correspondence and re-detection methods to validate
our design decision across a diverse range of tasks. Specifically, we collect
demonstrations of ten different tasks including pick-and-place tasks as well as
articulated object manipulation. Finally, we perform extensive evaluations on a
real robot system to demonstrate the effectiveness and utility of our approach
in real-world scenarios. We make the code publicly available at
http://ditto.cs.uni-freiburg.de.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, 3 tables, submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Your Image is My Video: Reshaping the Receptive Field via Image-To-Video
  Differentiable AutoAugmentation and Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sofia Casarin, Cynthia I. Ugwu, Sergio Escalera, Oswald Lanz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The landscape of deep learning research is moving towards innovative
strategies to harness the true potential of data. Traditionally, emphasis has
been on scaling model architectures, resulting in large and complex neural
networks, which can be difficult to train with limited computational resources.
However, independently of the model size, data quality (i.e. amount and
variability) is still a major factor that affects model generalization. In this
work, we propose a novel technique to exploit available data through the use of
automatic data augmentation for the tasks of image classification and semantic
segmentation. We introduce the first Differentiable Augmentation Search method
(DAS) to generate variations of images that can be processed as videos.
Compared to previous approaches, DAS is extremely fast and flexible, allowing
the search on very large search spaces in less than a GPU day. Our intuition is
that the increased receptive field in the temporal dimension provided by DAS
could lead to benefits also to the spatial receptive field. More specifically,
we leverage DAS to guide the reshaping of the spatial receptive field by
selecting task-dependant transformations. As a result, compared to standard
augmentation alternatives, we improve in terms of accuracy on ImageNet,
Cifar10, Cifar100, Tiny-ImageNet, Pascal-VOC-2012 and CityScapes datasets when
plugging-in our DAS over different light-weight video backbones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SFOD: Spiking Fusion Object Detector <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yimeng Fan, Wei Zhang, Changsong Liu, Mingyang Li, Wenrui Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras, characterized by high temporal resolution, high dynamic range,
low power consumption, and high pixel bandwidth, offer unique capabilities for
object detection in specialized contexts. Despite these advantages, the
inherent sparsity and asynchrony of event data pose challenges to existing
object detection algorithms. Spiking Neural Networks (SNNs), inspired by the
way the human brain codes and processes information, offer a potential solution
to these difficulties. However, their performance in object detection using
event cameras is limited in current implementations. In this paper, we propose
the Spiking Fusion Object Detector (SFOD), a simple and efficient approach to
SNN-based object detection. Specifically, we design a Spiking Fusion Module,
achieving the first-time fusion of feature maps from different scales in SNNs
applied to event cameras. Additionally, through integrating our analysis and
experiments conducted during the pretraining of the backbone network on the
NCAR dataset, we delve deeply into the impact of spiking decoding strategies
and loss functions on model performance. Thereby, we establish state-of-the-art
classification results based on SNNs, achieving 93.7\% accuracy on the NCAR
dataset. Experimental results on the GEN1 detection dataset demonstrate that
the SFOD achieves a state-of-the-art mAP of 32.1\%, outperforming existing
SNN-based approaches. Our research not only underscores the potential of SNNs
in object detection with event cameras but also propels the advancement of
SNNs. Code is available at https://github.com/yimeng-fan/SFOD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PDE-CNNs: Axiomatic Derivations and Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gijs Bellaard, Sei Sakata, Bart M. N. Smets, Remco Duits
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PDE-based Group Convolutional Neural Networks (PDE-G-CNNs) utilize solvers of
geometrically meaningful evolution PDEs as substitutes for the conventional
components in G-CNNs. PDE-G-CNNs offer several key benefits all at once: fewer
parameters, inherent equivariance, better performance, data efficiency, and
geometric interpretability. In this article we focus on Euclidean equivariant
PDE-G-CNNs where the feature maps are two dimensional throughout. We call this
variant of the framework a PDE-CNN. We list several practically desirable
axioms and derive from these which PDEs should be used in a PDE-CNN. Here our
approach to geometric learning via PDEs is inspired by the axioms of classical
linear and morphological scale-space theory, which we generalize by introducing
semifield-valued signals. Furthermore, we experimentally confirm for small
networks that PDE-CNNs offer fewer parameters, better performance, and data
efficiency in comparison to CNNs. We also investigate what effect the use of
different semifields has on the performance of the models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LSK3DNet: Towards Effective and Efficient 3D Perception with Large
  Sparse Kernels <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuo Feng, Wenguan Wang, Fan Ma, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous systems need to process large-scale, sparse, and irregular point
clouds with limited compute resources. Consequently, it is essential to develop
LiDAR perception methods that are both efficient and effective. Although
naively enlarging 3D kernel size can enhance performance, it will also lead to
a cubically-increasing overhead. Therefore, it is crucial to develop
streamlined 3D large kernel designs that eliminate redundant weights and work
effectively with larger kernels. In this paper, we propose an efficient and
effective Large Sparse Kernel 3D Neural Network (LSK3DNet) that leverages
dynamic pruning to amplify the 3D kernel size. Our method comprises two core
components: Spatial-wise Dynamic Sparsity (SDS) and Channel-wise Weight
Selection (CWS). SDS dynamically prunes and regrows volumetric weights from the
beginning to learn a large sparse 3D kernel. It not only boosts performance but
also significantly reduces model size and computational cost. Moreover, CWS
selects the most important channels for 3D convolution during training and
subsequently prunes the redundant channels to accelerate inference for 3D
vision tasks. We demonstrate the effectiveness of LSK3DNet on three benchmark
datasets and five tracks compared with classical models and large kernel
designs. Notably, LSK3DNet achieves the state-of-the-art performance on
SemanticKITTI (i.e., 75.6% on single-scan and 63.4% on multi-scan), with
roughly 40% model size reduction and 60% computing operations reduction
compared to the naive large 3D kernel model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024; Project page:
  https://github.com/FengZicai/LSK3DNet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastCAD: Real-Time CAD Retrieval and Alignment from Scans and Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Langer, Jihong Ju, Georgi Dikov, Gerhard Reitmayr, Mohsen Ghafoorian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digitising the 3D world into a clean, CAD model-based representation has
important applications for augmented reality and robotics. Current
state-of-the-art methods are computationally intensive as they individually
encode each detected object and optimise CAD alignments in a second stage. In
this work, we propose FastCAD, a real-time method that simultaneously retrieves
and aligns CAD models for all objects in a given scene. In contrast to previous
works, we directly predict alignment parameters and shape embeddings. We
achieve high-quality shape retrievals by learning CAD embeddings in a
contrastive learning framework and distilling those into FastCAD. Our
single-stage method accelerates the inference time by a factor of 50 compared
to other methods operating on RGB-D scans while outperforming them on the
challenging Scan2CAD alignment benchmark. Further, our approach collaborates
seamlessly with online 3D reconstruction techniques. This enables the real-time
generation of precise CAD model-based reconstructions from videos at 10 FPS.
Doing so, we significantly improve the Scan2CAD alignment accuracy in the video
setting from 43.0% to 48.2% and the reconstruction accuracy from 22.9% to
29.6%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Infrastructure-Assisted Collaborative Perception in Automated Valet
  Parking: A Safety Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukuan Jia, Jiawen Zhang, Shimeng Lu, Baokang Fan, Ruiqing Mao, Sheng Zhou, Zhisheng Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Environmental perception in Automated Valet Parking (AVP) has been a
challenging task due to severe occlusions in parking garages. Although
Collaborative Perception (CP) can be applied to broaden the field of view of
connected vehicles, the limited bandwidth of vehicular communications restricts
its application. In this work, we propose a BEV feature-based CP network
architecture for infrastructure-assisted AVP systems. The model takes the
roadside camera and LiDAR as optional inputs and adaptively fuses them with
onboard sensors in a unified BEV representation. Autoencoder and downsampling
are applied for channel-wise and spatial-wise dimension reduction, while
sparsification and quantization further compress the feature map with little
loss in data precision. Combining these techniques, the size of a BEV feature
map is effectively compressed to fit in the feasible data rate of the NR-V2X
network. With the synthetic AVP dataset, we observe that CP can effectively
increase perception performance, especially for pedestrians. Moreover, the
advantage of infrastructure-assisted CP is demonstrated in two typical
safety-critical scenarios in the AVP setting, increasing the maximum safe
cruising speed by up to 3m/s in both scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures, 4 tables, accepted by IEEE VTC2024-Spring</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multimodal Approach for Cross-Domain Image Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Iijima, Tania Stathaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image generators are gaining vast amount of popularity and have rapidly
changed how digital content is created. With the latest AI technology, millions
of high quality images are being generated by the public, which are constantly
motivating the research community to push the limits of generative models to
create more complex and realistic images. This paper focuses on Cross-Domain
Image Retrieval (CDIR) which can be used as an additional tool to inspect
collections of generated images by determining the level of similarity between
images in a dataset. An ideal retrieval system would be able to generalize to
unseen complex images from multiple domains (e.g., photos, drawings and
paintings). To address this goal, we propose a novel caption-matching approach
that leverages multimodal language-vision architectures pre-trained on large
datasets. The method is tested on DomainNet and Office-Home datasets and
consistently achieves state-of-the-art performance over the latest approaches
in the literature for cross-domain image retrieval. In order to verify the
effectiveness with AI-generated images, the method was also put to test with a
database composed by samples collected from Midjourney, which is a widely used
generative platform for content creation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An In-Depth Analysis of Data Reduction Methods for Sustainable Deep
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Víctor Toscano-Durán, Javier Perera-Lago, Eduardo Paluzo-Hidalgo, Rocío Gonzalez-Diaz, Miguel Ángel Gutierrez-Naranjo, Matteo Rucco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Deep Learning has gained popularity for its ability to solve
complex classification tasks, increasingly delivering better results thanks to
the development of more accurate models, the availability of huge volumes of
data and the improved computational capabilities of modern computers. However,
these improvements in performance also bring efficiency problems, related to
the storage of datasets and models, and to the waste of energy and time
involved in both the training and inference processes. In this context, data
reduction can help reduce energy consumption when training a deep learning
model. In this paper, we present up to eight different methods to reduce the
size of a tabular training dataset, and we develop a Python package to apply
them. We also introduce a representativeness metric based on topology to
measure how similar are the reduced datasets and the full training dataset.
Additionally, we develop a methodology to apply these data reduction methods to
image datasets for object detection tasks. Finally, we experimentally compare
how these data reduction methods affect the representativeness of the reduced
dataset, the energy consumption and the predictive performance of the model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modular Deep Active Learning Framework for Image Annotation: A Technical
  Report for the Ophthalmo-AI Project 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Abdul Kadir, Hasan Md Tusfiqur Alam, Pascale Maul, Hans-Jürgen Profitlich, Moritz Wolf, Daniel Sonntag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image annotation is one of the most essential tasks for guaranteeing proper
treatment for patients and tracking progress over the course of therapy in the
field of medical imaging and disease diagnosis. However, manually annotating a
lot of 2D and 3D imaging data can be extremely tedious. Deep Learning (DL)
based segmentation algorithms have completely transformed this process and made
it possible to automate image segmentation. By accurately segmenting medical
images, these algorithms can greatly minimize the time and effort necessary for
manual annotation. Additionally, by incorporating Active Learning (AL) methods,
these segmentation algorithms can perform far more effectively with a smaller
amount of ground truth data. We introduce MedDeepCyleAL, an end-to-end
framework implementing the complete AL cycle. It provides researchers with the
flexibility to choose the type of deep learning model they wish to employ and
includes an annotation tool that supports the classification and segmentation
of medical images. The user-friendly interface allows for easy alteration of
the AL and DL model settings through a configuration file, requiring no prior
programming experience. While MedDeepCyleAL can be applied to any kind of image
data, we have specifically applied it to ophthalmology data in this project.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>DFKI Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Generative Model based Rate-Distortion for Image Downscaling
  Assessment <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanbang Liang, Bhavesh Garg, Paul L Rosin, Yipeng Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose Image Downscaling Assessment by Rate-Distortion
(IDA-RD), a novel measure to quantitatively evaluate image downscaling
algorithms. In contrast to image-based methods that measure the quality of
downscaled images, ours is process-based that draws ideas from rate-distortion
theory to measure the distortion incurred during downscaling. Our main idea is
that downscaling and super-resolution (SR) can be viewed as the encoding and
decoding processes in the rate-distortion model, respectively, and that a
downscaling algorithm that preserves more details in the resulting
low-resolution (LR) images should lead to less distorted high-resolution (HR)
images in SR. In other words, the distortion should increase as the downscaling
algorithm deteriorates. However, it is non-trivial to measure this distortion
as it requires the SR algorithm to be blind and stochastic. Our key insight is
that such requirements can be met by recent SR algorithms based on deep
generative models that can find all matching HR images for a given LR image on
their learned image manifolds. Extensive experimental results show the
effectiveness of our IDA-RD measure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transfer CLIP for Generalizable Image Denoising <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Cheng, Dong Liang, Shan Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image denoising is a fundamental task in computer vision. While prevailing
deep learning-based supervised and self-supervised methods have excelled in
eliminating in-distribution noise, their susceptibility to out-of-distribution
(OOD) noise remains a significant challenge. The recent emergence of
contrastive language-image pre-training (CLIP) model has showcased exceptional
capabilities in open-world image recognition and segmentation. Yet, the
potential for leveraging CLIP to enhance the robustness of low-level tasks
remains largely unexplored. This paper uncovers that certain dense features
extracted from the frozen ResNet image encoder of CLIP exhibit
distortion-invariant and content-related properties, which are highly desirable
for generalizable denoising. Leveraging these properties, we devise an
asymmetrical encoder-decoder denoising network, which incorporates dense
features including the noisy image and its multi-scale features from the frozen
ResNet encoder of CLIP into a learnable image decoder to achieve generalizable
denoising. The progressive feature augmentation strategy is further proposed to
mitigate feature overfitting and improve the robustness of the learnable
decoder. Extensive experiments and comparisons conducted across diverse OOD
noises, including synthetic noise, real-world sRGB noise, and low-dose CT image
noise, demonstrate the superior generalization ability of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gradient-based Sampling for Class Imbalanced Semi-supervised Object
  Detection <span class="chip">ICCV2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Li, Xiangru Lin, Wei Zhang, Xiao Tan, Yingying Li, Junyu Han, Errui Ding, Jingdong Wang, Guanbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current semi-supervised object detection (SSOD) algorithms typically assume
class balanced datasets (PASCAL VOC etc.) or slightly class imbalanced datasets
(MS-COCO, etc). This assumption can be easily violated since real world
datasets can be extremely class imbalanced in nature, thus making the
performance of semi-supervised object detectors far from satisfactory. Besides,
the research for this problem in SSOD is severely under-explored. To bridge
this research gap, we comprehensively study the class imbalance problem for
SSOD under more challenging scenarios, thus forming the first experimental
setting for class imbalanced SSOD (CI-SSOD). Moreover, we propose a simple yet
effective gradient-based sampling framework that tackles the class imbalance
problem from the perspective of two types of confirmation biases. To tackle
confirmation bias towards majority classes, the gradient-based reweighting and
gradient-based thresholding modules leverage the gradients from each class to
fully balance the influence of the majority and minority classes. To tackle the
confirmation bias from incorrect pseudo labels of minority classes, the
class-rebalancing sampling module resamples unlabeled data following the
guidance of the gradient-based reweighting module. Experiments on three
proposed sub-tasks, namely MS-COCO, MS-COCO to Object365 and LVIS, suggest that
our method outperforms current class imbalanced object detectors by clear
margins, serving as a baseline for future research in CI-SSOD. Code will be
available at https://github.com/nightkeepers/CI-SSOD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EndoG<span class="highlight-title">SLAM</span>: Real-Time Dense Reconstruction and Tracking in Endoscopic
  Surgeries using Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kailing Wang, Chen Yang, Yuehao Wang, Sikuang Li, Yan Wang, Qi Dou, Xiaokang Yang, Wei Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise camera tracking, high-fidelity 3D tissue reconstruction, and
real-time online visualization are critical for intrabody medical imaging
devices such as endoscopes and capsule robots. However, existing SLAM
(Simultaneous Localization and Mapping) methods often struggle to achieve both
complete high-quality surgical field reconstruction and efficient computation,
restricting their intraoperative applications among endoscopic surgeries. In
this paper, we introduce EndoGSLAM, an efficient SLAM approach for endoscopic
surgeries, which integrates streamlined Gaussian representation and
differentiable rasterization to facilitate over 100 fps rendering speed during
online camera tracking and tissue reconstructing. Extensive experiments show
that EndoGSLAM achieves a better trade-off between intraoperative availability
and reconstruction quality than traditional or neural SLAM approaches, showing
tremendous potential for endoscopic surgeries. The project page is at
https://EndoGSLAM.loping151.com
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SYNCS: Synthetic Data and Contrastive Self-Supervised Training for
  Central Sulcus Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15121v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15121v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladyslav Zalevskyi, Kristoffer Hougaard Madsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bipolar disorder (BD) and schizophrenia (SZ) are severe mental disorders with
profound societal impact. Identifying risk markers early is crucial for
understanding disease progression and enabling preventive measures. The Danish
High Risk and Resilience Study (VIA) focuses on understanding early disease
processes, particularly in children with familial high risk (FHR).
Understanding structural brain changes associated with these diseases during
early stages is essential for effective interventions. The central sulcus (CS)
is a prominent brain landmark related to brain regions involved in motor and
sensory processing. Analyzing CS morphology can provide valuable insights into
neurodevelopmental abnormalities in the FHR group. However, segmenting the
central sulcus (CS) presents challenges due to its variability, especially in
adolescents. This study introduces two novel approaches to improve CS
segmentation: synthetic data generation to model CS variability and
self-supervised pre-training with multi-task learning to adapt models to new
cohorts. These methods aim to enhance segmentation performance across diverse
populations, eliminating the need for extensive preprocessing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Open-World, Diverse, Cross-Spatial-Temporal Benchmark for Dynamic
  Wild Person Re-Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Zhang, Xiaowei Fu, Fuxiang Huang, Yi Yang, Xinbo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Person re-identification (ReID) has made great strides thanks to the
data-driven deep learning techniques. However, the existing benchmark datasets
lack diversity, and models trained on these data cannot generalize well to
dynamic wild scenarios. To meet the goal of improving the explicit
generalization of ReID models, we develop a new Open-World, Diverse,
Cross-Spatial-Temporal dataset named OWD with several distinct features. 1)
Diverse collection scenes: multiple independent open-world and highly dynamic
collecting scenes, including streets, intersections, shopping malls, etc. 2)
Diverse lighting variations: long time spans from daytime to nighttime with
abundant illumination changes. 3) Diverse person status: multiple camera
networks in all seasons with normal/adverse weather conditions and diverse
pedestrian appearances (e.g., clothes, personal belongings, poses, etc.). 4)
Protected privacy: invisible faces for privacy critical applications. To
improve the implicit generalization of ReID, we further propose a Latent Domain
Expansion (LDE) method to develop the potential of source data, which decouples
discriminative identity-relevant and trustworthy domain-relevant features and
implicitly enforces domain-randomized identity feature space expansion with
richer domain diversity to facilitate domain invariant representations. Our
comprehensive evaluations with most benchmark datasets in the community are
crucial for progress, although this work is far from the grand goal toward
open-world and dynamic wild applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJCV in 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PseudoTouch: Efficiently Imaging the Surface Feel of Objects for Robotic
  Manipulation <span class="chip">IROS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Röfer, Nick Heppert, Abdallah Ayman, Eugenio Chisari, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans seemingly incorporate potential touch signals in their perception. Our
goal is to equip robots with a similar capability, which we term \ourmodel.
\ourmodel aims to predict the expected touch signal based on a visual patch
representing the touched area. We frame this problem as the task of learning a
low-dimensional visual-tactile embedding, wherein we encode a depth patch from
which we decode the tactile signal. To accomplish this task, we employ ReSkin,
an inexpensive and replaceable magnetic-based tactile sensor. Using ReSkin, we
collect and train PseudoTouch on a dataset comprising aligned tactile and
visual data pairs obtained through random touching of eight basic geometric
shapes. We demonstrate the efficacy of PseudoTouch through its application to
two downstream tasks: object recognition and grasp stability prediction. In the
object recognition task, we evaluate the learned embedding's performance on a
set of five basic geometric shapes and five household objects. Using
PseudoTouch, we achieve an object recognition accuracy 84% after just ten
touches, surpassing a proprioception baseline. For the grasp stability task, we
use ACRONYM labels to train and evaluate a grasp success predictor using
PseudoTouch's predictions derived from virtual depth information. Our approach
yields an impressive 32% absolute improvement in accuracy compared to the
baseline relying on partial point cloud data. We make the data, code, and
trained models publicly available at http://pseudotouch.cs.uni-freiburg.de.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, 2 tables, submitted to IROS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving cross-domain brain tissue segmentation in fetal MRI with
  synthetic data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladyslav Zalevskyi, Thomas Sanchez, Margaux Roulet, Jordina Aviles Verddera, Jana Hutter, Hamza Kebiri, Meritxell Bach Cuadra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation of fetal brain tissue from magnetic resonance imaging (MRI)
plays a crucial role in the study of in utero neurodevelopment. However,
automated tools face substantial domain shift challenges as they must be robust
to highly heterogeneous clinical data, often limited in numbers and lacking
annotations. Indeed, high variability of the fetal brain morphology, MRI
acquisition parameters, and superresolution reconstruction (SR) algorithms
adversely affect the model's performance when evaluated out-of-domain. In this
work, we introduce FetalSynthSeg, a domain randomization method to segment
fetal brain MRI, inspired by SynthSeg. Our results show that models trained
solely on synthetic data outperform models trained on real data in out-ofdomain
settings, validated on a 120-subject cross-domain dataset. Furthermore, we
extend our evaluation to 40 subjects acquired using lowfield (0.55T) MRI and
reconstructed with novel SR models, showcasing robustness across different
magnetic field strengths and SR algorithms. Leveraging a generative synthetic
approach, we tackle the domain shift problem in fetal brain MRI and offer
compelling prospects for applications in fields with limited and highly
heterogeneous data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lan Feng, Mohammadhossein Bahari, Kaouther Messaoud Ben Amor, Éloi Zablocki, Matthieu Cord, Alexandre Alahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicle trajectory prediction has increasingly relied on data-driven
solutions, but their ability to scale to different data domains and the impact
of larger dataset sizes on their generalization remain under-explored. While
these questions can be studied by employing multiple datasets, it is
challenging due to several discrepancies, \textit{e.g.,} in data formats, map
resolution, and semantic annotation types. To address these challenges, we
introduce UniTraj, a comprehensive framework that unifies various datasets,
models, and evaluation criteria, presenting new opportunities for the vehicle
trajectory prediction field. In particular, using UniTraj, we conduct extensive
experiments and find that model performance significantly drops when
transferred to other datasets. However, enlarging data size and diversity can
substantially improve performance, leading to a new state-of-the-art result for
the nuScenes dataset. We provide insights into dataset characteristics to
explain these findings. The code can be found here:
\hyperlink{https://github.com/vita-epfl/UniTraj}{https://github.com/vita-epfl/UniTraj}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IFSENet : Harnessing Sparse Iterations for Interactive Few-shot
  Segmentation Excellence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreyas Chandgothia, Ardhendu Sekhar, Amit Sethi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training a computer vision system to segment a novel class typically requires
collecting and painstakingly annotating lots of images with objects from that
class. Few-shot segmentation techniques reduce the required number of images to
learn to segment a new class, but careful annotations of object boundaries are
still required. On the other hand, interactive segmentation techniques only
focus on incrementally improving the segmentation of one object at a time
(typically, using clicks given by an expert) in a class-agnostic manner. We
combine the two concepts to drastically reduce the effort required to train
segmentation models for novel classes. Instead of trivially feeding interactive
segmentation masks as ground truth to a few-shot segmentation model, we propose
IFSENet, which can accept sparse supervision on a single or few support images
in the form of clicks to generate masks on support (training, at least clicked
upon once) as well as query (test, never clicked upon) images. To trade-off
effort for accuracy flexibly, the number of images and clicks can be
incrementally added to the support set to further improve the segmentation of
support as well as query images. The proposed model approaches the accuracy of
previous state-of-the-art few-shot segmentation models with considerably lower
annotation effort (clicks instead of maps), when tested on Pascal and SBD
datasets on query images. It also works well as an interactive segmentation
method on support images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cell Variational Information Bottleneck Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhonghua Zhai, Chen Ju, Jinsong Lan, Shuai Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose Cell Variational Information Bottleneck Network
(cellVIB), a convolutional neural network using information bottleneck
mechanism, which can be combined with the latest feedforward network
architecture in an end-to-end training method. Our Cell Variational Information
Bottleneck Network is constructed by stacking VIB cells, which generate feature
maps with uncertainty. As layers going deeper, the regularization effect will
gradually increase, instead of directly adding excessive regular constraints to
the output layer of the model as in Deep VIB. Under each VIB cell, the
feedforward process learns an independent mean term and an standard deviation
term, and predicts the Gaussian distribution based on them. The feedback
process is based on reparameterization trick for effective training. This work
performs an extensive analysis on MNIST dataset to verify the effectiveness of
each VIB cells, and provides an insightful analysis on how the VIB cells affect
mutual information. Experiments conducted on CIFAR-10 also prove that our
cellVIB is robust against noisy labels during training and against corrupted
images during testing. Then, we validate our method on PACS dataset, whose
results show that the VIB cells can significantly improve the generalization
performance of the basic model. Finally, in a more complex representation
learning task, face recognition, our network structure has also achieved very
competitive results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating multiscale topology in digital pathology with pyramidal
  graph convolutional networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Ibañez, Przemyslaw Szostak, Quincy Wong, Konstanty Korski, Samaneh Abbasi-Sureshjani, Alvaro Gomariz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph convolutional networks (GCNs) have emerged as a powerful alternative to
multiple instance learning with convolutional neural networks in digital
pathology, offering superior handling of structural information across various
spatial ranges - a crucial aspect of learning from gigapixel H&E-stained whole
slide images (WSI). However, graph message-passing algorithms often suffer from
oversmoothing when aggregating a large neighborhood. Hence, effective modeling
of multi-range interactions relies on the careful construction of the graph.
Our proposed multi-scale GCN (MS-GCN) tackles this issue by leveraging
information across multiple magnification levels in WSIs. MS-GCN enables the
simultaneous modeling of long-range structural dependencies at lower
magnifications and high-resolution cellular details at higher magnifications,
akin to analysis pipelines usually conducted by pathologists. The
architecture's unique configuration allows for the concurrent modeling of
structural patterns at lower magnifications and detailed cellular features at
higher ones, while also quantifying the contribution of each magnification
level to the prediction. Through testing on different datasets, MS-GCN
demonstrates superior performance over existing single-magnification GCN
methods. The enhancement in performance and interpretability afforded by our
method holds promise for advancing computational pathology models, especially
in tasks requiring extensive spatial context.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recent Trends in 3D Reconstruction of General Non-Rigid Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raza Yunus, Jan Eric Lenssen, Michael Niemeyer, Yiyi Liao, Christian Rupprecht, Christian Theobalt, Gerard Pons-Moll, Jia-Bin Huang, Vladislav Golyanik, Eddy Ilg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing models of the real world, including 3D geometry, appearance,
and motion of real scenes, is essential for computer graphics and computer
vision. It enables the synthesizing of photorealistic novel views, useful for
the movie industry and AR/VR applications. It also facilitates the content
creation necessary in computer games and AR/VR by avoiding laborious manual
design processes. Further, such models are fundamental for intelligent
computing systems that need to interpret real-world scenes and actions to act
and interact safely with the human world. Notably, the world surrounding us is
dynamic, and reconstructing models of dynamic, non-rigidly moving scenes is a
severely underconstrained and challenging problem. This state-of-the-art report
(STAR) offers the reader a comprehensive summary of state-of-the-art techniques
with monocular and multi-view inputs such as data from RGB and RGB-D sensors,
among others, conveying an understanding of different approaches, their
potential applications, and promising further research directions. The report
covers 3D reconstruction of general non-rigid scenes and further addresses the
techniques for scene decomposition, editing and controlling, and generalizable
and generative modeling. More specifically, we first review the common and
fundamental concepts necessary to understand and navigate the field and then
discuss the state-of-the-art techniques by reviewing recent approaches that use
traditional and machine-learning-based neural representations, including a
discussion on the newly enabled applications. The STAR is concluded with a
discussion of the remaining limitations and open challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 18 figures, 5 tables; State-of-the-Art Report at
  EUROGRAPHICS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Comprehensive, Efficient and Promptable Anatomic Structure
  Segmentation Model using 3D Whole-body CT Scans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Guo, Jianfeng Zhang, Jiaxing Huang, Tony C. W. Mok, Dazhou Guo, Ke Yan, Le Lu, Dakai Jin, Minfeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segment anything model (SAM) demonstrates strong generalization ability on
natural image segmentation. However, its direct adaption in medical image
segmentation tasks shows significant performance drops with inferior accuracy
and unstable results. It may also requires an excessive number of prompt points
to obtain a reasonable accuracy. For segmenting 3D radiological CT or MRI
scans, a 2D SAM model has to separately handle hundreds of 2D slices. Although
quite a few studies explore adapting SAM into medical image volumes, the
efficiency of 2D adaption methods is unsatisfactory and 3D adaptation methods
only capable of segmenting specific organs/tumors. In this work, we propose a
comprehensive and scalable 3D SAM model for whole-body CT segmentation, named
CT-SAM3D. Instead of adapting SAM, we propose a 3D promptable segmentation
model using a (nearly) fully labeled CT dataset. To train CT-SAM3D effectively,
ensuring the model's accurate responses to higher-dimensional spatial prompts
is crucial, and 3D patch-wise training is required due to GPU memory
constraints. For this purpose, we propose two key technical developments: 1) a
progressively and spatially aligned prompt encoding method to effectively
encode click prompts in local 3D space; and 2) a cross-patch prompt learning
scheme to capture more 3D spatial context, which is beneficial for reducing the
editing workloads when interactively prompting on large organs. CT-SAM3D is
trained and validated using a curated dataset of 1204 CT scans containing 107
whole-body anatomies, reporting significantly better quantitative performance
against all previous SAM-derived models by a large margin with much fewer click
prompts. Our model can handle segmenting unseen organ as well. Code, data, and
our 3D interactive segmentation tool with quasi-real-time responses will be
made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subjective Quality Assessment of Compressed Tone-Mapped High Dynamic
  Range Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15061v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15061v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinau K. Venkataramanan, Alan C. Bovik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High Dynamic Range (HDR) videos are able to represent wider ranges of
contrasts and colors than Standard Dynamic Range (SDR) videos, giving more
vivid experiences. Due to this, HDR videos are expected to grow into the
dominant video modality of the future. However, HDR videos are incompatible
with existing SDR displays, which form the majority of affordable consumer
displays on the market. Because of this, HDR videos must be processed by
tone-mapping them to reduced bit-depths to service a broad swath of SDR-limited
video consumers. Here, we analyze the impact of tone-mapping operators on the
visual quality of streaming HDR videos. To this end, we built the first
large-scale subjectively annotated open-source database of compressed
tone-mapped HDR videos, containing 15,000 tone-mapped sequences derived from 40
unique HDR source contents. The videos in the database were labeled with more
than 750,000 subjective quality annotations, collected from more than 1,600
unique human observers. We demonstrate the usefulness of the new subjective
database by benchmarking objective models of visual quality on it. We envision
that the new LIVE Tone-Mapped HDR (LIVE-TMHDR) database will enable significant
progress on HDR video tone mapping and quality assessment in the future. To
this end, we make the database freely available to the community at
https://live.ece.utexas.edu/research/LIVE_TMHDR/index.html
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MM-Diff: High-Fidelity Image Personalization via Multi-Modal Condition
  Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao Wei, Qingkun Su, Long Qin, Weizhi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in tuning-free personalized image generation based on
diffusion models are impressive. However, to improve subject fidelity, existing
methods either retrain the diffusion model or infuse it with dense visual
embeddings, both of which suffer from poor generalization and efficiency. Also,
these methods falter in multi-subject image generation due to the unconstrained
cross-attention mechanism. In this paper, we propose MM-Diff, a unified and
tuning-free image personalization framework capable of generating high-fidelity
images of both single and multiple subjects in seconds. Specifically, to
simultaneously enhance text consistency and subject fidelity, MM-Diff employs a
vision encoder to transform the input image into CLS and patch embeddings. CLS
embeddings are used on the one hand to augment the text embeddings, and on the
other hand together with patch embeddings to derive a small number of
detail-rich subject embeddings, both of which are efficiently integrated into
the diffusion model through the well-designed multimodal cross-attention
mechanism. Additionally, MM-Diff introduces cross-attention map constraints
during the training phase, ensuring flexible multi-subject image sampling
during inference without any predefined inputs (e.g., layout). Extensive
experiments demonstrate the superior performance of MM-Diff over other leading
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Vision-and-Language Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seongjun Jeong, Gi-Cheon Kang, Seongho Choi, Joochan Kim, Byoung-Tak Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-and-Language Navigation (VLN) agents navigate to a destination using
natural language instructions and the visual information they observe. Existing
methods for training VLN agents presuppose fixed datasets, leading to a
significant limitation: the introduction of new environments necessitates
retraining with previously encountered environments to preserve their
knowledge. This makes it difficult to train VLN agents that operate in the
ever-changing real world. To address this limitation, we present the Continual
Vision-and-Language Navigation (CVLN) paradigm, designed to evaluate agents
trained through a continual learning process. For the training and evaluation
of CVLN agents, we re-arrange existing VLN datasets to propose two datasets:
CVLN-I, focused on navigation via initial-instruction interpretation, and
CVLN-D, aimed at navigation through dialogue with other agents. Furthermore, we
propose two novel rehearsal-based methods for CVLN, Perplexity Replay (PerpR)
and Episodic Self-Replay (ESR). PerpR prioritizes replaying challenging
episodes based on action perplexity, while ESR replays previously predicted
action logits to preserve learned behaviors. We demonstrate the effectiveness
of the proposed methods on CVLN through extensive experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bumsoo Kim, Wonseop Shin, Kyuchul Lee, Sanghyun Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale Text-to-Image (TTI) models have become a common approach for
generating training data in various generative fields. However, visual
hallucinations, which contain perceptually critical defects, remain a concern,
especially in non-photorealistic styles like cartoon characters. We propose a
novel visual hallucination detection system for cartoon character images
generated by TTI models. Our approach leverages pose-aware in-context visual
learning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB
images and pose information. By incorporating pose guidance from a fine-tuned
pose estimator, we enable VLMs to make more accurate decisions. Experimental
results demonstrate significant improvements in identifying visual
hallucinations compared to baseline methods relying solely on RGB images. This
research advances TTI models by mitigating visual hallucinations, expanding
their potential in non-photorealistic domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 12 figures, 1 table, Project page:
  https://gh-bumsookim.github.io/Cartoon-Hallucinations-Detection/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Fusion with Pre-Trained Model Features in Affective Behaviour
  Analysis In-the-wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuofan Wen, Fengyu Zhang, Siyuan Zhang, Haiyang Sun, Mingyu Xu, Licai Sun, Zheng Lian, Bin Liu, Jianhua Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal fusion is a significant method for most multimodal tasks. With the
recent surge in the number of large pre-trained models, combining both
multimodal fusion methods and pre-trained model features can achieve
outstanding performance in many multimodal tasks. In this paper, we present our
approach, which leverages both advantages for addressing the task of Expression
(Expr) Recognition and Valence-Arousal (VA) Estimation. We evaluate the
Aff-Wild2 database using pre-trained models, then extract the final hidden
layers of the models as features. Following preprocessing and interpolation or
convolution to align the extracted features, different models are employed for
modal fusion. Our code is available at GitHub - FulgenceWen/ABAW6th.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Tiny and High-quality Facial Makeup with Data Amplify Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaoqiao Jin, Xuanhong Chen, Meiguang Jin, Ying Cheng, Rui Shi, Yucheng Zheng, Yupeng Zhu, Bingbing Ni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contemporary makeup approaches primarily hinge on unpaired learning
paradigms, yet they grapple with the challenges of inaccurate supervision
(e.g., face misalignment) and sophisticated facial prompts (including face
parsing, and landmark detection). These challenges prohibit low-cost deployment
of facial makeup models, especially on mobile devices. To solve above problems,
we propose a brand-new learning paradigm, termed "Data Amplify Learning (DAL),"
alongside a compact makeup model named "TinyBeauty." The core idea of DAL lies
in employing a Diffusion-based Data Amplifier (DDA) to "amplify" limited images
for the model training, thereby enabling accurate pixel-to-pixel supervision
with merely a handful of annotations. Two pivotal innovations in DDA facilitate
the above training approach: (1) A Residual Diffusion Model (RDM) is designed
to generate high-fidelity detail and circumvent the detail vanishing problem in
the vanilla diffusion models; (2) A Fine-Grained Makeup Module (FGMM) is
proposed to achieve precise makeup control and combination while retaining face
identity. Coupled with DAL, TinyBeauty necessitates merely 80K parameters to
achieve a state-of-the-art performance without intricate face prompts.
Meanwhile, TinyBeauty achieves a remarkable inference speed of up to 460 fps on
the iPhone 13. Extensive experiments show that DAL can produce highly
competitive makeup models using only 5 image pairs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Integrated Neighborhood and Scale Information Network for Open-Pit
  Mine Change Detection in High-Resolution Remote Sensing Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilin Xie, Kangning Li, Jinbao Jiang, Jinzhong Yang, Xiaojun Qiao, Deshuai Yuan, Cheng Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-pit mine change detection (CD) in high-resolution (HR) remote sensing
images plays a crucial role in mineral development and environmental
protection. Significant progress has been made in this field in recent years,
largely due to the advancement of deep learning techniques. However, existing
deep-learning-based CD methods encounter challenges in effectively integrating
neighborhood and scale information, resulting in suboptimal performance.
Therefore, by exploring the influence patterns of neighborhood and scale
information, this paper proposes an Integrated Neighborhood and Scale
Information Network (INSINet) for open-pit mine CD in HR remote sensing images.
Specifically, INSINet introduces 8-neighborhood-image information to acquire a
larger receptive field, improving the recognition of center image boundary
regions. Drawing on techniques of skip connection, deep supervision, and
attention mechanism, the multi-path deep supervised attention (MDSA) module is
designed to enhance multi-scale information fusion and change feature
extraction. Experimental analysis reveals that incorporating neighborhood and
scale information enhances the F1 score of INSINet by 6.40%, with improvements
of 3.08% and 3.32% respectively. INSINet outperforms existing methods with an
Overall Accuracy of 97.69%, Intersection over Union of 71.26%, and F1 score of
83.22%. INSINet shows significance for open-pit mine CD in HR remote sensing
images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Classification with Rotation-Invariant Variational Quantum
  Circuits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul San Sebastian, Mikel Cañizo, Román Orús
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational quantum algorithms are gaining attention as an early application
of Noisy Intermediate-Scale Quantum (NISQ) devices. One of the main problems of
variational methods lies in the phenomenon of Barren Plateaus, present in the
optimization of variational parameters. Adding geometric inductive bias to the
quantum models has been proposed as a potential solution to mitigate this
problem, leading to a new field called Geometric Quantum Machine Learning. In
this work, an equivariant architecture for variational quantum classifiers is
introduced to create a label-invariant model for image classification with
$C_4$ rotational label symmetry. The equivariant circuit is benchmarked against
two different architectures, and it is experimentally observed that the
geometric approach boosts the model's performance. Finally, a classical
equivariant convolution operation is proposed to extend the quantum model for
the processing of larger images, employing the resources available in NISQ
devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VRSO: Visual-Centric Reconstruction for Static Object Annotation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyao Yu, Yingfeng Cai, Jiaxin Zhang, Hui Kong, Wei Sui, Cong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a part of the perception results of intelligent driving systems, static
object detection (SOD) in 3D space provides crucial cues for driving
environment understanding. With the rapid deployment of deep neural networks
for SOD tasks, the demand for high-quality training samples soars. The
traditional, also reliable, way is manual labeling over the dense LiDAR point
clouds and reference images. Though most public driving datasets adopt this
strategy to provide SOD ground truth (GT), it is still expensive (requires
LiDAR scanners) and low-efficient (time-consuming and unscalable) in practice.
This paper introduces VRSO, a visual-centric approach for static object
annotation. VRSO is distinguished in low cost, high efficiency, and high
quality: (1) It recovers static objects in 3D space with only camera images as
input, and (2) manual labeling is barely involved since GT for SOD tasks is
generated based on an automatic reconstruction and annotation pipeline. (3)
Experiments on the Waymo Open Dataset show that the mean reprojection error
from VRSO annotation is only 2.6 pixels, around four times lower than the Waymo
labeling (10.6 pixels). Source code is available at:
https://github.com/CaiYingFeng/VRSO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to iros 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BSNet: Box-Supervised Simulation-assisted Mean Teacher for 3D Instance
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Lu, Jiacheng Deng, Tianzhu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D instance segmentation (3DIS) is a crucial task, but point-level
annotations are tedious in fully supervised settings. Thus, using bounding
boxes (bboxes) as annotations has shown great potential. The current mainstream
approach is a two-step process, involving the generation of pseudo-labels from
box annotations and the training of a 3DIS network with the pseudo-labels.
However, due to the presence of intersections among bboxes, not every point has
a determined instance label, especially in overlapping areas. To generate
higher quality pseudo-labels and achieve more precise weakly supervised 3DIS
results, we propose the Box-Supervised Simulation-assisted Mean Teacher for 3D
Instance Segmentation (BSNet), which devises a novel pseudo-labeler called
Simulation-assisted Transformer. The labeler consists of two main components.
The first is Simulation-assisted Mean Teacher, which introduces Mean Teacher
for the first time in this task and constructs simulated samples to assist the
labeler in acquiring prior knowledge about overlapping areas. To better model
local-global structure, we also propose Local-Global Aware Attention as the
decoder for teacher and student labelers. Extensive experiments conducted on
the ScanNetV2 and S3DIS datasets verify the superiority of our designs. Code is
available at
\href{https://github.com/peoplelu/BSNet}{https://github.com/peoplelu/BSNet}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vehicle Detection Performance in Nordic Region 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamam Mokayed, Rajkumar Saini, Oluwatosin Adewumi, Lama Alkhaled, Bjorn Backe, Palaiahnakote Shivakumara, Olle Hagner, Yan Chai Hum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the critical challenge of vehicle detection in the harsh
winter conditions in the Nordic regions, characterized by heavy snowfall,
reduced visibility, and low lighting. Due to their susceptibility to
environmental distortions and occlusions, traditional vehicle detection methods
have struggled in these adverse conditions. The advanced proposed deep learning
architectures brought promise, yet the unique difficulties of detecting
vehicles in Nordic winters remain inadequately addressed. This study uses the
Nordic Vehicle Dataset (NVD), which has UAV images from northern Sweden, to
evaluate the performance of state-of-the-art vehicle detection algorithms under
challenging weather conditions. Our methodology includes a comprehensive
evaluation of single-stage, two-stage, and transformer-based detectors against
the NVD. We propose a series of enhancements tailored to each detection
framework, including data augmentation, hyperparameter tuning, transfer
learning, and novel strategies designed explicitly for the DETR model. Our
findings not only highlight the limitations of current detection systems in the
Nordic environment but also offer promising directions for enhancing these
algorithms for improved robustness and accuracy in vehicle detection amidst the
complexities of winter landscapes. The code and the dataset are available at
https://nvd.ltu-ai.dev
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to ICPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extracting Human Attention through Crowdsourced Patch Labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minsuk Chang, Seokhyeon Park, Hyeon Jeon, Aeri Cho, Soohyun Lee, Jinwook Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In image classification, a significant problem arises from bias in the
datasets. When it contains only specific types of images, the classifier begins
to rely on shortcuts - simplistic and erroneous rules for decision-making. This
leads to high performance on the training dataset but inferior results on new,
varied images, as the classifier's generalization capability is reduced. For
example, if the images labeled as mustache consist solely of male figures, the
model may inadvertently learn to classify images by gender rather than the
presence of a mustache. One approach to mitigate such biases is to direct the
model's attention toward the target object's location, usually marked using
bounding boxes or polygons for annotation. However, collecting such annotations
requires substantial time and human effort. Therefore, we propose a novel
patch-labeling method that integrates AI assistance with crowdsourcing to
capture human attention from images, which can be a viable solution for
mitigating bias. Our method consists of two steps. First, we extract the
approximate location of a target using a pre-trained saliency detection model
supplemented by human verification for accuracy. Then, we determine the
human-attentive area in the image by iteratively dividing the image into
smaller patches and employing crowdsourcing to ascertain whether each patch can
be classified as the target object. We demonstrated the effectiveness of our
method in mitigating bias through improved classification accuracy and the
refined focus of the model. Also, crowdsourced experiments validate that our
method collects human annotation up to 3.4 times faster than annotating object
locations with polygons, significantly reducing the need for human resources.
We conclude the paper by discussing the advantages of our method in a
crowdsourcing context, mainly focusing on aspects of human errors and
accessibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cell Tracking according to Biological Needs -- Strong Mitosis-aware
  Random-finite Sets Tracker with Aleatoric Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timo Kaiser, Maximilian Schier, Bodo Rosenhahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cell tracking and segmentation assist biologists in extracting insights from
large-scale microscopy time-lapse data. Driven by local accuracy metrics,
current tracking approaches often suffer from a lack of long-term consistency.
To address this issue, we introduce an uncertainty estimation technique for
neural tracking-by-regression frameworks and incorporate it into our novel
extended Poisson multi-Bernoulli mixture tracker. Our uncertainty estimation
identifies uncertain associations within high-performing tracking-by-regression
methods using problem-specific test-time augmentations. Leveraging this
uncertainty, along with a novel mitosis-aware assignment problem formulation,
our tracker resolves false associations and mitosis detections stemming from
long-term conflicts. We evaluate our approach on nine competitive datasets and
demonstrate that it outperforms the current state-of-the-art on biologically
relevant metrics substantially, achieving improvements by a factor of
approximately $5.75$. Furthermore, we uncover new insights into the behavior of
tracking-by-regression uncertainty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 10 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clean-image Backdoor Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dazhong Rong, Shuheng Shen, Xinyi Fu, Peng Qian, Jianhai Chen, Qinming He, Xing Fu, Weiqiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To gather a significant quantity of annotated training data for
high-performance image classification models, numerous companies opt to enlist
third-party providers to label their unlabeled data. This practice is widely
regarded as secure, even in cases where some annotated errors occur, as the
impact of these minor inaccuracies on the final performance of the models is
negligible and existing backdoor attacks require attacker's ability to poison
the training images. Nevertheless, in this paper, we propose clean-image
backdoor attacks which uncover that backdoors can still be injected via a
fraction of incorrect labels without modifying the training images.
Specifically, in our attacks, the attacker first seeks a trigger feature to
divide the training images into two parts: those with the feature and those
without it. Subsequently, the attacker falsifies the labels of the former part
to a backdoor class. The backdoor will be finally implanted into the target
model after it is trained on the poisoned data. During the inference phase, the
attacker can activate the backdoor in two ways: slightly modifying the input
image to obtain the trigger feature, or taking an image that naturally has the
trigger feature as input. We conduct extensive experiments to demonstrate the
effectiveness and practicality of our attacks. According to the experimental
results, we conclude that our attacks seriously jeopardize the fairness and
robustness of image classification models, and it is necessary to be vigilant
about the incorrect labels in outsourced labeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TexRO: Generating Delicate Textures of 3D Models by Recursive
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinbo Wu, Xing Liu, Chenming Wu, Xiaobo Gao, Jialun Liu, Xinqi Liu, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents TexRO, a novel method for generating delicate textures of
a known 3D mesh by optimizing its UV texture. The key contributions are
two-fold. We propose an optimal viewpoint selection strategy, that finds the
most miniature set of viewpoints covering all the faces of a mesh. Our
viewpoint selection strategy guarantees the completeness of a generated result.
We propose a recursive optimization pipeline that optimizes a UV texture at
increasing resolutions, with an adaptive denoising method that re-uses existing
textures for new texture generation. Through extensive experimentation, we
demonstrate the superior performance of TexRO in terms of texture quality,
detail preservation, visual consistency, and, notably runtime speed,
outperforming other current methods. The broad applicability of TexRO is
further confirmed through its successful use on diverse 3D models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report. Project page:
  \href{https://3d-aigc.github.io/TexRO}{https://3d-aigc.github.io/TexRO}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tri-Perspective View Decomposition for Geometry-Aware Depth Completion <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Yan, Yuankai Lin, Kun Wang, Yupeng Zheng, Yufei Wang, Zhenyu Zhang, Jun Li, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth completion is a vital task for autonomous driving, as it involves
reconstructing the precise 3D geometry of a scene from sparse and noisy depth
measurements. However, most existing methods either rely only on 2D depth
representations or directly incorporate raw 3D point clouds for compensation,
which are still insufficient to capture the fine-grained 3D geometry of the
scene. To address this challenge, we introduce Tri-Perspective view
Decomposition (TPVD), a novel framework that can explicitly model 3D geometry.
In particular, (1) TPVD ingeniously decomposes the original point cloud into
three 2D views, one of which corresponds to the sparse depth input. (2) We
design TPV Fusion to update the 2D TPV features through recurrent 2D-3D-2D
aggregation, where a Distance-Aware Spherical Convolution (DASC) is applied.
(3) By adaptively choosing TPV affinitive neighbors, the newly proposed
Geometric Spatial Propagation Network (GSPN) further improves the geometric
consistency. As a result, our TPVD outperforms existing methods on KITTI,
NYUv2, and SUN RGBD. Furthermore, we build a novel depth completion dataset
named TOFDC, which is acquired by the time-of-flight (TOF) sensor and the color
camera on smartphones. Project page:
https://yanzq95.github.io/projectpage/TOFDC/index.html
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ParFormer: Vision Transformer Baseline with Parallel Local Global Token
  Mixer and Convolution Attention Patch Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Novendra Setyawan, Ghufron Wahyu Kurniawan, Chi-Chia Sun, Jun-Wei Hsieh, Hui-Kai Su, Wen-Kai Kuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents ParFormer as an enhanced transformer architecture that
allows the incorporation of different token mixers into a single stage, hence
improving feature extraction capabilities. Integrating both local and global
data allows for precise representation of short- and long-range spatial
relationships without the need for computationally intensive methods such as
shifting windows. Along with the parallel token mixer encoder, We offer the
Convolutional Attention Patch Embedding (CAPE) as an enhancement of standard
patch embedding to improve token mixer extraction with a convolutional
attention module. Our comprehensive evaluation demonstrates that our ParFormer
outperforms CNN-based and state-of-the-art transformer-based architectures in
image classification and several complex tasks such as object recognition. The
proposed CAPE has been demonstrated to benefit the overall MetaFormer
architecture, even while utilizing the Identity Mapping Token Mixer, resulting
in a 0.5\% increase in accuracy. The ParFormer models outperformed ConvNeXt and
Swin Transformer for the pure convolution and transformer model in accuracy.
Furthermore, our model surpasses the current leading hybrid transformer by
reaching competitive Top-1 scores in the ImageNet-1K classification test.
Specifically, our model variants with 11M, 23M, and 34M parameters achieve
scores of 80.4\%, 82.1\%, and 83.1\%, respectively. Code:
https://github.com/novendrastywn/ParFormer-CAPE-2024
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Magic for the Age of Quantized DNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoshihide Sawada, Ryuji Saiin, Kazuma Suetake
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the number of parameters in DNNs has explosively increased, as
exemplified by LLMs (Large Language Models), making inference on small-scale
computers more difficult. Model compression technology is, therefore, essential
for integration into products. In this paper, we propose a method of
quantization-aware training. We introduce a novel normalization (Layer-Batch
Normalization) that is independent of the mini-batch size and does not require
any additional computation cost during inference. Then, we quantize the weights
by the scaled round-clip function with the weight standardization. We also
quantize activation functions using the same function and apply surrogate
gradients to train the model with both quantized weights and the quantized
activation functions. We call this method Magic for the age of Quantised DNNs
(MaQD). Experimental results show that our quantization method can be achieved
with minimal accuracy degradation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improve Cross-domain Mixed Sampling with Guidance Training for Adaptive
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlve Zhou, Zhiheng Zhou, Tianlei Wang, Delu Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised Domain Adaptation (UDA) endeavors to adjust models trained on a
source domain to perform well on a target domain without requiring additional
annotations. In the context of domain adaptive semantic segmentation, which
tackles UDA for dense prediction, the goal is to circumvent the need for costly
pixel-level annotations. Typically, various prevailing methods baseline rely on
constructing intermediate domains via cross-domain mixed sampling techniques to
mitigate the performance decline caused by domain gaps. However, such
approaches generate synthetic data that diverge from real-world distributions,
potentially leading the model astray from the true target distribution. To
address this challenge, we propose a novel auxiliary task called Guidance
Training. This task facilitates the effective utilization of cross-domain mixed
sampling techniques while mitigating distribution shifts from the real world.
Specifically, Guidance Training guides the model to extract and reconstruct the
target-domain feature distribution from mixed data, followed by decoding the
reconstructed target-domain features to make pseudo-label predictions.
Importantly, integrating Guidance Training incurs minimal training overhead and
imposes no additional inference burden. We demonstrate the efficacy of our
approach by integrating it with existing methods, consistently improving
performance. The implementation will be available at
https://github.com/Wenlve-Zhou/Guidance-Training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Active Learning for Image Synthesis Personalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14987v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14987v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xulu Zhang, Wengyu Zhang, Xiao-Yong Wei, Jinlin Wu, Zhaoxiang Zhang, Zhen Lei, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a pilot study that explores the application of active
learning, traditionally studied in the context of discriminative models, to
generative models. We specifically focus on image synthesis personalization
tasks. The primary challenge in conducting active learning on generative models
lies in the open-ended nature of querying, which differs from the closed form
of querying in discriminative models that typically target a single concept. We
introduce the concept of anchor directions to transform the querying process
into a semi-open problem. We propose a direction-based uncertainty sampling
strategy to enable generative active learning and tackle the
exploitation-exploration dilemma. Extensive experiments are conducted to
validate the effectiveness of our approach, demonstrating that an open-source
model can achieve superior performance compared to closed-source models
developed by large companies, such as Google's StyleDrop. The source code is
available at https://github.com/zhangxulu1996/GAL4Personalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Piecewise-Linear Manifolds for Deep Metric Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubhang Bhatnagar, Narendra Ahuja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised deep metric learning (UDML) focuses on learning a semantic
representation space using only unlabeled data. This challenging problem
requires accurately estimating the similarity between data points, which is
used to supervise a deep network. For this purpose, we propose to model the
high-dimensional data manifold using a piecewise-linear approximation, with
each low-dimensional linear piece approximating the data manifold in a small
neighborhood of a point. These neighborhoods are used to estimate similarity
between data points. We empirically show that this similarity estimate
correlates better with the ground truth than the similarity estimates of
current state-of-the-art techniques. We also show that proxies, commonly used
in supervised metric learning, can be used to model the piecewise-linear
manifold in an unsupervised setting, helping improve performance. Our method
outperforms existing unsupervised metric learning approaches on standard
zero-shot image retrieval benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CPAL 2024 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AVT2-DWF: Improving Deepfake Detection with Audio-Visual Fusion and
  Dynamic Weighting Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Wang, Dengpan Ye, Long Tang, Yunming Zhang, Jiacheng Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the continuous improvements of deepfake methods, forgery messages have
transitioned from single-modality to multi-modal fusion, posing new challenges
for existing forgery detection algorithms. In this paper, we propose AVT2-DWF,
the Audio-Visual dual Transformers grounded in Dynamic Weight Fusion, which
aims to amplify both intra- and cross-modal forgery cues, thereby enhancing
detection capabilities. AVT2-DWF adopts a dual-stage approach to capture both
spatial characteristics and temporal dynamics of facial expressions. This is
achieved through a face transformer with an n-frame-wise tokenization strategy
encoder and an audio transformer encoder. Subsequently, it uses multi-modal
conversion with dynamic weight fusion to address the challenge of heterogeneous
information fusion between audio and visual modalities. Experiments on
DeepfakeTIMIT, FakeAVCeleb, and DFDC datasets indicate that AVT2-DWF achieves
state-of-the-art performance intra- and cross-dataset Deepfake detection. Code
is available at https://github.com/raining-dev/AVT2-DWF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trajectory Regularization Enhances Self-Supervised Geometric
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayun Wang, Stella X. Yu, Yubei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) has proven effective in learning high-quality
representations for various downstream tasks, with a primary focus on semantic
tasks. However, its application in geometric tasks remains underexplored,
partially due to the absence of a standardized evaluation method for geometric
representations. To address this gap, we introduce a new pose-estimation
benchmark for assessing SSL geometric representations, which demands training
without semantic or pose labels and achieving proficiency in both semantic and
geometric downstream tasks. On this benchmark, we study enhancing SSL geometric
representations without sacrificing semantic classification accuracy. We find
that leveraging mid-layer representations improves pose-estimation performance
by 10-20%. Further, we introduce an unsupervised trajectory-regularization
loss, which improves performance by an additional 4% and improves
generalization ability on out-of-distribution data. We hope the proposed
benchmark and methods offer new insights and improvements in self-supervised
geometric representation learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DreamFlow: High-Quality Text-to-3D Generation by Approximating
  Probability Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyungmin Lee, Kihyuk Sohn, Jinwoo Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in text-to-3D generation has been achieved through the
utilization of score distillation methods: they make use of the pre-trained
text-to-image (T2I) diffusion models by distilling via the diffusion model
training objective. However, such an approach inevitably results in the use of
random timesteps at each update, which increases the variance of the gradient
and ultimately prolongs the optimization process. In this paper, we propose to
enhance the text-to-3D optimization by leveraging the T2I diffusion prior in
the generative sampling process with a predetermined timestep schedule. To this
end, we interpret text-to3D optimization as a multi-view image-to-image
translation problem, and propose a solution by approximating the probability
flow. By leveraging the proposed novel optimization algorithm, we design
DreamFlow, a practical three-stage coarseto-fine text-to-3D optimization
framework that enables fast generation of highquality and high-resolution
(i.e., 1024x1024) 3D contents. For example, we demonstrate that DreamFlow is 5
times faster than the existing state-of-the-art text-to-3D method, while
producing more photorealistic 3D contents. Visit our project page
(https://kyungmnlee.github.io/dreamflow.github.io/) for visualizations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPT-Connect: Interaction between Text-Driven Human Motion Generator and
  3D Scenes in a Training-free Manner 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxuan Qu, Ziyan Guo, Jun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, while text-driven human motion generation has received massive
research attention, most existing text-driven motion generators are generally
only designed to generate motion sequences in a blank background. While this is
the case, in practice, human beings naturally perform their motions in 3D
scenes, rather than in a blank background. Considering this, we here aim to
perform scene-aware text-drive motion generation instead. Yet, intuitively
training a separate scene-aware motion generator in a supervised way can
require a large amount of motion samples to be troublesomely collected and
annotated in a large scale of different 3D scenes. To handle this task rather
in a relatively convenient manner, in this paper, we propose a novel
GPT-connect framework. In GPT-connect, we enable scene-aware motion sequences
to be generated directly utilizing the existing blank-background human motion
generator, via leveraging ChatGPT to connect the existing motion generator with
the 3D scene in a totally training-free manner. Extensive experiments
demonstrate the efficacy and generalizability of our proposed framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLIP-VQDiffusion : Langauge Free Training of Text To Image generation
  using CLIP and vector quantized diffusion model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungdae Han, Joohee Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been a significant progress in text conditional image generation
models. Recent advancements in this field depend not only on improvements in
model structures, but also vast quantities of text-image paired datasets.
However, creating these kinds of datasets is very costly and requires a
substantial amount of labor. Famous face datasets don't have corresponding text
captions, making it difficult to develop text conditional image generation
models on these datasets. Some research has focused on developing text to image
generation models using only images without text captions. Here, we propose
CLIP-VQDiffusion, which leverage the pretrained CLIP model to provide
multimodal text-image representations and strong image generation capabilities.
On the FFHQ dataset, our model outperformed previous state-of-the-art methods
by 4.4% in clipscore and generated very realistic images even when the text was
both in and out of distribution. The pretrained models and codes will soon be
available at https://github.com/INFINIQ-AI1/CLIPVQDiffusion
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun Cao, Yao Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in pre-trained diffusion models and 3D generation have
spurred interest in 4D content creation. However, achieving high-fidelity 4D
generation with spatial-temporal consistency remains a challenge. In this work,
we propose STAG4D, a novel framework that combines pre-trained diffusion models
with dynamic 3D Gaussian splatting for high-fidelity 4D generation. Drawing
inspiration from 3D generation techniques, we utilize a multi-view diffusion
model to initialize multi-view images anchoring on the input video frames,
where the video can be either real-world captured or generated by a video
diffusion model. To ensure the temporal consistency of the multi-view sequence
initialization, we introduce a simple yet effective fusion strategy to leverage
the first frame as a temporal anchor in the self-attention computation. With
the almost consistent multi-view sequences, we then apply the score
distillation sampling to optimize the 4D Gaussian point cloud. The 4D Gaussian
spatting is specially crafted for the generation task, where an adaptive
densification strategy is proposed to mitigate the unstable Gaussian gradient
for robust optimization. Notably, the proposed pipeline does not require any
pre-training or fine-tuning of diffusion networks, offering a more accessible
and practical solution for the 4D generation task. Extensive experiments
demonstrate that our method outperforms prior 4D generation works in rendering
quality, spatial-temporal consistency, and generation robustness, setting a new
state-of-the-art for 4D generation from diverse inputs, including text, image,
and video.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Survey</span> on Modeling of Articulated Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Liu, Manolis Savva, Ali Mahdavi-Amiri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D modeling of articulated objects is a research problem within computer
vision, graphics, and robotics. Its objective is to understand the shape and
motion of the articulated components, represent the geometry and mobility of
object parts, and create realistic models that reflect articulated objects in
the real world. This survey provides a comprehensive overview of the current
state-of-the-art in 3D modeling of articulated objects, with a specific focus
on the task of articulated part perception and articulated object creation
(reconstruction and generation). We systematically review and discuss the
relevant literature from two perspectives: geometry processing and articulation
modeling. Through this survey, we highlight the substantial progress made in
these areas, outline the ongoing challenges, and identify gaps for future
research. Our survey aims to serve as a foundational reference for researchers
and practitioners in computer vision and graphics, offering insights into the
complexities of articulated object modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Defying Imbalanced Forgetting in Class Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shixiong Xu, Gaofeng Meng, Xing Nie, Bolin Ni, Bin Fan, Shiming Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We observe a high level of imbalance in the accuracy of different classes in
the same old task for the first time. This intriguing phenomenon, discovered in
replay-based Class Incremental Learning (CIL), highlights the imbalanced
forgetting of learned classes, as their accuracy is similar before the
occurrence of catastrophic forgetting. This discovery remains previously
unidentified due to the reliance on average incremental accuracy as the
measurement for CIL, which assumes that the accuracy of classes within the same
task is similar. However, this assumption is invalid in the face of
catastrophic forgetting. Further empirical studies indicate that this
imbalanced forgetting is caused by conflicts in representation between
semantically similar old and new classes. These conflicts are rooted in the
data imbalance present in replay-based CIL methods. Building on these insights,
we propose CLass-Aware Disentanglement (CLAD) to predict the old classes that
are more likely to be forgotten and enhance their accuracy. Importantly, CLAD
can be seamlessly integrated into existing CIL methods. Extensive experiments
demonstrate that CLAD consistently improves current replay-based methods,
resulting in performance gains of up to 2.56%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaussian-<span class="highlight-title">SLAM</span>: Photo-realistic Dense <span class="highlight-title">SLAM</span> with Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10070v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10070v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladimir Yugay, Yue Li, Theo Gevers, Martin R. Oswald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a dense simultaneous localization and mapping (SLAM) method that
uses 3D Gaussians as a scene representation. Our approach enables
interactive-time reconstruction and photo-realistic rendering from real-world
single-camera RGBD videos. To this end, we propose a novel effective strategy
for seeding new Gaussians for newly explored areas and their effective online
optimization that is independent of the scene size and thus scalable to larger
scenes. This is achieved by organizing the scene into sub-maps which are
independently optimized and do not need to be kept in memory. We further
accomplish frame-to-model camera tracking by minimizing photometric and
geometric losses between the input and rendered frames. The Gaussian
representation allows for high-quality photo-realistic real-time rendering of
real-world scenes. Evaluation on synthetic and real-world datasets demonstrates
competitive or superior performance in mapping, tracking, and rendering
compared to existing neural dense SLAM methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Videoshop: Localized Semantic Video Editing with Noise-Extrapolated
  Diffusion Inversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14617v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14617v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Fan, Anand Bhattad, Ranjay Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Videoshop, a training-free video editing algorithm for localized
semantic edits. Videoshop allows users to use any editing software, including
Photoshop and generative inpainting, to modify the first frame; it
automatically propagates those changes, with semantic, spatial, and temporally
consistent motion, to the remaining frames. Unlike existing methods that enable
edits only through imprecise textual instructions, Videoshop allows users to
add or remove objects, semantically change objects, insert stock photos into
videos, etc. with fine-grained control over locations and appearance. We
achieve this through image-based video editing by inverting latents with noise
extrapolation, from which we generate videos conditioned on the edited image.
Videoshop produces higher quality edits against 6 baselines on 2 editing
benchmarks using 10 evaluation metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page at https://videoshop-editing.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VideoPoet: A Large Language Model for Zero-Shot Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14125v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14125v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, Krishna Somandepalli, Hassan Akbari, Yair Alon, Yong Cheng, Josh Dillon, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, Mikhail Sirotenko, Kihyuk Sohn, Xuan Yang, Hartwig Adam, Ming-Hsuan Yang, Irfan Essa, Huisheng Wang, David A. Ross, Bryan Seybold, Lu Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present VideoPoet, a language model capable of synthesizing high-quality
video, with matching audio, from a large variety of conditioning signals.
VideoPoet employs a decoder-only transformer architecture that processes
multimodal inputs -- including images, videos, text, and audio. The training
protocol follows that of Large Language Models (LLMs), consisting of two
stages: pretraining and task-specific adaptation. During pretraining, VideoPoet
incorporates a mixture of multimodal generative objectives within an
autoregressive Transformer framework. The pretrained LLM serves as a foundation
that can be adapted for a range of video generation tasks. We present empirical
results demonstrating the model's state-of-the-art capabilities in zero-shot
video generation, specifically highlighting VideoPoet's ability to generate
high-fidelity motions. Project page: http://sites.research.google/videopoet/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: http://sites.research.google/videopoet/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09611v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09611v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, Yinfei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we discuss building performant Multimodal Large Language Models
(MLLMs). In particular, we study the importance of various architecture
components and data choices. Through careful and comprehensive ablations of the
image encoder, the vision language connector, and various pre-training data
choices, we identified several crucial design lessons. For example, we
demonstrate that for large-scale multimodal pre-training using a careful mix of
image-caption, interleaved image-text, and text-only data is crucial for
achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,
compared to other published pre-training results. Further, we show that the
image encoder together with image resolution and the image token count has
substantial impact, while the vision-language connector design is of
comparatively negligible importance. By scaling up the presented recipe, we
build MM1, a family of multimodal models up to 30B parameters, including both
dense models and mixture-of-experts (MoE) variants, that are SOTA in
pre-training metrics and achieve competitive performance after supervised
fine-tuning on a range of established multimodal benchmarks. Thanks to
large-scale pre-training, MM1 enjoys appealing properties such as enhanced
in-context learning, and multi-image reasoning, enabling few-shot
chain-of-thought prompting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SkySense: A Multi-Modal Remote Sensing Foundation Model Towards
  Universal Interpretation for Earth Observation Imagery <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Guo, Jiangwei Lao, Bo Dang, Yingying Zhang, Lei Yu, Lixiang Ru, Liheng Zhong, Ziyuan Huang, Kang Wu, Dingxiang Hu, Huimei He, Jian Wang, Jingdong Chen, Ming Yang, Yongjun Zhang, Yansheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior studies on Remote Sensing Foundation Model (RSFM) reveal immense
potential towards a generic model for Earth Observation. Nevertheless, these
works primarily focus on a single modality without temporal and geo-context
modeling, hampering their capabilities for diverse tasks. In this study, we
present SkySense, a generic billion-scale model, pre-trained on a curated
multi-modal Remote Sensing Imagery (RSI) dataset with 21.5 million temporal
sequences. SkySense incorporates a factorized multi-modal spatiotemporal
encoder taking temporal sequences of optical and Synthetic Aperture Radar (SAR)
data as input. This encoder is pre-trained by our proposed Multi-Granularity
Contrastive Learning to learn representations across different modal and
spatial granularities. To further enhance the RSI representations by the
geo-context clue, we introduce Geo-Context Prototype Learning to learn
region-aware prototypes upon RSI's multi-modal spatiotemporal features. To our
best knowledge, SkySense is the largest Multi-Modal RSFM to date, whose modules
can be flexibly combined or used individually to accommodate various tasks. It
demonstrates remarkable generalization capabilities on a thorough evaluation
encompassing 16 datasets over 7 tasks, from single- to multi-modal, static to
temporal, and classification to localization. SkySense surpasses 18 recent
RSFMs in all test scenarios. Specifically, it outperforms the latest models
such as GFM, SatLas and Scale-MAE by a large margin, i.e., 2.76%, 3.67% and
3.61% on average respectively. We will release the pre-trained weights to
facilitate future research and Earth Observation applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast ODE-based Sampling for Diffusion Models in Around 5 Steps <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00094v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00094v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Zhou, Defang Chen, Can Wang, Chun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sampling from diffusion models can be treated as solving the corresponding
ordinary differential equations (ODEs), with the aim of obtaining an accurate
solution with as few number of function evaluations (NFE) as possible.
Recently, various fast samplers utilizing higher-order ODE solvers have emerged
and achieved better performance than the initial first-order one. However,
these numerical methods inherently result in certain approximation errors,
which significantly degrades sample quality with extremely small NFE (e.g.,
around 5). In contrast, based on the geometric observation that each sampling
trajectory almost lies in a two-dimensional subspace embedded in the ambient
space, we propose Approximate MEan-Direction Solver (AMED-Solver) that
eliminates truncation errors by directly learning the mean direction for fast
diffusion sampling. Besides, our method can be easily used as a plugin to
further improve existing ODE-based samplers. Extensive experiments on image
synthesis with the resolution ranging from 32 to 512 demonstrate the
effectiveness of our method. With only 5 NFE, we achieve 6.61 FID on CIFAR-10,
10.74 FID on ImageNet 64$\times$64, and 13.20 FID on LSUN Bedroom. Our code is
available at https://github.com/zju-pi/diff-sampler.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14520v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14520v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhao, Min Zhang, Wei Zhao, Pengxiang Ding, Siteng Huang, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the application of multimodal large language models (MLLM)
in various fields has achieved remarkable success. However, as the foundation
model for many downstream tasks, current MLLMs are composed of the well-known
Transformer network, which has a less efficient quadratic computation
complexity. To improve the efficiency of such basic models, we propose Cobra, a
linear computational complexity MLLM. Specifically, Cobra integrates the
efficient Mamba language model into the visual modality. Moreover, we explore
and study various modal fusion schemes to create an effective multi-modal
Mamba. Extensive experiments demonstrate that (1) Cobra achieves extremely
competitive performance with current computationally efficient state-of-the-art
methods, e.g., LLaVA-Phi, TinyLLaVA, and MobileVLM v2, and has faster speed due
to Cobra's linear sequential modeling. (2) Interestingly, the results of
closed-set challenging prediction benchmarks show that Cobra performs well in
overcoming visual illusions and spatial relationship judgments. (3) Notably,
Cobra even achieves comparable performance to LLaVA with about 43% of the
number of parameters. We will make all codes of Cobra open-source and hope that
the proposed method can facilitate future research on complexity problems in
MLLM. Our project page is available at: https://sites.google.com/view/cobravlm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self
  Attention at the Threadblock Level 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04690v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04690v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Hassani, Wen-Mei Hwu, Humphrey Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neighborhood attention reduces the cost of self attention by restricting each
token's attention span to its nearest neighbors. This restriction,
parameterized by a window size and dilation factor, draws a spectrum of
possible attention patterns between linear projection and self attention.
Neighborhood attention, and more generally sliding window attention patterns,
have long been bounded by infrastructure, particularly in higher-rank spaces
(2-D and 3-D), calling for the development of custom kernels, which have been
limited in either functionality, or performance, if not both. In this work, we
first show that neighborhood attention can be represented as a batched GEMM
problem, similar to standard attention, and implement it for 1-D and 2-D
neighborhood attention. These kernels on average provide 895% and 272%
improvement in full precision latency compared to existing naive kernels for
1-D and 2-D neighborhood attention respectively. We find certain inherent
inefficiencies in all unfused neighborhood attention kernels that bound their
performance and lower-precision scalability. We also developed fused
neighborhood attention; an adaptation of fused dot-product attention kernels
that allow fine-grained control over attention across different spatial axes.
Known for reducing the quadratic time complexity of self attention to a linear
complexity, neighborhood attention can now enjoy a reduced and constant memory
footprint, and record-breaking half precision latency. We observe that our
fused kernels successfully circumvent some of the unavoidable inefficiencies in
unfused implementations. While our unfused GEMM-based kernels only improve half
precision performance compared to naive kernels by an average of 496% and 113%
in 1-D and 2-D problems respectively, our fused kernels improve naive kernels
by an average of 1607% and 581% in 1-D and 2-D problems respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/SHI-Labs/NATTEN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantics, Distortion, and Style Matter: Towards Source-free UDA for
  Panoramic Segmentation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12505v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12505v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Zheng, Pengyuan Zhou, Athanasios V. Vasilakos, Lin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses an interesting yet challenging problem -- source-free
unsupervised domain adaptation (SFUDA) for pinhole-to-panoramic semantic
segmentation -- given only a pinhole image-trained model (i.e., source) and
unlabeled panoramic images (i.e., target). Tackling this problem is nontrivial
due to the semantic mismatches, style discrepancies, and inevitable distortion
of panoramic images. To this end, we propose a novel method that utilizes
Tangent Projection (TP) as it has less distortion and meanwhile slits the
equirectangular projection (ERP) with a fixed FoV to mimic the pinhole images.
Both projections are shown effective in extracting knowledge from the source
model. However, the distinct projection discrepancies between source and target
domains impede the direct knowledge transfer; thus, we propose a panoramic
prototype adaptation module (PPAM) to integrate panoramic prototypes from the
extracted knowledge for adaptation. We then impose the loss constraints on both
predictions and prototypes and propose a cross-dual attention module (CDAM) at
the feature level to better align the spatial and channel characteristics
across the domains and projections. Both knowledge extraction and transfer
processes are synchronously updated to reach the best performance. Extensive
experiments on the synthetic and real-world benchmarks, including outdoor and
indoor scenarios, demonstrate that our method achieves significantly better
performance than prior SFUDA methods for pinhole-to-panoramic adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Win-Win: Training High-Resolution Vision Transformers from Two Windows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00632v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00632v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Leroy, Jerome Revaud, Thomas Lucas, Philippe Weinzaepfel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have become the standard in state-of-the-art vision
architectures, achieving impressive performance on both image-level and dense
pixelwise tasks. However, training vision transformers for high-resolution
pixelwise tasks has a prohibitive cost. Typical solutions boil down to
hierarchical architectures, fast and approximate attention, or training on
low-resolution crops. This latter solution does not constrain architectural
choices, but it leads to a clear performance drop when testing at resolutions
significantly higher than that used for training, thus requiring ad-hoc and
slow post-processing schemes. In this paper, we propose a novel strategy for
efficient training and inference of high-resolution vision transformers. The
key principle is to mask out most of the high-resolution inputs during
training, keeping only N random windows. This allows the model to learn local
interactions between tokens inside each window, and global interactions between
tokens from different windows. As a result, the model can directly process the
high-resolution input at test time without any special trick. We show that this
strategy is effective when using relative positional embedding such as rotary
embeddings. It is 4 times faster to train than a full-resolution network, and
it is straightforward to use at test time compared to existing approaches. We
apply this strategy to three dense prediction tasks with high-resolution data.
First, we show on the task of semantic segmentation that a simple setting with
2 windows performs best, hence the name of our method: Win-Win. Second, we
confirm this result on the task of monocular depth prediction. Third, we
further extend it to the binocular task of optical flow, reaching
state-of-the-art performance on the Spring benchmark that contains Full-HD
images with an order of magnitude faster inference than the best competitor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inducing High Energy-Latency of Large Vision-Language Models with
  Verbose Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11170v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11170v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuofeng Gao, Yang Bai, Jindong Gu, Shu-Tao Xia, Philip Torr, Zhifeng Li, Wei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (VLMs) such as GPT-4 have achieved exceptional
performance across various multi-modal tasks. However, the deployment of VLMs
necessitates substantial energy consumption and computational resources. Once
attackers maliciously induce high energy consumption and latency time
(energy-latency cost) during inference of VLMs, it will exhaust computational
resources. In this paper, we explore this attack surface about availability of
VLMs and aim to induce high energy-latency cost during inference of VLMs. We
find that high energy-latency cost during inference of VLMs can be manipulated
by maximizing the length of generated sequences. To this end, we propose
verbose images, with the goal of crafting an imperceptible perturbation to
induce VLMs to generate long sentences during inference. Concretely, we design
three loss objectives. First, a loss is proposed to delay the occurrence of
end-of-sequence (EOS) token, where EOS token is a signal for VLMs to stop
generating further tokens. Moreover, an uncertainty loss and a token diversity
loss are proposed to increase the uncertainty over each generated token and the
diversity among all tokens of the whole generated sequence, respectively, which
can break output dependency at token-level and sequence-level. Furthermore, a
temporal weight adjustment algorithm is proposed, which can effectively balance
these losses. Extensive experiments demonstrate that our verbose images can
increase the length of generated sequences by 7.87 times and 8.56 times
compared to original images on MS-COCO and ImageNet datasets, which presents
potential challenges for various applications. Our code is available at
https://github.com/KuofengGao/Verbose_Images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Residual Denoising Diffusion Models <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.13712v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.13712v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Liu, Qiang Wang, Huijie Fan, Yinong Wang, Yandong Tang, Liangqiong Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose residual denoising diffusion models (RDDM), a novel dual diffusion
process that decouples the traditional single denoising diffusion process into
residual diffusion and noise diffusion. This dual diffusion framework expands
the denoising-based diffusion models, initially uninterpretable for image
restoration, into a unified and interpretable model for both image generation
and restoration by introducing residuals. Specifically, our residual diffusion
represents directional diffusion from the target image to the degraded input
image and explicitly guides the reverse generation process for image
restoration, while noise diffusion represents random perturbations in the
diffusion process. The residual prioritizes certainty, while the noise
emphasizes diversity, enabling RDDM to effectively unify tasks with varying
certainty or diversity requirements, such as image generation and restoration.
We demonstrate that our sampling process is consistent with that of DDPM and
DDIM through coefficient transformation, and propose a partially
path-independent generation process to better understand the reverse process.
Notably, our RDDM enables a generic UNet, trained with only an L1 loss and a
batch size of 1, to compete with state-of-the-art image restoration methods. We
provide code and pre-trained models to encourage further exploration,
application, and development of our innovative framework
(https://github.com/nachifur/RDDM).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09530v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09530v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Kelly, Luhui Hu, Jiayin Hu, Yu Tian, Deshun Yang, Bang Yang, Cindy Yang, Zihao Li, Zaoshan Huang, Yuexian Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of text to visual components facilitates people's daily lives,
such as generating image, videos from text and identifying the desired elements
within the images. Computer vision models involving the multimodal abilities in
the previous days are focused on image detection, classification based on
well-defined objects. Large language models (LLMs) introduces the
transformation from nature language to visual objects, which present the visual
layout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,
while the computer vision (CV) domain boasts a plethora of state-of-the-art
(SOTA) models and algorithms to convert 2D images to their 3D representations.
However, the mismatching between the algorithms with the problem could lead to
undesired results. In response to this challenge, we propose an unified
VisionGPT-3D framework to consolidate the state-of-the-art vision models,
thereby facilitating the development of vision-oriented AI. VisionGPT-3D
provides a versatile multimodal framework building upon the strengths of
multimodal foundation models. It seamlessly integrates various SOTA vision
models and brings the automation in the selection of SOTA vision models,
identifies the suitable 3D mesh creation algorithms corresponding to 2D depth
maps analysis, generates optimal results based on diverse multimodal inputs
such as text prompts.
  Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures, pending conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toulouse Hyperspectral Data Set: a benchmark data set to assess
  semi-supervised spectral representation learning and pixel-wise
  classification techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08863v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08863v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romain Thoreau, Laurent Risser, Véronique Achard, Béatrice Berthelot, Xavier Briottet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Airborne hyperspectral images can be used to map the land cover in large
urban areas, thanks to their very high spatial and spectral resolutions on a
wide spectral domain. While the spectral dimension of hyperspectral images is
highly informative of the chemical composition of the land surface, the use of
state-of-the-art machine learning algorithms to map the land cover has been
dramatically limited by the availability of training data. To cope with the
scarcity of annotations, semi-supervised and self-supervised techniques have
lately raised a lot of interest in the community. Yet, the publicly available
hyperspectral data sets commonly used to benchmark machine learning models are
not totally suited to evaluate their generalization performances due to one or
several of the following properties: a limited geographical coverage (which
does not reflect the spectral diversity in metropolitan areas), a small number
of land cover classes and a lack of appropriate standard train / test splits
for semi-supervised and self-supervised learning. Therefore, we release in this
paper the Toulouse Hyperspectral Data Set that stands out from other data sets
in the above-mentioned respects in order to meet key issues in spectral
representation learning and classification over large-scale hyperspectral
images with very few labeled pixels. Besides, we discuss and experiment
self-supervised techniques for spectral representation learning, including the
Masked Autoencoder, and establish a baseline for pixel-wise classification
achieving 85% overall accuracy and 77% F1 score. The Toulouse Hyperspectral
Data Set and our code are publicly available at
https://www.toulouse-hyperspectral-data-set.com and
https://www.github.com/Romain3Ch216/tlse-experiments, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ShapeFormer: Shape Prior Visible-to-Amodal Transformer-based Amodal
  Instance Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11376v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11376v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh Tran, Winston Bounsavy, Khoa Vo, Anh Nguyen, Tri Nguyen, Ngan Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Amodal Instance Segmentation (AIS) presents a challenging task as it involves
predicting both visible and occluded parts of objects within images. Existing
AIS methods rely on a bidirectional approach, encompassing both the transition
from amodal features to visible features (amodal-to-visible) and from visible
features to amodal features (visible-to-amodal). Our observation shows that the
utilization of amodal features through the amodal-to-visible can confuse the
visible features due to the extra information of occluded/hidden segments not
presented in visible display. Consequently, this compromised quality of visible
features during the subsequent visible-to-amodal transition. To tackle this
issue, we introduce ShapeFormer, a decoupled Transformer-based model with a
visible-to-amodal transition. It facilitates the explicit relationship between
output segmentations and avoids the need for amodal-to-visible transitions.
ShapeFormer comprises three key modules: (i) Visible-Occluding Mask Head for
predicting visible segmentation with occlusion awareness, (ii) Shape-Prior
Amodal Mask Head for predicting amodal and occluded masks, and (iii)
Category-Specific Shape Prior Retriever aims to provide shape prior knowledge.
Comprehensive experiments and extensive ablation studies across various AIS
benchmarks demonstrate the effectiveness of our ShapeFormer. The code is
available at: https://github.com/UARK-AICV/ShapeFormer
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IJCNN 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S-DyRF: Reference-Based Stylized Radiance Fields for Dynamic Scenes <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06205v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06205v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyi Li, Zhiguo Cao, Yizheng Wu, Kewei Wang, Ke Xian, Zhe Wang, Guosheng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current 3D stylization methods often assume static scenes, which violates the
dynamic nature of our real world. To address this limitation, we present
S-DyRF, a reference-based spatio-temporal stylization method for dynamic neural
radiance fields. However, stylizing dynamic 3D scenes is inherently challenging
due to the limited availability of stylized reference images along the temporal
axis. Our key insight lies in introducing additional temporal cues besides the
provided reference. To this end, we generate temporal pseudo-references from
the given stylized reference. These pseudo-references facilitate the
propagation of style information from the reference to the entire dynamic 3D
scene. For coarse style transfer, we enforce novel views and times to mimic the
style details present in pseudo-references at the feature level. To preserve
high-frequency details, we create a collection of stylized temporal pseudo-rays
from temporal pseudo-references. These pseudo-rays serve as detailed and
explicit stylization guidance for achieving fine style transfer. Experiments on
both synthetic and real-world datasets demonstrate that our method yields
plausible stylized results of space-time view synthesis on dynamic 3D scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024. Project page:
  https://xingyi-li.github.io/s-dyrf/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Inserting: Learning Identity Embedding for Semantic-Fidelity
  Personalized Diffusion Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00631v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00631v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Li, Songlin Yang, Wei Wang, Jing Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advanced diffusion-based Text-to-Image (T2I) models, such as the Stable
Diffusion Model, have made significant progress in generating diverse and
high-quality images using text prompts alone. However, when non-famous users
require personalized image generation for their identities (IDs), the T2I
models fail to accurately generate their ID-related images. The main problem is
that pre-trained T2I models do not learn the mapping between the new ID prompts
and their corresponding visual content. The previous methods either failed to
accurately fit the face region or lost the interactive generative ability with
other existing concepts in T2I models. In other words, they are unable to
generate T2I-aligned and semantic-fidelity images for the given prompts with
other concepts such as scenes (``Eiffel Tower''), actions (``holding a
basketball''), and facial attributes (``eyes closed''). In this paper, we focus
on inserting accurate and interactive ID embedding into the Stable Diffusion
Model for semantic-fidelity personalized generation. We address this challenge
from two perspectives: face-wise region fitting and semantic-fidelity token
optimization. Specifically, we first visualize the attention overfit problem
and propose a face-wise attention loss to fit the face region instead of
entangling ID-unrelated information, such as face layout and background. This
key trick significantly enhances the ID accuracy and interactive generative
ability with other existing concepts. Then, we optimize one ID representation
as multiple per-stage tokens where each token contains two disentangled
features. This expansion of the textual conditioning space improves
semantic-fidelity control. Extensive experiments validate that our results
exhibit superior ID accuracy, text-based manipulation ability, and
generalization compared to previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-conditioned Graph Diffusion for Neural Architecture Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06020v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06020v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohan Asthana, Joschua Conrad, Youssef Dawoud, Maurits Ortmanns, Vasileios Belagiannis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural architecture search automates the design of neural network
architectures usually by exploring a large and thus complex architecture search
space. To advance the architecture search, we present a graph diffusion-based
NAS approach that uses discrete conditional graph diffusion processes to
generate high-performing neural network architectures. We then propose a
multi-conditioned classifier-free guidance approach applied to graph diffusion
networks to jointly impose constraints such as high accuracy and low hardware
latency. Unlike the related work, our method is completely differentiable and
requires only a single model training. In our evaluations, we show promising
results on six standard benchmarks, yielding novel and unique architectures at
a fast speed, i.e. less than 0.2 seconds per architecture. Furthermore, we
demonstrate the generalisability and efficiency of our method through
experiments on ImageNet dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Transactions on Machine Learning Research (TMLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PIA: Your Personalized Image Animator via Plug-and-Play Modules in
  Text-to-Image Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.13964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.13964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing Fang, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in personalized text-to-image (T2I) models have
revolutionized content creation, empowering non-experts to generate stunning
images with unique styles. While promising, adding realistic motions into these
personalized images by text poses significant challenges in preserving distinct
styles, high-fidelity details, and achieving motion controllability by text. In
this paper, we present PIA, a Personalized Image Animator that excels in
aligning with condition images, achieving motion controllability by text, and
the compatibility with various personalized T2I models without specific tuning.
To achieve these goals, PIA builds upon a base T2I model with well-trained
temporal alignment layers, allowing for the seamless transformation of any
personalized T2I model into an image animation model. A key component of PIA is
the introduction of the condition module, which utilizes the condition frame
and inter-frame affinity as input to transfer appearance information guided by
the affinity hint for individual frame synthesis in the latent space. This
design mitigates the challenges of appearance-related image alignment within
and allows for a stronger focus on aligning with motion-related guidance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://pi-animator.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FunQA: Towards Surprising Video Comprehension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.14899v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.14899v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binzhu Xie, Sicheng Zhang, Zitang Zhou, Bo Li, Yuanhan Zhang, Jack Hessel, Jingkang Yang, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surprising videos, such as funny clips, creative performances, or visual
illusions, attract significant attention. Enjoyment of these videos is not
simply a response to visual stimuli; rather, it hinges on the human capacity to
understand (and appreciate) commonsense violations depicted in these videos. We
introduce FunQA, a challenging video question-answering (QA) dataset
specifically designed to evaluate and enhance the depth of video reasoning
based on counter-intuitive and fun videos. Unlike most video QA benchmarks
which focus on less surprising contexts, e.g., cooking or instructional videos,
FunQA covers three previously unexplored types of surprising videos: 1)
HumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous
QA tasks designed to assess the model's capability in counter-intuitive
timestamp localization, detailed video description, and reasoning around
counter-intuitiveness. We also pose higher-level tasks, such as attributing a
fitting and vivid title to the video and scoring the video creativity. In
total, the FunQA benchmark consists of 312K free-text QA pairs derived from
4.3K video clips, spanning a total of 24 video hours. Moreover, we propose
FunMentor, an agent designed for Vision-Language Models (VLMs) that uses
multi-turn dialogues to enhance models' understanding of counter-intuitiveness.
Extensive experiments with existing VLMs demonstrate the effectiveness of
FunMentor and reveal significant performance gaps for the FunQA videos across
spatial-temporal reasoning, visual-centered reasoning, and free-text
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://funqa-benchmark.github.io/ Codebase:
  https://github.com/Jingkang50/FunQA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ You Only Need Two Detectors to Achieve Multi-Modal 3D Multi-Object
  Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.08709v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.08709v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiyang Wang, Chunyun Fu, Jiawei He, Mingguang Huang, Ting Meng, Siyu Zhang, Hangning Zhou, Ziyao Xu, Chi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the classical tracking-by-detection (TBD) paradigm, detection and tracking
are separately and sequentially conducted, and data association must be
properly performed to achieve satisfactory tracking performance. In this paper,
a new end-to-end multi-object tracking framework is proposed, which integrates
object detection and multi-object tracking into a single model. The proposed
tracking framework eliminates the complex data association process in the
classical TBD paradigm, and requires no additional training. Secondly, the
regression confidence of historical trajectories is investigated, and the
possible states of a trajectory (weak object or strong object) in the current
frame are predicted. Then, a confidence fusion module is designed to guide
non-maximum suppression for trajectories and detections to achieve ordered and
robust tracking. Thirdly, by integrating historical trajectory features, the
regression performance of the detector is enhanced, which better reflects the
occlusion and disappearance patterns of objects in real world. Lastly,
extensive experiments are conducted on the commonly used KITTI and Waymo
datasets. The results show that the proposed framework can achieve robust
tracking by using only a 2D detector and a 3D detector, and it is proven more
accurate than many of the state-of-the-art TBD-based multi-modal tracking
methods. The source codes of the proposed method are available at
https://github.com/wangxiyang2022/YONTD-MOT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mora: Enabling Generalist Video Generation via A Multi-Agent Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengqing Yuan, Ruoxi Chen, Zhaoxu Li, Haolong Jia, Lifang He, Chi Wang, Lichao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sora is the first large-scale generalist video generation model that garnered
significant attention across society. Since its launch by OpenAI in February
2024, no other video generation models have paralleled {Sora}'s performance or
its capacity to support a broad spectrum of video generation tasks.
Additionally, there are only a few fully published video generation models,
with the majority being closed-source. To address this gap, this paper proposes
a new multi-agent framework Mora, which incorporates several advanced visual AI
agents to replicate generalist video generation demonstrated by Sora. In
particular, Mora can utilize multiple visual agents and successfully mimic
Sora's video generation capabilities in various tasks, such as (1)
text-to-video generation, (2) text-conditional image-to-video generation, (3)
extend generated videos, (4) video-to-video editing, (5) connect videos and (6)
simulate digital worlds. Our extensive experimental results show that Mora
achieves performance that is proximate to that of Sora in various tasks.
However, there exists an obvious performance gap between our work and Sora when
assessed holistically. In summary, we hope this project can guide the future
trajectory of video generation through collaborative AI agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MC-NeRF: Multi-Camera Neural Radiance Fields for Multi-Camera Image
  Acquisition Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07846v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07846v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Gao, Lutong Su, Hao Liang, Yufeng Yue, Yi Yang, Mengyin Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRF) use multi-view images for 3D scene
representation, demonstrating remarkable performance. As one of the primary
sources of multi-view images, multi-camera systems encounter challenges such as
varying intrinsic parameters and frequent pose changes. Most previous
NeRF-based methods assume a unique camera and rarely consider multi-camera
scenarios. Besides, some NeRF methods that can optimize intrinsic and extrinsic
parameters still remain susceptible to suboptimal solutions when these
parameters are poor initialized. In this paper, we propose MC-NeRF, a method
that enables joint optimization of both intrinsic and extrinsic parameters
alongside NeRF. The method also supports each image corresponding to
independent camera parameters. First, we tackle coupling issue and the
degenerate case that arise from the joint optimization between intrinsic and
extrinsic parameters. Second, based on the proposed solutions, we introduce an
efficient calibration image acquisition scheme for multi-camera systems,
including the design of calibration object. Finally, we present an end-to-end
network with training sequence that enables the estimation of intrinsic and
extrinsic parameters, along with the rendering network. Furthermore,
recognizing that most existing datasets are designed for a unique camera, we
construct a real multi-camera image acquisition system and create a
corresponding new dataset, which includes both simulated data and real-world
captured images. Experiments confirm the effectiveness of our method when each
image corresponds to different camera parameters. Specifically, we use
multi-cameras, each with different intrinsic and extrinsic parameters in
real-world system, to achieve 3D scene representation without providing initial
poses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript is currently under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ZePT: Zero-Shot Pan-Tumor Segmentation via Query-Disentangling and
  Self-Prompting <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yankai Jiang, Zhongzhen Huang, Rongzhao Zhang, Xiaofan Zhang, Shaoting Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The long-tailed distribution problem in medical image analysis reflects a
high prevalence of common conditions and a low prevalence of rare ones, which
poses a significant challenge in developing a unified model capable of
identifying rare or novel tumor categories not encountered during training. In
this paper, we propose a new zero-shot pan-tumor segmentation framework (ZePT)
based on query-disentangling and self-prompting to segment unseen tumor
categories beyond the training set. ZePT disentangles the object queries into
two subsets and trains them in two stages. Initially, it learns a set of
fundamental queries for organ segmentation through an object-aware feature
grouping strategy, which gathers organ-level visual features. Subsequently, it
refines the other set of advanced queries that focus on the auto-generated
visual prompts for unseen tumor segmentation. Moreover, we introduce
query-knowledge alignment at the feature level to enhance each query's
discriminative representation and generalizability. Extensive experiments on
various tumor segmentation tasks demonstrate the performance superiority of
ZePT, which surpasses the previous counterparts and evidence the promising
ability for zero-shot tumor segmentation in real-world settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FSC: Few-point Shape Completion <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07359v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07359v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianzu Wu, Xianfeng Wu, Tianyu Luan, Yajing Bai, Zhongyuan Lai, Junsong Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While previous studies have demonstrated successful 3D object shape
completion with a sufficient number of points, they often fail in scenarios
when a few points, e.g. tens of points, are observed. Surprisingly, via entropy
analysis, we find that even a few points, e.g. 64 points, could retain
substantial information to help recover the 3D shape of the object. To address
the challenge of shape completion with very sparse point clouds, we then
propose Few-point Shape Completion (FSC) model, which contains a novel
dual-branch feature extractor for handling extremely sparse inputs, coupled
with an extensive branch for maximal point utilization with a saliency branch
for dynamic importance assignment. This model is further bolstered by a
two-stage revision network that refines both the extracted features and the
decoder output, enhancing the detail and authenticity of the completed point
cloud. Our experiments demonstrate the feasibility of recovering 3D shapes from
a few points. The proposed Few-point Shape Completion (FSC) model outperforms
previous methods on both few-point inputs and many-point inputs, and shows good
generalizability to different object categories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RGBD GS-ICP <span class="highlight-title">SLAM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12550v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12550v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seongbo Ha, Jiung Yeon, Hyeonwoo Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous Localization and Mapping (SLAM) with dense representation plays
a key role in robotics, Virtual Reality (VR), and Augmented Reality (AR)
applications. Recent advancements in dense representation SLAM have highlighted
the potential of leveraging neural scene representation and 3D Gaussian
representation for high-fidelity spatial representation. In this paper, we
propose a novel dense representation SLAM approach with a fusion of Generalized
Iterative Closest Point (G-ICP) and 3D Gaussian Splatting (3DGS). In contrast
to existing methods, we utilize a single Gaussian map for both tracking and
mapping, resulting in mutual benefits. Through the exchange of covariances
between tracking and mapping processes with scale alignment techniques, we
minimize redundant computations and achieve an efficient system. Additionally,
we enhance tracking accuracy and mapping quality through our keyframe selection
methods. Experimental results demonstrate the effectiveness of our approach,
showing an incredibly fast speed up to 107 FPS (for the entire system) and
superior quality of the reconstructed map.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object
  Detection under Unknown Degradations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11220v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11220v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Zhang, Yan Wu, Yanming Liu, Xinyue Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection methods under known single degradations have been
extensively investigated. However, existing approaches require prior knowledge
of the degradation type and train a separate model for each, limiting their
practical applications in unpredictable environments. To address this
challenge, we propose a chain-of-thought (CoT) prompted adaptive enhancer,
CPA-Enhancer, for object detection under unknown degradations. Specifically,
CPA-Enhancer progressively adapts its enhancement strategy under the
step-by-step guidance of CoT prompts, that encode degradation-related
information. To the best of our knowledge, it's the first work that exploits
CoT prompting for object detection tasks. Overall, CPA-Enhancer is a
plug-and-play enhancement model that can be integrated into any generic
detectors to achieve substantial gains on degraded images, without knowing the
degradation type priorly. Experimental results demonstrate that CPA-Enhancer
not only sets the new state of the art for object detection but also boosts the
performance of other downstream vision tasks under unknown degradations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S2DM: Sector-Shaped Diffusion Models for Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13408v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13408v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Lang, Yuxuan Ge, Zheng Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have achieved great success in image generation. However,
when leveraging this idea for video generation, we face significant challenges
in maintaining the consistency and continuity across video frames. This is
mainly caused by the lack of an effective framework to align frames of videos
with desired temporal features while preserving consistent semantic and
stochastic features. In this work, we propose a novel Sector-Shaped Diffusion
Model (S2DM) whose sector-shaped diffusion region is formed by a set of
ray-shaped reverse diffusion processes starting at the same noise point. S2DM
can generate a group of intrinsically related data sharing the same semantic
and stochastic features while varying on temporal features with appropriate
guided conditions. We apply S2DM to video generation tasks, and explore the use
of optical flow as temporal conditions. Our experimental results show that S2DM
outperforms many existing methods in the task of video generation without any
temporal-feature modelling modules. For text-to-video generation tasks where
temporal conditions are not explicitly given, we propose a two-stage generation
strategy which can decouple the generation of temporal features from
semantic-content features. We show that, without additional training, our model
integrated with another temporal conditions generative model can still achieve
comparable performance with existing works. Our results can be viewd at
https://s2dm.github.io/S2DM/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI-Dentify: Deep learning for proximal caries detection on bitewing
  x-ray -- HUNT4 Oral Health Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00354v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00354v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier Pérez de Frutos, Ragnhild Holden Helland, Shreya Desai, Line Cathrine Nymoen, Thomas Langø, Theodor Remman, Abhijit Sen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Dental caries diagnosis requires the manual inspection of
diagnostic bitewing images of the patient, followed by a visual inspection and
probing of the identified dental pieces with potential lesions. Yet the use of
artificial intelligence, and in particular deep-learning, has the potential to
aid in the diagnosis by providing a quick and informative analysis of the
bitewing images.
  Methods: A dataset of 13,887 bitewings from the HUNT4 Oral Health Study were
annotated individually by six different experts, and used to train three
different object detection deep-learning architectures: RetinaNet (ResNet50),
YOLOv5 (M size), and EfficientDet (D0 and D1 sizes). A consensus dataset of 197
images, annotated jointly by the same six dentist, was used for evaluation. A
five-fold cross validation scheme was used to evaluate the performance of the
AI models.
  Results: he trained models show an increase in average precision and
F1-score, and decrease of false negative rate, with respect to the dental
clinicians. When compared against the dental clinicians, the YOLOv5 model shows
the largest improvement, reporting 0.647 mean average precision, 0.548 mean
F1-score, and 0.149 mean false negative rate. Whereas the best annotators on
each of these metrics reported 0.299, 0.495, and 0.164 respectively.
  Conclusion: Deep-learning models have shown the potential to assist dental
professionals in the diagnosis of caries. Yet, the task remains challenging due
to the artifacts natural to the bitewing images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 5 figure, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Event-based Simultaneous Localization and <span class="highlight-title">Mapping</span>: A Comprehensive
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.09793v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.09793v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunping Huang, Sen Zhang, Jing Zhang, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent decades, visual simultaneous localization and mapping (vSLAM) has
gained significant interest in both academia and industry. It estimates camera
motion and reconstructs the environment concurrently using visual sensors on a
moving robot. However, conventional cameras are limited by hardware, including
motion blur and low dynamic range, which can negatively impact performance in
challenging scenarios like high-speed motion and high dynamic range
illumination. Recent studies have demonstrated that event cameras, a new type
of bio-inspired visual sensor, offer advantages such as high temporal
resolution, dynamic range, low power consumption, and low latency. This paper
presents a timely and comprehensive review of event-based vSLAM algorithms that
exploit the benefits of asynchronous and irregular event streams for
localization and mapping tasks. The review covers the working principle of
event cameras and various event representations for preprocessing event data.
It also categorizes event-based vSLAM methods into four main categories:
feature-based, direct, motion-compensation, and deep learning methods, with
detailed discussions and practical guidance for each approach. Furthermore, the
paper evaluates the state-of-the-art methods on various benchmarks,
highlighting current challenges and future opportunities in this emerging
research area. A public repository will be maintained to keep track of the
rapid developments in this field at
{\url{https://github.com/kun150kun/ESLAM-survey}}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SyncTweedies: A General Generative Framework Based on Synchronized
  Diffusions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaihoon Kim, Juil Koo, Kyeongmin Yeo, Minhyuk Sung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a general framework for generating diverse visual content,
including ambiguous images, panorama images, mesh textures, and Gaussian splat
textures, by synchronizing multiple diffusion processes. We present exhaustive
investigation into all possible scenarios for synchronizing multiple diffusion
processes through a canonical space and analyze their characteristics across
applications. In doing so, we reveal a previously unexplored case: averaging
the outputs of Tweedie's formula while conducting denoising in multiple
instance spaces. This case also provides the best quality with the widest
applicability to downstream tasks. We name this case SyncTweedies. In our
experiments generating visual content aforementioned, we demonstrate the
superior quality of generation by SyncTweedies compared to other
synchronization methods, optimization-based and iterative-update-based methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://synctweedies.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detection Is Tracking: Point Cloud Multi-Sweep Deep Learning Models
  Revisited 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15756v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15756v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingji Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional tracking paradigm takes in instantaneous measurements such as
range and bearing, and produces object tracks across time. In applications such
as autonomous driving, lidar measurements in the form of point clouds are
usually passed through a "virtual sensor" realized by a deep learning model, to
produce "measurements" such as bounding boxes, which are in turn ingested by a
tracking module to produce object tracks. Very often multiple lidar sweeps are
accumulated in a buffer to merge and become the input to the virtual sensor. We
argue in this paper that such an input already contains temporal information,
and therefore the virtual sensor output should also contain temporal
information, not just instantaneous values for the time corresponding to the
end of the buffer. In particular, we present the deep learning model called
MULti-Sweep PAired Detector (MULSPAD) that produces, for each detected object,
a pair of bounding boxes at both the end time and the beginning time of the
input buffer. This is achieved with fairly straightforward changes in commonly
used lidar detection models, and with only marginal extra processing, but the
resulting symmetry is satisfying. Such paired detections make it possible not
only to construct rudimentary trackers fairly easily, but also to construct
more sophisticated trackers that can exploit the extra information conveyed by
the pair and be robust to choices of motion models and object birth/death
models. We have conducted preliminary training and experimentation using Waymo
Open Dataset, which shows the efficacy of our proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>My previous employer Motional is requiring a review and approval
  process before I can publish this paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixture of Cluster-conditional LoRA Experts for Vision-language
  Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12379v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12379v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhao Gou, Zhili Liu, Kai Chen, Lanqing Hong, Hang Xu, Aoxue Li, Dit-Yan Yeung, James T. Kwok, Yu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning of Large Vision-language Models (LVLMs) has revolutionized
the development of versatile models with zero-shot generalization across a wide
range of downstream vision-language tasks. However, the diversity of training
tasks of different sources and formats would lead to inevitable task conflicts,
where different tasks conflict for the same set of model parameters, resulting
in sub-optimal instructionfollowing abilities. To address that, we propose the
Mixture of Clusterconditional LoRA Experts (MoCLE), a novel Mixture of Experts
(MoE) architecture designed to activate the task-customized model parameters
based on the instruction clusters. A separate universal expert is further
incorporated to improve generalization capabilities of MoCLE for novel
instructions. Extensive experiments on 11 zero-shot tasks demonstrate the
effectiveness of MoCLE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://gyhdog99.github.io/projects/mocle/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02733v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02733v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bumsoo Kim, Abdul Muqeet, Kyuchul Lee, Sanghyun Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face re-aging is a prominent field in computer vision and graphics, with
significant applications in photorealistic domains such as movies, advertising,
and live streaming. Recently, the need to apply face re-aging to
non-photorealistic images, like comics, illustrations, and animations, has
emerged as an extension in various entertainment sectors. However, the lack of
a network that can seamlessly edit the apparent age in NPR images has limited
these tasks to a naive, sequential approach. This often results in unpleasant
artifacts and a loss of facial attributes due to domain discrepancies. In this
paper, we introduce a novel one-stage method for face re-aging combined with
portrait style transfer, executed in a single generative step. We leverage
existing face re-aging and style transfer networks, both trained within the
same PR domain. Our method uniquely fuses distinct latent vectors, each
responsible for managing aging-related attributes and NPR appearance. By
adopting an exemplar-based approach, our method offers greater flexibility
compared to domain-level fine-tuning approaches, which typically require
separate training or fine-tuning for each domain. This effectively addresses
the limitation of requiring paired datasets for re-aging and domain-level,
data-driven approaches for stylization. Our experiments show that our model can
effortlessly generate re-aged images while simultaneously transferring the
style of examples, maintaining both natural appearance and controllability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 15 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text
  Transformation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09572v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09572v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhao Gou, Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung, James T. Kwok, Yu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have shown impressive reasoning
abilities, which, however, are also more vulnerable to jailbreak attacks than
their LLM predecessors. Although still capable of detecting unsafe responses,
we observe that safety mechanisms of the pre-aligned LLMs in MLLMs can be
easily bypassed due to the introduction of image features. To construct robust
MLLMs, we propose ECSO(Eyes Closed, Safety On), a novel training-free
protecting approach that exploits the inherent safety awareness of MLLMs, and
generates safer responses via adaptively transforming unsafe images into texts
to activate intrinsic safety mechanism of pre-aligned LLMs in MLLMs.
Experiments on five state-of-the-art (SoTA) MLLMs demonstrate that our ECSO
enhances model safety significantly (e.g., a 37.6% improvement on the
MM-SafetyBench (SD+OCR), and 71.3% on VLSafe for the LLaVA-1.5-7B), while
consistently maintaining utility results on common MLLM benchmarks.
Furthermore, we show that ECSO can be used as a data engine to generate
supervised-finetuning (SFT) data for MLLM alignment without extra human
intervention.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://gyhdog99.github.io/projects/ecso/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MV-ROPE: Multi-view Constraints for Robust Category-level Object Pose
  and Size Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.08856v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.08856v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Yang, Yucong Chen, Xiangting Meng, Chenxin Yan, Min Li, Ran Cheng, Lige Liu, Tao Sun, Laurent Kneip
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently there has been a growing interest in category-level object pose and
size estimation, and prevailing methods commonly rely on single view RGB-D
images. However, one disadvantage of such methods is that they require accurate
depth maps which cannot be produced by consumer-grade sensors. Furthermore,
many practical real-world situations involve a moving camera that continuously
observes its surroundings, and the temporal information of the input video
streams is simply overlooked by single-view methods. We propose a novel
solution that makes use of RGB video streams. Our framework consists of three
modules: a scale-aware monocular dense SLAM solution, a lightweight object pose
predictor, and an object-level pose graph optimizer. The SLAM module utilizes a
video stream and additional scale-sensitive readings to estimate camera poses
and metric depth. The object pose predictor then generates canonical object
representations from RGB images. The object pose is estimated through geometric
registration of these canonical object representations with estimated object
depth points. All per-view estimates finally undergo optimization within a pose
graph, culminating in the output of robust and accurate canonical object poses.
Our experimental results demonstrate that when utilizing public dataset
sequences with high-quality depth information, the proposed method exhibits
comparable performance to state-of-the-art RGB-D methods. We also collect and
evaluate on new datasets containing depth maps of varying quality to further
quantitatively benchmark the proposed method alongside previous RGB-D based
methods. We demonstrate a significant advantage in scenarios where depth input
is absent or the quality of depth sensing is limited.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ D-SCo: Dual-Stream Conditional Diffusion for Monocular Hand-Held Object
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14189v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14189v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Fu, Gu Wang, Chenyangguang Zhang, Yan Di, Ziqin Huang, Zhiying Leng, Fabian Manhardt, Xiangyang Ji, Federico Tombari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing hand-held objects from a single RGB image is a challenging
task in computer vision. In contrast to prior works that utilize deterministic
modeling paradigms, we employ a point cloud denoising diffusion model to
account for the probabilistic nature of this problem. In the core, we introduce
centroid-fixed dual-stream conditional diffusion for monocular hand-held object
reconstruction (D-SCo), tackling two predominant challenges. First, to avoid
the object centroid from deviating, we utilize a novel hand-constrained
centroid fixing paradigm, enhancing the stability of diffusion and reverse
processes and the precision of feature projection. Second, we introduce a
dual-stream denoiser to semantically and geometrically model hand-object
interactions with a novel unified hand-object semantic embedding, enhancing the
reconstruction performance of the hand-occluded region of the object.
Experiments on the synthetic ObMan dataset and three real-world datasets HO3D,
MOW and DexYCB demonstrate that our approach can surpass all other
state-of-the-art methods. Codes will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Listen to Look into the Future: Audio-Visual Egocentric Gaze
  Anticipation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03907v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03907v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bolin Lai, Fiona Ryan, Wenqi Jia, Miao Liu, James M. Rehg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Egocentric gaze anticipation serves as a key building block for the emerging
capability of Augmented Reality. Notably, gaze behavior is driven by both
visual cues and audio signals during daily activities. Motivated by this
observation, we introduce the first model that leverages both the video and
audio modalities for egocentric gaze anticipation. Specifically, we propose a
Contrastive Spatial-Temporal Separable (CSTS) fusion approach that adopts two
modules to separately capture audio-visual correlations in spatial and temporal
dimensions, and applies a contrastive loss on the re-weighted audio-visual
features from fusion modules for representation learning. We conduct extensive
ablation studies and thorough analysis using two egocentric video datasets:
Ego4D and Aria, to validate our model design. We demonstrate the audio improves
the performance by +2.5% and +2.4% on the two datasets. Our model also
outperforms the prior state-of-the-art methods by at least +1.9% and +1.6%.
Moreover, we provide visualizations to show the gaze anticipation results and
provide additional insights into audio-visual representation learning. The code
and data split are available on our website
(https://bolinlai.github.io/CSTS-EgoGazeAnticipation/).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 6D-Diff: A Keypoint Diffusion Framework for 6D Object Pose Estimation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00029v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00029v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Xu, Haoxuan Qu, Yujun Cai, Jun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the 6D object pose from a single RGB image often involves noise
and indeterminacy due to challenges such as occlusions and cluttered
backgrounds. Meanwhile, diffusion models have shown appealing performance in
generating high-quality images from random noise with high indeterminacy
through step-by-step denoising. Inspired by their denoising capability, we
propose a novel diffusion-based framework (6D-Diff) to handle the noise and
indeterminacy in object pose estimation for better performance. In our
framework, to establish accurate 2D-3D correspondence, we formulate 2D
keypoints detection as a reverse diffusion (denoising) process. To facilitate
such a denoising process, we design a Mixture-of-Cauchy-based forward diffusion
process and condition the reverse process on the object features. Extensive
experiments on the LM-O and YCB-V datasets demonstrate the effectiveness of our
framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024 CAMERA-READY</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Promoting Segment Anything Model towards Highly Accurate Dichotomous
  Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianjie Liu, Keren Fu, Qijun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Segment Anything Model (SAM) represents a significant breakthrough into
foundation models for computer vision, providing a large-scale image
segmentation model. However, despite SAM's zero-shot performance, its
segmentation masks lack fine-grained details, particularly in accurately
delineating object boundaries. We have high expectations regarding whether SAM,
as a foundation model, can be improved towards highly accurate object
segmentation, which is known as dichotomous image segmentation (DIS). To
address this issue, we propose DIS-SAM, which advances SAM towards DIS with
extremely accurate details. DIS-SAM is a framework specifically tailored for
highly accurate segmentation, maintaining SAM's promptable design. DIS-SAM
employs a two-stage approach, integrating SAM with a modified IS-Net dedicated
to DIS. Despite its simplicity, DIS-SAM demonstrates significantly enhanced
segmentation accuracy compared to SAM and HQ-SAM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instance-aware Exploration-Verification-Exploitation for Instance
  ImageGoal Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17587v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17587v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Lei, Min Wang, Wengang Zhou, Li Li, Houqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a new embodied vision task, Instance ImageGoal Navigation (IIN) aims to
navigate to a specified object depicted by a goal image in an unexplored
environment.
  The main challenge of this task lies in identifying the target object from
different viewpoints while rejecting similar distractors.
  Existing ImageGoal Navigation methods usually adopt the simple
Exploration-Exploitation framework and ignore the identification of specific
instance during navigation.
  In this work, we propose to imitate the human behaviour of ``getting closer
to confirm" when distinguishing objects from a distance.
  Specifically, we design a new modular navigation framework named
Instance-aware Exploration-Verification-Exploitation (IEVE) for instance-level
image goal navigation.
  Our method allows for active switching among the exploration, verification,
and exploitation actions, thereby facilitating the agent in making reasonable
decisions under different situations.
  On the challenging HabitatMatterport 3D semantic (HM3D-SEM) dataset, our
method surpasses previous state-of-the-art work, with a classical segmentation
model (0.684 vs. 0.561 success) or a robust model (0.702 vs. 0.561 success)
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ mPLUG-Owl: Modularization Empowers Large Language Models with
  Multimodality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.14178v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.14178v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qi Qian, Ji Zhang, Fei Huang, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated impressive zero-shot abilities
on a variety of open-ended tasks, while recent research has also explored the
use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl,
a novel training paradigm that equips LLMs with multi-modal abilities through
modularized learning of foundation LLM, a visual knowledge module, and a visual
abstractor module. This approach can support multiple modalities and facilitate
diverse unimodal and multimodal abilities through modality collaboration. The
training paradigm of mPLUG-Owl involves a two-stage method for aligning image
and text, which learns visual knowledge with the assistance of LLM while
maintaining and even improving the generation abilities of LLM. In the first
stage, the visual knowledge module and abstractor module are trained with a
frozen LLM module to align the image and text. In the second stage,
language-only and multi-modal supervised datasets are used to jointly fine-tune
a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing
the visual knowledge module. We carefully build a visually-related instruction
evaluation set OwlEval. Experimental results show that our model outperforms
existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction
and visual understanding ability, multi-turn conversation ability, and
knowledge reasoning ability. Besides, we observe some unexpected and exciting
abilities such as multi-image correlation and scene text understanding, which
makes it possible to leverage it for harder real scenarios, such as vision-only
document comprehension. Our code, pre-trained model, instruction-tuned models,
and evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The
online demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working in Process</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ID-like Prompt Learning for Few-Shot Out-of-Distribution Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15243v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15243v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen Bai, Zongbo Han, Changqing Zhang, Bing Cao, Xiaoheng Jiang, Qinghua Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection methods often exploit auxiliary outliers
to train model identifying OOD samples, especially discovering challenging
outliers from auxiliary outliers dataset to improve OOD detection. However,
they may still face limitations in effectively distinguishing between the most
challenging OOD samples that are much like in-distribution (ID) data, i.e.,
\idlike samples. To this end, we propose a novel OOD detection framework that
discovers \idlike outliers using CLIP \cite{DBLP:conf/icml/RadfordKHRGASAM21}
from the vicinity space of the ID samples, thus helping to identify these most
challenging OOD samples. Then a prompt learning framework is proposed that
utilizes the identified \idlike outliers to further leverage the capabilities
of CLIP for OOD detection. Benefiting from the powerful CLIP, we only need a
small number of ID samples to learn the prompts of the model without exposing
other auxiliary outlier datasets. By focusing on the most challenging \idlike
OOD samples and elegantly exploiting the capabilities of CLIP, our method
achieves superior few-shot learning performance on various real-world image
datasets (e.g., in 4-shot OOD detection on the ImageNet-1k dataset, our method
reduces the average FPR95 by 12.16\% and improves the average AUROC by 2.76\%,
compared to state-of-the-art methods). Code is available at
https://github.com/ycfate/ID-like.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BigGait: Learning Gait Representation You Want by Large Vision Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19122v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19122v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingqiang Ye, Chao Fan, Jingzhe Ma, Xiaoming Liu, Shiqi Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gait recognition stands as one of the most pivotal remote identification
technologies and progressively expands across research and industry
communities. However, existing gait recognition methods heavily rely on
task-specific upstream driven by supervised learning to provide explicit gait
representations like silhouette sequences, which inevitably introduce expensive
annotation costs and potential error accumulation. Escaping from this trend,
this work explores effective gait representations based on the all-purpose
knowledge produced by task-agnostic Large Vision Models (LVMs) and proposes a
simple yet efficient gait framework, termed BigGait. Specifically, the Gait
Representation Extractor (GRE) within BigGait draws upon design principles from
established gait representations, effectively transforming all-purpose
knowledge into implicit gait representations without requiring third-party
supervision signals. Experiments on CCPG, CAISA-B* and SUSTech1K indicate that
BigGait significantly outperforms the previous methods in both within-domain
and cross-domain tasks in most cases, and provides a more practical paradigm
for learning the next-generation gait representation. Finally, we delve into
prospective challenges and promising directions in LVMs-based gait recognition,
aiming to inspire future work in this emerging topic. The source code is
available at https://github.com/ShiqiYu/OpenGait.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image super-resolution via dynamic network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10413v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10413v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunwei Tian, Xuanyu Zhang, Qi Zhang, Mingming Yang, Zhaojie Ju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) depend on deep network architectures to
extract accurate information for image super-resolution. However, obtained
information of these CNNs cannot completely express predicted high-quality
images for complex scenes. In this paper, we present a dynamic network for
image super-resolution (DSRNet), which contains a residual enhancement block,
wide enhancement block, feature refinement block and construction block. The
residual enhancement block is composed of a residual enhanced architecture to
facilitate hierarchical features for image super-resolution. To enhance
robustness of obtained super-resolution model for complex scenes, a wide
enhancement block achieves a dynamic architecture to learn more robust
information to enhance applicability of an obtained super-resolution model for
varying scenes. To prevent interference of components in a wide enhancement
block, a refinement block utilizes a stacked architecture to accurately learn
obtained features. Also, a residual learning operation is embedded in the
refinement block to prevent long-term dependency problem. Finally, a
construction block is responsible for reconstructing high-quality images.
Designed heterogeneous architecture can not only facilitate richer structural
information, but also be lightweight, which is suitable for mobile digital
devices. Experimental results shows that our method is more competitive in
terms of performance and recovering time of image super-resolution and
complexity. The code of DSRNet can be obtained at
https://github.com/hellloxiaotian/DSRNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open-sourced Data Ecosystem in Autonomous Driving: the Present and
  Future 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03408v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03408v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyang Li, Yang Li, Huijie Wang, Jia Zeng, Huilin Xu, Pinlong Cai, Li Chen, Junchi Yan, Feng Xu, Lu Xiong, Jingdong Wang, Futang Zhu, Chunjing Xu, Tiancai Wang, Fei Xia, Beipeng Mu, Zhihui Peng, Dahua Lin, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the continuous maturation and application of autonomous driving
technology, a systematic examination of open-source autonomous driving datasets
becomes instrumental in fostering the robust evolution of the industry
ecosystem. Current autonomous driving datasets can broadly be categorized into
two generations. The first-generation autonomous driving datasets are
characterized by relatively simpler sensor modalities, smaller data scale, and
is limited to perception-level tasks. KITTI, introduced in 2012, serves as a
prominent representative of this initial wave. In contrast, the
second-generation datasets exhibit heightened complexity in sensor modalities,
greater data scale and diversity, and an expansion of tasks from perception to
encompass prediction and control. Leading examples of the second generation
include nuScenes and Waymo, introduced around 2019. This comprehensive review,
conducted in collaboration with esteemed colleagues from both academia and
industry, systematically assesses over seventy open-source autonomous driving
datasets from domestic and international sources. It offers insights into
various aspects, such as the principles underlying the creation of high-quality
datasets, the pivotal role of data engine systems, and the utilization of
generative foundation models to facilitate scalable data generation.
Furthermore, this review undertakes an exhaustive analysis and discourse
regarding the characteristics and data scales that future third-generation
autonomous driving datasets should possess. It also delves into the scientific
and technical challenges that warrant resolution. These endeavors are pivotal
in advancing autonomous innovation and fostering technological enhancement in
critical domains. For further details, please refer to
https://github.com/OpenDriveLab/DriveAGI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article is a simplified English translation of corresponding
  Chinese article. Please refer to Chinese version for the complete content</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LSKNet: A Foundation Lightweight Backbone for Remote Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11735v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11735v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Li, Xiang Li, Yimain Dai, Qibin Hou, Li Liu, Yongxiang Liu, Ming-Ming Cheng, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote sensing images pose distinct challenges for downstream tasks due to
their inherent complexity. While a considerable amount of research has been
dedicated to remote sensing classification, object detection and semantic
segmentation, most of these studies have overlooked the valuable prior
knowledge embedded within remote sensing scenarios. Such prior knowledge can be
useful because remote sensing objects may be mistakenly recognized without
referencing a sufficiently long-range context, which can vary for different
objects. This paper considers these priors and proposes a lightweight Large
Selective Kernel Network (LSKNet) backbone. LSKNet can dynamically adjust its
large spatial receptive field to better model the ranging context of various
objects in remote sensing scenarios. To our knowledge, large and selective
kernel mechanisms have not been previously explored in remote sensing images.
Without bells and whistles, our lightweight LSKNet sets new state-of-the-art
scores on standard remote sensing classification, object detection and semantic
segmentation benchmarks. Our comprehensive analysis further validated the
significance of the identified priors and the effectiveness of LSKNet. The code
is available at https://github.com/zcablii/LSKNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2303.09030</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual
  Tokenization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04669v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04669v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Quzhe Huang, Bin Chen, Chenyi Lei, An Liu, Chengru Song, Xiaoqiang Lei, Di Zhang, Wenwu Ou, Kun Gai, Yadong Mu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the remarkable advance of the Large Language Model (LLM) has
inspired researchers to transfer its extraordinary reasoning capability to both
vision and language data. However, the prevailing approaches primarily regard
the visual input as a prompt and focus exclusively on optimizing the text
generation process conditioned upon vision content by a frozen LLM. Such an
inequitable treatment of vision and language heavily constrains the model's
potential. In this paper, we break through this limitation by representing both
vision and language in a unified form. Specifically, we introduce a
well-designed visual tokenizer to translate the non-linguistic image into a
sequence of discrete tokens like a foreign language that LLM can read. The
resulting visual tokens encompass high-level semantics worthy of a word and
also support dynamic sequence length varying from the image. Coped with this
tokenizer, the presented foundation model called LaVIT can handle both image
and text indiscriminately under the same generative learning paradigm. This
unification empowers LaVIT to serve as an impressive generalist interface to
understand and generate multi-modal content simultaneously. Extensive
experiments further showcase that it outperforms the existing models by a large
margin on massive vision-language tasks. Our code and models are available at
https://github.com/jy0205/LaVIT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction
  Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03849v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03849v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bolin Lai, Xiaoliang Dai, Lawrence Chen, Guan Pang, James M. Rehg, Miao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating instructional images of human daily actions from an egocentric
viewpoint serves as a key step towards efficient skill transfer. In this paper,
we introduce a novel problem -- egocentric action frame generation. The goal is
to synthesize an image depicting an action in the user's context (i.e., action
frame) by conditioning on a user prompt and an input egocentric image. Notably,
existing egocentric action datasets lack the detailed annotations that describe
the execution of actions. Additionally, existing diffusion-based image
manipulation models are sub-optimal in controlling the state transition of an
action in egocentric image pixel space because of the domain gap. To this end,
we propose to Learn EGOcentric (LEGO) action frame generation via visual
instruction tuning. First, we introduce a prompt enhancement scheme to generate
enriched action descriptions from a visual large language model (VLLM) by
visual instruction tuning. Then we propose a novel method to leverage image and
text embeddings from the VLLM as additional conditioning to improve the
performance of a diffusion model. We validate our model on two egocentric
datasets -- Ego4D and Epic-Kitchens. Our experiments show substantial
improvement over prior image manipulation models in both quantitative and
qualitative evaluation. We also conduct detailed ablation studies and analysis
to provide insights in our method. More details of the dataset and code are
available on the website (https://bolinlai.github.io/Lego_EgoActGen/).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicting Generalization of AI Colonoscopy Models to Unseen Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09920v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09920v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joel Shor, Carson McNeil, Yotam Intrator, Joseph R Ledsam, Hiro-o Yamano, Daisuke Tsurumaru, Hiroki Kayama, Atsushi Hamabe, Koji Ando, Mitsuhiko Ota, Haruei Ogino, Hiroshi Nakase, Kaho Kobayashi, Masaaki Miyo, Eiji Oki, Ichiro Takemasa, Ehud Rivlin, Roman Goldenberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  $\textbf{Background}$: Generalizability of AI colonoscopy algorithms is
important for wider adoption in clinical practice. However, current techniques
for evaluating performance on unseen data require expensive and time-intensive
labels.
  $\textbf{Methods}$: We use a "Masked Siamese Network" (MSN) to identify novel
phenomena in unseen data and predict polyp detector performance. MSN is trained
to predict masked out regions of polyp images, without any labels. We test
MSN's ability to be trained on data only from Israel and detect unseen
techniques, narrow-band imaging (NBI) and chromendoscoy (CE), on colonoscopes
from Japan (354 videos, 128 hours). We also test MSN's ability to predict
performance of Computer Aided Detection (CADe) of polyps on colonoscopies from
both countries, even though MSN is not trained on data from Japan.
  $\textbf{Results}$: MSN correctly identifies NBI and CE as less similar to
Israel whitelight than Japan whitelight (bootstrapped z-test, |z| > 496, p <
10^-8 for both) using the label-free Frechet distance. MSN detects NBI with 99%
accuracy, predicts CE better than our heuristic (90% vs 79% accuracy) despite
being trained only on whitelight, and is the only method that is robust to
noisy labels. MSN predicts CADe polyp detector performance on in-domain Israel
and out-of-domain Japan colonoscopies (r=0.79, 0.37 respectively). With few
examples of Japan detector performance to train on, MSN prediction of Japan
performance improves (r=0.56).
  $\textbf{Conclusion}$: Our technique can identify distribution shifts in
clinical data and can predict CADe detector performance on unseen data, without
labels. Our self-supervised approach can aid in detecting when data in practice
is different from training, such as between hospitals or data has meaningfully
shifted from training. MSN has potential for application to medical image
domains beyond colonoscopy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-Augmented Layout Transformer for Content-Aware Layout
  Generation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13602v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13602v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daichi Horita, Naoto Inoue, Kotaro Kikuchi, Kota Yamaguchi, Kiyoharu Aizawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Content-aware graphic layout generation aims to automatically arrange visual
elements along with a given content, such as an e-commerce product image. In
this paper, we argue that the current layout generation approaches suffer from
the limited training data for the high-dimensional layout structure. We show
that a simple retrieval augmentation can significantly improve the generation
quality. Our model, which is named Retrieval-Augmented Layout Transformer
(RALF), retrieves nearest neighbor layout examples based on an input image and
feeds these results into an autoregressive generator. Our model can apply
retrieval augmentation to various controllable generation tasks and yield
high-quality layouts within a unified architecture. Our extensive experiments
show that RALF successfully generates content-aware layouts in both constrained
and unconstrained settings and significantly outperforms the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024, Project website:
  https://udonda.github.io/RALF/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands
  from a Single Image <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08262v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08262v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minje Kim, Tae-Kyun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating personalized hand avatars is important to offer a realistic
experience to users on AR / VR platforms. While most prior studies focused on
reconstructing 3D hand shapes, some recent work has tackled the reconstruction
of hand textures on top of shapes. However, these methods are often limited to
capturing pixels on the visible side of a hand, requiring diverse views of the
hand in a video or multiple images as input. In this paper, we propose a novel
method, BiTT(Bi-directional Texture reconstruction of Two hands), which is the
first end-to-end trainable method for relightable, pose-free texture
reconstruction of two interacting hands taking only a single RGB image, by
three novel components: 1) bi-directional (left $\leftrightarrow$ right)
texture reconstruction using the texture symmetry of left / right hands, 2)
utilizing a texture parametric model for hand texture recovery, and 3) the
overall coarse-to-fine stage pipeline for reconstructing personalized texture
of two interacting hands. BiTT first estimates the scene light condition and
albedo image from an input image, then reconstructs the texture of both hands
through the texture parametric model and bi-directional texture reconstructor.
In experiments using InterHand2.6M and RGB2Hands datasets, our method
significantly outperforms state-of-the-art hand texture reconstruction methods
quantitatively and qualitatively. The code is available at
https://github.com/yunminjin2/BiTT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Image Search in Histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08699v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08699v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H. R. Tizhoosh, Liron Pantanowitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pathology images of histopathology can be acquired from camera-mounted
microscopes or whole slide scanners. Utilizing similarity calculations to match
patients based on these images holds significant potential in research and
clinical contexts. Recent advancements in search technologies allow for
implicit quantification of tissue morphology across diverse primary sites,
facilitating comparisons and enabling inferences about diagnosis, and
potentially prognosis, and predictions for new patients when compared against a
curated database of diagnosed and treated cases. In this paper, we
comprehensively review the latest developments in image search technologies for
histopathology, offering a concise overview tailored for computational
pathology researchers seeking effective, fast and efficient image search
methods in their work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A chapter in the Book "Artificial INtelligence in Digital Pathology"
  by Cohen and Chauhan, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video Super-Resolution Transformer with Masked Inter&Intra-Frame
  Attention <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06312v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06312v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Zhou, Leheng Zhang, Xiaorui Zhao, Keze Wang, Leida Li, Shuhang Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Vision Transformer has achieved great success in recovering missing
details in low-resolution sequences, i.e., the video super-resolution (VSR)
task. Despite its superiority in VSR accuracy, the heavy computational burden
as well as the large memory footprint hinder the deployment of
Transformer-based VSR models on constrained devices. In this paper, we address
the above issue by proposing a novel feature-level masked processing framework:
VSR with Masked Intra and inter frame Attention (MIA-VSR). The core of MIA-VSR
is leveraging feature-level temporal continuity between adjacent frames to
reduce redundant computations and make more rational use of previously enhanced
SR features. Concretely, we propose an intra-frame and inter-frame attention
block which takes the respective roles of past features and input features into
consideration and only exploits previously enhanced features to provide
supplementary information. In addition, an adaptive block-wise mask prediction
module is developed to skip unimportant computations according to feature
similarity between adjacent frames. We conduct detailed ablation studies to
validate our contributions and compare the proposed method with recent
state-of-the-art VSR approaches. The experimental results demonstrate that
MIA-VSR improves the memory and computation efficiency over state-of-the-art
methods, without trading off PSNR accuracy. The code is available at
https://github.com/LabShuHangGU/MIA-VSR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational
  Score Distillation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05239v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05239v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thuan Hoang Nguyen, Anh Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their ability to generate high-resolution and diverse images from
text prompts, text-to-image diffusion models often suffer from slow iterative
sampling processes. Model distillation is one of the most effective directions
to accelerate these models. However, previous distillation methods fail to
retain the generation quality while requiring a significant amount of images
for training, either from real data or synthetically generated by the teacher
model. In response to this limitation, we present a novel image-free
distillation scheme named $\textbf{SwiftBrush}$. Drawing inspiration from
text-to-3D synthesis, in which a 3D neural radiance field that aligns with the
input prompt can be obtained from a 2D text-to-image diffusion prior via a
specialized loss without the use of any 3D data ground-truth, our approach
re-purposes that same loss for distilling a pretrained multi-step text-to-image
model to a student network that can generate high-fidelity images with just a
single inference step. In spite of its simplicity, our model stands as one of
the first one-step text-to-image generators that can produce images of
comparable quality to Stable Diffusion without reliance on any training image
data. Remarkably, SwiftBrush achieves an FID score of $\textbf{16.67}$ and a
CLIP score of $\textbf{0.29}$ on the COCO-30K benchmark, achieving competitive
results or even substantially surpassing existing state-of-the-art distillation
techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024; Project Page:
  https://thuanz123.github.io/swiftbrush/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-Enhanced Multi-fidelity Learning for Optical Surface Imprint 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10278v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10278v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human fingerprints serve as one unique and powerful characteristic for each
person, from which policemen can recognize the identity. Similar to humans,
many natural bodies and intrinsic mechanical qualities can also be uniquely
identified from surface characteristics. To measure the elasto-plastic
properties of one material, one formally sharp indenter is pushed into the
measured body under constant force and retracted, leaving a unique residual
imprint of the minute size from several micrometers to nanometers. However, one
great challenge is how to map the optical image of this residual imprint into
the real wanted mechanical properties, \ie, the tensile force curve. In this
paper, we propose a novel method to use multi-fidelity neural networks (MFNN)
to solve this inverse problem. We first build up the NN model via pure
simulation data, and then bridge the sim-to-real gap via transfer learning.
Considering the difficulty of collecting real experimental data, we use NN to
dig out the unknown physics and also implant the known physics into the
transfer learning framework, thus highly improving the model stability and
decreasing the data requirement. The final constructed model only needs
three-shot calibration of real materials. We tested the final model across 20
real materials and achieved satisfying accuracy. This work serves as one great
example of applying machine learning into scientific research, especially under
the constraints of data limitation and fidelity variance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Boundary Discontinuity Problem for Oriented Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10061v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10061v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Xu, Xinyuan Liu, Haonan Xu, Yike Ma, Zunjie Zhu, Chenggang Yan, Feng Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Oriented object detection has been developed rapidly in the past few years,
where rotation equivariance is crucial for detectors to predict rotated boxes.
It is expected that the prediction can maintain the corresponding rotation when
objects rotate, but severe mutation in angular prediction is sometimes observed
when objects rotate near the boundary angle, which is well-known boundary
discontinuity problem. The problem has been long believed to be caused by the
sharp loss increase at the angular boundary, and widely used joint-optim
IoU-like methods deal with this problem by loss-smoothing. However, we
experimentally find that even state-of-the-art IoU-like methods actually fail
to solve the problem. On further analysis, we find that the key to solution
lies in encoding mode of the smoothing function rather than in joint or
independent optimization. In existing IoU-like methods, the model essentially
attempts to fit the angular relationship between box and object, where the
break point at angular boundary makes the predictions highly unstable.To deal
with this issue, we propose a dual-optimization paradigm for angles. We
decouple reversibility and joint-optim from single smoothing function into two
distinct entities, which for the first time achieves the objectives of both
correcting angular boundary and blending angle with other parameters.Extensive
experiments on multiple datasets show that boundary discontinuity problem is
well-addressed. Moreover, typical IoU-like methods are improved to the same
level without obvious performance gap. The code is available at
https://github.com/hangxu-cv/cvpr24acm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>cvpr 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16194v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16194v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawang Bai, Kuofeng Gao, Shaobo Min, Shu-Tao Xia, Zhifeng Li, Wei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Vision-Language Pre-training, known as CLIP, has shown promising
effectiveness in addressing downstream image recognition tasks. However, recent
works revealed that the CLIP model can be implanted with a downstream-oriented
backdoor. On downstream tasks, one victim model performs well on clean samples
but predicts a specific target class whenever a specific trigger is present.
For injecting a backdoor, existing attacks depend on a large amount of
additional data to maliciously fine-tune the entire pre-trained CLIP model,
which makes them inapplicable to data-limited scenarios. In this work,
motivated by the recent success of learnable prompts, we address this problem
by injecting a backdoor into the CLIP model in the prompt learning stage. Our
method named BadCLIP is built on a novel and effective mechanism in backdoor
attacks on CLIP, i.e., influencing both the image and text encoders with the
trigger. It consists of a learnable trigger applied to images and a
trigger-aware context generator, such that the trigger can change text features
via trigger-aware prompts, resulting in a powerful and generalizable attack.
Extensive experiments conducted on 11 datasets verify that the clean accuracy
of BadCLIP is similar to those of advanced prompt learning methods and the
attack success rate is higher than 99% in most cases. BadCLIP is also
generalizable to unseen classes, and shows a strong generalization capability
under cross-dataset and cross-domain settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hulk: A Universal Knowledge Translator for Human-Centric Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01697v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01697v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhou Wang, Yixuan Wu, Shixiang Tang, Weizhen He, Xun Guo, Feng Zhu, Lei Bai, Rui Zhao, Jian Wu, Tong He, Wanli Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-centric perception tasks, e.g., pedestrian detection, skeleton-based
action recognition, and pose estimation, have wide industrial applications,
such as metaverse and sports analysis. There is a recent surge to develop
human-centric foundation models that can benefit a broad range of human-centric
perception tasks. While many human-centric foundation models have achieved
success, they did not explore 3D and vision-language tasks for human-centric
and required task-specific finetuning. These limitations restrict their
application to more downstream tasks and situations. To tackle these problems,
we present Hulk, the first multimodal human-centric generalist model, capable
of addressing 2D vision, 3D vision, skeleton-based, and vision-language tasks
without task-specific finetuning. The key to achieving this is condensing
various task-specific heads into two general heads, one for discrete
representations, e.g., languages, and the other for continuous representations,
e.g., location coordinates. The outputs of two heads can be further stacked
into four distinct input and output modalities. This uniform representation
enables Hulk to treat diverse human-centric tasks as modality translation,
integrating knowledge across a wide range of tasks. Comprehensive evaluations
of Hulk on 12 benchmarks covering 8 human-centric tasks demonstrate the
superiority of our proposed method, achieving state-of-the-art performance in
11 benchmarks. The code is available on https://github.com/OpenGVLab/Hulk.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CBNet: A Plug-and-Play Network for Segmentation-Based Scene Text
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02340v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02340v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Zhao, Wei Feng, Zheng Zhang, Jingjing Lv, Xin Zhu, Zhangang Lin, Jinghe Hu, Jingping Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, segmentation-based methods are quite popular in scene text
detection, which mainly contain two steps: text kernel segmentation and
expansion. However, the segmentation process only considers each pixel
independently, and the expansion process is difficult to achieve a favorable
accuracy-speed trade-off. In this paper, we propose a Context-aware and
Boundary-guided Network (CBN) to tackle these problems. In CBN, a basic text
detector is firstly used to predict initial segmentation results. Then, we
propose a context-aware module to enhance text kernel feature representations,
which considers both global and local contexts. Finally, we introduce a
boundary-guided module to expand enhanced text kernels adaptively with only the
pixels on the contours, which not only obtains accurate text boundaries but
also keeps high speed, especially on high-resolution output maps. In
particular, with a lightweight backbone, the basic detector equipped with our
proposed CBN achieves state-of-the-art results on several popular benchmarks,
and our proposed CBN can be plugged into several segmentation-based methods.
Code is available at https://github.com/XiiZhao/cbn.pytorch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJCV 2024. Code is available at
  https://github.com/XiiZhao/cbn.pytorch</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Multilingual Models Pivot Zero-Shot Multimodal Learning across
  Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12038v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12038v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, Xu Han, Yankai Lin, Jiao Xue, Dahai Li, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently there has been a significant surge in multimodal learning in terms
of both image-to-text and text-to-image generation. However, the success is
typically limited to English, leaving other languages largely behind. Building
a competitive counterpart in other languages is highly challenging due to the
low-resource nature of non-English multimodal data (i.e., lack of large-scale,
high-quality image-text data). In this work, we propose MPM, an effective
training paradigm for training large multimodal models in non-English
languages. MPM demonstrates that Multilingual language models can Pivot
zero-shot Multimodal learning across languages. Specifically, based on a strong
multilingual large language model, multimodal models pretrained on English-only
image-text data can well generalize to other languages in a (quasi)-zero-shot
manner, even surpassing models trained on image-text data in native languages.
Taking Chinese as a practice of MPM, we build large multimodal models VisCPM in
image-to-text and text-to-image generation, which achieve state-of-the-art
(open-source) performance in Chinese. To facilitate future research, we
open-source codes and model weights at https://github.com/OpenBMB/VisCPM.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/OpenBMB/VisCPM.git</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bidirectional Temporal Diffusion Model for Temporally Consistent Human
  Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00574v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00574v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tserendorj Adiya, Jae Shin Yoon, Jungeun Lee, Sanghun Kim, Hwasup Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a method to generate temporally coherent human animation from a
single image, a video, or a random noise. This problem has been formulated as
modeling of an auto-regressive generation, i.e., to regress past frames to
decode future frames. However, such unidirectional generation is highly prone
to motion drifting over time, generating unrealistic human animation with
significant artifacts such as appearance distortion. We claim that
bidirectional temporal modeling enforces temporal coherence on a generative
network by largely suppressing the motion ambiguity of human appearance. To
prove our claim, we design a novel human animation framework using a denoising
diffusion model: a neural network learns to generate the image of a person by
denoising temporal Gaussian noises whose intermediate results are
cross-conditioned bidirectionally between consecutive frames. In the
experiments, our method demonstrates strong performance compared to existing
unidirectional approaches with realistic temporal coherence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: see https://typest.github.io/btdm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14468v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14468v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Ku, Cong Wei, Weiming Ren, Harry Yang, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-to-video editing involves editing a source video along with additional
control (such as text prompts, subjects, or styles) to generate a new video
that aligns with the source video and the provided control. Traditional methods
have been constrained to certain editing types, limiting their ability to meet
the wide range of user demands. In this paper, we introduce AnyV2V, a novel
training-free framework designed to simplify video editing into two primary
steps: (1) employing an off-the-shelf image editing model (e.g.
InstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an
existing image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion
and feature injection. In the first stage, AnyV2V can plug in any existing
image editing tools to support an extensive array of video editing tasks.
Beyond the traditional prompt-based editing methods, AnyV2V also can support
novel video editing tasks, including reference-based style transfer,
subject-driven editing, and identity manipulation, which were unattainable by
previous methods. In the second stage, AnyV2V can plug in any existing
image-to-video models to perform DDIM inversion and intermediate feature
injection to maintain the appearance and motion consistency with the source
video. On the prompt-based editing, we show that AnyV2V can outperform the
previous best approach by 35\% on prompt alignment, and 25\% on human
preference. On the three novel tasks, we show that AnyV2V also achieves a high
success rate. We believe AnyV2V will continue to thrive due to its ability to
seamlessly integrate the fast-evolving image editing methods. Such
compatibility can help AnyV2V to increase its versatility to cater to diverse
user demands.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ i<span class="highlight-title">SLAM</span>: Imperative <span class="highlight-title">SLAM</span> <span class="chip">RA-L</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07894v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07894v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taimeng Fu, Shaoshu Su, Yiren Lu, Chen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous Localization and Mapping (SLAM) stands as one of the critical
challenges in robot navigation. A SLAM system often consists of a front-end
component for motion estimation and a back-end system for eliminating
estimation drifts. Recent advancements suggest that data-driven methods are
highly effective for front-end tasks, while geometry-based methods continue to
be essential in the back-end processes. However, such a decoupled paradigm
between the data-driven front-end and geometry-based back-end can lead to
sub-optimal performance, consequently reducing the system's capabilities and
generalization potential. To solve this problem, we proposed a novel
self-supervised imperative learning framework, named imperative SLAM (iSLAM),
which fosters reciprocal correction between the front-end and back-end, thus
enhancing performance without necessitating any external supervision.
Specifically, we formulate the SLAM problem as a bilevel optimization so that
the front-end and back-end are bidirectionally connected. As a result, the
front-end model can learn global geometric knowledge obtained through pose
graph optimization by back-propagating the residuals from the back-end
component. We showcase the effectiveness of this new framework through an
application of stereo-inertial SLAM. The experiments show that the iSLAM
training strategy achieves an accuracy improvement of 22% on average over a
baseline model. To the best of our knowledge, iSLAM is the first SLAM system
showing that the front-end and back-end components can mutually correct each
other in a self-supervised manner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted by IEEE Robotics and Automation Letters
  (RA-L)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NAYER: Noisy Layer Data Generation for Efficient and Effective Data-free
  Knowledge Distillation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh-Tuan Tran, Trung Le, Xuan-May Le, Mehrtash Harandi, Quan Hung Tran, Dinh Phung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-Free Knowledge Distillation (DFKD) has made significant recent strides
by transferring knowledge from a teacher neural network to a student neural
network without accessing the original data. Nonetheless, existing approaches
encounter a significant challenge when attempting to generate samples from
random noise inputs, which inherently lack meaningful information.
Consequently, these models struggle to effectively map this noise to the
ground-truth sample distribution, resulting in prolonging training times and
low-quality outputs. In this paper, we propose a novel Noisy Layer Generation
method (NAYER) which relocates the random source from the input to a noisy
layer and utilizes the meaningful constant label-text embedding (LTE) as the
input. LTE is generated by using the language model once, and then it is stored
in memory for all subsequent training processes. The significance of LTE lies
in its ability to contain substantial meaningful inter-class information,
enabling the generation of high-quality samples with only a few training steps.
Simultaneously, the noisy layer plays a key role in addressing the issue of
diversity in sample generation by preventing the model from overemphasizing the
constrained label information. By reinitializing the noisy layer in each
iteration, we aim to facilitate the generation of diverse samples while still
retaining the method's efficiency, thanks to the ease of learning provided by
LTE. Experiments carried out on multiple datasets demonstrate that our NAYER
not only outperforms the state-of-the-art methods but also achieves speeds 5 to
15 times faster than previous approaches. The code is available at
https://github.com/tmtuan1307/nayer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniChest: Conquer-and-Divide Pre-training for Multi-Source Chest X-Ray
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11038v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11038v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianjie Dai, Ruipeng Zhang, Feng Hong, Jiangchao Yao, Ya Zhang, Yanfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Pre-training (VLP) that utilizes the multi-modal information
to promote the training efficiency and effectiveness, has achieved great
success in vision recognition of natural domains and shown promise in medical
imaging diagnosis for the Chest X-Rays (CXRs). However, current works mainly
pay attention to the exploration on single dataset of CXRs, which locks the
potential of this powerful paradigm on larger hybrid of multi-source CXRs
datasets. We identify that although blending samples from the diverse sources
offers the advantages to improve the model generalization, it is still
challenging to maintain the consistent superiority for the task of each source
due to the existing heterogeneity among sources. To handle this dilemma, we
design a Conquer-and-Divide pre-training framework, termed as UniChest, aiming
to make full use of the collaboration benefit of multiple sources of CXRs while
reducing the negative influence of the source heterogeneity. Specially, the
``Conquer" stage in UniChest encourages the model to sufficiently capture
multi-source common patterns, and the ``Divide" stage helps squeeze
personalized patterns into different small experts (query networks). We conduct
thorough experiments on many benchmarks, e.g., ChestX-ray14, CheXpert,
Vindr-CXR, Shenzhen, Open-I and SIIM-ACR Pneumothorax, verifying the
effectiveness of UniChest over a range of baselines, and release our codes and
pre-training models at https://github.com/Elfenreigen/UniChest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE Transactions on Medical Imaging</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spacewalk-18: A Benchmark for Multimodal and Long-form Procedural Video
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18773v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18773v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohan Myer Krishnan, Zitian Tang, Zhiqiu Yu, Chen Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning from videos is an emerging research area that enables robots to
acquire skills from human demonstrations, such as procedural videos. To do
this, video-language models must be able to obtain structured understandings,
such as the temporal segmentation of a demonstration into sequences of actions
and skills, and to generalize the understandings to novel domains. In pursuit
of this goal, we introduce Spacewalk-18, a benchmark containing two tasks: (1)
step recognition and (2) intra-video retrieval over a dataset of temporally
segmented and labeled tasks in International Space Station spacewalk
recordings. In tandem, the two tasks quantify a model's ability to make use of:
(1) out-of-domain visual information; (2) a high temporal context window; and
(3) multimodal (e.g. visual and speech) domains. This departs from existing
benchmarks for procedural video understanding, which typically deal with short
context lengths and can be solved with a single modality. Spacewalk-18, with
its inherent multimodal and long-form complexity, exposes the high difficulty
of task recognition and segmentation. We find that state-of-the-art methods
perform poorly on our benchmark, but improvements can be obtained by
incorporating information from longer-range temporal context across different
modalities. Our experiments underscore the need to develop new approaches to
these tasks. Data, model, and code will be released at
https://brown-palm.github.io/Spacewalk-18/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under submission. Code and models will be released at
  https://brown-palm.github.io/Spacewalk-18/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-21T00:00:00Z">2024-03-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">61</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ODTFormer: Efficient Obstacle Detection and Tracking with Stereo Cameras
  Based on Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianye Ding, Hongyu Li, Huaizu Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Obstacle detection and tracking represent a critical component in robot
autonomous navigation. In this paper, we propose ODTFormer, a Transformer-based
model to address both obstacle detection and tracking problems. For the
detection task, our approach leverages deformable attention to construct a 3D
cost volume, which is decoded progressively in the form of voxel occupancy
grids. We further track the obstacles by matching the voxels between
consecutive frames. The entire model can be optimized in an end-to-end manner.
Through extensive experiments on DrivingStereo and KITTI benchmarks, our model
achieves state-of-the-art performance in the obstacle detection task. We also
report comparable accuracy to state-of-the-art obstacle tracking models while
requiring only a fraction of their computation cost, typically ten-fold to
twenty-fold less. The code and model weights will be publicly released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SDP Synthesis of Maximum Coverage Trees for Probabilistic Planning under
  Control Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naman Aggarwal, Jonathan P. How
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper presents Maximal Covariance Backward Reachable Trees (MAXCOVAR
BRT), which is a multi-query algorithm for planning of dynamic systems under
stochastic motion uncertainty and constraints on the control input with
explicit coverage guarantees. In contrast to existing roadmap-based
probabilistic planning methods that sample belief nodes randomly and draw edges
between them \cite{csbrm_tro2024}, under control constraints, the reachability
of belief nodes needs to be explicitly established and is determined by
checking the feasibility of a non-convex program. Moreover, there is no
explicit consideration of coverage of the roadmap while adding nodes and edges
during the construction procedure for the existing methods. Our contribution is
a novel optimization formulation to add nodes and construct the corresponding
edge controllers such that the generated roadmap results in provably maximal
coverage under control constraints as compared to any other method of adding
nodes and edges. We characterize formally the notion of coverage of a roadmap
in this stochastic domain via introduction of the h-$\operatorname{BRS}$
(Backward Reachable Set of Distributions) of a tree of distributions under
control constraints, and also support our method with extensive simulations on
a 6 DoF model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extended Reality for Enhanced Human-Robot Collaboration: a
  Human-in-the-Loop Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yehor Karpichev, Todd Charter, Homayoun Najjaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of automation has provided an opportunity to achieve higher
efficiency in manufacturing processes, yet it often compromises the flexibility
required to promptly respond to evolving market needs and meet the demand for
customization. Human-robot collaboration attempts to tackle these challenges by
combining the strength and precision of machines with human ingenuity and
perceptual understanding. In this paper, we conceptualize and propose an
implementation framework for an autonomous, machine learning-based manipulator
that incorporates human-in-the-loop principles and leverages Extended Reality
(XR) to facilitate intuitive communication and programming between humans and
robots. Furthermore, the conceptual framework foresees human involvement
directly in the robot learning process, resulting in higher adaptability and
task generalization. The paper highlights key technologies enabling the
proposed framework, emphasizing the importance of developing the digital
ecosystem as a whole. Additionally, we review the existent implementation
approaches of XR in human-robot collaboration, showcasing diverse perspectives
and methodologies. The challenges and future outlooks are discussed, delving
into the major obstacles and potential research avenues of XR for more natural
human-robot interaction and integration in the industrial landscape.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VXP: Voxel-Cross-Pixel Large-scale Image-<span class="highlight-title">LiDAR</span> Place Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun-Jin Li, Mariia Gladkova, Yan Xia, Rui Wang, Daniel Cremers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works on the global place recognition treat the task as a retrieval
problem, where an off-the-shelf global descriptor is commonly designed in
image-based and LiDAR-based modalities. However, it is non-trivial to perform
accurate image-LiDAR global place recognition since extracting consistent and
robust global descriptors from different domains (2D images and 3D point
clouds) is challenging. To address this issue, we propose a novel
Voxel-Cross-Pixel (VXP) approach, which establishes voxel and pixel
correspondences in a self-supervised manner and brings them into a shared
feature space. Specifically, VXP is trained in a two-stage manner that first
explicitly exploits local feature correspondences and enforces similarity of
global descriptors. Extensive experiments on the three benchmarks (Oxford
RobotCar, ViViD++ and KITTI) demonstrate our method surpasses the
state-of-the-art cross-modal retrieval by a large margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page https://yunjinli.github.io/projects-vxp/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Co-Optimization of Environment and Policies for Decentralized
  Multi-Agent Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhan Gao, Guang Yang, Amanda Prorok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work views the multi-agent system and its surrounding environment as a
co-evolving system, where the behavior of one affects the other. The goal is to
take both agent actions and environment configurations as decision variables,
and optimize these two components in a coordinated manner to improve some
measure of interest. Towards this end, we consider the problem of decentralized
multi-agent navigation in cluttered environments. By introducing two
sub-objectives of multi-agent navigation and environment optimization, we
propose an $\textit{agent-environment co-optimization}$ problem and develop a
$\textit{coordinated algorithm}$ that alternates between these sub-objectives
to search for an optimal synthesis of agent actions and obstacle configurations
in the environment; ultimately, improving the navigation performance. Due to
the challenge of explicitly modeling the relation between agents, environment
and performance, we leverage policy gradient to formulate a model-free learning
mechanism within the coordinated framework. A formal convergence analysis shows
that our coordinated algorithm tracks the local minimum trajectory of an
associated time-varying non-convex optimization problem. Extensive numerical
results corroborate theoretical findings and show the benefits of
co-optimization over baselines. Interestingly, the results also indicate that
optimized environment configurations are able to offer structural guidance that
is key to de-conflicting agents in motion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Hierarchical Control For Constrained Dynamic Task Assignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charlott Vallon, Alessandro Pinto, Bartolomeo Stellato, Francesco Borrelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel data-driven hierarchical control scheme for
managing a fleet of nonlinear, capacity-constrained autonomous agents in an
iterative environment. We propose a control framework consisting of a
high-level dynamic task assignment and routing layer and low-level motion
planning and tracking layer. Each layer of the control hierarchy uses a
data-driven MPC policy, maintaining bounded computational complexity at each
calculation of a new task assignment or actuation input. We utilize collected
data to iteratively refine estimates of agent capacity usage, and update MPC
policy parameters accordingly. Our approach leverages tools from iterative
learning control to integrate learning at both levels of the hierarchy, and
coordinates learning between levels in order to maintain closed-loop
feasibility and performance improvement of the connected architecture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion
  Descriptors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Tsagkas, Jack Rome, Subramanian Ramamoorthy, Oisin Mac Aodha, Chris Xiaoxuan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise manipulation that is generalizable across scenes and objects remains
a persistent challenge in robotics. Current approaches for this task heavily
depend on having a significant number of training instances to handle objects
with pronounced visual and/or geometric part ambiguities. Our work explores the
grounding of fine-grained part descriptors for precise manipulation in a
zero-shot setting by utilizing web-trained text-to-image diffusion-based
generative models. We tackle the problem by framing it as a dense semantic part
correspondence task. Our model returns a gripper pose for manipulating a
specific part, using as reference a user-defined click from a source image of a
visually different instance of the same object. We require no manual grasping
demonstrations as we leverage the intrinsic object geometry and features.
Practical experiments in a real-world tabletop scenario validate the efficacy
of our approach, demonstrating its potential for advancing semantic-aware
robotics manipulation. Web page: https://tsagkas.github.io/click2grasp
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Based Causal Reasoning for Safe & Robust Next-Best Action
  Selection in Robot Manipulation Tasks <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ricardo Cannizzaro, Michael Groom, Jonathan Routley, Robert Osazuwa Ness, Lars Kunze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe and efficient object manipulation is a key enabler of many real-world
robot applications. However, this is challenging because robot operation must
be robust to a range of sensor and actuator uncertainties. In this paper, we
present a physics-informed causal-inference-based framework for a robot to
probabilistically reason about candidate actions in a block stacking task in a
partially observable setting. We integrate a physics-based simulation of the
rigid-body system dynamics with a causal Bayesian network (CBN) formulation to
define a causal generative probabilistic model of the robot decision-making
process. Using simulation-based Monte Carlo experiments, we demonstrate our
framework's ability to successfully: (1) predict block tower stability with
high accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-best
action for the block stacking task, for execution by an integrated robot
system, achieving 94.2% task success rate. We also demonstrate our framework's
suitability for real-world robot systems by demonstrating successful task
executions with a domestic support robot, with perception and manipulation
sub-system integration. Hence, we show that by embedding physics-based causal
reasoning into robots' decision-making processes, we can make robot task
execution safer, more reliable, and more robust to various types of
uncertainty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures, submitted to 2024 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bringing Robots Home: The Rise of AI Robots in Consumer Electronics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiwei Dong, Yang Liu, Ted Chu, Abdulmotaleb El Saddik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On March 18, 2024, NVIDIA unveiled Project GR00T, a general-purpose
multimodal generative AI model designed specifically for training humanoid
robots. Preceding this event, Tesla's unveiling of the Optimus Gen 2 humanoid
robot on December 12, 2023, underscored the profound impact robotics is poised
to have on reshaping various facets of our daily lives. While robots have long
dominated industrial settings, their presence within our homes is a burgeoning
phenomenon. This can be attributed, in part, to the complexities of domestic
environments and the challenges of creating robots that can seamlessly
integrate into our daily routines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Consumer Electronics Magazine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring 3D Human Pose Estimation and Forecasting from the Robot's
  Perspective: The HARPER <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Avogaro. Andrea Toaiari, Federico Cunico, Xiangmin Xu, Haralambos Dafas, Alessandro Vinciarelli, Emma Li, Marco Cristani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce HARPER, a novel dataset for 3D body pose estimation and forecast
in dyadic interactions between users and \spot, the quadruped robot
manufactured by Boston Dynamics. The key-novelty is the focus on the robot's
perspective, i.e., on the data captured by the robot's sensors. These make 3D
body pose analysis challenging because being close to the ground captures
humans only partially. The scenario underlying HARPER includes 15 actions, of
which 10 involve physical contact between the robot and users. The Corpus
contains not only the recordings of the built-in stereo cameras of Spot, but
also those of a 6-camera OptiTrack system (all recordings are synchronized).
This leads to ground-truth skeletal representations with a precision lower than
a millimeter. In addition, the Corpus includes reproducible benchmarks on 3D
Human Pose Estimation, Human Pose Forecasting, and Collision Prediction, all
based on publicly available baseline approaches. This enables future HARPER
users to rigorously compare their results with those we provide in this work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Model Learning and Adaptive Tracking Control of Magnetic
  Micro-Robots for Non-Contact Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongyi Jia, Shu Miao, Junjian Zhou, Niandong Jiao, Lianqing Liu, Xiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic microrobots can be navigated by an external magnetic field to
autonomously move within living organisms with complex and unstructured
environments. Potential applications include drug delivery, diagnostics, and
therapeutic interventions. Existing techniques commonly impart magnetic
properties to the target object,or drive the robot to contact and then
manipulate the object, both probably inducing physical damage. This paper
considers a non-contact formulation, where the robot spins to generate a
repulsive field to push the object without physical contact. Under such a
formulation, the main challenge is that the motion model between the input of
the magnetic field and the output velocity of the target object is commonly
unknown and difficult to analyze. To deal with it, this paper proposes a
data-driven-based solution. A neural network is constructed to efficiently
estimate the motion model. Then, an approximate model-based optimal control
scheme is developed to push the object to track a time-varying trajectory,
maintaining the non-contact with distance constraints. Furthermore, a
straightforward planner is introduced to assess the adaptability of non-contact
manipulation in a cluttered unstructured environment. Experimental results are
presented to show the tracking and navigation performance of the proposed
scheme.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, received by 2024 IEEE International Conference on
  Robotics and Automation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DaCapo: Accelerating Continuous Learning in Autonomous Systems for Video
  Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoonsung Kim, Changhun Oh, Jinwoo Hwang, Wonung Kim, Seongryong Oh, Yubin Lee, Hardik Sharma, Amir Yazdanbakhsh, Jongse Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural network (DNN) video analytics is crucial for autonomous systems
such as self-driving vehicles, unmanned aerial vehicles (UAVs), and security
robots. However, real-world deployment faces challenges due to their limited
computational resources and battery power. To tackle these challenges,
continuous learning exploits a lightweight "student" model at deployment
(inference), leverages a larger "teacher" model for labeling sampled data
(labeling), and continuously retrains the student model to adapt to changing
scenarios (retraining). This paper highlights the limitations in
state-of-the-art continuous learning systems: (1) they focus on computations
for retraining, while overlooking the compute needs for inference and labeling,
(2) they rely on power-hungry GPUs, unsuitable for battery-operated autonomous
systems, and (3) they are located on a remote centralized server, intended for
multi-tenant scenarios, again unsuitable for autonomous systems due to privacy,
network availability, and latency concerns. We propose a hardware-algorithm
co-designed solution for continuous learning, DaCapo, that enables autonomous
systems to perform concurrent executions of inference, labeling, and training
in a performant and energy-efficient manner. DaCapo comprises (1) a
spatially-partitionable and precision-flexible accelerator enabling parallel
execution of kernels on sub-accelerators at their respective precisions, and
(2) a spatiotemporal resource allocation algorithm that strategically navigates
the resource-accuracy tradeoff space, facilitating optimal decisions for
resource allocation to achieve maximal accuracy. Our evaluation shows that
DaCapo achieves 6.5% and 5.5% higher accuracy than a state-of-the-art GPU-based
continuous learning systems, Ekya and EOMU, respectively, while consuming 254x
less power.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Study of Real-Time Implementable Cooperative Aerial
  Manipulation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stamatina C. Barakou, Costas S. Tzafestas, Kimon P. Valavanis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey paper focuses on quadrotor- and multirotor- based cooperative
aerial manipulation. Emphasis is first given on comparing and evaluating
prototype systems that have been implemented and tested in real-time in diverse
application environments. Underlying modeling and control approaches are also
discussed and compared. The outcome of the survey allows for understanding the
motivation and rationale to develop such systems, their applicability and
implementability in diverse applications and also challenges that need to be
addressed and overcome. Moreover, the survey provides a guide to develop the
next generation of prototype systems based on preferred characteristics,
functionality, operability and application domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to MDPI Drones</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tell Me What You Want (What You Really, Really Want): Addressing the
  Expectation Gap for Goal Conveyance from Humans to Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Leahy, Ho Chit Siu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conveying human goals to autonomous systems (AS) occurs both when the system
is being designed and when it is being operated. The design-step conveyance is
typically mediated by robotics and AI engineers, who must appropriately capture
end-user requirements and concepts of operations, while the operation-step
conveyance is mediated by the design, interfaces, and behavior of the AI.
However, communication can be difficult during both these periods because of
mismatches in the expectations and expertise of the end-user and the
roboticist, necessitating more design cycles to resolve. We examine some of the
barriers in communicating system design requirements, and develop an
augmentation for applied cognitive task analysis (ACTA) methods, that we call
robot task analysis (RTA), pertaining specifically to the development of
autonomous systems. Further, we introduce a top-down view of an underexplored
area of friction between requirements communication -- implied human
expectations -- utilizing a collection of work primarily from experimental
psychology and social sciences. We show how such expectations can be used in
conjunction with task-specific expectations and the system design process for
AS to improve design team communication, alleviate barriers to user rejection,
and reduce the number of design cycles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the End-User Development for Human-Robot Interaction
  (EUD4HRI) workshop at HRI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distilling Reinforcement Learning Policies for Interpretable Robot
  Locomotion: Gradient Boosting Machines and Symbolic Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando Acero, Zhibin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in reinforcement learning (RL) have led to remarkable
achievements in robot locomotion capabilities. However, the complexity and
``black-box'' nature of neural network-based RL policies hinder their
interpretability and broader acceptance, particularly in applications demanding
high levels of safety and reliability. This paper introduces a novel approach
to distill neural RL policies into more interpretable forms using Gradient
Boosting Machines (GBMs), Explainable Boosting Machines (EBMs) and Symbolic
Regression. By leveraging the inherent interpretability of generalized additive
models, decision trees, and analytical expressions, we transform opaque neural
network policies into more transparent ``glass-box'' models. We train expert
neural network policies using RL and subsequently distill them into (i) GBMs,
(ii) EBMs, and (iii) symbolic policies. To address the inherent distribution
shift challenge of behavioral cloning, we propose to use the Dataset
Aggregation (DAgger) algorithm with a curriculum of episode-dependent
alternation of actions between expert and distilled policies, to enable
efficient distillation of feedback control policies. We evaluate our approach
on various robot locomotion gaits -- walking, trotting, bounding, and pacing --
and study the importance of different observations in joint actions for
distilled policies using various methods. We train neural expert policies for
205 hours of simulated experience and distill interpretable policies with only
10 minutes of simulated interaction for each gait using the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation and Deployment of <span class="highlight-title">LiDAR</span>-based Place Recognition in Dense
  Forests 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haedam Oh, Nived Chebrolu, Matias Mattamala, Leonard Freißmuth, Maurice Fallon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many LiDAR place recognition systems have been developed and tested
specifically for urban driving scenarios. Their performance in natural
environments such as forests and woodlands have been studied less closely. In
this paper, we analyzed the capabilities of four different LiDAR place
recognition systems, both handcrafted and learning-based methods, using LiDAR
data collected with a handheld device and legged robot within dense forest
environments. In particular, we focused on evaluating localization where there
is significant translational and orientation difference between corresponding
LiDAR scan pairs. This is particularly important for forest survey systems
where the sensor or robot does not follow a defined road or path. Extending our
analysis we then incorporated the best performing approach, Logg3dNet, into a
full 6-DoF pose estimation system -- introducing several verification layers
for precise registration. We demonstrated the performance of our methods in
three operational modes: online SLAM, offline multi-mission SLAM map merging,
and relocalization into a prior map. We evaluated these modes using data
captured in forests from three different countries, achieving 80% of correct
loop closures candidates with baseline distances up to 5m, and 60% up to 10m.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exosense: A Vision-Centric Scene Understanding System For Safe
  Exoskeleton Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianeng Wang, Matias Mattamala, Christina Kassab, Lintong Zhang, Maurice Fallon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exoskeletons for daily use by those with mobility impairments are being
developed. They will require accurate and robust scene understanding systems.
Current research has used vision to identify immediate terrain and geometric
obstacles, however these approaches are constrained to detections directly in
front of the user and are limited to classifying a finite range of terrain
types (e.g., stairs, ramps and level-ground). This paper presents Exosense, a
vision-centric scene understanding system which is capable of generating rich,
globally-consistent elevation maps, incorporating both semantic and terrain
traversability information. It features an elastic Atlas mapping framework
associated with a visual SLAM pose graph, embedded with open-vocabulary room
labels from a Vision-Language Model (VLM). The device's design includes a wide
field-of-view (FoV) fisheye multi-camera system to mitigate the challenges
introduced by the exoskeleton walking pattern. We demonstrate the system's
robustness to the challenges of typical periodic walking gaits, and its ability
to construct accurate semantically-rich maps in indoor settings. Additionally,
we showcase its potential for motion planning -- providing a step towards safe
navigation for exoskeletons.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Optimization for Sample-Efficient Policy Improvement in Robotic
  Manipulation <span class="chip">IROS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Röfer, Iman Nematollahi, Tim Welschehold, Wolfram Burgard, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sample efficient learning of manipulation skills poses a major challenge in
robotics. While recent approaches demonstrate impressive advances in the type
of task that can be addressed and the sensing modalities that can be
incorporated, they still require large amounts of training data. Especially
with regard to learning actions on robots in the real world, this poses a major
problem due to the high costs associated with both demonstrations and
real-world robot interactions. To address this challenge, we introduce
BOpt-GMM, a hybrid approach that combines imitation learning with own
experience collection. We first learn a skill model as a dynamical system
encoded in a Gaussian Mixture Model from a few demonstrations. We then improve
this model with Bayesian optimization building on a small number of autonomous
skill executions in a sparse reward setting. We demonstrate the sample
efficiency of our approach on multiple complex manipulation skills in both
simulations and real-world experiments. Furthermore, we make the code and
pre-trained models publicly available at http://bopt-gmm. cs.uni-freiburg.de.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, 2 tables, submitted to IROS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DexDribbler: Learning Dexterous Soccer Manipulation via Dynamic
  Supervision <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Hu, Kehan Wen, Fisher Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning dexterous locomotion policy for legged robots is becoming
increasingly popular due to its ability to handle diverse terrains and resemble
intelligent behaviors. However, joint manipulation of moving objects and
locomotion with legs, such as playing soccer, receive scant attention in the
learning community, although it is natural for humans and smart animals. A key
challenge to solve this multitask problem is to infer the objectives of
locomotion from the states and targets of the manipulated objects. The implicit
relation between the object states and robot locomotion can be hard to capture
directly from the training experience. We propose adding a feedback control
block to compute the necessary body-level movement accurately and using the
outputs as dynamic joint-level locomotion supervision explicitly. We further
utilize an improved ball dynamic model, an extended context-aided estimator,
and a comprehensive ball observer to facilitate transferring policy learned in
simulation to the real world. We observe that our learning scheme can not only
make the policy network converge faster but also enable soccer robots to
perform sophisticated maneuvers like sharp cuts and turns on flat surfaces, a
capability that was lacking in previous methods. Video and code are available
at https://github.com/SysCV/soccer-player
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Reactions to Incorrect Answers from Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ponkoj Chandra Shill, Md. Azizul Hakim, Muhammad Jahanzeb Khan, Bashira Akter Anima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As robots grow more and more integrated into numerous industries, it is
critical to comprehend how humans respond to their failures. This paper
systematically studies how trust dynamics and system design are affected by
human responses to robot failures. The three-stage survey used in the study
provides a thorough understanding of human-robot interactions. While the second
stage concentrates on interaction details, such as robot precision and error
acknowledgment, the first stage collects demographic data and initial levels of
trust. In the last phase, participants' perceptions are examined after the
encounter, and trust dynamics, forgiveness, and propensity to suggest robotic
technologies are evaluated. Results show that participants' trust in robotic
technologies increased significantly when robots acknowledged their errors or
limitations to participants and their willingness to suggest robots for
activities in the future points to a favorable change in perception,
emphasizing the role that direct engagement has in influencing trust dynamics.
By providing useful advice for creating more sympathetic, responsive, and
reliable robotic systems, the study advances the science of human-robot
interaction and promotes a wider adoption of robotic technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures, 1 table, Ro-Man 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UAV-Assisted Maritime Search and Rescue: A Holistic Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Messmer, Benjamin Kiefer, Leon Amadeus Varga, Andreas Zell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore the application of Unmanned Aerial Vehicles (UAVs)
in maritime search and rescue (mSAR) missions, focusing on medium-sized
fixed-wing drones and quadcopters. We address the challenges and limitations
inherent in operating some of the different classes of UAVs, particularly in
search operations. Our research includes the development of a comprehensive
software framework designed to enhance the efficiency and efficacy of SAR
operations. This framework combines preliminary detection onboard UAVs with
advanced object detection at ground stations, aiming to reduce visual strain
and improve decision-making for operators. It will be made publicly available
upon publication. We conduct experiments to evaluate various Region of Interest
(RoI) proposal methods, especially by imposing simulated limited bandwidth on
them, an important consideration when flying remote or offshore operations.
This forces the algorithm to prioritize some predictions over others.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Salzmann, Markus Ryll, Alex Bewley, Matthias Minderer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual relationship detection aims to identify objects and their
relationships in images. Prior methods approach this task by adding separate
relationship modules or decoders to existing object detection architectures.
This separation increases complexity and hinders end-to-end training, which
limits performance. We propose a simple and highly efficient decoder-free
architecture for open-vocabulary visual relationship detection. Our model
consists of a Transformer-based image encoder that represents objects as tokens
and models their relationships implicitly. To extract relationship information,
we introduce an attention mechanism that selects object pairs likely to form a
relationship. We provide a single-stage recipe to train this model on a mixture
of object and relationship detection data. Our approach achieves
state-of-the-art relationship detection performance on Visual Genome and on the
large-vocabulary GQA benchmark at real-time inference speeds. We provide
analyses of zero-shot performance, ablations, and real-world qualitative
examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReFeree: Radar-based efficient global descriptor using a Feature and
  Free space for Place Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byunghee Choi, Hogyun Kim, Younggun Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radar is highlighted for robust sensing capabilities in adverse weather
conditions (e.g. dense fog, heavy rain, or snowfall). In addition, Radar can
cover wide areas and penetrate small particles. Despite these advantages,
Radar-based place recognition remains in the early stages compared to other
sensors due to its unique characteristics such as low resolution, and
significant noise. In this paper, we propose a Radarbased place recognition
utilizing a descriptor called ReFeree using a feature and free space. Unlike
traditional methods, we overwhelmingly summarize the Radar image. Despite being
lightweight, it contains semi-metric information and is also outstanding from
the perspective of place recognition performance. For concrete validation, we
test a single session from the MulRan dataset and a multi-session from the
Oxford Radar RobotCar and the Boreas dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HCTO: Optimality-Aware <span class="highlight-title">LiDAR</span> Inertial <span class="highlight-title">Odometry</span> with Hybrid Continuous
  Time Optimization for Compact Wearable <span class="highlight-title">Mapping</span> System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianping Li, Shenghai Yuan, Muqing Cao, Thien-Minh Nguyen, Kun Cao, Lihua Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compact wearable mapping system (WMS) has gained significant attention due to
their convenience in various applications. Specifically, it provides an
efficient way to collect prior maps for 3D structure inspection and robot-based
"last-mile delivery" in complex environments. However, vibrations in human
motion and the uneven distribution of point cloud features in complex
environments often lead to rapid drift, which is a prevalent issue when
applying existing LiDAR Inertial Odometry (LIO) methods on low-cost WMS. To
address these limitations, we propose a novel LIO for WMSs based on Hybrid
Continuous Time Optimization (HCTO) considering the optimality of Lidar
correspondences. First, HCTO recognizes patterns in human motion
(high-frequency part, low-frequency part, and constant velocity part) by
analyzing raw IMU measurements. Second, HCTO constructs hybrid IMU factors
according to different motion states, which enables robust and accurate
estimation against vibration-induced noise in the IMU measurements. Third, the
best point correspondences are selected using optimal design to achieve
real-time performance and better odometry accuracy. We conduct experiments on
head-mounted WMS datasets to evaluate the performance of our system,
demonstrating significant advantages over state-of-the-art methods. Video
recordings of experiments can be found on the project page of HCTO:
\href{https://github.com/kafeiyin00/HCTO}{https://github.com/kafeiyin00/HCTO}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Large Language Model-based Room-Object Relationships
  Knowledge for Enhancing Multimodal-Input Object Goal Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leyuan Sun, Asako Kanezaki, Guillaume Caron, Yusuke Yoshiyasu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-goal navigation is a crucial engineering task for the community of
embodied navigation; it involves navigating to an instance of a specified
object category within unseen environments. Although extensive investigations
have been conducted on both end-to-end and modular-based, data-driven
approaches, fully enabling an agent to comprehend the environment through
perceptual knowledge and perform object-goal navigation as efficiently as
humans remains a significant challenge. Recently, large language models have
shown potential in this task, thanks to their powerful capabilities for
knowledge extraction and integration. In this study, we propose a data-driven,
modular-based approach, trained on a dataset that incorporates common-sense
knowledge of object-to-room relationships extracted from a large language
model. We utilize the multi-channel Swin-Unet architecture to conduct
multi-task learning incorporating with multimodal inputs. The results in the
Habitat simulator demonstrate that our framework outperforms the baseline by an
average of 10.6% in the efficiency metric, Success weighted by Path Length
(SPL). The real-world demonstration shows that the proposed approach can
efficiently conduct this task by traversing several rooms. For more details and
real-world demonstrations, please check our project webpage
(https://sunleyuan.github.io/ObjectNav).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>will soon submit to the Elsevier journal, Advanced Engineering
  Informatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extrinsic Calibration of Multiple <span class="highlight-title">LiDAR</span>s for a Mobile Robot based on
  Floor Plane And Object Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shun Niijima, Atsushi Suzuki, Ryoichi Tsuzaki, Masaya Kinoshita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile robots equipped with multiple light detection and ranging (LiDARs) and
capable of recognizing their surroundings are increasing due to the
minitualization and cost reduction of LiDAR. This paper proposes a target-less
extrinsic calibration method of multiple LiDARs with non-overlapping field of
view (FoV). The proposed method uses accumulated point clouds of floor plane
and objects while in motion. It enables accurate calibration with challenging
configuration of LiDARs that directed towards the floor plane, caused by biased
feature values. Additionally, the method includes a noise removal module that
considers the scanning pattern to address bleeding points, which are noises of
significant source of error in point cloud alignment using high-density LiDARs.
Evaluations through simulation demonstrate that the proposed method achieved
higher accuracy extrinsic calibration with two and four LiDARs than
conventional methods, regardless type of objects. Furthermore, the experiments
using a real mobile robot has shown that our proposed noise removal module can
eliminate noise more precisely than conventional methods, and the estimated
extrinsic parameters have successfully created consistent 3D maps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8pages, 10figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Development of a Compact Robust Passive Transformable Omni-Ball for
  Enhanced Step-Climbing and Vibration Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuo Hongo, Takashi Kito, Yasuhisa Kamikawa, Masaya Kinoshita, Yasunori Kawanami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the Passive Transformable Omni-Ball (PTOB), an advanced
omnidirectional wheel engineered to enhance step-climbing performance,
incorporate built-in actuators, diminish vibrations, and fortify structural
integrity. By modifying the omni-ball's structure from two to three segments,
we have achieved improved in-wheel actuation and a reduction in vibrational
feedback. Additionally, we have implemented a sliding mechanism in the follower
wheels to boost the wheel's step-climbing abilities. A prototype with a 127 mm
diameter PTOB was constructed, which confirmed its functionality for
omnidirectional movement and internal actuation. Compared to a traditional
omni-wheel, the PTOB demonstrated a comparable level of vibration while
offering superior capabilities. Extensive testing in varied settings showed
that the PTOB can adeptly handle step obstacles up to 45 mm, equivalent to 35
$\%$ of the wheel's diameter, in both the forward and lateral directions. The
PTOB showcased robust construction and proved to be versatile in navigating
through environments with diverse obstacles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Locomotion via Zero-order Stochastic Nonlinear Model Predictive
  Control with Guard Saltation Matrix 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sotaro Katayama, Noriaki Takasugi, Mitsuhisa Kaneko, Norio Nagatsuka, and Masaya Kinoshita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a stochastic/robust nonlinear model predictive control
(NMPC) to enhance the robustness of legged locomotion against contact
uncertainties. We integrate the contact uncertainties into the covariance
propagation of stochastic/robust NMPC framework by leveraging the guard
saltation matrix and an extended Kalman filter-like covariance update. We
achieve fast stochastic/robust NMPC computation by utilizing the zero-order
stochastic/robust NMPC algorithm with additional improvements in computational
efficiency concerning the feedback gains. We conducted numerical experiments
and demonstrate that the proposed method can accurately forecast future state
covariance and generate trajectories that satisfies constraints even in the
presence of the contact uncertainties. Hardware experiments on the perceptive
locomotion of a wheeled-legged robot were also carried out, validating the
feasibility of the proposed method in a real-world system with limited on-board
computation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evidential Semantic <span class="highlight-title">Mapping</span> in Off-road Environments with
  Uncertainty-aware Bayesian Kernel Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyoung Kim, Junwon Seo, Jihong Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic mapping with Bayesian Kernel Inference (BKI) has shown promise in
creating semantic maps by effectively leveraging local spatial information.
However, existing semantic mapping methods face challenges in constructing
reliable maps in unstructured outdoor scenarios due to unreliable semantic
predictions. To address this issue, we propose an evidential semantic mapping,
which can enhance reliability in perceptually challenging off-road
environments. We integrate Evidential Deep Learning into the semantic
segmentation network to obtain the uncertainty estimate of semantic prediction.
Subsequently, this semantic uncertainty is incorporated into an
uncertainty-aware BKI, tailored to prioritize more confident semantic
predictions when accumulating semantic information. By adaptively handling
semantic uncertainties, the proposed framework constructs robust
representations of the surroundings even in previously unseen environments.
Comprehensive experiments across various off-road datasets demonstrate that our
framework enhances accuracy and robustness, consistently outperforming existing
methods in scenes with high perceptual uncertainties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our project website can be found at
  https://kjyoung.github.io/Homepage/#/Projects/Evidential-Semantic-Mapping</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantics from Space: Satellite-Guided Thermal Semantic Segmentation
  Annotation for Aerial Field Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Connor Lee, Saraswati Soedarmadji, Matthew Anderson, Anthony J. Clark, Soon-Jo Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new method to automatically generate semantic segmentation
annotations for thermal imagery captured from an aerial vehicle by utilizing
satellite-derived data products alongside onboard global positioning and
attitude estimates. This new capability overcomes the challenge of developing
thermal semantic perception algorithms for field robots due to the lack of
annotated thermal field datasets and the time and costs of manual annotation,
enabling precise and rapid annotation of thermal data from field collection
efforts at a massively-parallelizable scale. By incorporating a
thermal-conditioned refinement step with visual foundation models, our approach
can produce highly-precise semantic segmentation labels using low-resolution
satellite land cover data for little-to-no cost. It achieves 98.5% of the
performance from using costly high-resolution options and demonstrates between
70-160% improvement over popular zero-shot semantic segmentation methods based
on large vision-language models currently used for generating annotations for
RGB imagery. Code will be available at:
https://github.com/connorlee77/aerial-auto-segment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Roadmap Towards Automated and Regulated Robotic Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Liu, Mehran Armand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of generative technology opens up possibility for
higher level of automation, and artificial intelligence (AI) embodiment in
robotic systems is imminent. However, due to the blackbox nature of the
generative technology, the generation of the knowledge and workflow scheme is
uncontrolled, especially in a dynamic environment and a complex scene. This
poses challenges to regulations in safety-demanding applications such as
medical scenes. We argue that the unregulated generative processes from AI is
fitted for low level end tasks, but intervention in the form of manual or
automated regulation should happen post-workflow-generation and
pre-robotic-execution. To address this, we propose a roadmap that can lead to
fully automated and regulated robotic systems. In this paradigm, the high level
policies are generated as structured graph data, enabling regulatory oversight
and reusability, while the code base for lower level tasks is generated by
generative models. Our approach aims the transitioning from expert knowledge to
regulated action, akin to the iterative processes of study, practice, scrutiny,
and execution in human tasks. We identify the generative and deterministic
processes in a design cycle, where generative processes serve as a text-based
world simulator and the deterministic processes generate the executable system.
We propose State Machine Seralization Language (SMSL) to be the conversion
point between text simulator and executable workflow control. From there, we
analyze the modules involved based on the current literature, and discuss human
in the loop. As a roadmap, this work identifies the current possible
implementation and future work. This work does not provide an implemented
system but envisions to inspire the researchers working on the direction in the
roadmap. We implement the SMSL and D-SFO paradigm that serve as the starting
point of the roadmap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GelLink: A Compact Multi-phalanx Finger with Vision-based Tactile
  Sensing and Proprioception <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Ma,  Jialiang,  Zhao, Edward Adelson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared to fully-actuated robotic end-effectors, underactuated ones are
generally more adaptive, robust, and cost-effective. However, state estimation
for underactuated hands is usually more challenging. Vision-based tactile
sensors, like Gelsight, can mitigate this issue by providing high-resolution
tactile sensing and accurate proprioceptive sensing. As such, we present
GelLink, a compact, underactuated, linkage-driven robotic finger with low-cost,
high-resolution vision-based tactile sensing and proprioceptive sensing
capabilities. In order to reduce the amount of embedded hardware, i.e. the
cameras and motors, we optimize the linkage transmission with a planar linkage
mechanism simulator and develop a planar reflection simulator to simplify the
tactile sensing hardware. As a result, GelLink only requires one motor to
actuate the three phalanges, and one camera to capture tactile signals along
the entire finger. Overall, GelLink is a compact robotic finger that shows
adaptability and robustness when performing grasping tasks. The integration of
vision-based tactile sensors can significantly enhance the capabilities of
underactuated fingers and potentially broaden their future usage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Supplement video: https://www.youtube.com/watch?v=hZwUpAig5C0 . 7
  pages, 9 figures. ICRA 2024 (IEEE International Conference on Robotics and
  Automation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Change: Choreographing Mixed Traffic Through Lateral Control
  and Hierarchical Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dawei Wang, Weizi Li, Lei Zhu, Jia Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The management of mixed traffic that consists of robot vehicles (RVs) and
human-driven vehicles (HVs) at complex intersections presents a multifaceted
challenge. Traditional signal controls often struggle to adapt to dynamic
traffic conditions and heterogeneous vehicle types. Recent advancements have
turned to strategies based on reinforcement learning (RL), leveraging its
model-free nature, real-time operation, and generalizability over different
scenarios. We introduce a hierarchical RL framework to manage mixed traffic
through precise longitudinal and lateral control of RVs. Our proposed
hierarchical framework combines the state-of-the-art mixed traffic control
algorithm as a high level decision maker to improve the performance and
robustness of the whole system. Our experiments demonstrate that the framework
can reduce the average waiting time by up to 54% compared to the
state-of-the-art mixed traffic control method. When the RV penetration rate
exceeds 60%, our technique consistently outperforms conventional traffic signal
control programs in terms of the average waiting time for all vehicles at the
intersection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TEeVTOL: Balancing Energy and Time Efficiency in eVTOL Aircraft Path
  Planning Across City-Scale Wind Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songyang Liu, Shuai Li, Haochen Li, Weizi Li, Jindong Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electric vertical-takeoff and landing (eVTOL) aircraft, recognized for their
maneuverability and flexibility, offer a promising alternative to our
transportation system. However, the operational effectiveness of these aircraft
faces many challenges, such as the delicate balance between energy and time
efficiency, stemming from unpredictable environmental factors, including wind
fields. Mathematical modeling-based approaches have been adopted to plan
aircraft flight path in urban wind fields with the goal to save energy and time
costs. While effective, they are limited in adapting to dynamic and complex
environments. To optimize energy and time efficiency in eVTOL's flight through
dynamic wind fields, we introduce a novel path planning method leveraging deep
reinforcement learning. We assess our method with extensive experiments,
comparing it to Dijkstra's algorithm -- the theoretically optimal approach for
determining shortest paths in a weighted graph, where weights represent either
energy or time cost. The results show that our method achieves a graceful
balance between energy and time efficiency, closely resembling the
theoretically optimal values for both objectives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Quadruped Locomotion Using Differentiable Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunlong Song, Sangbae Kim, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While most recent advancements in legged robot control have been driven by
model-free reinforcement learning, we explore the potential of differentiable
simulation. Differentiable simulation promises faster convergence and more
stable training by computing low-variant first-order gradients using the robot
model, but so far, its use for legged robot control has remained limited to
simulation. The main challenge with differentiable simulation lies in the
complex optimization landscape of robotic tasks due to discontinuities in
contact-rich environments, e.g., quadruped locomotion. This work proposes a
new, differentiable simulation framework to overcome these challenges. The key
idea involves decoupling the complex whole-body simulation, which may exhibit
discontinuities due to contact, into two separate continuous domains.
Subsequently, we align the robot state resulting from the simplified model with
a more precise, non-differentiable simulator to maintain sufficient simulation
accuracy. Our framework enables learning quadruped walking in minutes using a
single simulated robot without any parallelization. When augmented with GPU
parallelization, our approach allows the quadruped robot to master diverse
locomotion skills, including trot, pace, bound, and gallop, on challenging
terrains in minutes. Additionally, our policy achieves robust locomotion
performance in the real world zero-shot. To the best of our knowledge, this
work represents the first demonstration of using differentiable simulation for
controlling a real quadruped robot. This work provides several important
insights into using differentiable simulations for legged locomotion in the
real world.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-agent Task-Driven Exploration via Intelligent Map Compression and
  Sharing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evangelos Psomiadis, Dipankar Maity, Panagiotis Tsiotras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the task-driven exploration of unknown environments
with mobile sensors communicating compressed measurements. The sensors explore
the area and transmit their compressed data to another robot, assisting it in
reaching a goal location. We propose a novel communication framework and a
tractable multi-agent exploration algorithm to select the sensors' actions. The
algorithm uses a task-driven measure of uncertainty, resulting from map
compression, as a reward function. We validate the efficacy of our algorithm
through numerical simulations conducted on a realistic map and compare it with
two alternative approaches. The results indicate that the proposed algorithm
effectively decreases the time required for the robot to reach its target
without causing excessive load on the communication network.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Modular Aerial System Based on Homogeneous Quadrotors with
  Fault-Tolerant Control <span class="chip">ICRA2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01477v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01477v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengguang Li, Kai Cui, Heinz Koeppl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The standard quadrotor is one of the most popular and widely used aerial
vehicle of recent decades, offering great maneuverability with mechanical
simplicity. However, the under-actuation characteristic limits its
applications, especially when it comes to generating desired wrench with six
degrees of freedom (DOF). Therefore, existing work often compromises between
mechanical complexity and the controllable DOF of the aerial system. To take
advantage of the mechanical simplicity of a standard quadrotor, we propose a
modular aerial system, IdentiQuad, that combines only homogeneous
quadrotor-based modules. Each IdentiQuad can be operated alone like a standard
quadrotor, but at the same time allows task-specific assembly, increasing the
controllable DOF of the system. Each module is interchangeable within its
assembly. We also propose a general controller for different configurations of
assemblies, capable of tolerating rotor failures and balancing the energy
consumption of each module. The functionality and robustness of the system and
its controller are validated using physics-based simulations for different
assembly configurations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instance-aware Exploration-Verification-Exploitation for Instance
  ImageGoal Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17587v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17587v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Lei, Min Wang, Wengang Zhou, Li Li, Houqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a new embodied vision task, Instance ImageGoal Navigation (IIN) aims to
navigate to a specified object depicted by a goal image in an unexplored
environment.
  The main challenge of this task lies in identifying the target object from
different viewpoints while rejecting similar distractors.
  Existing ImageGoal Navigation methods usually adopt the simple
Exploration-Exploitation framework and ignore the identification of specific
instance during navigation.
  In this work, we propose to imitate the human behaviour of ``getting closer
to confirm" when distinguishing objects from a distance.
  Specifically, we design a new modular navigation framework named
Instance-aware Exploration-Verification-Exploitation (IEVE) for instance-level
image goal navigation.
  Our method allows for active switching among the exploration, verification,
and exploitation actions, thereby facilitating the agent in making reasonable
decisions under different situations.
  On the challenging HabitatMatterport 3D semantic (HM3D-SEM) dataset, our
method surpasses previous state-of-the-art work, with a classical segmentation
model (0.684 vs. 0.561 success) or a robust model (0.702 vs. 0.561 success).
Our code will be made publicly available at https://github.com/XiaohanLei/IEVE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning a Depth Covariance Function <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Dexheimer, Andrew J. Davison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose learning a depth covariance function with applications to
geometric vision tasks. Given RGB images as input, the covariance function can
be flexibly used to define priors over depth functions, predictive
distributions given observations, and methods for active point selection. We
leverage these techniques for a selection of downstream tasks: depth
completion, bundle adjustment, and monocular dense visual odometry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Project page: https://edexheim.github.io/DepthCov/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep learning reduces sensor requirements for gust rejection on a small
  uncrewed aerial vehicle morphing wing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03133v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03133v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin PT. Haughn, Christina Harvey, Daniel J. Inman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a growing need for uncrewed aerial vehicles (UAVs) to operate in
cities. However, the uneven urban landscape and complex street systems cause
large-scale wind gusts that challenge the safe and effective operation of UAVs.
Current gust alleviation methods rely on traditional control surfaces and
computationally expensive modeling to select a control action, leading to a
slower response. Here, we used deep reinforcement learning to create an
autonomous gust alleviation controller for a camber-morphing wing. This method
reduced gust impact by 84%, directly from real-time, on-board pressure signals.
Notably, we found that gust alleviation using signals from only three pressure
taps was statistically indistinguishable from using six signals. This
reduced-sensor fly-by-feel control opens the door to UAV missions in previously
inoperable locations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Human's Gender Perception and Bias toward Non-Humanoid Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12001v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12001v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahya Ramezani, Jose Luis Sanchez-Lopez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As non-humanoid robots increasingly permeate various sectors, understanding
their design implications for human acceptance becomes paramount. Despite their
ubiquity, studies on how to improve human interaction are sparse. Our
investigation, conducted through two surveys, addresses this gap. The first
survey emphasizes non-humanoid robots and human perceptions about gender
attributions, suggesting that both design and perceived gender influence
acceptance. Survey 2 investigates the effects of varying gender cues on robot
designs and their consequent impacts on human-robot interactions. Our findings
highlighted that distinct gender cues can bolster or impede interaction
comfort.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Star-Searcher: A Complete and Efficient Aerial System for Autonomous
  Target Search in Complex Unknown Environments <span class="chip">RA-L</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16348v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16348v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Luo, Zixuan Zhuang, Neng Pan, Chen Feng, Shaojie Shen, Fei Gao, Hui Cheng, Boyu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper tackles the challenge of autonomous target search using unmanned
aerial vehicles (UAVs) in complex unknown environments. To fill the gap in
systematic approaches for this task, we introduce Star-Searcher, an aerial
system featuring specialized sensor suites, mapping, and planning modules to
optimize searching. Path planning challenges due to increased inspection
requirements are addressed through a hierarchical planner with a
visibility-based viewpoint clustering method. This simplifies planning by
breaking it into global and local sub-problems, ensuring efficient global and
local path coverage in real-time. Furthermore, our global path planning employs
a history-aware mechanism to reduce motion inconsistency from frequent map
changes, significantly enhancing search efficiency. We conduct comparisons with
state-of-the-art methods in both simulation and the real world, demonstrating
shorter flight paths, reduced time, and higher target search completeness. Our
approach will be open-sourced for community benefit at
https://github.com/SYSU-STAR/STAR-Searcher.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Aceepted to IEEE RA-L. Code:
  https://github.com/SYSU-STAR/STAR-Searcher. Video:
  https://www.youtube.com/watch?v=08ll_oo_DtU</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models for Multi-Modal Human-Robot Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15174v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15174v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Wang, Stephan Hasler, Daniel Tanneberg, Felix Ocker, Frank Joublin, Antonello Ceravola, Joerg Deigmoeller, Michael Gienger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an innovative large language model (LLM)-based robotic
system for enhancing multi-modal human-robot interaction (HRI). Traditional HRI
systems relied on complex designs for intent estimation, reasoning, and
behavior generation, which were resource-intensive. In contrast, our system
empowers researchers and practitioners to regulate robot behavior through three
key aspects: providing high-level linguistic guidance, creating "atomics" for
actions and expressions the robot can use, and offering a set of examples.
Implemented on a physical robot, it demonstrates proficiency in adapting to
multi-modal inputs and determining the appropriate manner of action to assist
humans with its arms, following researchers' defined guidelines.
Simultaneously, it coordinates the robot's lid, neck, and ear movements with
speech output to produce dynamic, multi-modal expressions. This showcases the
system's potential to revolutionize HRI by shifting from conventional, manual
state-and-flow design methods to an intuitive, guidance-based, and
example-driven approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAkEable: Memory-centered and Affordance-based Task Execution Framework
  for Transferable Mobile Manipulation Skills 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16899v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16899v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Pohl, Fabian Reister, Fabian Peller-Konrad, Tamim Asfour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To perform versatile mobile manipulation tasks in human-centered
environments, the ability to efficiently transfer learned tasks and experiences
from one robot to another or across different environments is key. In this
paper, we present MAkEable, a versatile uni- and multi-manual mobile
manipulation framework that facilitates the transfer of capabilities and
knowledge across different tasks, environments, and robots. Our framework
integrates an affordance-based task description into the memory-centric
cognitive architecture of the ARMAR humanoid robot family, which supports the
sharing of experiences and demonstrations for transfer learning. By
representing mobile manipulation actions through affordances, i.e., interaction
possibilities of the robot with its environment, we provide a unifying
framework for the autonomous uni- and multi-manual manipulation of known and
unknown objects in various environments. We demonstrate the applicability of
the framework in real-world experiments for multiple robots, tasks, and
environments. This includes grasping known and unknown objects, object placing,
bimanual object grasping, memory-enabled skill transfer in a drawer opening
scenario across two different humanoid robots, and a pouring task learned from
human demonstration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Driving Animatronic Robot Facial Expression From Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boren Li, Hang Li, Hangxin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Animatronic robots aim to enable natural human-robot interaction through
lifelike facial expressions. However, generating realistic, speech-synchronized
robot expressions is challenging due to the complexities of facial biomechanics
and responsive motion synthesis. This paper presents a principled,
skinning-centric approach to drive animatronic robot facial expressions from
speech. The proposed approach employs linear blend skinning (LBS) as the core
representation to guide tightly integrated innovations in embodiment design and
motion synthesis. LBS informs the actuation topology, enables human expression
retargeting, and allows speech-driven facial motion generation. The proposed
approach is capable of generating highly realistic, real-time facial
expressions from speech on an animatronic face, significantly advancing robots'
ability to replicate nuanced human expressions for natural interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review. For associated project page, see
  https://library87.github.io/animatronic-face-iros24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ROS-Causal: A ROS-based Causal Analysis Framework for Human-Robot
  Interaction Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16068v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16068v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Castri, Gloria Beraldo, Sariah Mghames, Marc Hanheide, Nicola Bellotto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying robots in human-shared spaces requires understanding interactions
among nearby agents and objects. Modelling cause-and-effect relations through
causal inference aids in predicting human behaviours and anticipating robot
interventions. However, a critical challenge arises as existing causal
discovery methods currently lack an implementation inside the ROS ecosystem,
the standard de facto in robotics, hindering effective utilisation in robotics.
To address this gap, this paper introduces ROS-Causal, a ROS-based framework
for onboard data collection and causal discovery in human-robot spatial
interactions. An ad-hoc simulator, integrated with ROS, illustrates the
approach's effectiveness, showcasing the robot onboard generation of causal
models during data collection. ROS-Causal is available on GitHub:
https://github.com/lcastri/roscausal.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the "Causal-HRI: Causal Learning for Human-Robot
  Interaction" workshop at the 2024 ACM/IEEE International Conference on
  Human-Robot Interaction (HRI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Large Language Models to Facilitate Variable Autonomy for
  Human-Robot Teaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07214v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07214v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Younes Lakhnati, Max Pascher, Jens Gerken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a rapidly evolving digital landscape autonomous tools and robots are
becoming commonplace. Recognizing the significance of this development, this
paper explores the integration of Large Language Models (LLMs) like Generative
pre-trained transformer (GPT) into human-robot teaming environments to
facilitate variable autonomy through the means of verbal human-robot
communication. In this paper, we introduce a novel framework for such a
GPT-powered multi-robot testbed environment, based on a Unity Virtual Reality
(VR) setting. This system allows users to interact with robot agents through
natural language, each powered by individual GPT cores. By means of OpenAI's
function calling, we bridge the gap between unstructured natural language input
and structure robot actions. A user study with 12 participants explores the
effectiveness of GPT-4 and, more importantly, user strategies when being given
the opportunity to converse in natural language within a multi-robot
environment. Our findings suggest that users may have preconceived expectations
on how to converse with robots and seldom try to explore the actual language
and cognitive capabilities of their robot collaborators. Still, those users who
did explore where able to benefit from a much more natural flow of
communication and human-like back-and-forth. We provide a set of lessons
learned for future research and technical implementations of similar systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Frontiers in Robotics and AI, Variable Autonomy for Human-Robot
  Teaming</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SLIM: Skill Learning with Multiple Critics <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Emukpere, Bingbing Wu, Julien Perez, Jean-Michel Renders
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised skill learning aims to acquire useful behaviors that leverage
the underlying dynamics of the environment. Latent variable models, based on
mutual information maximization, have been successful in this task but still
struggle in the context of robotic manipulation. As it requires impacting a
possibly large set of degrees of freedom composing the environment, mutual
information maximization fails alone in producing useful and safe manipulation
behaviors. Furthermore, tackling this by augmenting skill discovery rewards
with additional rewards through a naive combination might fail to produce
desired behaviors. To address this limitation, we introduce SLIM, a
multi-critic learning approach for skill discovery with a particular focus on
robotic manipulation. Our main insight is that utilizing multiple critics in an
actor-critic framework to gracefully combine multiple reward functions leads to
a significant improvement in latent-variable skill discovery for robotic
manipulation while overcoming possible interference occurring among rewards
which hinders convergence to useful skills. Furthermore, in the context of
tabletop manipulation, we demonstrate the applicability of our novel skill
discovery approach to acquire safe and efficient motor primitives in a
hierarchical reinforcement learning fashion and leverage them through planning,
significantly surpassing baseline approaches for skill discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ R2SNet: Scalable Domain Adaptation for Object Detection in Cloud-Based
  Robots Ecosystems via Proposal Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11567v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11567v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michele Antonazzi, Matteo Luperto, N. Alberto Borghese, Nicola Basilico
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel approach for scalable domain adaptation in cloud
robotics scenarios where robots rely on third-party AI inference services
powered by large pre-trained deep neural networks. Our method is based on a
downstream proposal-refinement stage running locally on the robots, exploiting
a new lightweight DNN architecture, R2SNet. This architecture aims to mitigate
performance degradation from domain shifts by adapting the object detection
process to the target environment, focusing on relabeling, rescoring, and
suppression of bounding-box proposals. Our method allows for local execution on
robots, addressing the scalability challenges of domain adaptation without
incurring significant computational costs. Real-world results on mobile service
robots performing door detection show the effectiveness of the proposed method
in achieving scalable domain adaptation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoBRA: A Composable Benchmark for Robotics Applications <span class="chip">ICRA'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09337v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09337v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Mayer, Jonathan Külz, Matthias Althoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Selecting an optimal robot, its base pose, and trajectory for a given task is
currently mainly done by human expertise or trial and error. To evaluate
automatic approaches to this combined optimization problem, we introduce a
benchmark suite encompassing a unified format for robots, environments, and
task descriptions. Our benchmark suite is especially useful for modular robots,
where the multitude of robots that can be assembled creates a host of
additional parameters to optimize. We include tasks such as machine tending and
welding in synthetic environments and 3D scans of real-world machine shops. All
benchmarks are accessible through https://cobra.cps.cit.tum.de, a platform to
conveniently share, reference, and compare tasks, robot models, and solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 Figures, 5 Tables Final version for IEEE ICRA'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Early Stopping in Evolutionary Direct Policy Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Etor Arza, Leni K. Le Goff, Emma Hart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lengthy evaluation times are common in many optimization problems such as
direct policy search tasks, especially when they involve conducting evaluations
in the physical world, e.g. in robotics applications. Often when evaluating
solution over a fixed time period it becomes clear that the objective value
will not increase with additional computation time (for example when a two
wheeled robot continuously spins on the spot). In such cases, it makes sense to
stop the evaluation early to save computation time. However, most approaches to
stop the evaluation are problem specific and need to be specifically designed
for the task at hand. Therefore, we propose an early stopping method for direct
policy search. The proposed method only looks at the objective value at each
time step and requires no problem specific knowledge. We test the introduced
stopping criterion in five direct policy search environments drawn from games,
robotics and classic control domains, and show that it can save up to 75% of
the computation time. We also compare it with problem specific stopping
criteria and show that it performs comparably, while being more generally
applicable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language and Sketching: An LLM-driven Interactive Multimodal Multitask
  Robot Navigation Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08244v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08244v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiqin Zu, Wenbin Song, Ruiqing Chen, Ze Guo, Fanglei Sun, Zheng Tian, Wei Pan, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The socially-aware navigation system has evolved to adeptly avoid various
obstacles while performing multiple tasks, such as point-to-point navigation,
human-following, and -guiding. However, a prominent gap persists: in
Human-Robot Interaction (HRI), the procedure of communicating commands to
robots demands intricate mathematical formulations. Furthermore, the transition
between tasks does not quite possess the intuitive control and user-centric
interactivity that one would desire. In this work, we propose an LLM-driven
interactive multimodal multitask robot navigation framework, termed LIM2N, to
solve the above new challenge in the navigation field. We achieve this by first
introducing a multimodal interaction framework where language and hand-drawn
inputs can serve as navigation constraints and control objectives. Next, a
reinforcement learning agent is built to handle multiple tasks with the
received information. Crucially, LIM2N creates smooth cooperation among the
reasoning of multimodal input, multitask planning, and adaptation and
processing of the intelligent sensing modules in the complicated system.
Extensive experiments are conducted in both simulation and the real world
demonstrating that LIM2N has superior user needs understanding, alongside an
enhanced interactive experience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-time Perceptive Motion Control using Control Barrier Functions with
  Analytical Smoothing for Six-Wheeled-Telescopic-Legged Robot Tachyon 3 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11792v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11792v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noriaki Takasugi, Masaya Kinoshita, Yasuhisa Kamikawa, Ryoichi Tsuzaki, Atsushi Sakamoto, Toshimitsu Kai, Yasunori Kawanami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve safe legged locomotion, it is important to generate motion in
real-time considering various constraints in robots and environments. In this
study, we propose a lightweight real-time perspective motion control system for
the newly developed six-wheeled-telescopic-legged robot, Tachyon 3. In the
proposed method, analytically smoothed constraints including Smooth Separating
Axis Theorem (Smooth SAT) as a novel higher order differentiable collision
detection for 3D shapes is applied to the Control Barrier Function (CBF). The
proposed system integrating the CBF achieves online motion generation in a
short control cycle of 1 ms that satisfies joint limitations, environmental
collision avoidance and safe convex foothold constraints. The efficiency of
Smooth SAT is shown from the collision detection time of 1 us or less and the
CBF constraint computation time for Tachyon3 of several us. Furthermore, the
effectiveness of the proposed system is verified through the stair-climbing
motion, integrating online recognition in a simulation and a real machine.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures, This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distilling and Retrieving Generalizable Knowledge for Robot Manipulation
  via Language Corrections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10678v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10678v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lihan Zha, Yuchen Cui, Li-Heng Lin, Minae Kwon, Montserrat Gonzalez Arenas, Andy Zeng, Fei Xia, Dorsa Sadigh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today's robot policies exhibit subpar performance when faced with the
challenge of generalizing to novel environments. Human corrective feedback is a
crucial form of guidance to enable such generalization. However, adapting to
and learning from online human corrections is a non-trivial endeavor: not only
do robots need to remember human feedback over time to retrieve the right
information in new settings and reduce the intervention rate, but also they
would need to be able to respond to feedback that can be arbitrary corrections
about high-level human preferences to low-level adjustments to skill
parameters. In this work, we present Distillation and Retrieval of Online
Corrections (DROC), a large language model (LLM)-based system that can respond
to arbitrary forms of language feedback, distill generalizable knowledge from
corrections, and retrieve relevant past experiences based on textual and visual
similarity for improving performance in novel settings. DROC is able to respond
to a sequence of online language corrections that address failures in both
high-level task plans and low-level skill primitives. We demonstrate that DROC
effectively distills the relevant information from the sequence of online
corrections in a knowledge base and retrieves that knowledge in settings with
new task or object instances. DROC outperforms other techniques that directly
generate robot code via LLMs by using only half of the total number of
corrections needed in the first round and requires little to no corrections
after two iterations. We show further results, videos, prompts and code on
https://sites.google.com/stanford.edu/droc .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, videos and code links on website
  https://sites.google.com/stanford.edu/droc</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning for Inertial Positioning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03757v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03757v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changhao Chen, Xianfei Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inertial sensors are widely utilized in smartphones, drones, robots, and IoT
devices, playing a crucial role in enabling ubiquitous and reliable
localization. Inertial sensor-based positioning is essential in various
applications, including personal navigation, location-based security, and
human-device interaction. However, low-cost MEMS inertial sensors' measurements
are inevitably corrupted by various error sources, leading to unbounded drifts
when integrated doubly in traditional inertial navigation algorithms,
subjecting inertial positioning to the problem of error drifts. In recent
years, with the rapid increase in sensor data and computational power, deep
learning techniques have been developed, sparking significant research into
addressing the problem of inertial positioning. Relevant literature in this
field spans across mobile computing, robotics, and machine learning. In this
article, we provide a comprehensive review of deep learning-based inertial
positioning and its applications in tracking pedestrians, drones, vehicles, and
robots. We connect efforts from different fields and discuss how deep learning
can be applied to address issues such as sensor calibration, positioning error
drift reduction, and multi-sensor fusion. This article aims to attract readers
from various backgrounds, including researchers and practitioners interested in
the potential of deep learning-based techniques to solve inertial positioning
problems. Our review demonstrates the exciting possibilities that deep learning
brings to the table and provides a roadmap for future research in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Intelligent Transportation Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AMP: Autoregressive Motion Prediction Revisited with Next Token
  Prediction for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaosong Jia, Shaoshuai Shi, Zijun Chen, Li Jiang, Wenlong Liao, Tao He, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As an essential task in autonomous driving (AD), motion prediction aims to
predict the future states of surround objects for navigation. One natural
solution is to estimate the position of other agents in a step-by-step manner
where each predicted time-step is conditioned on both observed time-steps and
previously predicted time-steps, i.e., autoregressive prediction. Pioneering
works like SocialLSTM and MFP design their decoders based on this intuition.
However, almost all state-of-the-art works assume that all predicted time-steps
are independent conditioned on observed time-steps, where they use a single
linear layer to generate positions of all time-steps simultaneously. They
dominate most motion prediction leaderboards due to the simplicity of training
MLPs compared to autoregressive networks.
  In this paper, we introduce the GPT style next token prediction into motion
forecasting. In this way, the input and output could be represented in a
unified space and thus the autoregressive prediction becomes more feasible.
However, different from language data which is composed of homogeneous units
-words, the elements in the driving scene could have complex spatial-temporal
and semantic relations. To this end, we propose to adopt three factorized
attention modules with different neighbors for information aggregation and
different position encoding styles to capture their relations, e.g., encoding
the transformation between coordinate systems for spatial relativity while
adopting RoPE for temporal relativity. Empirically, by equipping with the
aforementioned tailored designs, the proposed method achieves state-of-the-art
performance in the Waymo Open Motion and Waymo Interaction datasets. Notably,
AMP outperforms other recent autoregressive motion prediction methods: MotionLM
and StateTransformer, which demonstrates the effectiveness of the proposed
designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Redundancy parameterization and inverse kinematics of 7-DOF revolute
  manipulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.13122v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.13122v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander J. Elias, John T. Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Seven degree-of-freedom (DOF) robot arms have one redundant DOF which does
not change the motion of the end effector. The redundant DOF offers greater
manipulability of the arm configuration to avoid obstacles and singularities,
but it must be parameterized to fully specify the joint angles for a given end
effector pose. For 7-DOF revolute (7R) manipulators, we introduce a new concept
of generalized shoulder-elbow-wrist (SEW) angle, a generalization of the
conventional SEW angle but with an arbitrary choice of the reference direction
function. The SEW angle is widely used and easy for human operators to
visualize as a rotation of the elbow about the shoulder-wrist line. Since other
redundancy parameterizations including the conventional SEW angle encounter an
algorithmic singularity along a line in the workspace, we introduce a special
choice of the reference direction function called the stereographic SEW angle
which has a singularity only along a half-line, which can be placed out of
reach. We prove that such a singularity is unavoidable for any
parameterization. We also include expressions for the SEW angle Jacobian along
with singularity analysis. Finally, we provide efficient and singularity-robust
inverse kinematics solutions for most known 7R manipulators using the general
SEW angle and the subproblem decomposition method. These solutions are often
closed-form but may sometimes involve a 1D or 2D search in the general case.
Search-based solutions may be converted to finding zeros of a high-order
polynomial. Inverse kinematics solutions, examples, and evaluations are
available in a publicly accessible repository.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 14 figures. Update: Sawyer IK using polynomial method, two
  video extensions, expanded related literature</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open X-Embodiment: Robotic Learning <span class="highlight-title">Dataset</span>s and RT-X Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08864v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08864v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Open X-Embodiment Collaboration, Abby O'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi "Jim" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick "Tree" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large, high-capacity models trained on diverse datasets have shown remarkable
successes on efficiently tackling downstream applications. In domains from NLP
to Computer Vision, this has led to a consolidation of pretrained models, with
general pretrained backbones serving as a starting point for many applications.
Can such a consolidation happen in robotics? Conventionally, robotic learning
methods train a separate model for every application, every robot, and even
every environment. Can we instead train generalist X-robot policy that can be
adapted efficiently to new robots, tasks, and environments? In this paper, we
provide datasets in standardized data formats and models to make it possible to
explore this possibility in the context of robotic manipulation, alongside
experimental results that provide an example of effective X-robot policies. We
assemble a dataset from 22 different robots collected through a collaboration
between 21 institutions, demonstrating 527 skills (160266 tasks). We show that
a high-capacity model trained on this data, which we call RT-X, exhibits
positive transfer and improves the capabilities of multiple robots by
leveraging experience from other platforms. More details can be found on the
project website https://robotics-transformer-x.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://robotics-transformer-x.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TD-MPC2: Scalable, Robust World Models for Continuous Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16828v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16828v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicklas Hansen, Hao Su, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  TD-MPC is a model-based reinforcement learning (RL) algorithm that performs
local trajectory optimization in the latent space of a learned implicit
(decoder-free) world model. In this work, we present TD-MPC2: a series of
improvements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves
significantly over baselines across 104 online RL tasks spanning 4 diverse task
domains, achieving consistently strong results with a single set of
hyperparameters. We further show that agent capabilities increase with model
and data size, and successfully train a single 317M parameter agent to perform
80 tasks across multiple task domains, embodiments, and action spaces. We
conclude with an account of lessons, opportunities, and risks associated with
large TD-MPC2 agents. Explore videos, models, data, code, and more at
https://tdmpc2.com
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024. Explore videos, models, data, code, and more at
  https://tdmpc2.com</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TeleMoMa: A Modular and Versatile Teleoperation System for Mobile
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07869v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07869v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivin Dass, Wensi Ai, Yuqian Jiang, Samik Singh, Jiaheng Hu, Ruohan Zhang, Peter Stone, Ben Abbatematteo, Roberto Martín-Martín
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A critical bottleneck limiting imitation learning in robotics is the lack of
data. This problem is more severe in mobile manipulation, where collecting
demonstrations is harder than in stationary manipulation due to the lack of
available and easy-to-use teleoperation interfaces. In this work, we
demonstrate TeleMoMa, a general and modular interface for whole-body
teleoperation of mobile manipulators. TeleMoMa unifies multiple human
interfaces including RGB and depth cameras, virtual reality controllers,
keyboard, joysticks, etc., and any combination thereof. In its more accessible
version, TeleMoMa works using simply vision (e.g., an RGB-D camera), lowering
the entry bar for humans to provide mobile manipulation demonstrations. We
demonstrate the versatility of TeleMoMa by teleoperating several existing
mobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in simulation and
the real world. We demonstrate the quality of the demonstrations collected with
TeleMoMa by training imitation learning policies for mobile manipulation tasks
involving synchronized whole-body motion. Finally, we also show that TeleMoMa's
teleoperation channel enables teleoperation on site, looking at the robot, or
remote, sending commands and observations through a computer network, and
perform user studies to evaluate how easy it is for novice users to learn to
collect demonstrations with different combinations of human interfaces enabled
by our system. We hope TeleMoMa becomes a helpful tool for the community
enabling researchers to collect whole-body mobile manipulation demonstrations.
For more information and video results,
https://robin-lab.cs.utexas.edu/telemoma-web.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OASIS: Optimal Arrangements for Sensing in <span class="highlight-title">SLAM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10698v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10698v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pushyami Kaveti, Matthew Giamou, Hanumant Singh, David M. Rosen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The number and arrangement of sensors on mobile robot dramatically influence
its perception capabilities. Ensuring that sensors are mounted in a manner that
enables accurate detection, localization, and mapping is essential for the
success of downstream control tasks. However, when designing a new robotic
platform, researchers and practitioners alike usually mimic standard
configurations or maximize simple heuristics like field-of-view (FOV) coverage
to decide where to place exteroceptive sensors. In this work, we conduct an
information-theoretic investigation of this overlooked element of robotic
perception in the context of simultaneous localization and mapping (SLAM). We
show how to formalize the sensor arrangement problem as a form of subset
selection under the E-optimality performance criterion. While this formulation
is NP-hard in general, we show that a combination of greedy sensor selection
and fast convex relaxation-based post-hoc verification enables the efficient
recovery of certifiably optimal sensor designs in practice. Results from
synthetic experiments reveal that sensors placed with OASIS outperform
benchmarks in terms of mean squared error of visual SLAM estimates.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">152</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Multi-Object Shape Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shun Iwase, Katherine Liu, Vitor Guizilini, Adrien Gaidon, Kris Kitani, Rares Ambrus, Sergey Zakharov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a 3D shape completion method that recovers the complete geometry
of multiple objects in complex scenes from a single RGB-D image. Despite
notable advancements in single object 3D shape completion, high-quality
reconstructions in highly cluttered real-world multi-object scenes remains a
challenge. To address this issue, we propose OctMAE, an architecture that
leverages an Octree U-Net and a latent 3D MAE to achieve high-quality and near
real-time multi-object shape completion through both local and global geometric
reasoning. Because a na\"ive 3D MAE can be computationally intractable and
memory intensive even in the latent space, we introduce a novel occlusion
masking strategy and adopt 3D rotary embeddings, which significantly improves
the runtime and shape completion quality. To generalize to a wide range of
objects in diverse scenes, we create a large-scale photorealistic dataset,
featuring a diverse set of 12K 3D object models from the Objaverse dataset
which are rendered in multi-object scenes with physics-based positioning. Our
method outperforms the current state-of-the-art on both synthetic and
real-world datasets and demonstrates a strong zero-shot capability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 8 figues</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, Jianfei Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose MVSplat, an efficient feed-forward 3D Gaussian Splatting model
learned from sparse multi-view images. To accurately localize the Gaussian
centers, we propose to build a cost volume representation via plane sweeping in
the 3D space, where the cross-view feature similarities stored in the cost
volume can provide valuable geometry cues to the estimation of depth. We learn
the Gaussian primitives' opacities, covariances, and spherical harmonics
coefficients jointly with the Gaussian centers while only relying on
photometric supervision. We demonstrate the importance of the cost volume
representation in learning feed-forward Gaussian Splatting models via extensive
experimental evaluations. On the large-scale RealEstate10K and ACID benchmarks,
our model achieves state-of-the-art performance with the fastest feed-forward
inference speed (22 fps). Compared to the latest state-of-the-art method
pixelSplat, our model uses $10\times $ fewer parameters and infers more than
$2\times$ faster while providing higher appearance and geometry quality as well
as better cross-dataset generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://donydchen.github.io/mvsplat Code:
  https://github.com/donydchen/mvsplat</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT
  Descriptors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saksham Suri, Matthew Walmer, Kamal Gupta, Abhinav Shrivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a simple self-supervised method to enhance the performance of ViT
features for dense downstream tasks. Our Lightweight Feature Transform (LiFT)
is a straightforward and compact postprocessing network that can be applied to
enhance the features of any pre-trained ViT backbone. LiFT is fast and easy to
train with a self-supervised objective, and it boosts the density of ViT
features for minimal extra inference cost. Furthermore, we demonstrate that
LiFT can be applied with approaches that use additional task-specific
downstream modules, as we integrate LiFT with ViTDet for COCO detection and
segmentation. Despite the simplicity of LiFT, we find that it is not simply
learning a more complex version of bilinear interpolation. Instead, our LiFT
training protocol leads to several desirable emergent properties that benefit
ViT features in dense downstream tasks. This includes greater scale invariance
for features, and better object boundary maps. By simply training LiFT for a
few epochs, we show improved performance on keypoint correspondence, detection,
segmentation, and object discovery tasks. Overall, LiFT provides an easy way to
unlock the benefits of denser feature arrays for a fraction of the
computational cost. For more details, refer to our project page at
https://www.cs.umd.edu/~sakshams/LiFT/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ODTFormer: Efficient Obstacle Detection and Tracking with Stereo Cameras
  Based on Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianye Ding, Hongyu Li, Huaizu Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Obstacle detection and tracking represent a critical component in robot
autonomous navigation. In this paper, we propose ODTFormer, a Transformer-based
model to address both obstacle detection and tracking problems. For the
detection task, our approach leverages deformable attention to construct a 3D
cost volume, which is decoded progressively in the form of voxel occupancy
grids. We further track the obstacles by matching the voxels between
consecutive frames. The entire model can be optimized in an end-to-end manner.
Through extensive experiments on DrivingStereo and KITTI benchmarks, our model
achieves state-of-the-art performance in the obstacle detection task. We also
report comparable accuracy to state-of-the-art obstacle tracking models while
requiring only a fraction of their computation cost, typically ten-fold to
twenty-fold less. The code and model weights will be publicly released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual
  Math Problems? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable progress of Multi-modal Large Language Models (MLLMs) has
garnered unparalleled attention, due to their superior performance in visual
contexts. However, their capabilities in visual math problem-solving remain
insufficiently evaluated and understood. We investigate current benchmarks to
incorporate excessive visual content within textual questions, which
potentially assist MLLMs in deducing answers without truly interpreting the
input diagrams. To this end, we introduce MathVerse, an all-around visual math
benchmark designed for an equitable and in-depth evaluation of MLLMs. We
meticulously collect 2,612 high-quality, multi-subject math problems with
diagrams from publicly available sources. Each problem is then transformed by
human annotators into six distinct versions, each offering varying degrees of
information content in multi-modality, contributing to 15K test samples in
total. This approach allows MathVerse to comprehensively assess whether and how
much MLLMs can truly understand the visual diagrams for mathematical reasoning.
In addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a
fine-grained assessment of the output answers. Rather than naively judging True
or False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and
then score each step with detailed error analysis, which can reveal the
intermediate CoT reasoning quality by MLLMs. We hope the MathVerse benchmark
may provide unique insights to guide the future development of MLLMs. Project
page: https://mathverse-cuhk.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 Pages, Work in Progress, Benchmark Project Page:
  https://mathverse-cuhk.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simplified Diffusion Schrödinger Bridge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicong Tang, Tiankai Hang, Shuyang Gu, Dong Chen, Baining Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel theoretical simplification of the Diffusion
Schr\"odinger Bridge (DSB) that facilitates its unification with Score-based
Generative Models (SGMs), addressing the limitations of DSB in complex data
generation and enabling faster convergence and enhanced performance. By
employing SGMs as an initial solution for DSB, our approach capitalizes on the
strengths of both frameworks, ensuring a more efficient training process and
improving the performance of SGM. We also propose a reparameterization
technique that, despite theoretical approximations, practically improves the
network's fitting capabilities. Our extensive experimental evaluations confirm
the effectiveness of the simplified DSB, demonstrating its significant
improvements. We believe the contributions of this work pave the way for
advanced generative modeling. The code is available at
https://github.com/tzco/Simplified-Diffusion-Schrodinger-Bridge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Repository for Long Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kumara Kahatapitiya, Kanchana Ranasinghe, Jongwoo Park, Michael S. Ryoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language has become a prominent modality in computer vision with the rise of
multi-modal LLMs. Despite supporting long context-lengths, their effectiveness
in handling long-term information gradually declines with input length. This
becomes critical, especially in applications such as long-form video
understanding. In this paper, we introduce a Language Repository (LangRepo) for
LLMs, that maintains concise and structured information as an interpretable
(i.e., all-textual) representation. Our repository is updated iteratively based
on multi-scale video chunks. We introduce write and read operations that focus
on pruning redundancies in text, and extracting information at various temporal
scales. The proposed framework is evaluated on zero-shot visual
question-answering benchmarks including EgoSchema, NExT-QA, IntentQA and
NExT-GQA, showing state-of-the-art performance at its scale. Our code is
available at https://github.com/kkahatapitiya/LangRepo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction
  and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, Gordon Wetzstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce GRM, a large-scale reconstructor capable of recovering a 3D
asset from sparse-view images in around 0.1s. GRM is a feed-forward
transformer-based model that efficiently incorporates multi-view information to
translate the input pixels into pixel-aligned Gaussians, which are unprojected
to create a set of densely distributed 3D Gaussians representing a scene.
Together, our transformer architecture and the use of 3D Gaussians unlock a
scalable and efficient reconstruction framework. Extensive experimental results
demonstrate the superiority of our method over alternatives regarding both
reconstruction quality and efficiency. We also showcase the potential of GRM in
generative tasks, i.e., text-to-3D and image-to-3D, by integrating it with
existing multi-view diffusion models. Our project website is at:
https://justimyhxu.github.io/projects/grm/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://justimyhxu.github.io/projects/grm/ Code:
  https://github.com/justimyhxu/GRM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D
  Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhao Wu, Chuanxia Zheng, Tat-Jen Cham, Qianyi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D decomposition/segmentation still remains a challenge as large-scale 3D
annotated data is not readily available. Contemporary approaches typically
leverage 2D machine-generated segments, integrating them for 3D consistency.
While the majority of these methods are based on NeRFs, they face a potential
weakness that the instance/semantic embedding features derive from independent
MLPs, thus preventing the segmentation network from learning the geometric
details of the objects directly through radiance and density. In this paper, we
propose ClusteringSDF, a novel approach to achieve both segmentation and
reconstruction in 3D via the neural implicit surface representation,
specifically Signal Distance Function (SDF), where the segmentation rendering
is directly integrated with the volume rendering of neural implicit surfaces.
Although based on ObjectSDF++, ClusteringSDF no longer requires the
ground-truth segments for supervision while maintaining the capability of
reconstructing individual object surfaces, but purely with the noisy and
inconsistent labels from pre-trained models.As the core of ClusteringSDF, we
introduce a high-efficient clustering mechanism for lifting the 2D labels to 3D
and the experimental results on the challenging scenes from ScanNet and Replica
datasets show that ClusteringSDF can achieve competitive performance compared
against the state-of-the-art with significantly reduced training time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://sm0kywu.github.io/ClusteringSDF/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Videoshop: Localized Semantic Video Editing with Noise-Extrapolated
  Diffusion Inversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Fan, Anand Bhattad, Ranjay Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Videoshop, a training-free video editing algorithm for localized
semantic edits. Videoshop allows users to use any editing software, including
Photoshop and generative inpainting, to modify the first frame; it
automatically propagates those changes, with semantic, spatial, and temporally
consistent motion, to the remaining frames. Unlike existing methods that enable
edits only through imprecise textual instructions, Videoshop allows users to
add or remove objects, semantically change objects, insert stock photos into
videos, etc. with fine-grained control over locations and appearance. We
achieve this through image-based video editing by inverting latents with noise
extrapolation, from which we generate videos conditioned on the edited image.
Videoshop produces higher quality edits against 6 baselines on 2 editing
benchmarks using 10 evaluation metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Text-to-Vision Self Supervised Alignment for Improved
  Histopathology Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hasindri Watawana, Kanchana Ranasinghe, Tariq Mahmood, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised representation learning has been highly promising for
histopathology image analysis with numerous approaches leveraging their
patient-slide-patch hierarchy to learn better representations. In this paper,
we explore how the combination of domain specific natural language information
with such hierarchical visual representations can benefit rich representation
learning for medical image tasks. Building on automated language description
generation for features visible in histopathology images, we present a novel
language-tied self-supervised learning framework, Hierarchical Language-tied
Self-Supervision (HLSS) for histopathology images. We explore contrastive
objectives and granular language description based text alignment at multiple
hierarchies to inject language modality information into the visual
representations. Our resulting model achieves state-of-the-art performance on
two medical imaging benchmarks, OpenSRH and TCGA datasets. Our framework also
provides better interpretability with our language aligned representation
space. Code is available at https://github.com/Hasindri/HLSS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages and 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaIR: Adaptive All-in-One Image Restoration via Frequency Mining and
  Modulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuning Cui, Syed Waqas Zamir, Salman Khan, Alois Knoll, Mubarak Shah, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the image acquisition process, various forms of degradation, including
noise, haze, and rain, are frequently introduced. These degradations typically
arise from the inherent limitations of cameras or unfavorable ambient
conditions. To recover clean images from degraded versions, numerous
specialized restoration methods have been developed, each targeting a specific
type of degradation. Recently, all-in-one algorithms have garnered significant
attention by addressing different types of degradations within a single model
without requiring prior information of the input degradation type. However,
these methods purely operate in the spatial domain and do not delve into the
distinct frequency variations inherent to different degradation types. To
address this gap, we propose an adaptive all-in-one image restoration network
based on frequency mining and modulation. Our approach is motivated by the
observation that different degradation types impact the image content on
different frequency subbands, thereby requiring different treatments for each
restoration task. Specifically, we first mine low- and high-frequency
information from the input features, guided by the adaptively decoupled spectra
of the degraded image. The extracted features are then modulated by a
bidirectional operator to facilitate interactions between different frequency
components. Finally, the modulated features are merged into the original input
for a progressively guided restoration. With this approach, the model achieves
adaptive reconstruction by accentuating the informative frequency subbands
according to different input degradations. Extensive experiments demonstrate
that the proposed method achieves state-of-the-art performance on different
image restoration tasks, including denoising, dehazing, deraining, motion
deblurring, and low-light image enhancement. Our code is available at
https://github.com/c-yn/AdaIR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages,15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DreamReward: Text-to-3D Generation with Human Preference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junliang Ye, Fangfu Liu, Qixiu Li, Zhengyi Wang, Yikai Wang, Xinzhou Wang, Yueqi Duan, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D content creation from text prompts has shown remarkable success recently.
However, current text-to-3D methods often generate 3D results that do not align
well with human preferences. In this paper, we present a comprehensive
framework, coined DreamReward, to learn and improve text-to-3D models from
human preference feedback. To begin with, we collect 25k expert comparisons
based on a systematic annotation pipeline including rating and ranking. Then,
we build Reward3D -- the first general-purpose text-to-3D human preference
reward model to effectively encode human preferences. Building upon the 3D
reward model, we finally perform theoretical analysis and present the Reward3D
Feedback Learning (DreamFL), a direct tuning algorithm to optimize the
multi-view diffusion models with a redefined scorer. Grounded by theoretical
proof and extensive experiment comparisons, our DreamReward successfully
generates high-fidelity and 3D consistent results with significant boosts in
prompt alignment with human intention. Our results demonstrate the great
potential for learning from human feedback to improve text-to-3D models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://jamesyjl.github.io/DreamReward</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explorative Inbetweening of Time and Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiwen Feng, Zheng Ding, Zhihao Xia, Simon Niklaus, Victoria Abrevaya, Michael J. Black, Xuaner Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce bounded generation as a generalized task to control video
generation to synthesize arbitrary camera and subject motion based only on a
given start and end frame. Our objective is to fully leverage the inherent
generalization capability of an image-to-video model without additional
training or fine-tuning of the original model. This is achieved through the
proposed new sampling strategy, which we call Time Reversal Fusion, that fuses
the temporally forward and backward denoising paths conditioned on the start
and end frame, respectively. The fused path results in a video that smoothly
connects the two frames, generating inbetweening of faithful subject motion,
novel views of static scenes, and seamless video looping when the two bounding
frames are identical. We curate a diverse evaluation dataset of image pairs and
compare against the closest existing methods. We find that Time Reversal Fusion
outperforms related work on all subtasks, exhibiting the ability to generate
complex motions and 3D-consistent views guided by bounded frames. See project
page at https://time-reversal.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page at https://time-reversal.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qing Jiang, Feng Li, Zhaoyang Zeng, Tianhe Ren, Shilong Liu, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present T-Rex2, a highly practical model for open-set object detection.
Previous open-set object detection methods relying on text prompts effectively
encapsulate the abstract concept of common objects, but struggle with rare or
complex object representation due to data scarcity and descriptive limitations.
Conversely, visual prompts excel in depicting novel objects through concrete
visual examples, but fall short in conveying the abstract concept of objects as
effectively as text prompts. Recognizing the complementary strengths and
weaknesses of both text and visual prompts, we introduce T-Rex2 that synergizes
both prompts within a single model through contrastive learning. T-Rex2 accepts
inputs in diverse formats, including text prompts, visual prompts, and the
combination of both, so that it can handle different scenarios by switching
between the two prompt modalities. Comprehensive experiments demonstrate that
T-Rex2 exhibits remarkable zero-shot object detection capabilities across a
wide spectrum of scenarios. We show that text prompts and visual prompts can
benefit from each other within the synergy, which is essential to cover massive
and complicated real-world scenarios and pave the way towards generic object
detection. Model API is now available at
\url{https://github.com/IDEA-Research/T-Rex}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReNoise: Real Image Inversion Through Iterative Noising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar Averbuch-Elor, Daniel Cohen-Or
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in text-guided diffusion models have unlocked powerful
image manipulation capabilities. However, applying these methods to real images
necessitates the inversion of the images into the domain of the pretrained
diffusion model. Achieving faithful inversion remains a challenge, particularly
for more recent models trained to generate images with a small number of
denoising steps. In this work, we introduce an inversion method with a high
quality-to-operation ratio, enhancing reconstruction accuracy without
increasing the number of operations. Building on reversing the diffusion
sampling process, our method employs an iterative renoising mechanism at each
inversion sampling step. This mechanism refines the approximation of a
predicted point along the forward diffusion trajectory, by iteratively applying
the pretrained diffusion model, and averaging these predictions. We evaluate
the performance of our ReNoise technique using various sampling algorithms and
models, including recent accelerated diffusion models. Through comprehensive
evaluations and comparisons, we show its effectiveness in terms of both
accuracy and speed. Furthermore, we confirm that our method preserves
editability by demonstrating text-driven image editing on real images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page at: https://garibida.github.io/ReNoise-Inversion/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MyVLM: Personalizing VLMs for User-Specific Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuval Alaluf, Elad Richardson, Sergey Tulyakov, Kfir Aberman, Daniel Cohen-Or
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large-scale vision-language models (VLMs) have demonstrated remarkable
capabilities in understanding and generating textual descriptions for visual
content. However, these models lack an understanding of user-specific concepts.
In this work, we take a first step toward the personalization of VLMs, enabling
them to learn and reason over user-provided concepts. For example, we explore
whether these models can learn to recognize you in an image and communicate
what you are doing, tailoring the model to reflect your personal experiences
and relationships. To effectively recognize a variety of user-specific
concepts, we augment the VLM with external concept heads that function as
toggles for the model, enabling the VLM to identify the presence of specific
target concepts in a given image. Having recognized the concept, we learn a new
concept embedding in the intermediate feature space of the VLM. This embedding
is tasked with guiding the language model to naturally integrate the target
concept in its generated response. We apply our technique to BLIP-2 and LLaVA
for personalized image captioning and further show its applicability for
personalized visual question-answering. Our experiments demonstrate our ability
to generalize to unseen images of learned concepts while preserving the model
behavior on unrelated inputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://snap-research.github.io/MyVLM/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Zhang, Yeyao Ma, Enming Zhang, Xiang Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PSALM is a powerful extension of the Large Multi-modal Model (LMM) to address
the segmentation task challenges. To overcome the limitation of the LMM being
limited to textual output, PSALM incorporates a mask decoder and a
well-designed input schema to handle a variety of segmentation tasks. This
schema includes images, task instructions, conditional prompts, and mask
tokens, which enable the model to generate and classify segmentation masks
effectively. The flexible design of PSALM supports joint training across
multiple datasets and tasks, leading to improved performance and task
generalization. PSALM achieves superior results on several benchmarks, such as
RefCOCO/RefCOCO+/RefCOCOg, COCO Panoptic Segmentation, and COCO-Interactive,
and further exhibits zero-shot capabilities on unseen tasks, such as
open-vocabulary segmentation, generalized referring expression segmentation and
video object segmentation, making a significant step towards a GPT moment in
computer vision. Through extensive experiments, PSALM demonstrates its
potential to transform the domain of image segmentation, leveraging the robust
visual understanding capabilities of LMMs as seen in natural language
processing. Code and models are available at https://github.com/zamling/PSALM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VXP: Voxel-Cross-Pixel Large-scale Image-<span class="highlight-title">LiDAR</span> Place Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun-Jin Li, Mariia Gladkova, Yan Xia, Rui Wang, Daniel Cremers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works on the global place recognition treat the task as a retrieval
problem, where an off-the-shelf global descriptor is commonly designed in
image-based and LiDAR-based modalities. However, it is non-trivial to perform
accurate image-LiDAR global place recognition since extracting consistent and
robust global descriptors from different domains (2D images and 3D point
clouds) is challenging. To address this issue, we propose a novel
Voxel-Cross-Pixel (VXP) approach, which establishes voxel and pixel
correspondences in a self-supervised manner and brings them into a shared
feature space. Specifically, VXP is trained in a two-stage manner that first
explicitly exploits local feature correspondences and enforces similarity of
global descriptors. Extensive experiments on the three benchmarks (Oxford
RobotCar, ViViD++ and KITTI) demonstrate our method surpasses the
state-of-the-art cross-modal retrieval by a large margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page https://yunjinli.github.io/projects-vxp/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implicit Style-Content Separation using B-LoRA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yarden Frenkel, Yael Vinker, Ariel Shamir, Daniel Cohen-Or
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image stylization involves manipulating the visual appearance and texture
(style) of an image while preserving its underlying objects, structures, and
concepts (content). The separation of style and content is essential for
manipulating the image's style independently from its content, ensuring a
harmonious and visually pleasing result. Achieving this separation requires a
deep understanding of both the visual and semantic characteristics of images,
often necessitating the training of specialized models or employing heavy
optimization. In this paper, we introduce B-LoRA, a method that leverages LoRA
(Low-Rank Adaptation) to implicitly separate the style and content components
of a single image, facilitating various image stylization tasks. By analyzing
the architecture of SDXL combined with LoRA, we find that jointly learning the
LoRA weights of two specific blocks (referred to as B-LoRAs) achieves
style-content separation that cannot be achieved by training each B-LoRA
independently. Consolidating the training into only two blocks and separating
style and content allows for significantly improving style manipulation and
overcoming overfitting issues often associated with model fine-tuning. Once
trained, the two B-LoRAs can be used as independent components to allow various
image stylization tasks, including image style transfer, text-based image
stylization, consistent style generation, and style-content mixing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visibility-Aware Keypoint Localization for 6DoF Object Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14559v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14559v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruyi Lian, Haibin Ling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Localizing predefined 3D keypoints in a 2D image is an effective way to
establish 3D-2D correspondences for 6DoF object pose estimation. However,
unreliable localization results of invisible keypoints degrade the quality of
correspondences. In this paper, we address this issue by localizing the
important keypoints in terms of visibility. Since keypoint visibility
information is currently missing in dataset collection process, we propose an
efficient way to generate binary visibility labels from available object-level
annotations, for keypoints of both asymmetric objects and symmetric objects. We
further derive real-valued visibility-aware importance from binary labels based
on PageRank algorithm. Taking advantage of the flexibility of our
visibility-aware importance, we construct VAPO (Visibility-Aware POse
estimator) by integrating the visibility-aware importance with a
state-of-the-art pose estimation algorithm, along with additional positional
encoding. Extensive experiments are conducted on popular pose estimation
benchmarks including Linemod, Linemod-Occlusion, and YCB-V. The results show
that, VAPO improves both the keypoint correspondences and final estimated
poses, and clearly achieves state-of-the-art performances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gaussian Frosting: Editable Complex Radiance Fields with Real-Time
  Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Guédon, Vincent Lepetit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Gaussian Frosting, a novel mesh-based representation for
high-quality rendering and editing of complex 3D effects in real-time. Our
approach builds on the recent 3D Gaussian Splatting framework, which optimizes
a set of 3D Gaussians to approximate a radiance field from images. We propose
first extracting a base mesh from Gaussians during optimization, then building
and refining an adaptive layer of Gaussians with a variable thickness around
the mesh to better capture the fine details and volumetric effects near the
surface, such as hair or grass. We call this layer Gaussian Frosting, as it
resembles a coating of frosting on a cake. The fuzzier the material, the
thicker the frosting. We also introduce a parameterization of the Gaussians to
enforce them to stay inside the frosting layer and automatically adjust their
parameters when deforming, rescaling, editing or animating the mesh. Our
representation allows for efficient rendering using Gaussian splatting, as well
as editing and animation by modifying the base mesh. We demonstrate the
effectiveness of our method on various synthetic and real scenes, and show that
it outperforms existing surface-based approaches. We will release our code and
a web-based viewer as additional contributions. Our project page is the
following: https://anttwo.github.io/frosting/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Webpage: https://anttwo.github.io/frosting/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token Transformation Matters: Towards Faithful Post-hoc Explanation for
  Vision Transformer <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyi Wu, Bin Duan, Weitai Kang, Hao Tang, Yan Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Transformers have rapidly gained popularity in various computer vision
applications, post-hoc explanations of their internal mechanisms remain largely
unexplored. Vision Transformers extract visual information by representing
image regions as transformed tokens and integrating them via attention weights.
However, existing post-hoc explanation methods merely consider these attention
weights, neglecting crucial information from the transformed tokens, which
fails to accurately illustrate the rationales behind the models' predictions.
To incorporate the influence of token transformation into interpretation, we
propose TokenTM, a novel post-hoc explanation method that utilizes our
introduced measurement of token transformation effects. Specifically, we
quantify token transformation effects by measuring changes in token lengths and
correlations in their directions pre- and post-transformation. Moreover, we
develop initialization and aggregation rules to integrate both attention
weights and token transformation effects across all layers, capturing holistic
token contributions throughout the model. Experimental results on segmentation
and perturbation tests demonstrate the superiority of our proposed TokenTM
compared to state-of-the-art Vision Transformer explanation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DINO-Tracker: Taming DINO for Self-Supervised Point Tracking in a Single
  Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Narek Tumanyan, Assaf Singer, Shai Bagon, Tali Dekel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DINO-Tracker -- a new framework for long-term dense tracking in
video. The pillar of our approach is combining test-time training on a single
video, with the powerful localized semantic features learned by a pre-trained
DINO-ViT model. Specifically, our framework simultaneously adopts DINO's
features to fit to the motion observations of the test video, while training a
tracker that directly leverages the refined features. The entire framework is
trained end-to-end using a combination of self-supervised losses, and
regularization that allows us to retain and benefit from DINO's semantic prior.
Extensive evaluation demonstrates that our method achieves state-of-the-art
results on known benchmarks. DINO-tracker significantly outperforms
self-supervised methods and is competitive with state-of-the-art supervised
trackers, while outperforming them in challenging cases of tracking under
long-term occlusions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating Physical Information Consistency of Channel Data Augmentation
  for Remote Sensing Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Burgert, Begüm Demir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of data augmentation for deep learning (DL) methods plays an
important role in achieving state-of-the-art results in supervised,
semi-supervised, and self-supervised image classification. In particular,
channel transformations (e.g., solarize, grayscale, brightness adjustments) are
integrated into data augmentation pipelines for remote sensing (RS) image
classification tasks. However, contradicting beliefs exist about their proper
applications to RS images. A common point of critique is that the application
of channel augmentation techniques may lead to physically inconsistent spectral
data (i.e., pixel signatures). To shed light on the open debate, we propose an
approach to estimate whether a channel augmentation technique affects the
physical information of RS images. To this end, the proposed approach estimates
a score that measures the alignment of a pixel signature within a time series
that can be naturally subject to deviations caused by factors such as
acquisition conditions or phenological states of vegetation. We compare the
scores associated with original and augmented pixel signatures to evaluate the
physical consistency. Experimental results on a multi-label image
classification task show that channel augmentations yielding a score that
exceeds the expected deviation of original pixel signatures can not improve the
performance of a baseline model trained without augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the IEEE International Geoscience and Remote Sensing
  Symposium</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object-Centric Domain Randomization for 3D Shape Reconstruction in the
  Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhyeong Cho, Kim Youwang, Hunmin Yang, Tae-Hyun Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the biggest challenges in single-view 3D shape reconstruction in the
wild is the scarcity of <3D shape, 2D image>-paired data from real-world
environments. Inspired by remarkable achievements via domain randomization, we
propose ObjectDR which synthesizes such paired data via a random simulation of
visual variations in object appearances and backgrounds. Our data synthesis
framework exploits a conditional generative model (e.g., ControlNet) to
generate images conforming to spatial conditions such as 2.5D sketches, which
are obtainable through a rendering process of 3D shapes from object collections
(e.g., Objaverse-XL). To simulate diverse variations while preserving object
silhouettes embedded in spatial conditions, we also introduce a disentangled
framework which leverages an initial object guidance. After synthesizing a wide
range of data, we pre-train a model on them so that it learns to capture a
domain-invariant geometry prior which is consistent across various domains. We
validate its effectiveness by substantially improving 3D shape reconstruction
models on a real-world benchmark. In a scale-up evaluation, our pre-training
achieves 23.6% superior results compared with the pre-training on high-quality
computer graphics renderings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://ObjectDR.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transfer Learning for Cross-<span class="highlight-title">dataset</span> Isolated Sign Language Recognition
  in Under-Resourced <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmet Alp Kindiroglu, Ozgur Kara, Ogulcan Ozdemir, Lale Akarun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sign language recognition (SLR) has recently achieved a breakthrough in
performance thanks to deep neural networks trained on large annotated sign
datasets. Of the many different sign languages, these annotated datasets are
only available for a select few. Since acquiring gloss-level labels on sign
language videos is difficult, learning by transferring knowledge from existing
annotated sources is useful for recognition in under-resourced sign languages.
This study provides a publicly available cross-dataset transfer learning
benchmark from two existing public Turkish SLR datasets. We use a temporal
graph convolution-based sign language recognition approach to evaluate five
supervised transfer learning approaches and experiment with closed-set and
partial-set cross-dataset transfer learning. Experiments demonstrate that
improvement over finetuning based transfer learning is possible with
specialized supervised transfer learning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to The 18th IEEE International Conference on Automatic Face
  and Gesture Recognition 2024, Code available in
  https://github.com/alpk/tid-supervised-transfer-learning-dataset</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihang Chen, Qianyi Wu, Jianfei Cai, Mehrtash Harandi, Weiyao Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel
view synthesis, boasting rapid rendering speed with high fidelity. However, the
substantial Gaussians and their associated attributes necessitate effective
compression techniques. Nevertheless, the sparse and unorganized nature of the
point cloud of Gaussians (or anchors in our paper) presents challenges for
compression. To address this, we make use of the relations between the
unorganized anchors and the structured hash grid, leveraging their mutual
information for context modeling, and propose a Hash-grid Assisted Context
(HAC) framework for highly compact 3DGS representation. Our approach introduces
a binary hash grid to establish continuous spatial consistencies, allowing us
to unveil the inherent spatial relations of anchors through a carefully
designed context model. To facilitate entropy coding, we utilize Gaussian
distributions to accurately estimate the probability of each quantized
attribute, where an adaptive quantization module is proposed to enable
high-precision quantization of these attributes for improved fidelity
restoration. Additionally, we incorporate an adaptive masking strategy to
eliminate invalid Gaussians and anchors. Importantly, our work is the pioneer
to explore context-based compression for 3DGS representation, resulting in a
remarkable size reduction of over $75\times$ compared to vanilla 3DGS, while
simultaneously improving fidelity, and achieving over $11\times$ size reduction
over SOTA 3DGS compression approach Scaffold-GS. Our code is available here:
https://github.com/YihangChen-ee/HAC
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://yihangchen-ee.github.io/project_hac/ Code:
  https://github.com/YihangChen-ee/HAC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion
  Descriptors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Tsagkas, Jack Rome, Subramanian Ramamoorthy, Oisin Mac Aodha, Chris Xiaoxuan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise manipulation that is generalizable across scenes and objects remains
a persistent challenge in robotics. Current approaches for this task heavily
depend on having a significant number of training instances to handle objects
with pronounced visual and/or geometric part ambiguities. Our work explores the
grounding of fine-grained part descriptors for precise manipulation in a
zero-shot setting by utilizing web-trained text-to-image diffusion-based
generative models. We tackle the problem by framing it as a dense semantic part
correspondence task. Our model returns a gripper pose for manipulating a
specific part, using as reference a user-defined click from a source image of a
visually different instance of the same object. We require no manual grasping
demonstrations as we leverage the intrinsic object geometry and features.
Practical experiments in a real-world tabletop scenario validate the efficacy
of our approach, demonstrating its potential for advancing semantic-aware
robotics manipulation. Web page: https://tsagkas.github.io/click2grasp
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Invisible Needle Detection in Ultrasound: Leveraging Mechanism-Induced
  Vibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Li, Dianye Huang, Angelos Karlas, Nassir Navab, Zhongliang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In clinical applications that involve ultrasound-guided intervention, the
visibility of the needle can be severely impeded due to steep insertion and
strong distractors such as speckle noise and anatomical occlusion. To address
this challenge, we propose VibNet, a learning-based framework tailored to
enhance the robustness and accuracy of needle detection in ultrasound images,
even when the target becomes invisible to the naked eye. Inspired by Eulerian
Video Magnification techniques, we utilize an external step motor to induce
low-amplitude periodic motion on the needle. These subtle vibrations offer the
potential to generate robust frequency features for detecting the motion
patterns around the needle. To robustly and precisely detect the needle
leveraging these vibrations, VibNet integrates learning-based
Short-Time-Fourier-Transform and Hough-Transform modules to achieve successive
sub-goals, including motion feature extraction in the spatiotemporal space,
frequency feature aggregation, and needle detection in the Hough space. Based
on the results obtained on distinct ex vivo porcine and bovine tissue samples,
the proposed algorithm exhibits superior detection performance with efficient
computation and generalization capability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhao, Min Zhang, Wei Zhao, Pengxiang Ding, Siteng Huang, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the application of multimodal large language models (MLLM)
in various fields has achieved remarkable success. However, as the foundation
model for many downstream tasks, current MLLMs are composed of the well-known
Transformer network, which has a less efficient quadratic computation
complexity. To improve the efficiency of such basic models, we propose Cobra, a
linear computational complexity MLLM. Specifically, Cobra integrates the
efficient Mamba language model into the visual modality. Moreover, we explore
and study various modal fusion schemes to create an effective multi-modal
Mamba. Extensive experiments demonstrate that (1) Cobra achieves extremely
competitive performance with current computationally efficient state-of-the-art
methods, \textit{e.g.}, LLaVA-Phi, TinyLLaVA, and MobileVLM v2, and has faster
speed due to Cobra's linear sequential modeling. (2) Interestingly, the results
of closed-set challenging prediction benchmarks show that Cobra performs well
in overcoming visual illusions and spatial relationship judgments. (3) Notably,
Cobra even achieves comparable performance to LLaVA with about 43% of the
number of parameters. We will make all codes of Cobra open-source and hope that
the proposed method can facilitate future research on complexity problems in
MLLM. Our project page is available at: https://sites.google.com/view/cobravlm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ View-decoupled Transformer for Person Re-identification under
  Aerial-ground Camera Network <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Zhang, Lei Wang, Vishal M. Patel, Xiaohua Xie, Jianhuang Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing person re-identification methods have achieved remarkable advances
in appearance-based identity association across homogeneous cameras, such as
ground-ground matching. However, as a more practical scenario, aerial-ground
person re-identification (AGPReID) among heterogeneous cameras has received
minimal attention. To alleviate the disruption of discriminative identity
representation by dramatic view discrepancy as the most significant challenge
in AGPReID, the view-decoupled transformer (VDT) is proposed as a simple yet
effective framework. Two major components are designed in VDT to decouple
view-related and view-unrelated features, namely hierarchical subtractive
separation and orthogonal loss, where the former separates these two features
inside the VDT, and the latter constrains these two to be independent. In
addition, we contribute a large-scale AGPReID dataset called CARGO, consisting
of five/eight aerial/ground cameras, 5,000 identities, and 108,563 images.
Experiments on two datasets show that VDT is a feasible and effective solution
for AGPReID, surpassing the previous method on mAP/Rank1 by up to 5.0%/2.7% on
CARGO and 3.7%/5.2% on AG-ReID, keeping the same magnitude of computational
complexity. Our project is available at https://github.com/LinlyAC/VDT-AGPReID
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Denoising Diffusion Models for 3D Healthy Brain Tissue Inpainting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alicia Durrer, Julia Wolleb, Florentin Bieder, Paul Friedrich, Lester Melie-Garcia, Mario Ocampo-Pineda, Cosmin I. Bercea, Ibrahim E. Hamamci, Benedikt Wiestler, Marie Piraud, Özgür Yaldizli, Cristina Granziera, Bjoern H. Menze, Philippe C. Cattin, Florian Kofler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monitoring diseases that affect the brain's structural integrity requires
automated analysis of magnetic resonance (MR) images, e.g., for the evaluation
of volumetric changes. However, many of the evaluation tools are optimized for
analyzing healthy tissue. To enable the evaluation of scans containing
pathological tissue, it is therefore required to restore healthy tissue in the
pathological areas. In this work, we explore and extend denoising diffusion
models for consistent inpainting of healthy 3D brain tissue. We modify
state-of-the-art 2D, pseudo-3D, and 3D methods working in the image space, as
well as 3D latent and 3D wavelet diffusion models, and train them to synthesize
healthy brain tissue. Our evaluation shows that the pseudo-3D model performs
best regarding the structural-similarity index, peak signal-to-noise ratio, and
mean squared error. To emphasize the clinical relevance, we fine-tune this
model on data containing synthetic MS lesions and evaluate it on a downstream
brain tissue segmentation task, whereby it outperforms the established FMRIB
Software Library (FSL) lesion-filling method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MULDE: Multiscale Log-Density Estimation via Denoising Score Matching
  for Video Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Micorek, Horst Possegger, Dominik Narnhofer, Horst Bischof, Mateusz Kozinski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel approach to video anomaly detection: we treat feature
vectors extracted from videos as realizations of a random variable with a fixed
distribution and model this distribution with a neural network. This lets us
estimate the likelihood of test videos and detect video anomalies by
thresholding the likelihood estimates. We train our video anomaly detector
using a modification of denoising score matching, a method that injects
training data with noise to facilitate modeling its distribution. To eliminate
hyperparameter selection, we model the distribution of noisy video features
across a range of noise levels and introduce a regularizer that tends to align
the models for different levels of noise. At test time, we combine anomaly
indications at multiple noise scales with a Gaussian mixture model. Running our
video anomaly detector induces minimal delays as inference requires merely
extracting the features and forward-propagating them through a shallow neural
network and a Gaussian mixture model. Our experiments on five popular video
anomaly detection benchmarks demonstrate state-of-the-art performance, both in
the object-centric and in the frame-centric setup.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Project for Cross-Task Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Auty, Roy Miles, Benedikt Kolbeinsson, Krystian Mikolajczyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional knowledge distillation (KD) relies on a proficient teacher
trained on the target task, which is not always available. In this setting,
cross-task distillation can be used, enabling the use of any teacher model
trained on a different task. However, many KD methods prove ineffective when
applied to this cross-task setting. To address this limitation, we propose a
simple modification: the use of an inverted projection. We show that this
drop-in replacement for a standard projector is effective by learning to
disregard any task-specific features which might degrade the student's
performance. We find that this simple modification is sufficient for extending
many KD methods to the cross-task setting, where the teacher and student tasks
can be very different. In doing so, we obtain up to a 1.9% improvement in the
cross-task setting compared to the traditional projection, at no additional
cost. Our method can obtain significant performance improvements (up to 7%)
when using even a randomly-initialised teacher on various tasks such as depth
estimation, image translation, and semantic segmentation, despite the lack of
any learned knowledge to transfer. To provide conceptual and analytical
insights into this result, we show that using an inverted projection allows the
distillation loss to be decomposed into a knowledge transfer and a spectral
regularisation component. Through this analysis we are additionally able to
propose a novel regularisation loss that allows teacher-free distillation,
enabling performance improvements of up to 8.57% on ImageNet with no additional
training costs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversary-Robust Graph-Based Learning of WSIs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saba Heidari Gheshlaghi, Milan Aryal, Nasim Yahyasoltani, Masoud Ganji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enhancing the robustness of deep learning models against adversarial attacks
is crucial, especially in critical domains like healthcare where significant
financial interests heighten the risk of such attacks. Whole slide images
(WSIs) are high-resolution, digitized versions of tissue samples mounted on
glass slides, scanned using sophisticated imaging equipment. The digital
analysis of WSIs presents unique challenges due to their gigapixel size and
multi-resolution storage format. In this work, we aim at improving the
robustness of cancer Gleason grading classification systems against adversarial
attacks, addressing challenges at both the image and graph levels. As regards
the proposed algorithm, we develop a novel and innovative graph-based model
which utilizes GNN to extract features from the graph representation of WSIs. A
denoising module, along with a pooling layer is incorporated to manage the
impact of adversarial attacks on the WSIs. The process concludes with a
transformer module that classifies various grades of prostate cancer based on
the processed data. To assess the effectiveness of the proposed method, we
conducted a comparative analysis using two scenarios. Initially, we trained and
tested the model without the denoiser using WSIs that had not been exposed to
any attack. We then introduced a range of attacks at either the image or graph
level and processed them through the proposed network. The performance of the
model was evaluated in terms of accuracy and kappa scores. The results from
this comparison showed a significant improvement in cancer diagnosis accuracy,
highlighting the robustness and efficiency of the proposed method in handling
adversarial challenges in the context of medical imaging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DesignEdit: Multi-Layered Latent Decomposition and Fusion for Unified &
  Accurate Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueru Jia, Yuhui Yuan, Aosong Cheng, Chuke Wang, Ji Li, Huizhu Jia, Shanghang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, how to achieve precise image editing has attracted increasing
attention, especially given the remarkable success of text-to-image generation
models. To unify various spatial-aware image editing abilities into one
framework, we adopt the concept of layers from the design domain to manipulate
objects flexibly with various operations. The key insight is to transform the
spatial-aware image editing task into a combination of two sub-tasks:
multi-layered latent decomposition and multi-layered latent fusion. First, we
segment the latent representations of the source images into multiple layers,
which include several object layers and one incomplete background layer that
necessitates reliable inpainting. To avoid extra tuning, we further explore the
inner inpainting ability within the self-attention mechanism. We introduce a
key-masking self-attention scheme that can propagate the surrounding context
information into the masked region while mitigating its impact on the regions
outside the mask. Second, we propose an instruction-guided latent fusion that
pastes the multi-layered latent representations onto a canvas latent. We also
introduce an artifact suppression scheme in the latent space to enhance the
inpainting quality. Due to the inherent modular advantages of such
multi-layered representations, we can achieve accurate image editing, and we
demonstrate that our approach consistently surpasses the latest spatial editing
methods, including Self-Guidance and DiffEditor. Last, we show that our
approach is a unified framework that supports various accurate image editing
tasks on more than six different editing tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>technical report, 15 pages, webpage: https://design-edit.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyperGALE: ASD Classification via Hypergraph Gated Attention with
  Learnable Hyperedges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehul Arora, Chirag Shantilal Jain, Lalith Bharadwaj Baru, Kamalaker Dadi, Bapi Raju Surampudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autism Spectrum Disorder (ASD) is a neurodevelopmental condition
characterized by varied social cognitive challenges and repetitive behavioral
patterns. Identifying reliable brain imaging-based biomarkers for ASD has been
a persistent challenge due to the spectrum's diverse symptomatology. Existing
baselines in the field have made significant strides in this direction, yet
there remains room for improvement in both performance and interpretability. We
propose \emph{HyperGALE}, which builds upon the hypergraph by incorporating
learned hyperedges and gated attention mechanisms. This approach has led to
substantial improvements in the model's ability to interpret complex brain
graph data, offering deeper insights into ASD biomarker characterization.
Evaluated on the extensive ABIDE II dataset, \emph{HyperGALE} not only improves
interpretability but also demonstrates statistically significant enhancements
in key performance metrics compared to both previous baselines and the
foundational hypergraph model. The advancement \emph{HyperGALE} brings to ASD
research highlights the potential of sophisticated graph-based techniques in
neurodevelopmental studies. The source code and implementation instructions are
available at GitHub:https://github.com/mehular0ra/HyperGALE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IJCNN 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detoxifying Large Language Models via Knowledge Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates using knowledge editing techniques to detoxify Large
Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine
unsafe categories with various powerful attack prompts and equips comprehensive
metrics for systematic evaluation. We conduct experiments to compare knowledge
editing approaches with previous baselines, indicating that knowledge editing
has the potential to efficiently detoxify LLMs with limited impact on general
performance. Then, we propose a simple yet effective baseline, dubbed
Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the
toxicity of LLMs within a few tuning steps via only one instance. We further
provide an in-depth analysis of the internal mechanism for various detoxify
approaches, demonstrating that previous methods like SFT and DPO may merely
suppress the activations of toxic parameters, while DINM mitigates the toxicity
of the toxic parameters to a certain extent, making permanent adjustments. We
hope that these insights could shed light on future work of developing
detoxifying approaches and the underlying knowledge mechanisms of LLMs. Code
and benchmark are available at https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work. Project website:
  https://zjunlp.github.io/project/SafeEdit Benchmark:
  https://huggingface.co/datasets/zjunlp/SafeEdit Code:
  https://github.com/zjunlp/EasyEdit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Ku, Cong Wei, Weiming Ren, Huan Yang, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-to-video editing involves editing a source video along with additional
control (such as text prompts, subjects, or styles) to generate a new video
that aligns with the source video and the provided control. Traditional methods
have been constrained to certain editing types, limiting their ability to meet
the wide range of user demands. In this paper, we introduce AnyV2V, a novel
training-free framework designed to simplify video editing into two primary
steps: (1) employing an off-the-shelf image editing model (e.g.
InstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an
existing image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion
and feature injection. In the first stage, AnyV2V can plug in any existing
image editing tools to support an extensive array of video editing tasks.
Beyond the traditional prompt-based editing methods, AnyV2V also can support
novel video editing tasks, including reference-based style transfer,
subject-driven editing, and identity manipulation, which were unattainable by
previous methods. In the second stage, AnyV2V can plug in any existing
image-to-video models to perform DDIM inversion and intermediate feature
injection to maintain the appearance and motion consistency with the source
video. On the prompt-based editing, we show that AnyV2V can outperform the
previous best approach by 35\% on prompt alignment, and 25\% on human
preference. On the three novel tasks, we show that AnyV2V also achieves a high
success rate. We believe AnyV2V will continue to thrive due to its ability to
seamlessly integrate the fast-evolving image editing methods. Such
compatibility can help AnyV2V to increase its versatility to cater to diverse
user demands.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CathFlow: Self-Supervised Segmentation of Catheters in Interventional
  Ultrasound Using Optical Flow and Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Ranne, Liming Kuang, Yordanka Velikova, Nassir Navab, Ferdinando Rodriguez y Baena
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In minimally invasive endovascular procedures, contrast-enhanced angiography
remains the most robust imaging technique. However, it is at the expense of the
patient and clinician's health due to prolonged radiation exposure. As an
alternative, interventional ultrasound has notable benefits such as being
radiation-free, fast to deploy, and having a small footprint in the operating
room. Yet, ultrasound is hard to interpret, and highly prone to artifacts and
noise. Additionally, interventional radiologists must undergo extensive
training before they become qualified to diagnose and treat patients
effectively, leading to a shortage of staff, and a lack of open-source
datasets. In this work, we seek to address both problems by introducing a
self-supervised deep learning architecture to segment catheters in longitudinal
ultrasound images, without demanding any labeled data. The network architecture
builds upon AiAReSeg, a segmentation transformer built with the Attention in
Attention mechanism, and is capable of learning feature changes across time and
space. To facilitate training, we used synthetic ultrasound data based on
physics-driven catheter insertion simulations, and translated the data into a
unique CT-Ultrasound common domain, CACTUSS, to improve the segmentation
performance. We generated ground truth segmentation masks by computing the
optical flow between adjacent frames using FlowNet2, and performed thresholding
to obtain a binary map estimate. Finally, we validated our model on a test
dataset, consisting of unseen synthetic data and images collected from silicon
aorta phantoms, thus demonstrating its potential for applications to clinical
data in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring 3D Human Pose Estimation and Forecasting from the Robot's
  Perspective: The HARPER <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Avogaro. Andrea Toaiari, Federico Cunico, Xiangmin Xu, Haralambos Dafas, Alessandro Vinciarelli, Emma Li, Marco Cristani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce HARPER, a novel dataset for 3D body pose estimation and forecast
in dyadic interactions between users and \spot, the quadruped robot
manufactured by Boston Dynamics. The key-novelty is the focus on the robot's
perspective, i.e., on the data captured by the robot's sensors. These make 3D
body pose analysis challenging because being close to the ground captures
humans only partially. The scenario underlying HARPER includes 15 actions, of
which 10 involve physical contact between the robot and users. The Corpus
contains not only the recordings of the built-in stereo cameras of Spot, but
also those of a 6-camera OptiTrack system (all recordings are synchronized).
This leads to ground-truth skeletal representations with a precision lower than
a millimeter. In addition, the Corpus includes reproducible benchmarks on 3D
Human Pose Estimation, Human Pose Forecasting, and Collision Prediction, all
based on publicly available baseline approaches. This enables future HARPER
users to rigorously compare their results with those we provide in this work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RoDLA: Benchmarking the Robustness of Document Layout Analysis Models <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufan Chen, Jiaming Zhang, Kunyu Peng, Junwei Zheng, Ruiping Liu, Philip Torr, Rainer Stiefelhagen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Before developing a Document Layout Analysis (DLA) model in real-world
applications, conducting comprehensive robustness testing is essential.
However, the robustness of DLA models remains underexplored in the literature.
To address this, we are the first to introduce a robustness benchmark for DLA
models, which includes 450K document images of three datasets. To cover
realistic corruptions, we propose a perturbation taxonomy with 36 common
document perturbations inspired by real-world document processing.
Additionally, to better understand document perturbation impacts, we propose
two metrics, Mean Perturbation Effect (mPE) for perturbation assessment and
Mean Robustness Degradation (mRD) for robustness evaluation. Furthermore, we
introduce a self-titled model, i.e., Robust Document Layout Analyzer (RoDLA),
which improves attention mechanisms to boost extraction of robust features.
Experiments on the proposed benchmarks (PubLayNet-P, DocLayNet-P, and
M$^6$Doc-P) demonstrate that RoDLA obtains state-of-the-art mRD scores of
115.7, 135.4, and 150.4, respectively. Compared to previous methods, RoDLA
achieves notable improvements in mAP of +3.8%, +7.1% and +12.1%, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024. Project page:
  https://yufanchen96.github.io/projects/RoDLA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysing Diffusion Segmentation for Medical Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathias Öttl, Siyuan Mei, Frauke Wilm, Jana Steenpass, Matthias Rübner, Arndt Hartmann, Matthias Beckmann, Peter Fasching, Andreas Maier, Ramona Erber, Katharina Breininger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising Diffusion Probabilistic models have become increasingly popular due
to their ability to offer probabilistic modeling and generate diverse outputs.
This versatility inspired their adaptation for image segmentation, where
multiple predictions of the model can produce segmentation results that not
only achieve high quality but also capture the uncertainty inherent in the
model. Here, powerful architectures were proposed for improving diffusion
segmentation performance. However, there is a notable lack of analysis and
discussions on the differences between diffusion segmentation and image
generation, and thorough evaluations are missing that distinguish the
improvements these architectures provide for segmentation in general from their
benefit for diffusion segmentation specifically. In this work, we critically
analyse and discuss how diffusion segmentation for medical images differs from
diffusion image generation, with a particular focus on the training behavior.
Furthermore, we conduct an assessment how proposed diffusion segmentation
architectures perform when trained directly for segmentation. Lastly, we
explore how different medical segmentation tasks influence the diffusion
segmentation behavior and the diffusion process could be adapted accordingly.
With these analyses, we aim to provide in-depth insights into the behavior of
diffusion segmentation that allow for a better design and evaluation of
diffusion segmentation methods in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Raw Instinct: Trust Your Classifiers and Skip the Conversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christos Kantas, Bjørk Antoniussen, Mathias V. Andersen, Rasmus Munksø, Shobhit Kotnala, Simon B. Jensen, Andreas Møgelmose, Lau Nørgaard, Thomas B. Moeslund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using RAW-images in computer vision problems is surprisingly underexplored
considering that converting from RAW to RGB does not introduce any new capture
information. In this paper, we show that a sufficiently advanced classifier can
yield equivalent results on RAW input compared to RGB and present a new public
dataset consisting of RAW images and the corresponding converted RGB images.
Classifying images directly from RAW is attractive, as it allows for skipping
the conversion to RGB, lowering computation time significantly. Two CNN
classifiers are used to classify the images in both formats, confirming that
classification performance can indeed be preserved. We furthermore show that
the total computation time from RAW image data to classification results for
RAW images can be up to 8.46 times faster than RGB. These results contribute to
the evidence found in related works, that using RAW images as direct input to
computer vision algorithms looks very promising.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://www.kaggle.com/datasets/mathiasviborg/raw-instinct</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Biased Binary Attribute Classifiers Ignore the Majority Classes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Zhang, Johanna Sophie Bieri, Manuel Günther
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To visualize the regions of interest that classifiers base their decisions
on, different Class Activation Mapping (CAM) methods have been developed.
However, all of these techniques target categorical classifiers only, though
most real-world tasks are binary classification. In this paper, we extend
gradient-based CAM techniques to work with binary classifiers and visualize the
active regions for binary facial attribute classifiers. When training an
unbalanced binary classifier on an imbalanced dataset, it is well-known that
the majority class, i.e. the class with many training samples, is mostly
predicted much better than minority class with few training instances. In our
experiments on the CelebA dataset, we verify these results, when training an
unbalanced classifier to extract 40 facial attributes simultaneously. One would
expect that the biased classifier has learned to extract features mainly for
the majority classes and that the proportional energy of the activations mainly
reside in certain specific regions of the image where the attribute is located.
However, we find very little regular activation for samples of majority
classes, while the active regions for minority classes seem mostly reasonable
and overlap with our expectations. These results suggest that biased
classifiers mainly rely on bias activation for majority classes. When training
a balanced classifier on the imbalanced data by employing attribute-specific
class weights, majority and minority classes are classified similarly well and
show expected activations for almost all attributes
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ranking Distillation for Open-Ended Video Question Answering with
  Insufficient Labels <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianming Liang, Chaolei Tan, Beihao Xia, Wei-Shi Zheng, Jian-Fang Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on open-ended video question answering, which aims to find
the correct answers from a large answer set in response to a video-related
question. This is essentially a multi-label classification task, since a
question may have multiple answers. However, due to annotation costs, the
labels in existing benchmarks are always extremely insufficient, typically one
answer per question. As a result, existing works tend to directly treat all the
unlabeled answers as negative labels, leading to limited ability for
generalization. In this work, we introduce a simple yet effective ranking
distillation framework (RADI) to mitigate this problem without additional
manual annotation. RADI employs a teacher model trained with incomplete labels
to generate rankings for potential answers, which contain rich knowledge about
label priority as well as label-associated visual cues, thereby enriching the
insufficient labeling information. To avoid overconfidence in the imperfect
teacher model, we further present two robust and parameter-free ranking
distillation approaches: a pairwise approach which introduces adaptive soft
margins to dynamically refine the optimization constraints on various pairwise
rankings, and a listwise approach which adopts sampling-based partial listwise
learning to resist the bias in teacher ranking. Extensive experiments on five
popular benchmarks consistently show that both our pairwise and listwise RADIs
outperform state-of-the-art methods. Further analysis demonstrates the
effectiveness of our methods on the insufficient labeling problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Style-Extracting Diffusion Models for Semi-Supervised Histopathology
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathias Öttl, Frauke Wilm, Jana Steenpass, Jingna Qiu, Matthias Rübner, Arndt Hartmann, Matthias Beckmann, Peter Fasching, Andreas Maier, Ramona Erber, Bernhard Kainz, Katharina Breininger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based image generation has seen significant advancements with
diffusion models, notably improving the quality of generated images. Despite
these developments, generating images with unseen characteristics beneficial
for downstream tasks has received limited attention. To bridge this gap, we
propose Style-Extracting Diffusion Models, featuring two conditioning
mechanisms. Specifically, we utilize 1) a style conditioning mechanism which
allows to inject style information of previously unseen images during image
generation and 2) a content conditioning which can be targeted to a downstream
task, e.g., layout for segmentation. We introduce a trainable style encoder to
extract style information from images, and an aggregation block that merges
style information from multiple style inputs. This architecture enables the
generation of images with unseen styles in a zero-shot manner, by leveraging
styles from unseen images, resulting in more diverse generations. In this work,
we use the image layout as target condition and first show the capability of
our method on a natural image dataset as a proof-of-concept. We further
demonstrate its versatility in histopathology, where we combine prior knowledge
about tissue composition and unannotated data to create diverse synthetic
images with known layouts. This allows us to generate additional synthetic data
to train a segmentation network in a semi-supervised fashion. We verify the
added value of the generated images by showing improved segmentation results
and lower performance variability between patients when synthetic images are
included during segmentation training. Our code will be made publicly available
at [LINK].
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Lebensold, Maziar Sanjabi, Pietro Astolfi, Adriana Romero-Soriano, Kamalika Chaudhuri, Mike Rabbat, Chuan Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have been shown to suffer from sample-level
memorization, possibly reproducing near-perfect replica of images that they are
trained on, which may be undesirable. To remedy this issue, we develop the
first differentially private (DP) retrieval-augmented generation algorithm that
is capable of generating high-quality image samples while providing provable
privacy guarantees. Specifically, we assume access to a text-to-image diffusion
model trained on a small amount of public data, and design a DP retrieval
mechanism to augment the text prompt with samples retrieved from a private
retrieval dataset. Our \emph{differentially private retrieval-augmented
diffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to
adapt to another domain, and can use state-of-the-art generative models to
generate high-quality image samples while satisfying rigorous DP guarantees.
For instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a
privacy budget of $\epsilon=10$, while providing a $3.5$ point improvement in
FID compared to public-only retrieval for up to $10,000$ queries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14418v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14418v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohao Peng, Xiaoyang Wu, Li Jiang, Yukang Chen, Hengshuang Zhao, Zhuotao Tian, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The booming of 3D recognition in the 2020s began with the introduction of
point cloud transformers. They quickly overwhelmed sparse CNNs and became
state-of-the-art models, especially in 3D semantic segmentation. However,
sparse CNNs are still valuable networks, due to their efficiency treasure, and
ease of application. In this work, we reexamine the design distinctions and
test the limits of what a sparse CNN can achieve. We discover that the key
credit to the performance difference is adaptivity. Specifically, we propose
two key components, i.e., adaptive receptive fields (spatially) and adaptive
relation, to bridge the gap. This exploration led to the creation of
Omni-Adaptive 3D CNNs (OA-CNNs), a family of networks that integrates a
lightweight module to greatly enhance the adaptivity of sparse CNNs at minimal
computational cost. Without any self-attention modules, OA-CNNs favorably
surpass point transformers in terms of accuracy in both indoor and outdoor
scenes, with much less latency and memory cost. Notably, it achieves 76.1%,
78.9%, and 70.6% mIoU on ScanNet v2, nuScenes, and SemanticKITTI validation
benchmarks respectively, while maintaining at most 5x better speed than
transformer counterparts. This revelation highlights the potential of pure
sparse CNNs to outperform transformer-related networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CombiNeRF: A Combination of Regularization Techniques for Few-Shot
  Neural Radiance Field View Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Bonotto, Luigi Sarrocco, Daniele Evangelista, Marco Imperoli, Alberto Pretto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRFs) have shown impressive results for novel view
synthesis when a sufficiently large amount of views are available. When dealing
with few-shot settings, i.e. with a small set of input views, the training
could overfit those views, leading to artifacts and geometric and chromatic
inconsistencies in the resulting rendering. Regularization is a valid solution
that helps NeRF generalization. On the other hand, each of the most recent NeRF
regularization techniques aim to mitigate a specific rendering problem.
Starting from this observation, in this paper we propose CombiNeRF, a framework
that synergically combines several regularization techniques, some of them
novel, in order to unify the benefits of each. In particular, we regularize
single and neighboring rays distributions and we add a smoothness term to
regularize near geometries. After these geometric approaches, we propose to
exploit Lipschitz regularization to both NeRF density and color networks and to
use encoding masks for input features regularization. We show that CombiNeRF
outperforms the state-of-the-art methods with few-shot settings in several
publicly available datasets. We also present an ablation study on the LLFF and
NeRF-Synthetic datasets that support the choices made. We release with this
paper the open-source implementation of our framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication at the 2024
  International Conference on 3D Vision (3DV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLC++: Source-Free Universal Domain Adaptation through Global-Local
  Clustering and Contrastive Affinity Learning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanqing Qu, Tianpei Zou, Florian Röhrbein, Cewu Lu, Guang Chen, Dacheng Tao, Changjun Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks often exhibit sub-optimal performance under covariate
and category shifts. Source-Free Domain Adaptation (SFDA) presents a promising
solution to this dilemma, yet most SFDA approaches are restricted to closed-set
scenarios. In this paper, we explore Source-Free Universal Domain Adaptation
(SF-UniDA) aiming to accurately classify "known" data belonging to common
categories and segregate them from target-private "unknown" data. We propose a
novel Global and Local Clustering (GLC) technique, which comprises an adaptive
one-vs-all global clustering algorithm to discern between target classes,
complemented by a local k-NN clustering strategy to mitigate negative transfer.
Despite the effectiveness, the inherent closed-set source architecture leads to
uniform treatment of "unknown" data, impeding the identification of distinct
"unknown" categories. To address this, we evolve GLC to GLC++, integrating a
contrastive affinity learning strategy. We examine the superiority of GLC and
GLC++ across multiple benchmarks and category shift scenarios. Remarkably, in
the most challenging open-partial-set scenarios, GLC and GLC++ surpass GATE by
16.7% and 18.6% in H-score on VisDA, respectively. GLC++ enhances the novel
category clustering accuracy of GLC by 4.3% in open-set scenarios on
Office-Home. Furthermore, the introduced contrastive learning strategy not only
enhances GLC but also significantly facilitates existing methodologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a substantial extension of the CVPR 2023 paper "Upcycling
  Models under Domain and Category Shift"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pensieve: Retrospect-then-Compare Mitigates Visual Hallucination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingchen Yang, Bowen Cao, Guang Chen, Changjun Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal Large Language Models (MLLMs) demonstrate remarkable success
across various vision-language tasks. However, they suffer from visual
hallucination, where the generated responses diverge from the provided image.
Are MLLMs completely oblivious to accurate visual cues when they hallucinate?
Our investigation reveals that the visual branch may simultaneously advocate
both accurate and non-existent content. To address this issue, we propose
Pensieve, a training-free method inspired by our observation that analogous
visual hallucinations can arise among images sharing common semantic and
appearance characteristics. During inference, Pensieve enables MLLMs to
retrospect relevant images as references and compare them with the test image.
This paradigm assists MLLMs in downgrading hallucinatory content mistakenly
supported by the visual input. Experiments on Whoops, MME, POPE, and LLaVA
Bench demonstrate the efficacy of Pensieve in mitigating visual hallucination,
surpassing other advanced decoding strategies. Additionally, Pensieve aids
MLLMs in identifying details in the image and enhancing the specificity of
image descriptions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Bag of Tricks for Few-Shot Class-Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuvendu Roy, Chunjong Park, Aldi Fahrezi, Ali Etemad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a bag of tricks framework for few-shot class-incremental learning
(FSCIL), which is a challenging form of continual learning that involves
continuous adaptation to new tasks with limited samples. FSCIL requires both
stability and adaptability, i.e., preserving proficiency in previously learned
tasks while learning new ones. Our proposed bag of tricks brings together eight
key and highly influential techniques that improve stability, adaptability, and
overall performance under a unified framework for FSCIL. We organize these
tricks into three categories: stability tricks, adaptability tricks, and
training tricks. Stability tricks aim to mitigate the forgetting of previously
learned classes by enhancing the separation between the embeddings of learned
classes and minimizing interference when learning new ones. On the other hand,
adaptability tricks focus on the effective learning of new classes. Finally,
training tricks improve the overall performance without compromising stability
or adaptability. We perform extensive experiments on three benchmark datasets,
CIFAR-100, CUB-200, and miniIMageNet, to evaluate the impact of our proposed
framework. Our detailed analysis shows that our approach substantially improves
both stability and adaptability, establishing a new state-of-the-art by
outperforming prior works in the area. We believe our method provides a go-to
solution and establishes a robust baseline for future research in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tensor network compressibility of convolutional models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sukhbinder Singh, Saeed S. Jahromi, Roman Orus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) represent one of the most widely used
neural network architectures, showcasing state-of-the-art performance in
computer vision tasks. Although larger CNNs generally exhibit higher accuracy,
their size can be effectively reduced by "tensorization" while maintaining
accuracy. Tensorization consists of replacing the convolution kernels with
compact decompositions such as Tucker, Canonical Polyadic decompositions, or
quantum-inspired decompositions such as matrix product states, and directly
training the factors in the decompositions to bias the learning towards
low-rank decompositions. But why doesn't tensorization seem to impact the
accuracy adversely? We explore this by assessing how truncating the convolution
kernels of dense (untensorized) CNNs impact their accuracy. Specifically, we
truncated the kernels of (i) a vanilla four-layer CNN and (ii) ResNet-50
pre-trained for image classification on CIFAR-10 and CIFAR-100 datasets. We
found that kernels (especially those inside deeper layers) could often be
truncated along several cuts resulting in significant loss in kernel norm but
not in classification accuracy. This suggests that such ``correlation
compression'' (underlying tensorization) is an intrinsic feature of how
information is encoded in dense CNNs. We also found that aggressively truncated
models could often recover the pre-truncation accuracy after only a few epochs
of re-training, suggesting that compressing the internal correlations of
convolution layers does not often transport the model to a worse minimum. Our
results can be applied to tensorize and compress CNN models more effectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 21 images</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InfNeRF: Towards Infinite Scale NeRF Rendering with O(log n) Space
  Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiabin Liang, Lanqing Zhang, Zhuoran Zhao, Xiangyu Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conventional mesh-based Level of Detail (LoD) technique, exemplified by
applications such as Google Earth and many game engines, exhibits the
capability to holistically represent a large scene even the Earth, and achieves
rendering with a space complexity of O(log n). This constrained data
requirement not only enhances rendering efficiency but also facilitates dynamic
data fetching, thereby enabling a seamless 3D navigation experience for users.
In this work, we extend this proven LoD technique to Neural Radiance Fields
(NeRF) by introducing an octree structure to represent the scenes in different
scales. This innovative approach provides a mathematically simple and elegant
representation with a rendering space complexity of O(log n), aligned with the
efficiency of mesh-based LoD techniques. We also present a novel training
strategy that maintains a complexity of O(n). This strategy allows for parallel
training with minimal overhead, ensuring the scalability and efficiency of our
proposed method. Our contribution is not only in extending the capabilities of
existing techniques but also in establishing a foundation for scalable and
efficient large-scale scene representation using NeRF and octree structures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SyncTweedies: A General Generative Framework Based on Synchronized
  Diffusions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaihoon Kim, Juil Koo, Kyeongmin Yeo, Minhyuk Sung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a general framework for generating diverse visual content,
including ambiguous images, panorama images, mesh textures, and Gaussian splat
textures, by synchronizing multiple diffusion processes. We present exhaustive
investigation into all possible scenarios for synchronizing multiple diffusion
processes through a canonical space and analyze their characteristics across
applications. In doing so, we reveal a previously unexplored case: averaging
the outputs of Tweedie's formula while conducting denoising in multiple
instance spaces. This case also provides the best quality with the widest
applicability to downstream tasks. We name this case SyncTweedies. In our
experiments generating visual content aforementioned, we demonstrate the
superior quality of generation by SyncTweedies compared to other
synchronization methods, optimization-based and iterative-update-based methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://synctweedies.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enabling Visual Composition and Animation in Unsupervised Video
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aram Davtyan, Sepehr Sameni, Björn Ommer, Paolo Favaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we propose a novel method for unsupervised controllable video
generation. Once trained on a dataset of unannotated videos, at inference our
model is capable of both composing scenes of predefined object parts and
animating them in a plausible and controlled way. This is achieved by
conditioning video generation on a randomly selected subset of local
pre-trained self-supervised features during training. We call our model CAGE
for visual Composition and Animation for video GEneration. We conduct a series
of experiments to demonstrate capabilities of CAGE in various settings. Project
website: https://araachie.github.io/cage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://araachie.github.io/cage</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SurroundSDF: Implicit 3D Scene Understanding Based on Signed Distance
  Field 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lizhe Liu, Bohua Wang, Hongwei Xie, Daqi Liu, Li Liu, Zhiqiang Tian, Kuiyuan Yang, Bing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-centric 3D environment understanding is both vital and challenging for
autonomous driving systems. Recently, object-free methods have attracted
considerable attention. Such methods perceive the world by predicting the
semantics of discrete voxel grids but fail to construct continuous and accurate
obstacle surfaces. To this end, in this paper, we propose SurroundSDF to
implicitly predict the signed distance field (SDF) and semantic field for the
continuous perception from surround images. Specifically, we introduce a
query-based approach and utilize SDF constrained by the Eikonal formulation to
accurately describe the surfaces of obstacles. Furthermore, considering the
absence of precise SDF ground truth, we propose a novel weakly supervised
paradigm for SDF, referred to as the Sandwich Eikonal formulation, which
emphasizes applying correct and dense constraints on both sides of the surface,
thereby enhancing the perceptual accuracy of the surface. Experiments suggest
that our method achieves SOTA for both occupancy prediction and 3D scene
reconstruction tasks on the nuScenes dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen
  Domains by Intrinsic Learning from Redundant LLM Semantics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Yue, Jiancheng Zhao, Chunhui Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen
classes against domain shift problem (DSP) where data of unseen classes may be
misclassified as seen classes. However, existing GZSL is still limited to seen
domains. In the current work, we pioneer cross-domain GZSL (CDGZSL) which
addresses GZSL towards unseen domains. Different from existing GZSL methods
which alleviate DSP by generating features of unseen classes with semantics,
CDGZSL needs to construct a common feature space across domains and acquire the
corresponding intrinsic semantics shared among domains to transfer from seen to
unseen domains. Considering the information asymmetry problem caused by
redundant class semantics annotated with large language models (LLMs), we
present Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR
consists of two parts: Inter-class Similarity Alignment (ISA), which eliminates
the non-intrinsic semantics not shared across all domains under the guidance of
inter-class feature relationships, and Unseen-class Meta Generation (UMG),
which preserves intrinsic semantics to maintain connectivity between seen and
unseen classes by simulating feature generation. MDASR effectively aligns the
redundant semantic space with the common feature space, mitigating the
information asymmetry in CDGZSL. The effectiveness of MDASR is demonstrated on
the Office-Home and Mini-DomainNet, and we have shared the LLM-based semantics
for these datasets as the benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work is submitted to IEEE TNNLS and is subject to IEEE copyright</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Varroa destructor detection on honey bees using hyperspectral imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14359v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14359v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zina-Sabrina Duma, Tomas Zemcik, Simon Bilik, Tuomas Sihvonen, Peter Honec, Satu-Pia Reinikainen, Karel Horak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral (HS) imagery in agriculture is becoming increasingly common.
These images have the advantage of higher spectral resolution. Advanced
spectral processing techniques are required to unlock the information potential
in these HS images. The present paper introduces a method rooted in
multivariate statistics designed to detect parasitic Varroa destructor mites on
the body of western honey bee Apis mellifera, enabling easier and continuous
monitoring of the bee hives. The methodology explores unsupervised (K-means++)
and recently developed supervised (Kernel Flows - Partial Least-Squares,
KF-PLS) methods for parasitic identification. Additionally, in light of the
emergence of custom-band multispectral cameras, the present research outlines a
strategy for identifying the specific wavelengths necessary for effective
bee-mite separation, suitable for implementation in a custom-band camera.
Illustrated with a real-case dataset, our findings demonstrate that as few as
four spectral bands are sufficient for accurate parasite identification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LDTR: Transformer-based Lane Detection with Anchor-chain Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongyu Yang, Chen Shen, Wei Shao, Tengfei Xing, Runbo Hu, Pengfei Xu, Hua Chai, Ruini Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances in lane detection methods, scenarios with limited- or
no-visual-clue of lanes due to factors such as lighting conditions and
occlusion remain challenging and crucial for automated driving. Moreover,
current lane representations require complex post-processing and struggle with
specific instances. Inspired by the DETR architecture, we propose LDTR, a
transformer-based model to address these issues. Lanes are modeled with a novel
anchor-chain, regarding a lane as a whole from the beginning, which enables
LDTR to handle special lanes inherently. To enhance lane instance perception,
LDTR incorporates a novel multi-referenced deformable attention module to
distribute attention around the object. Additionally, LDTR incorporates two
line IoU algorithms to improve convergence efficiency and employs a Gaussian
heatmap auxiliary branch to enhance model representation capability during
training. To evaluate lane detection models, we rely on Frechet distance,
parameterized F1-score, and additional synthetic metrics. Experimental results
demonstrate that LDTR achieves state-of-the-art performance on well-known
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVM 2024 and CVMJ. 16 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Annotation-Efficient Polyp Segmentation via Active Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duojun Huang, Xinyu Xiong, De-Jun Fan, Feng Gao, Xiao-Jian Wu, Guanbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based techniques have proven effective in polyp segmentation
tasks when provided with sufficient pixel-wise labeled data. However, the high
cost of manual annotation has created a bottleneck for model generalization. To
minimize annotation costs, we propose a deep active learning framework for
annotation-efficient polyp segmentation. In practice, we measure the
uncertainty of each sample by examining the similarity between features masked
by the prediction map of the polyp and the background area. Since the
segmentation model tends to perform weak in samples with indistinguishable
features of foreground and background areas, uncertainty sampling facilitates
the fitting of under-learning data. Furthermore, clustering image-level
features weighted by uncertainty identify samples that are both uncertain and
representative. To enhance the selectivity of the active selection strategy, we
propose a novel unsupervised feature discrepancy learning mechanism. The
selection strategy and feature optimization work in tandem to achieve optimal
performance with a limited annotation budget. Extensive experimental results
have demonstrated that our proposed method achieved state-of-the-art
performance compared to other competitors on both a public dataset and a
large-scale in-house dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 IEEE 21th International Symposium on Biomedical Imaging (ISBI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Concept Trustworthiness in Concept Bottleneck Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qihan Huang, Jie Song, Jingwen Hu, Haofei Zhang, Yong Wang, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept Bottleneck Models (CBMs), which break down the reasoning process into
the input-to-concept mapping and the concept-to-label prediction, have garnered
significant attention due to their remarkable interpretability achieved by the
interpretable concept bottleneck. However, despite the transparency of the
concept-to-label prediction, the mapping from the input to the intermediate
concept remains a black box, giving rise to concerns about the trustworthiness
of the learned concepts (i.e., these concepts may be predicted based on
spurious cues). The issue of concept untrustworthiness greatly hampers the
interpretability of CBMs, thereby hindering their further advancement. To
conduct a comprehensive analysis on this issue, in this study we establish a
benchmark to assess the trustworthiness of concepts in CBMs. A pioneering
metric, referred to as concept trustworthiness score, is proposed to gauge
whether the concepts are derived from relevant regions. Additionally, an
enhanced CBM is introduced, enabling concept predictions to be made
specifically from distinct parts of the feature map, thereby facilitating the
exploration of their related regions. Besides, we introduce three modules,
namely the cross-layer alignment (CLA) module, the cross-image alignment (CIA)
module, and the prediction alignment (PA) module, to further enhance the
concept trustworthiness within the elaborated CBM. The experiments on five
datasets across ten architectures demonstrate that without using any concept
localization annotations during training, our model improves the concept
trustworthiness by a large margin, meanwhile achieving superior accuracy to the
state-of-the-arts. Our code is available at https://github.com/hqhQAQ/ProtoCBM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Efficient Information Fusion: Concentric Dual Fusion Attention
  Based Multiple Instance Learning for Whole Slide Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujian Liu, Ruoxuan Wu, Xinjie Shen, Zihuang Lu, Lingyu Liang, Haiyu Zhou, Shipu Xu, Shaoai Cai, Shidang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of digital pathology, multi-magnification Multiple Instance
Learning (multi-mag MIL) has proven effective in leveraging the hierarchical
structure of Whole Slide Images (WSIs) to reduce information loss and redundant
data. However, current methods fall short in bridging the domain gap between
pretrained models and medical imaging, and often fail to account for spatial
relationships across different magnifications. Addressing these challenges, we
introduce the Concentric Dual Fusion Attention-MIL (CDFA-MIL) framework,which
innovatively combines point-to-area feature-colum attention and point-to-point
concentric-row attention using concentric patch. This approach is designed to
effectively fuse correlated information, enhancing feature representation and
providing stronger correlation guidance for WSI analysis. CDFA-MIL
distinguishes itself by offering a robust fusion strategy that leads to
superior WSI recognition. Its application has demonstrated exceptional
performance, significantly surpassing existing MIL methods in accuracy and F1
scores on prominent datasets like Camelyon16 and TCGA-NSCLC. Specifically,
CDFA-MIL achieved an average accuracy and F1-score of 93.7\% and 94.1\%
respectively on these datasets, marking a notable advancement over traditional
MIL approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $\nabla τ$: Gradient-based and Task-Agnostic machine Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Trippa, Cesare Campagnano, Maria Sofia Bucarelli, Gabriele Tolomei, Fabrizio Silvestri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Unlearning, the process of selectively eliminating the influence of
certain data examples used during a model's training, has gained significant
attention as a means for practitioners to comply with recent data protection
regulations. However, existing unlearning methods face critical drawbacks,
including their prohibitively high cost, often associated with a large number
of hyperparameters, and the limitation of forgetting only relatively small data
portions. This often makes retraining the model from scratch a quicker and more
effective solution. In this study, we introduce Gradient-based and
Task-Agnostic machine Unlearning ($\nabla \tau$), an optimization framework
designed to remove the influence of a subset of training data efficiently. It
applies adaptive gradient ascent to the data to be forgotten while using
standard gradient descent for the remaining data. $\nabla \tau$ offers multiple
benefits over existing approaches. It enables the unlearning of large sections
of the training dataset (up to 30%). It is versatile, supporting various
unlearning tasks (such as subset forgetting or class removal) and applicable
across different domains (images, text, etc.). Importantly, $\nabla \tau$
requires no hyperparameter adjustments, making it a more appealing option than
retraining the model from scratch. We evaluate our framework's effectiveness
using a set of well-established Membership Inference Attack metrics,
demonstrating up to 10% enhancements in performance compared to
state-of-the-art methods without compromising the original model's accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FFT-based Selection and Optimization of Statistics for Robust
  Recognition of Severely Corrupted Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elena Camuffo, Umberto Michieli, Jijoong Moon, Daehyun Kim, Mete Ozay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving model robustness in case of corrupted images is among the key
challenges to enable robust vision systems on smart devices, such as robotic
agents. Particularly, robust test-time performance is imperative for most of
the applications. This paper presents a novel approach to improve robustness of
any classification model, especially on severely corrupted images. Our method
(FROST) employs high-frequency features to detect input image corruption type,
and select layer-wise feature normalization statistics. FROST provides the
state-of-the-art results for different models and datasets, outperforming
competitors on ImageNet-C by up to 37.1% relative gain, improving baseline of
40.9% mCE on severe corruptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2024. Copyright 2024 IEEE. Personal use of this material is
  permitted. Permission from IEEE must be obtained for all other uses, in any
  current or future media, including reprinting/republishing this material for
  advertising or promotional purposes, creating new collective works, for
  resale or redistribution to servers or lists, or reuse of any copyrighted
  component of this work in other</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CFPL-FAS: Class Free Prompt Learning for Generalizable Face
  Anti-spoofing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ajian Liu, Shuai Xue, Jianwen Gan, Jun Wan, Yanyan Liang, Jiankang Deng, Sergio Escalera, Zhen Lei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain generalization (DG) based Face Anti-Spoofing (FAS) aims to improve the
model's performance on unseen domains. Existing methods either rely on domain
labels to align domain-invariant feature spaces, or disentangle generalizable
features from the whole sample, which inevitably lead to the distortion of
semantic feature structures and achieve limited generalization. In this work,
we make use of large-scale VLMs like CLIP and leverage the textual feature to
dynamically adjust the classifier's weights for exploring generalizable visual
features. Specifically, we propose a novel Class Free Prompt Learning (CFPL)
paradigm for DG FAS, which utilizes two lightweight transformers, namely
Content Q-Former (CQF) and Style Q-Former (SQF), to learn the different
semantic prompts conditioned on content and style features by using a set of
learnable query vectors, respectively. Thus, the generalizable prompt can be
learned by two improvements: (1) A Prompt-Text Matched (PTM) supervision is
introduced to ensure CQF learns visual representation that is most informative
of the content description. (2) A Diversified Style Prompt (DSP) technology is
proposed to diversify the learning of style prompts by mixing feature
statistics between instance-specific styles. Finally, the learned text features
modulate visual features to generalization through the designed Prompt
Modulation (PM). Extensive experiments show that the CFPL is effective and
outperforms the state-of-the-art methods on several cross-domain datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Network-Based Processing and Reconstruction of Compromised
  Biophotonic Image Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael John Fanous, Paloma Casteleiro Costa, Cagatay Isil, Luzhe Huang, Aydogan Ozcan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of deep learning techniques with biophotonic setups has
opened new horizons in bioimaging. A compelling trend in this field involves
deliberately compromising certain measurement metrics to engineer better
bioimaging tools in terms of cost, speed, and form-factor, followed by
compensating for the resulting defects through the utilization of deep learning
models trained on a large amount of ideal, superior or alternative data. This
strategic approach has found increasing popularity due to its potential to
enhance various aspects of biophotonic imaging. One of the primary motivations
for employing this strategy is the pursuit of higher temporal resolution or
increased imaging speed, critical for capturing fine dynamic biological
processes. This approach also offers the prospect of simplifying hardware
requirements/complexities, thereby making advanced imaging standards more
accessible in terms of cost and/or size. This article provides an in-depth
review of the diverse measurement aspects that researchers intentionally impair
in their biophotonic setups, including the point spread function,
signal-to-noise ratio, sampling density, and pixel resolution. By deliberately
compromising these metrics, researchers aim to not only recuperate them through
the application of deep learning networks, but also bolster in return other
crucial parameters, such as the field-of-view, depth-of-field, and
space-bandwidth product. Here, we discuss various biophotonic methods that have
successfully employed this strategic approach. These techniques span broad
applications and showcase the versatility and effectiveness of deep learning in
the context of compromised biophotonic data. Finally, by offering our
perspectives on the future possibilities of this rapidly evolving concept, we
hope to motivate our readers to explore novel ways of balancing hardware
compromises with compensation via AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 Pages, 4 Figures, 1 Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exosense: A Vision-Centric Scene Understanding System For Safe
  Exoskeleton Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianeng Wang, Matias Mattamala, Christina Kassab, Lintong Zhang, Maurice Fallon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exoskeletons for daily use by those with mobility impairments are being
developed. They will require accurate and robust scene understanding systems.
Current research has used vision to identify immediate terrain and geometric
obstacles, however these approaches are constrained to detections directly in
front of the user and are limited to classifying a finite range of terrain
types (e.g., stairs, ramps and level-ground). This paper presents Exosense, a
vision-centric scene understanding system which is capable of generating rich,
globally-consistent elevation maps, incorporating both semantic and terrain
traversability information. It features an elastic Atlas mapping framework
associated with a visual SLAM pose graph, embedded with open-vocabulary room
labels from a Vision-Language Model (VLM). The device's design includes a wide
field-of-view (FoV) fisheye multi-camera system to mitigate the challenges
introduced by the exoskeleton walking pattern. We demonstrate the system's
robustness to the challenges of typical periodic walking gaits, and its ability
to construct accurate semantically-rich maps in indoor settings. Additionally,
we showcase its potential for motion planning -- providing a step towards safe
navigation for exoskeletons.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Lightweight Attention-based Deep Network via Multi-Scale Feature
  Fusion for Multi-View Facial Expression Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Ezati, Mohammadreza Dezyani, Rajib Rana, Roozbeh Rajabi, Ahmad Ayatollahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) and their variations have shown
effectiveness in facial expression recognition (FER). However, they face
challenges when dealing with high computational complexity and multi-view head
poses in real-world scenarios. We introduce a lightweight attentional network
incorporating multi-scale feature fusion (LANMSFF) to tackle these issues. For
the first challenge, we have carefully designed a lightweight fully
convolutional network (FCN). We address the second challenge by presenting two
novel components, namely mass attention (MassAtt) and point wise feature
selection (PWFS) blocks. The MassAtt block simultaneously generates channel and
spatial attention maps to recalibrate feature maps by emphasizing important
features while suppressing irrelevant ones. On the other hand, the PWFS block
employs a feature selection mechanism that discards less meaningful features
prior to the fusion process. This mechanism distinguishes it from previous
methods that directly fuse multi-scale features. Our proposed approach achieved
results comparable to state-of-the-art methods in terms of parameter counts and
robustness to pose variation, with accuracy rates of 90.77% on KDEF, 70.44% on
FER-2013, and 86.96% on FERPlus datasets. The code for LANMSFF is available at
https://github.com/AE-1129/LANMSFF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, two-column, submitted to journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpikingResformer: Bridging ResNet and Vision Transformer in Spiking
  Neural Networks <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Shi, Zecheng Hao, Zhaofei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable success of Vision Transformers in Artificial Neural Networks
(ANNs) has led to a growing interest in incorporating the self-attention
mechanism and transformer-based architecture into Spiking Neural Networks
(SNNs). While existing methods propose spiking self-attention mechanisms that
are compatible with SNNs, they lack reasonable scaling methods, and the overall
architectures proposed by these methods suffer from a bottleneck in effectively
extracting local features. To address these challenges, we propose a novel
spiking self-attention mechanism named Dual Spike Self-Attention (DSSA) with a
reasonable scaling method. Based on DSSA, we propose a novel spiking Vision
Transformer architecture called SpikingResformer, which combines the
ResNet-based multi-stage architecture with our proposed DSSA to improve both
performance and energy efficiency while reducing parameters. Experimental
results show that SpikingResformer achieves higher accuracy with fewer
parameters and lower energy consumption than other spiking Vision Transformer
counterparts. Notably, our SpikingResformer-L achieves 79.40% top-1 accuracy on
ImageNet with 4 time-steps, which is the state-of-the-art result in the SNN
field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the 2024 IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact Assessment of Missing Data in Model Predictions for Earth
  Observation Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco Mena, Diego Arenas, Marcela Charfuelan, Marlon Nuske, Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Earth observation (EO) applications involving complex and heterogeneous data
sources are commonly approached with machine learning models. However, there is
a common assumption that data sources will be persistently available. Different
situations could affect the availability of EO sources, like noise, clouds, or
satellite mission failures. In this work, we assess the impact of missing
temporal and static EO sources in trained models across four datasets with
classification and regression tasks. We compare the predictive quality of
different methods and find that some are naturally more robust to missing data.
The Ensemble strategy, in particular, achieves a prediction robustness up to
100%. We evidence that missing scenarios are significantly more challenging in
regression than classification tasks. Finally, we find that the optical view is
the most critical view when it is missing individually.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE International Geoscience and Remote Sensing
  Symposium 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HySim: An Efficient Hybrid Similarity Measure for Patch Matching in
  Image Inpainting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saad Noufel, Nadir Maaroufi, Mehdi Najib, Mohamed Bakhouya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inpainting, for filling missing image regions, is a crucial task in various
applications, such as medical imaging and remote sensing. Trending data-driven
approaches efficiency, for image inpainting, often requires extensive data
preprocessing. In this sense, there is still a need for model-driven approaches
in case of application constrained with data availability and quality,
especially for those related for time series forecasting using image inpainting
techniques. This paper proposes an improved modeldriven approach relying on
patch-based techniques. Our approach deviates from the standard Sum of Squared
Differences (SSD) similarity measure by introducing a Hybrid Similarity
(HySim), which combines both strengths of Chebychev and Minkowski distances.
This hybridization enhances patch selection, leading to high-quality inpainting
results with reduced mismatch errors. Experimental results proved the
effectiveness of our approach against other model-driven techniques, such as
diffusion or patch-based approaches, showcasing its effectiveness in achieving
visually pleasing restorations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-Vocabulary Attention Maps with Token Optimization for Semantic
  Segmentation in Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Marcos-Manchón, Roberto Alcover-Couso, Juan C. SanMiguel, Jose M. Martínez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models represent a new paradigm in text-to-image generation. Beyond
generating high-quality images from text prompts, models such as Stable
Diffusion have been successfully extended to the joint generation of semantic
segmentation pseudo-masks. However, current extensions primarily rely on
extracting attentions linked to prompt words used for image synthesis. This
approach limits the generation of segmentation masks derived from word tokens
not contained in the text prompt. In this work, we introduce Open-Vocabulary
Attention Maps (OVAM)-a training-free method for text-to-image diffusion models
that enables the generation of attention maps for any word. In addition, we
propose a lightweight optimization process based on OVAM for finding tokens
that generate accurate attention maps for an object class with a single
annotation. We evaluate these tokens within existing state-of-the-art Stable
Diffusion extensions. The best-performing model improves its mIoU from 52.1 to
86.6 for the synthetic images' pseudo-masks, demonstrating that our optimized
tokens are an efficient way to improve the performance of existing methods
without architectural changes or retraining.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Green AI for Audio Deepfake Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhajit Saha, Md Sahidullah, Swagatam Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The state-of-the-art audio deepfake detectors leveraging deep neural networks
exhibit impressive recognition performance. Nonetheless, this advantage is
accompanied by a significant carbon footprint. This is mainly due to the use of
high-performance computing with accelerators and high training time. Studies
show that average deep NLP model produces around 626k lbs of
CO\textsubscript{2} which is equivalent to five times of average US car
emission at its lifetime. This is certainly a massive threat to the
environment. To tackle this challenge, this study presents a novel framework
for audio deepfake detection that can be seamlessly trained using standard CPU
resources. Our proposed framework utilizes off-the-shelve self-supervised
learning (SSL) based models which are pre-trained and available in public
repositories. In contrast to existing methods that fine-tune SSL models and
employ additional deep neural networks for downstream tasks, we exploit
classical machine learning algorithms such as logistic regression and shallow
neural networks using the SSL embeddings extracted using the pre-trained model.
Our approach shows competitive results compared to the commonly used
high-carbon footprint approaches. In experiments with the ASVspoof 2019 LA
dataset, we achieve a 0.90\% equal error rate (EER) with less than 1k trainable
model parameters. To encourage further research in this direction and support
reproducible results, the Python code will be made publicly accessible
following acceptance. Github: https://github.com/sahasubhajit/Speech-Spoofing-
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript is under review in a conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Historical Image Retrieval with Compositional Cues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingyu Lin, Robert Sablatnig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In analyzing vast amounts of digitally stored historical image data, existing
content-based retrieval methods often overlook significant non-semantic
information, limiting their effectiveness for flexible exploration across
varied themes. To broaden the applicability of image retrieval methods for
diverse purposes and uncover more general patterns, we innovatively introduce a
crucial factor from computational aesthetics, namely image composition, into
this topic. By explicitly integrating composition-related information extracted
by CNN into the designed retrieval model, our method considers both the image's
composition rules and semantic information. Qualitative and quantitative
experiments demonstrate that the image retrieval network guided by composition
information outperforms those relying solely on content information,
facilitating the identification of images in databases closer to the target
image in human perception. Please visit https://github.com/linty5/CCBIR to try
our codes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing the Robustness of Spectral Clustering for Deep Speaker
  Diarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Raghav, Md Sahidullah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clustering speaker embeddings is crucial in speaker diarization but hasn't
received as much focus as other components. Moreover, the robustness of speaker
diarization across various datasets hasn't been explored when the development
and evaluation data are from different domains. To bridge this gap, this study
thoroughly examines spectral clustering for both same-domain and cross-domain
speaker diarization. Our extensive experiments on two widely used corpora, AMI
and DIHARD, reveal the performance trend of speaker diarization in the presence
of domain mismatch. We observe that the performance difference between two
different domain conditions can be attributed to the role of spectral
clustering. In particular, keeping other modules unchanged, we show that
differences in optimal tuning parameters as well as speaker count estimation
originates due to the mismatch. This study opens several future directions for
speaker diarization research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Manuscript Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero123-6D: Zero-shot Novel View Synthesis for RGB Category-level 6D
  Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Di Felice, Alberto Remus, Stefano Gasperini, Benjamin Busam, Lionel Ott, Federico Tombari, Roland Siegwart, Carlo Alberto Avizzano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the pose of objects through vision is essential to make robotic
platforms interact with the environment. Yet, it presents many challenges,
often related to the lack of flexibility and generalizability of
state-of-the-art solutions. Diffusion models are a cutting-edge neural
architecture transforming 2D and 3D computer vision, outlining remarkable
performances in zero-shot novel-view synthesis. Such a use case is particularly
intriguing for reconstructing 3D objects. However, localizing objects in
unstructured environments is rather unexplored. To this end, this work presents
Zero123-6D to demonstrate the utility of Diffusion Model-based
novel-view-synthesizers in enhancing RGB 6D pose estimation at category-level
by integrating them with feature extraction techniques. The outlined method
exploits such a novel view synthesizer to expand a sparse set of RGB-only
reference views for the zero-shot 6D pose estimation task. Experiments are
quantitatively analyzed on the CO3D dataset, showcasing increased performance
over baselines, a substantial reduction in data requirements, and the removal
of the necessity of depth information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 reference pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Salzmann, Markus Ryll, Alex Bewley, Matthias Minderer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual relationship detection aims to identify objects and their
relationships in images. Prior methods approach this task by adding separate
relationship modules or decoders to existing object detection architectures.
This separation increases complexity and hinders end-to-end training, which
limits performance. We propose a simple and highly efficient decoder-free
architecture for open-vocabulary visual relationship detection. Our model
consists of a Transformer-based image encoder that represents objects as tokens
and models their relationships implicitly. To extract relationship information,
we introduce an attention mechanism that selects object pairs likely to form a
relationship. We provide a single-stage recipe to train this model on a mixture
of object and relationship detection data. Our approach achieves
state-of-the-art relationship detection performance on Visual Genome and on the
large-vocabulary GQA benchmark at real-time inference speeds. We provide
analyses of zero-shot performance, ablations, and real-world qualitative
examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Framework for Portrait Stylization with Skin-Tone Awareness and Nudity
  Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungkwon Kim, Sangyeon Kim, Seung-Hun Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Portrait stylization is a challenging task involving the transformation of an
input portrait image into a specific style while preserving its inherent
characteristics. The recent introduction of Stable Diffusion (SD) has
significantly improved the quality of outcomes in this field. However, a
practical stylization framework that can effectively filter harmful input
content and preserve the distinct characteristics of an input, such as
skin-tone, while maintaining the quality of stylization remains lacking. These
challenges have hindered the wide deployment of such a framework. To address
these issues, this study proposes a portrait stylization framework that
incorporates a nudity content identification module (NCIM) and a
skin-tone-aware portrait stylization module (STAPSM). In experiments, NCIM
showed good performance in enhancing explicit content filtering, and STAPSM
accurately represented a diverse range of skin tones. Our proposed framework
has been successfully deployed in practice, and it has effectively satisfied
critical requirements of real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Models with Ensembled Structure-Based Anomaly Scoring for
  Unsupervised Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Finn Behrendt, Debayan Bhattacharya, Lennart Maack, Julia Krüger, Roland Opfer, Robin Mieling, Alexander Schlaefer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised deep learning techniques show promise in medical image analysis.
However, they require comprehensive annotated data sets, which poses
challenges, particularly for rare diseases. Consequently, unsupervised anomaly
detection (UAD) emerges as a viable alternative for pathology segmentation, as
only healthy data is required for training. However, recent UAD anomaly scoring
functions often focus on intensity only and neglect structural differences,
which impedes the segmentation performance. This work investigates the
potential of Structural Similarity (SSIM) to bridge this gap. SSIM captures
both intensity and structural disparities and can be advantageous over the
classical $l1$ error. However, we show that there is more than one optimal
kernel size for the SSIM calculation for different pathologies. Therefore, we
investigate an adaptive ensembling strategy for various kernel sizes to offer a
more pathology-agnostic scoring mechanism. We demonstrate that this ensembling
strategy can enhance the performance of DMs and mitigate the sensitivity to
different kernel sizes across varying pathologies, highlighting its promise for
brain MRI anomaly detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE ISBI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LayoutLLM: Large Language Model Instruction Tuning for Visually Rich
  Document Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masato Fujitake
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes LayoutLLM, a more flexible document analysis method for
understanding imaged documents. Visually Rich Document Understanding tasks,
such as document image classification and information extraction, have gained
significant attention due to their importance. Existing methods have been
developed to enhance document comprehension by incorporating pre-training
awareness of images, text, and layout structure. However, these methods require
fine-tuning for each task and dataset, and the models are expensive to train
and operate. To overcome this limitation, we propose a new LayoutLLM that
integrates these with large-scale language models (LLMs). By leveraging the
strengths of existing research in document image understanding and LLMs'
superior language understanding capabilities, the proposed model, fine-tuned
with multimodal instruction datasets, performs an understanding of document
images in a single model. Our experiments demonstrate improvement over the
baseline model in various document analysis tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safeguarding Medical Image Segmentation <span class="highlight-title">Dataset</span>s against Unauthorized
  Training via Contour- and Texture-Aware Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xun Lin, Yi Yu, Song Xia, Jue Jiang, Haoran Wang, Zitong Yu, Yizhong Liu, Ying Fu, Shuai Wang, Wenzhong Tang, Alex Kot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread availability of publicly accessible medical images has
significantly propelled advancements in various research and clinical fields.
Nonetheless, concerns regarding unauthorized training of AI systems for
commercial purposes and the duties of patient privacy protection have led
numerous institutions to hesitate to share their images. This is particularly
true for medical image segmentation (MIS) datasets, where the processes of
collection and fine-grained annotation are time-intensive and laborious.
Recently, Unlearnable Examples (UEs) methods have shown the potential to
protect images by adding invisible shortcuts. These shortcuts can prevent
unauthorized deep neural networks from generalizing. However, existing UEs are
designed for natural image classification and fail to protect MIS datasets
imperceptibly as their protective perturbations are less learnable than
important prior knowledge in MIS, e.g., contour and texture features. To this
end, we propose an Unlearnable Medical image generation method, termed UMed.
UMed integrates the prior knowledge of MIS by injecting contour- and
texture-aware perturbations to protect images. Given that our target is to only
poison features critical to MIS, UMed requires only minimal perturbations
within the ROI and its contour to achieve greater imperceptibility (average
PSNR is 50.03) and protective performance (clean average DSC degrades from
82.18% to 6.80%).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ResNet101 and DAE for Enhance Quality and Classification Accuracy in
  Skin Cancer Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sibasish Dhibar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skin cancer is a crucial health issue that requires timely detection for
higher survival rates. Traditional computer vision techniques face challenges
in addressing the advanced variability of skin lesion features, a gap partially
bridged by convolutional neural networks (CNNs). To overcome the existing
issues, we introduce an innovative convolutional ensemble network approach
named deep autoencoder (DAE) with ResNet101. This method utilizes
convolution-based deep neural networks for the detection of skin cancer. The
ISIC-2018 public data taken from the source is used for experimental results,
which demonstrate remarkable performance with the different in terms of
performance metrics. The methods result in 96.03% of accuracy, 95.40 % of
precision, 96.05% of recall, 0.9576 of F-measure, 0.98 of AUC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 Pages; 14 figures; 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Isotropic Gaussian Splatting for Real-Time Radiance Field Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhao Gong, Lantao Yu, Guanghui Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The 3D Gaussian splatting method has drawn a lot of attention, thanks to its
high performance in training and high quality of the rendered image. However,
it uses anisotropic Gaussian kernels to represent the scene. Although such
anisotropic kernels have advantages in representing the geometry, they lead to
difficulties in terms of computation, such as splitting or merging two kernels.
In this paper, we propose to use isotropic Gaussian kernels to avoid such
difficulties in the computation, leading to a higher performance method. The
experiments confirm that the proposed method is about {\bf 100X} faster without
losing the geometry representation accuracy. The proposed method can be applied
in a large range applications where the radiance field is needed, such as 3D
reconstruction, view synthesis, and dynamic object modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large
  Language Models with Machine Learning in tele-dermatology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios P. Panagoulias, Evridiki Tsoureli-Nikita, Maria Virvou, George A. Tsihrintzis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of Artificial Intelligence creates great promise in the field of
medical discovery, diagnostics and patient management. However, the vast
complexity of all medical domains require a more complex approach that combines
machine learning algorithms, classifiers, segmentation algorithms and, lately,
large language models. In this paper, we describe, implement and assess an
Artificial Intelligence-empowered system and methodology aimed at assisting the
diagnosis process of skin lesions and other skin conditions within the field of
dermatology that aims to holistically address the diagnostic process in this
domain. The workflow integrates large language, transformer-based vision models
and sophisticated machine learning tools. This holistic approach achieves a
nuanced interpretation of dermatological conditions that simulates and
facilitates a dermatologist's workflow. We assess our proposed methodology
through a thorough cross-model validation technique embedded in an evaluation
pipeline that utilizes publicly available medical case studies of skin
conditions and relevant images. To quantitatively score the system performance,
advanced machine learning and natural language processing tools are employed
which focus on similarity comparison and natural language inference.
Additionally, we incorporate a human expert evaluation process based on a
structured checklist to further validate our results. We implemented the
proposed methodology in a system which achieved approximate (weighted) scores
of 0.87 for both contextual understanding and diagnostic accuracy,
demonstrating the efficacy of our approach in enhancing dermatological
analysis. The proposed methodology is expected to prove useful in the
development of next-generation tele-dermatology applications, enhancing remote
consultation capabilities and access to care, especially in underserved areas.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weak Supervision with Arbitrary Single Frame for Micro- and
  Macro-expression Spotting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wang-Wang Yu, Xian-Shi Zhang, Fu-Ya Luo, Yijun Cao, Kai-Fu Yang, Hong-Mei Yan, Yong-Jie Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Frame-level micro- and macro-expression spotting methods require
time-consuming frame-by-frame observation during annotation. Meanwhile,
video-level spotting lacks sufficient information about the location and number
of expressions during training, resulting in significantly inferior performance
compared with fully-supervised spotting. To bridge this gap, we propose a
point-level weakly-supervised expression spotting (PWES) framework, where each
expression requires to be annotated with only one random frame (i.e., a point).
To mitigate the issue of sparse label distribution, the prevailing solution is
pseudo-label mining, which, however, introduces new problems: localizing
contextual background snippets results in inaccurate boundaries and discarding
foreground snippets leads to fragmentary predictions. Therefore, we design the
strategies of multi-refined pseudo label generation (MPLG) and
distribution-guided feature contrastive learning (DFCL) to address these
problems. Specifically, MPLG generates more reliable pseudo labels by merging
class-specific probabilities, attention scores, fused features, and point-level
labels. DFCL is utilized to enhance feature similarity for the same categories
and feature variability for different categories while capturing global
representations across the entire datasets. Extensive experiments on the
CAS(ME)^2, CAS(ME)^3, and SAMM-LV datasets demonstrate PWES achieves promising
performance comparable to that of recent fully-supervised methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RG-CAT: Detection Pipeline and Catalogue of Radio Galaxies in the EMU
  Pilot <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhel Gupta, Ray P. Norris, Zeeshan Hayder, Minh Huynh, Lars Petersson, X. Rosalind Wang, Andrew M. Hopkins, Heinz Andernach, Yjan Gordon, Simone Riggi, Miranda Yew, Evan J. Crawford, Bärbel Koribalski, Miroslav D. Filipović, Anna D. Kapinśka, Stanislav Shabala, Tessa Vernstrom, Joshua R. Marvil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present source detection and catalogue construction pipelines to build the
first catalogue of radio galaxies from the 270 $\rm deg^2$ pilot survey of the
Evolutionary Map of the Universe (EMU-PS) conducted with the Australian Square
Kilometre Array Pathfinder (ASKAP) telescope. The detection pipeline uses
Gal-DINO computer-vision networks (Gupta et al., 2024) to predict the
categories of radio morphology and bounding boxes for radio sources, as well as
their potential infrared host positions. The Gal-DINO network is trained and
evaluated on approximately 5,000 visually inspected radio galaxies and their
infrared hosts, encompassing both compact and extended radio morphologies. We
find that the Intersection over Union (IoU) for the predicted and ground truth
bounding boxes is larger than 0.5 for 99% of the radio sources, and 98% of
predicted host positions are within $3^{\prime \prime}$ of the ground truth
infrared host in the evaluation set. The catalogue construction pipeline uses
the predictions of the trained network on the radio and infrared image cutouts
based on the catalogue of radio components identified using the Selavy source
finder algorithm. Confidence scores of the predictions are then used to
prioritize Selavy components with higher scores and incorporate them first into
the catalogue. This results in identifications for a total of 211,625 radio
sources, with 201,211 classified as compact and unresolved. The remaining
10,414 are categorized as extended radio morphologies, including 582 FR-I,
5,602 FR-II, 1,494 FR-x (uncertain whether FR-I or FR-II), 2,375 R (single-peak
resolved) radio galaxies, and 361 with peculiar and other rare morphologies. We
cross-match the radio sources in the catalogue with the infrared and optical
catalogues, finding infrared cross-matches for 73% and photometric redshifts
for 36% of the radio galaxies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in PASA. The paper has 22 pages, 12 figures
  and 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SoftPatch: Unsupervised Anomaly Detection with Noisy Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Jiang, Ying Chen, Qiang Nie, Yong Liu, Jianlin Liu, Bin-Bin Gao, Jun Liu, Chengjie Wang, Feng Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although mainstream unsupervised anomaly detection (AD) algorithms perform
well in academic datasets, their performance is limited in practical
application due to the ideal experimental setting of clean training data.
Training with noisy data is an inevitable problem in real-world anomaly
detection but is seldom discussed. This paper considers label-level noise in
image sensory anomaly detection for the first time. To solve this problem, we
proposed a memory-based unsupervised AD method, SoftPatch, which efficiently
denoises the data at the patch level. Noise discriminators are utilized to
generate outlier scores for patch-level noise elimination before coreset
construction. The scores are then stored in the memory bank to soften the
anomaly detection boundary. Compared with existing methods, SoftPatch maintains
a strong modeling ability of normal data and alleviates the overconfidence
problem in coreset. Comprehensive experiments in various noise scenes
demonstrate that SoftPatch outperforms the state-of-the-art AD methods on the
MVTecAD and BTAD benchmarks and is comparable to those methods under the
setting without noise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36th Conference on Neural Information Processing Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Multi-class Anomaly Detection: Exploring Class-aware Unified
  Model against Inter-class Interference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Jiang, Ying Chen, Qiang Nie, Jianlin Liu, Yong Liu, Chengjie Wang, Feng Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of high usability in single-class anomaly detection models,
recent academic research has become concerned about the more complex
multi-class anomaly detection. Although several papers have designed unified
models for this task, they often overlook the utility of class labels, a potent
tool for mitigating inter-class interference. To address this issue, we
introduce a Multi-class Implicit Neural representation Transformer for unified
Anomaly Detection (MINT-AD), which leverages the fine-grained category
information in the training stage. By learning the multi-class distributions,
the model generates class-aware query embeddings for the transformer decoder,
mitigating inter-class interference within the reconstruction model. Utilizing
such an implicit neural representation network, MINT-AD can project category
and position information into a feature embedding space, further supervised by
classification and prior probability loss functions. Experimental results on
multiple datasets demonstrate that MINT-AD outperforms existing unified
training models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Audio-Visual Segmentation with Modality Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Swapnil Bhosale, Haosen Yang, Diptesh Kanojia, Jiangkang Deng, Xiatian Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-Visual Segmentation (AVS) aims to identify, at the pixel level, the
object in a visual scene that produces a given sound. Current AVS methods rely
on costly fine-grained annotations of mask-audio pairs, making them impractical
for scalability. To address this, we introduce unsupervised AVS, eliminating
the need for such expensive annotation. To tackle this more challenging
problem, we propose an unsupervised learning method, named Modality
Correspondence Alignment (MoCA), which seamlessly integrates off-the-shelf
foundation models like DINO, SAM, and ImageBind. This approach leverages their
knowledge complementarity and optimizes their joint usage for multi-modality
association. Initially, we estimate positive and negative image pairs in the
feature space. For pixel-level association, we introduce an audio-visual
adapter and a novel pixel matching aggregation strategy within the image-level
contrastive learning framework. This allows for a flexible connection between
object appearance and audio signal at the pixel level, with tolerance to
imaging variations such as translation and rotation. Extensive experiments on
the AVSBench (single and multi-object splits) and AVSS datasets demonstrate
that our MoCA outperforms strongly designed baseline methods and approaches
supervised counterparts, particularly in complex scenarios with multiple
auditory objects. Notably when comparing mIoU, MoCA achieves a substantial
improvement over baselines in both the AVSBench (S4: +17.24%; MS3: +67.64%) and
AVSS (+19.23%) audio-visual segmentation challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Debiasing surgeon: fantastic weights and how to find them 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rémi Nahon, Ivan Luiz De Moura Matos, Van-Tam Nguyen, Enzo Tartaglione
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays an ever-growing concerning phenomenon, the emergence of algorithmic
biases that can lead to unfair models, emerges. Several debiasing approaches
have been proposed in the realm of deep learning, employing more or less
sophisticated approaches to discourage these models from massively employing
these biases. However, a question emerges: is this extra complexity really
necessary? Is a vanilla-trained model already embodying some ``unbiased
sub-networks'' that can be used in isolation and propose a solution without
relying on the algorithmic biases? In this work, we show that such a
sub-network typically exists, and can be extracted from a vanilla-trained model
without requiring additional training. We further validate that such specific
architecture is incapable of learning a specific bias, suggesting that there
are possible architectural countermeasures to the problem of biases in deep
neural networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unleashing Unlabeled Data: A Paradigm for Cross-View Geo-Localization <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guopeng Li, Ming Qian, Gui-Song Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the effective utilization of unlabeled data for
large-area cross-view geo-localization (CVGL), encompassing both unsupervised
and semi-supervised settings. Common approaches to CVGL rely on
ground-satellite image pairs and employ label-driven supervised training.
However, the cost of collecting precise cross-view image pairs hinders the
deployment of CVGL in real-life scenarios. Without the pairs, CVGL will be more
challenging to handle the significant imaging and spatial gaps between ground
and satellite images. To this end, we propose an unsupervised framework
including a cross-view projection to guide the model for retrieving initial
pseudo-labels and a fast re-ranking mechanism to refine the pseudo-labels by
leveraging the fact that ``the perfectly paired ground-satellite image is
located in a unique and identical scene". The framework exhibits competitive
performance compared with supervised works on three open-source benchmarks. Our
code and models will be released on https://github.com/liguopeng0923/UCVGL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PECI-Net: Bolus segmentation from video fluoroscopic swallowing study
  images using preprocessing ensemble and cascaded inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dougho Park, Younghun Kim, Harim Kang, Junmyeoung Lee, Jinyoung Choi, Taeyeon Kim, Sangeok Lee, Seokil Son, Minsol Kim, Injung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bolus segmentation is crucial for the automated detection of swallowing
disorders in videofluoroscopic swallowing studies (VFSS). However, it is
difficult for the model to accurately segment a bolus region in a VFSS image
because VFSS images are translucent, have low contrast and unclear region
boundaries, and lack color information. To overcome these challenges, we
propose PECI-Net, a network architecture for VFSS image analysis that combines
two novel techniques: the preprocessing ensemble network (PEN) and the cascaded
inference network (CIN). PEN enhances the sharpness and contrast of the VFSS
image by combining multiple preprocessing algorithms in a learnable way. CIN
reduces ambiguity in bolus segmentation by using context from other regions
through cascaded inference. Moreover, CIN prevents undesirable side effects
from unreliably segmented regions by referring to the context in an asymmetric
way. In experiments, PECI-Net exhibited higher performance than four recently
developed baseline models, outperforming TernausNet, the best among the
baseline models, by 4.54\% and the widely used UNet by 10.83\%. The results of
the ablation studies confirm that CIN and PEN are effective in improving bolus
segmentation performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained
  StyleGAN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jongwoo Choi, Kwanggyoon Seo, Amirsaman Ashtari, Junyong Noh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method that can generate cinemagraphs automatically from a still
landscape image using a pre-trained StyleGAN. Inspired by the success of recent
unconditional video generation, we leverage a powerful pre-trained image
generator to synthesize high-quality cinemagraphs. Unlike previous approaches
that mainly utilize the latent space of a pre-trained StyleGAN, our approach
utilizes its deep feature space for both GAN inversion and cinemagraph
generation. Specifically, we propose multi-scale deep feature warping (MSDFW),
which warps the intermediate features of a pre-trained StyleGAN at different
resolutions. By using MSDFW, the generated cinemagraphs are of high resolution
and exhibit plausible looping animation. We demonstrate the superiority of our
method through user studies and quantitative comparisons with state-of-the-art
cinemagraph generation methods and a video generation method that uses a
pre-trained StyleGAN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://jeolpyeoni.github.io/stylecinegan_project/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling Typographic Deceptions: Insights of the Typographic
  Vulnerability in Large Vision-Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19150v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19150v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Cheng, Erjia Xiao, Jindong Gu, Le Yang, Jinhao Duan, Jize Zhang, Jiahang Cao, Kaidi Xu, Renjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) rely on vision encoders and Large
Language Models (LLMs) to exhibit remarkable capabilities on various
multi-modal tasks in the joint space of vision and language. However, the
Typographic Attack, which disrupts vision-language models (VLMs) such as
Contrastive Language-Image Pretraining (CLIP), has also been expected to be a
security threat to LVLMs. Firstly, we verify typographic attacks on current
well-known commercial and open-source LVLMs and uncover the widespread
existence of this threat. Secondly, to better assess this vulnerability, we
propose the most comprehensive and largest-scale Typographic Dataset to date.
The Typographic Dataset not only considers the evaluation of typographic
attacks under various multi-modal tasks but also evaluates the effects of
typographic attacks, influenced by texts generated with diverse factors. Based
on the evaluation results, we investigate the causes why typographic attacks
may impact VLMs and LVLMs, leading to three highly insightful discoveries. By
the examination of our discoveries and experimental validation in the
Typographic Dataset, we reduce the performance degradation from $42.07\%$ to
$13.90\%$ when LVLMs confront typographic attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The All-Seeing Project V2: Towards General Relation Comprehension of the
  Open World 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19474v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19474v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, Yu Qiao, Jifeng Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the All-Seeing Project V2: a new model and dataset designed for
understanding object relations in images. Specifically, we propose the
All-Seeing Model V2 (ASMv2) that integrates the formulation of text generation,
object localization, and relation comprehension into a relation conversation
(ReC) task. Leveraging this unified task, our model excels not only in
perceiving and recognizing all objects within the image but also in grasping
the intricate relation graph between them, diminishing the relation
hallucination often encountered by Multi-modal Large Language Models (MLLMs).
To facilitate training and evaluation of MLLMs in relation understanding, we
created the first high-quality ReC dataset ({AS-V2) which is aligned with the
format of standard instruction tuning data. In addition, we design a new
benchmark, termed Circular-based Relation Probing Evaluation (CRPE) for
comprehensively evaluating the relation comprehension capabilities of MLLMs.
Notably, our ASMv2 achieves an overall accuracy of 52.04 on this relation-aware
benchmark, surpassing the 43.14 of LLaVA-1.5 by a large margin. We hope that
our work can inspire more future research and contribute to the evolution
towards artificial general intelligence. Our project is released at
https://github.com/OpenGVLab/all-seeing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11085v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11085v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixian Ma, Weikai Huang, Jieyu Zhang, Tanmay Gupta, Ranjay Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world multi-modal problems are rarely solved by a single machine
learning model, and often require multi-step computational plans that involve
stitching several models. Tool-augmented LLMs hold tremendous promise for
automating the generation of such computational plans. However, the lack of
standardized benchmarks for evaluating LLMs as planners for multi-step
multi-modal tasks has prevented a systematic study of planner design decisions.
Should LLMs generate a full plan in a single shot or step-by-step? Should they
invoke tools directly with Python code or through structured data formats like
JSON? Does feedback improve planning? To answer these questions and more, we
introduce m&m's: a benchmark containing 4K+ multi-step multi-modal tasks
involving 33 tools that include multi-modal models, (free) public APIs, and
image processing modules. For each of these task queries, we provide
automatically generated plans using this realistic toolset. We further provide
a high-quality subset of 1,565 task plans that are human-verified and correctly
executable. With m&m's, we evaluate 6 popular LLMs with 2 planning strategies
(multi-step vs. step-by-step planning), 2 plan formats (JSON vs. code), and 3
types of feedback (parsing/verification/execution). Finally, we summarize
takeaways from our extensive experiments. Our dataset and code are available on
HuggingFace (https://huggingface.co/datasets/zixianma/mnms) and Github
(https://github.com/RAIVNLab/mnms).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedCycle: Unpaired Medical Report Generation via Cycle-Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13444v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13444v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elad Hirsch, Gefen Dawidowicz, Ayellet Tal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating medical reports for X-ray images presents a significant challenge,
particularly in unpaired scenarios where access to paired image-report data for
training is unavailable. Previous works have typically learned a joint
embedding space for images and reports, necessitating a specific labeling
schema for both. We introduce an innovative approach that eliminates the need
for consistent labeling schemas, thereby enhancing data accessibility and
enabling the use of incompatible datasets. This approach is based on
cycle-consistent mapping functions that transform image embeddings into report
embeddings, coupled with report auto-encoding for medical report generation.
Our model and objectives consider intricate local details and the overarching
semantic context within images and reports. This approach facilitates the
learning of effective mapping functions, resulting in the generation of
coherent reports. It outperforms state-of-the-art results in unpaired chest
X-ray report generation, demonstrating improvements in both language and
clinical metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Geospatial Approach to Predicting Desert Locust Breeding Grounds in
  Africa 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06860v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06860v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ibrahim Salihu Yusuf, Mukhtar Opeyemi Yusuf, Kobby Panford-Quainoo, Arnu Pretorius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Desert locust swarms present a major threat to agriculture and food security.
Addressing this challenge, our study develops an operationally-ready model for
predicting locust breeding grounds, which has the potential to enhance early
warning systems and targeted control measures. We curated a dataset from the
United Nations Food and Agriculture Organization's (UN-FAO) locust observation
records and analyzed it using two types of spatio-temporal input features:
remotely-sensed environmental and climate data as well as multi-spectral earth
observation images. Our approach employed custom deep learning models
(three-dimensional and LSTM-based recurrent convolutional networks), along with
the geospatial foundational model Prithvi recently released by Jakubik et al.,
2023. These models notably outperformed existing baselines, with the
Prithvi-based model, fine-tuned on multi-spectral images from NASA's Harmonized
Landsat and Sentinel-2 (HLS) dataset, achieving the highest accuracy, F1 and
ROC-AUC scores (83.03%, 81.53% and 87.69%, respectively). A significant finding
from our research is that multi-spectral earth observation images alone are
sufficient for effective locust breeding ground prediction without the need to
explicitly incorporate climatic or environmental features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Flexible, Scalable, and Adaptive Multi-Modal Conditioned Face
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingjing Ren, Cheng Xu, Haoyu Chen, Xinran Qin, Lei Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in multi-modal conditioned face synthesis has enabled the
creation of visually striking and accurately aligned facial images. Yet,
current methods still face issues with scalability, limited flexibility, and a
one-size-fits-all approach to control strength, not accounting for the
differing levels of conditional entropy, a measure of unpredictability in data
given some condition, across modalities. To address these challenges, we
introduce a novel uni-modal training approach with modal surrogates, coupled
with an entropy-aware modal-adaptive modulation, to support flexible, scalable,
and scalable multi-modal conditioned face synthesis network. Our uni-modal
training with modal surrogate that only leverage uni-modal data, use modal
surrogate to decorate condition with modal-specific characteristic and serve as
linker for inter-modal collaboration , fully learns each modality control in
face synthesis process as well as inter-modal collaboration. The entropy-aware
modal-adaptive modulation finely adjust diffusion noise according to
modal-specific characteristics and given conditions, enabling well-informed
step along denoising trajectory and ultimately leading to synthesis results of
high fidelity and quality. Our framework improves multi-modal face synthesis
under various conditions, surpassing current methods in image quality and
fidelity, as demonstrated by our thorough experimental results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedMamba: Vision Mamba for Medical Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03849v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03849v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubiao Yue, Zhenzhang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image classification is a very fundamental and crucial task in the
field of computer vision. These years, CNN-based and Transformer-based models
have been widely used to classify various medical images. Unfortunately, The
limitation of CNNs in long-range modeling capabilities prevents them from
effectively extracting features in medical images, while Transformers are
hampered by their quadratic computational complexity. Recent research has shown
that the state space model (SSM) represented by Mamba can efficiently model
long-range interactions while maintaining linear computational complexity.
Inspired by this, we propose Vision Mamba for medical image classification
(MedMamba). More specifically, we introduce a novel Conv-SSM module. Conv-SSM
combines the local feature extraction ability of convolutional layers with the
ability of SSM to capture long-range dependency, thereby modeling medical
images with different modalities. To demonstrate the potential of MedMamba, we
conducted extensive experiments using 14 publicly available medical datasets
with different imaging techniques and two private datasets built by ourselves.
Extensive experimental results demonstrate that the proposed MedMamba performs
well in detecting lesions in various medical images. To the best of our
knowledge, this is the first Vision Mamba tailored for medical image
classification. The purpose of this work is to establish a new baseline for
medical image classification tasks and provide valuable insights for the future
development of more efficient and effective SSM-based artificial intelligence
algorithms and application systems in the medical. Source code has been
available at https://github.com/YubiaoYue/MedMamba.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instance-aware Exploration-Verification-Exploitation for Instance
  ImageGoal Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17587v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17587v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Lei, Min Wang, Wengang Zhou, Li Li, Houqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a new embodied vision task, Instance ImageGoal Navigation (IIN) aims to
navigate to a specified object depicted by a goal image in an unexplored
environment.
  The main challenge of this task lies in identifying the target object from
different viewpoints while rejecting similar distractors.
  Existing ImageGoal Navigation methods usually adopt the simple
Exploration-Exploitation framework and ignore the identification of specific
instance during navigation.
  In this work, we propose to imitate the human behaviour of ``getting closer
to confirm" when distinguishing objects from a distance.
  Specifically, we design a new modular navigation framework named
Instance-aware Exploration-Verification-Exploitation (IEVE) for instance-level
image goal navigation.
  Our method allows for active switching among the exploration, verification,
and exploitation actions, thereby facilitating the agent in making reasonable
decisions under different situations.
  On the challenging HabitatMatterport 3D semantic (HM3D-SEM) dataset, our
method surpasses previous state-of-the-art work, with a classical segmentation
model (0.684 vs. 0.561 success) or a robust model (0.702 vs. 0.561 success).
Our code will be made publicly available at https://github.com/XiaohanLei/IEVE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalizing deep learning models for medical image classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12167v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12167v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matta Sarah, Lamard Mathieu, Zhang Philippe, Alexandre Le Guilcher, Laurent Borderie, Béatrice Cochener, Gwenolé Quellec
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous Deep Learning (DL) models have been developed for a large spectrum
of medical image analysis applications, which promises to reshape various
facets of medical practice. Despite early advances in DL model validation and
implementation, which encourage healthcare institutions to adopt them, some
fundamental questions remain: are the DL models capable of generalizing? What
causes a drop in DL model performances? How to overcome the DL model
performance drop? Medical data are dynamic and prone to domain shift, due to
multiple factors such as updates to medical equipment, new imaging workflow,
and shifts in patient demographics or populations can induce this drift over
time. In this paper, we review recent developments in generalization methods
for DL-based classification models. We also discuss future challenges,
including the need for improved evaluation protocols and benchmarks, and
envisioned future developments to achieve robust, generalized models for
medical image classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12966v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12966v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuyan Liu, Yuhao Dong, Yongming Rao, Jie Zhou, Jiwen Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of vision-language understanding, the proficiency of models in
interpreting and reasoning over visual content has become a cornerstone for
numerous applications. However, it is challenging for the visual encoder in
Large Vision-Language Models (LVLMs) to extract useful features tailored to
questions that aid the language model's response. Furthermore, a common
practice among existing LVLMs is to utilize lower-resolution images, which
restricts the ability for visual recognition. Our work introduces the
Chain-of-Spot (CoS) method, which we describe as Interactive Reasoning, a novel
approach that enhances feature extraction by focusing on key regions of
interest (ROI) within the image, corresponding to the posed questions or
instructions. This technique allows LVLMs to access more detailed visual
information without altering the original image resolution, thereby offering
multi-granularity image features. By integrating Chain-of-Spot with
instruct-following LLaVA-1.5 models, the process of image reasoning
consistently improves performance across a wide range of multimodal datasets
and benchmarks without bells and whistles and achieves new state-of-the-art
results. Our empirical findings demonstrate a significant improvement in LVLMs'
ability to understand and reason about visual content, paving the way for more
sophisticated visual instruction-following applications. Code and models are
available at https://github.com/dongyh20/Chain-of-Spot
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://sites.google.com/view/chain-of-spot/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Radiance Fields in Medical Imaging: Challenges and Next Steps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17797v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17797v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Wang, Shu Hu, Heng Fan, Hongtu Zhu, Xin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRF), as a pioneering technique in computer vision,
offer great potential to revolutionize medical imaging by synthesizing
three-dimensional representations from the projected two-dimensional image
data. However, they face unique challenges when applied to medical
applications. This paper presents a comprehensive examination of applications
of NeRFs in medical imaging, highlighting four imminent challenges, including
fundamental imaging principles, inner structure requirement, object boundary
definition, and color density significance. We discuss current methods on
different organs and discuss related limitations. We also review several
datasets and evaluation metrics and propose several promising directions for
future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning a Depth Covariance Function <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Dexheimer, Andrew J. Davison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose learning a depth covariance function with applications to
geometric vision tasks. Given RGB images as input, the covariance function can
be flexibly used to define priors over depth functions, predictive
distributions given observations, and methods for active point selection. We
leverage these techniques for a selection of downstream tasks: depth
completion, bundle adjustment, and monocular dense visual odometry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Project page: https://edexheim.github.io/DepthCov/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ T-MAE: Temporal Masked Autoencoders for Point Cloud Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10217v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10217v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijie Wei, Fatemeh Karimi Nejadasl, Theo Gevers, Martin R. Oswald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scarcity of annotated data in LiDAR point cloud understanding hinders
effective representation learning. Consequently, scholars have been actively
investigating efficacious self-supervised pre-training paradigms. Nevertheless,
temporal information, which is inherent in the LiDAR point cloud sequence, is
consistently disregarded. To better utilize this property, we propose an
effective pre-training strategy, namely Temporal Masked Auto-Encoders (T-MAE),
which takes as input temporally adjacent frames and learns temporal dependency.
A SiamWCA backbone, containing a Siamese encoder and a windowed cross-attention
(WCA) module, is established for the two-frame input. Considering that the
movement of an ego-vehicle alters the view of the same instance, temporal
modeling also serves as a robust and natural data augmentation, enhancing the
comprehension of target objects. SiamWCA is a powerful architecture but heavily
relies on annotated data. Our T-MAE pre-training strategy alleviates its demand
for annotated data. Comprehensive experiments demonstrate that T-MAE achieves
the best performance on both Waymo and ONCE datasets among competitive
self-supervised approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ins-HOI: Instance Aware Human-Object Interactions Recovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09641v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09641v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Zhang, Yuxiang Zhang, Hongwen Zhang, Xiao Zhou, Boyao Zhou, Ruizhi Shao, Zonghai Hu, Yebin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately modeling detailed interactions between human/hand and object is an
appealing yet challenging task. Current multi-view capture systems are only
capable of reconstructing multiple subjects into a single, unified mesh, which
fails to model the states of each instance individually during interactions. To
address this, previous methods use template-based representations to track
human/hand and object. However, the quality of the reconstructions is limited
by the descriptive capabilities of the templates so that these methods are
inherently struggle with geometry details, pressing deformations and invisible
contact surfaces. In this work, we propose an end-to-end Instance-aware
Human-Object Interactions recovery (Ins-HOI) framework by introducing an
instance-level occupancy field representation. However, the real-captured data
is presented as a holistic mesh, unable to provide instance-level supervision.
To address this, we further propose a complementary training strategy that
leverages synthetic data to introduce instance-level shape priors, enabling the
disentanglement of occupancy fields for different instances. Specifically,
synthetic data, created by randomly combining individual scans of humans/hands
and objects, guides the network to learn a coarse prior of instances.
Meanwhile, real-captured data helps in learning the overall geometry and
restricting interpenetration in contact areas. As demonstrated in experiments,
our method Ins-HOI supports instance-level reconstruction and provides
reasonable and realistic invisible contact surfaces even in cases of extremely
close interaction. To facilitate the research of this task, we collect a
large-scale, high-fidelity 3D scan dataset, including 5.2k high-quality scans
with real-world human-chair and hand-object interactions. The code and data
will be public for research purposes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://jiajunzhang16.github.io/ins-hoi/ , Code and
  Dataset Page: https://github.com/jiajunzhang16/ins-hoi</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GIVT: Generative Infinite-Vocabulary Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02116v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02116v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Tschannen, Cian Eastwood, Fabian Mentzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce generative infinite-vocabulary transformers (GIVT) which
generate vector sequences with real-valued entries, instead of discrete tokens
from a finite vocabulary. To this end, we propose two surprisingly simple
modifications to decoder-only transformers: 1) at the input, we replace the
finite-vocabulary lookup table with a linear projection of the input vectors;
and 2) at the output, we replace the logits prediction (usually mapped to a
categorical distribution) with the parameters of a multivariate Gaussian
mixture model. Inspired by the image-generation paradigm of VQ-GAN and MaskGIT,
where transformers are used to model the discrete latent sequences of a VQ-VAE,
we use GIVT to model the unquantized real-valued latent sequences of a
$\beta$-VAE. In class-conditional image generation GIVT outperforms VQ-GAN (and
improved variants thereof) as well as MaskGIT, and achieves performance
competitive with recent latent diffusion models. Finally, we obtain strong
results outside of image generation when applying GIVT to panoptic segmentation
and depth estimation with a VAE variant of the UViM framework
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2: add related NLP work, loss details. v3: Improved GMM formulation,
  added adapter module, larger models, better image generation results. Code
  and model checkpoints are available at:
  https://github.com/google-research/big_vision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Closing the Gap: Achieving Better Accuracy-Robustness Tradeoffs against
  Query-Based Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Zimmer, Sébastien Andreina, Giorgia Azzurra Marson, Ghassan Karame
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although promising, existing defenses against query-based attacks share a
common limitation: they offer increased robustness against attacks at the price
of a considerable accuracy drop on clean samples. In this work, we show how to
efficiently establish, at test-time, a solid tradeoff between robustness and
accuracy when mitigating query-based attacks. Given that these attacks
necessarily explore low-confidence regions, our insight is that activating
dedicated defenses, such as random noise defense and random image
transformations, only for low-confidence inputs is sufficient to prevent them.
Our approach is independent of training and supported by theory. We verify the
effectiveness of our approach for various existing defenses by conducting
extensive experiments on CIFAR-10, CIFAR-100, and ImageNet. Our results confirm
that our proposal can indeed enhance these defenses by providing better
tradeoffs between robustness and accuracy when compared to state-of-the-art
approaches while being completely training-free.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Proceedings of the AAAI Conference on Artificial
  Intelligence (AAAI) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Supervised Class-Agnostic Motion Prediction with Spatial and
  Temporal Consistency Regularizations <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13261v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13261v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kewei Wang, Yizheng Wu, Jun Cen, Zhiyu Pan, Xingyi Li, Zhe Wang, Zhiguo Cao, Guosheng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The perception of motion behavior in a dynamic environment holds significant
importance for autonomous driving systems, wherein class-agnostic motion
prediction methods directly predict the motion of the entire point cloud. While
most existing methods rely on fully-supervised learning, the manual labeling of
point cloud data is laborious and time-consuming. Therefore, several
annotation-efficient methods have been proposed to address this challenge.
Although effective, these methods rely on weak annotations or additional
multi-modal data like images, and the potential benefits inherent in the point
cloud sequence are still underexplored. To this end, we explore the feasibility
of self-supervised motion prediction with only unlabeled LiDAR point clouds.
Initially, we employ an optimal transport solver to establish coarse
correspondences between current and future point clouds as the coarse pseudo
motion labels. Training models directly using such coarse labels leads to
noticeable spatial and temporal prediction inconsistencies. To mitigate these
issues, we introduce three simple spatial and temporal regularization losses,
which facilitate the self-supervised training process effectively. Experimental
results demonstrate the significant superiority of our approach over the
state-of-the-art self-supervised methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ColonNeRF: High-Fidelity Neural Reconstruction of Long Colonoscopy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02015v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02015v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufei Shi, Beijia Lu, Jia-Wei Liu, Ming Li, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Colonoscopy reconstruction is pivotal for diagnosing colorectal cancer.
However, accurate long-sequence colonoscopy reconstruction faces three major
challenges: (1) dissimilarity among segments of the colon due to its meandering
and convoluted shape; (2) co-existence of simple and intricately folded
geometry structures; (3) sparse viewpoints due to constrained camera
trajectories. To tackle these challenges, we introduce a new reconstruction
framework based on neural radiance field (NeRF), named ColonNeRF, which
leverages neural rendering for novel view synthesis of long-sequence
colonoscopy. Specifically, to reconstruct the entire colon in a piecewise
manner, our ColonNeRF introduces a region division and integration module,
effectively reducing shape dissimilarity and ensuring geometric consistency in
each segment. To learn both the simple and complex geometry in a unified
framework, our ColonNeRF incorporates a multi-level fusion module that
progressively models the colon regions from easy to hard. Additionally, to
overcome the challenges from sparse views, we devise a DensiNet module for
densifying camera poses under the guidance of semantic consistency. We conduct
extensive experiments on both synthetic and real-world datasets to evaluate our
ColonNeRF. Quantitatively, ColonNeRF exhibits a 67%-85% increase in LPIPS-ALEX
scores. Qualitatively, our reconstruction visualizations show much clearer
textures and more accurate geometric details. These sufficiently demonstrate
our superior performance over the state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>for Project Page, see https://showlab.github.io/ColonNeRF/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neuromorphic Imaging and Classification with Graph Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15627v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15627v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pei Zhang, Chutian Wang, Edmund Y. Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bio-inspired neuromorphic cameras asynchronously record pixel brightness
changes and generate sparse event streams. They can capture dynamic scenes with
little motion blur and more details in extreme illumination conditions. Due to
the multidimensional address-event structure, most existing vision algorithms
cannot properly handle asynchronous event streams. While several event
representations and processing methods have been developed to address such an
issue, they are typically driven by a large number of events, leading to
substantial overheads in runtime and memory. In this paper, we propose a new
graph representation of the event data and couple it with a Graph Transformer
to perform accurate neuromorphic classification. Extensive experiments show
that our approach leads to better results and excels at the challenging
realistic situations where only a small number of events and limited
computational resources are available, paving the way for neuromorphic
applications embedded into mobile facilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures, and 7 tables. Accepted by Elsevier
  Neurocomputing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands
  from a Single Image <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minje Kim, Tae-Kyun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating personalized hand avatars is important to offer a realistic
experience to users on AR / VR platforms. While most prior studies focused on
reconstructing 3D hand shapes, some recent work has tackled the reconstruction
of hand textures on top of shapes. However, these methods are often limited to
capturing pixels on the visible side of a hand, requiring diverse views of the
hand in a video or multiple images as input. In this paper, we propose a novel
method, BiTT(Bi-directional Texture reconstruction of Two hands), which is the
first end-to-end trainable method for relightable, pose-free texture
reconstruction of two interacting hands taking only a single RGB image, by
three novel components: 1) bi-directional (left $\leftrightarrow$ right)
texture reconstruction using the texture symmetry of left / right hands, 2)
utilizing a texture parametric model for hand texture recovery, and 3) the
overall coarse-to-fine stage pipeline for reconstructing personalized texture
of two interacting hands. BiTT first estimates the scene light condition and
albedo image from an input image, then reconstructs the texture of both hands
through the texture parametric model and bi-directional texture reconstructor.
In experiments using InterHand2.6M and RGB2Hands datasets, our method
significantly outperforms state-of-the-art hand texture reconstruction methods
quantitatively and qualitatively. The code is available at
https://github.com/yunminjin2/BiTT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An explainable three dimension framework to uncover learning patterns: A
  unified look in variable sulci recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00903v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00903v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michail Mamalakis, Heloise de Vareilles, Atheer AI-Manea, Samantha C. Mitchell, Ingrid Arartz, Lynn Egeland Morch-Johnsen, Jane Garrison, Jon Simons, Pietro Lio, John Suckling, Graham Murray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable AI is crucial in medical imaging. In the challenging field of
neuroscience, visual topics present a high level of complexity, particularly
within three-dimensional space. The application of neuroscience, which involves
identifying brain sulcal features from MRI, faces significant hurdles due to
varying annotation protocols among experts and the intricate three-dimension
functionality of the brain. Consequently, traditional explainability approaches
fall short in effectively validating and evaluating these networks. To address
this, we first present a mathematical formulation delineating various
categories of explanation needs across diverse computer vision tasks,
categorized into self-explanatory, semi-explanatory, non-explanatory, and
new-pattern learning applications based on the reliability of the validation
protocol. With respect to this mathematical formulation, we propose a 3D
explainability framework aimed at validating the outputs of deep learning
networks in detecting the paracingulate sulcus an essential brain anatomical
feature. The framework integrates local 3D explanations, global explanations
through dimensionality reduction, concatenated global explanations, and
statistical shape features, unveiling new insights into pattern learning. We
trained and tested two advanced 3D deep learning networks on the challenging
TOP-OSLO dataset, significantly improving sulcus detection accuracy,
particularly on the left hemisphere. During evaluation with diverse annotation
protocols for this dataset, we highlighted the crucial role of an unbiased
annotation process in achieving precise predictions and effective pattern
learning within our proposed 3D framework. The proposed framework not only
annotates the variable sulcus but also uncovers hidden AI knowledge, promising
to advance our understanding of brain anatomy and function.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Generative Approach for Wikipedia-Scale Visual Entity Recognition <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02041v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02041v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathilde Caron, Ahmet Iscen, Alireza Fathi, Cordelia Schmid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address web-scale visual entity recognition, specifically
the task of mapping a given query image to one of the 6 million existing
entities in Wikipedia. One way of approaching a problem of such scale is using
dual-encoder models (eg CLIP), where all the entity names and query images are
embedded into a unified space, paving the way for an approximate k-NN search.
Alternatively, it is also possible to re-purpose a captioning model to directly
generate the entity names for a given image. In contrast, we introduce a novel
Generative Entity Recognition (GER) framework, which given an input image
learns to auto-regressively decode a semantic and discriminative ``code''
identifying the target entity. Our experiments demonstrate the efficacy of this
GER paradigm, showcasing state-of-the-art performance on the challenging OVEN
benchmark. GER surpasses strong captioning, dual-encoder, visual matching and
hierarchical classification baselines, affirming its advantage in tackling the
complexities of web-scale recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analyzing Local Representations of Self-supervised Vision Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ani Vanyan, Alvard Barseghyan, Hakob Tamazyan, Vahan Huroyan, Hrant Khachatrian, Martin Danelljan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a comparative analysis of various self-supervised
Vision Transformers (ViTs), focusing on their local representative power.
Inspired by large language models, we examine the abilities of ViTs to perform
various computer vision tasks with little to no fine-tuning. We design
evaluation framework to analyze the quality of local, i.e.\ patch-level,
representations in the context of few-shot semantic segmentation, instance
identification, object retrieval and tracking. We discover that contrastive
learning based methods like DINO produce more universal patch representations
that can be immediately applied for downstream tasks with no parameter tuning,
compared to masked image modeling. The embeddings learned using the latter
approach, e.g. in masked autoencoders, have high variance features that harm
distance-based algorithms, such as k-NN, and do not contain useful information
for most downstream tasks. Furthermore, we demonstrate that removing these
high-variance features enhances k-NN for MAE, as well as for its recent
extension Scale-MAE. Finally, we find an object instance retrieval setting
where DINOv2, a model pretrained on two orders of magnitude more data, falls
short of its less compute intensive counterpart DINO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Few-Shot Class-Incremental Learning via Ensemble Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07208v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07208v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingli Zhu, Zihao Zhu, Sihong Chen, Chen Chen, Baoyuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot class-incremental learning (FSCIL) aims to continually fit new
classes with limited training data, while maintaining the performance of
previously learned classes. The main challenges are overfitting the rare new
training samples and forgetting old classes. While catastrophic forgetting has
been extensively studied, the overfitting problem has attracted less attention
in FSCIL. To tackle overfitting challenge, we design a new ensemble model
framework cooperated with data augmentation to boost generalization. In this
way, the enhanced model works as a library storing abundant features to
guarantee fast adaptation to downstream tasks. Specifically, the multi-input
multi-output ensemble structure is applied with a spatial-aware data
augmentation strategy, aiming at diversifying the feature extractor and
alleviating overfitting in incremental sessions. Moreover, self-supervised
learning is also integrated to further improve the model generalization.
Comprehensive experimental results show that the proposed method can indeed
mitigate the overfitting problem in FSCIL, and outperform the state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Separate and Conquer: Decoupling Co-occurrence via Decomposition and
  Representation for Weakly Supervised Semantic Segmentation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18467v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18467v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Yang, Kexue Fu, Minghong Duan, Linhao Qu, Shuo Wang, Zhijian Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly supervised semantic segmentation (WSSS) with image-level labels aims
to achieve segmentation tasks without dense annotations. However, attributed to
the frequent coupling of co-occurring objects and the limited supervision from
image-level labels, the challenging co-occurrence problem is widely present and
leads to false activation of objects in WSSS. In this work, we devise a
'Separate and Conquer' scheme SeCo to tackle this issue from dimensions of
image space and feature space. In the image space, we propose to 'separate' the
co-occurring objects with image decomposition by subdividing images into
patches. Importantly, we assign each patch a category tag from Class Activation
Maps (CAMs), which spatially helps remove the co-context bias and guide the
subsequent representation. In the feature space, we propose to 'conquer' the
false activation by enhancing semantic representation with multi-granularity
knowledge contrast. To this end, a dual-teacher-single-student architecture is
designed and tag-guided contrast is conducted, which guarantee the correctness
of knowledge and further facilitate the discrepancy among co-contexts. We
streamline the multi-staged WSSS pipeline end-to-end and tackle this issue
without external supervision. Extensive experiments are conducted, validating
the efficiency of our method and the superiority over previous single-staged
and even multi-staged competitors on PASCAL VOC and MS COCO. Code is available
at https://github.com/zwyang6/SeCo.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visually-Aware Context Modeling for News Image Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.08325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.08325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingyu Qu, Tinne Tuytelaars, Marie-Francine Moens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  News Image Captioning aims to create captions from news articles and images,
emphasizing the connection between textual context and visual elements.
Recognizing the significance of human faces in news images and the face-name
co-occurrence pattern in existing datasets, we propose a face-naming module for
learning better name embeddings. Apart from names, which can be directly linked
to an image area (faces), news image captions mostly contain context
information that can only be found in the article. We design a retrieval
strategy using CLIP to retrieve sentences that are semantically close to the
image, mimicking human thought process of linking articles to images.
Furthermore, to tackle the problem of the imbalanced proportion of article
context and image context in captions, we introduce a simple yet effective
method Contrasting with Language Model backbone (CoLaM) to the training
pipeline. We conduct extensive experiments to demonstrate the efficacy of our
framework. We out-perform the previous state-of-the-art (without external data)
by 7.97/5.80 CIDEr scores on GoodNews/NYTimes800k. Our code is available at
https://github.com/tingyu215/VACNIC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Active Contour Model Driven By the Hybrid Signed Pressure Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07570v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07570v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the influence of imaging equipment and complex imaging environments,
most images in daily life have features of intensity inhomogeneity and noise.
Therefore, many scholars have designed many image segmentation algorithms to
address these issues. Among them, the active contour model is one of the most
effective image segmentation algorithms.This paper proposes an active contour
model driven by the hybrid signed pressure function that combines global and
local information construction. Firstly, a new global region-based signed
pressure function is introduced by combining the average intensity of the inner
and outer regions of the curve with the median intensity of the inner region of
the evolution curve. Then, the paper uses the energy differences between the
inner and outer regions of the curve in the local region to design the signed
pressure function of the local term. Combine the two SPF function to obtain a
new signed pressure function and get the evolution equation of the new model.
Finally, experiments and numerical analysis show that the model has excellent
segmentation performance for both intensity inhomogeneous images and noisy
images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Direct2.5: Diverse Text-to-3D Generation via Multi-view 2.5D Diffusion <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15980v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15980v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanxun Lu, Jingyang Zhang, Shiwei Li, Tian Fang, David McKinnon, Yanghai Tsin, Long Quan, Xun Cao, Yao Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in generative AI have unveiled significant potential for the
creation of 3D content. However, current methods either apply a pre-trained 2D
diffusion model with the time-consuming score distillation sampling (SDS), or a
direct 3D diffusion model trained on limited 3D data losing generation
diversity. In this work, we approach the problem by employing a multi-view 2.5D
diffusion fine-tuned from a pre-trained 2D diffusion model. The multi-view 2.5D
diffusion directly models the structural distribution of 3D data, while still
maintaining the strong generalization ability of the original 2D diffusion
model, filling the gap between 2D diffusion-based and direct 3D diffusion-based
methods for 3D content generation. During inference, multi-view normal maps are
generated using the 2.5D diffusion, and a novel differentiable rasterization
scheme is introduced to fuse the almost consistent multi-view normal maps into
a consistent 3D model. We further design a normal-conditioned multi-view image
generation module for fast appearance generation given the 3D geometry. Our
method is a one-pass diffusion process and does not require any SDS
optimization as post-processing. We demonstrate through extensive experiments
that, our direct 2.5D generation with the specially-designed fusion scheme can
achieve diverse, mode-seeking-free, and high-fidelity 3D content generation in
only 10 seconds. Project page: https://nju-3dv.github.io/projects/direct25.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024 camera ready, including more evaluations and discussions.
  Project webpage: https://nju-3dv.github.io/projects/direct25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Alleviating Exposure Bias in Diffusion Models through Sampling with
  Shifted Time Steps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15583v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15583v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxiao Li, Tingyu Qu, Ruicong Yao, Wei Sun, Marie-Francine Moens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Probabilistic Models (DPM) have shown remarkable efficacy in the
synthesis of high-quality images. However, their inference process
characteristically requires numerous, potentially hundreds, of iterative steps,
which could exaggerate the problem of exposure bias due to the training and
inference discrepancy. Previous work has attempted to mitigate this issue by
perturbing inputs during training, which consequently mandates the retraining
of the DPM. In this work, we conduct a systematic study of exposure bias in DPM
and, intriguingly, we find that the exposure bias could be alleviated with a
novel sampling method that we propose, without retraining the model. We
empirically and theoretically show that, during inference, for each backward
time step $t$ and corresponding state $\hat{x}_t$, there might exist another
time step $t_s$ which exhibits superior coupling with $\hat{x}_t$. Based on
this finding, we introduce a sampling method named Time-Shift Sampler. Our
framework can be seamlessly integrated to existing sampling algorithms, such as
DDPM, DDIM and other high-order solvers, inducing merely minimal additional
computations. Experimental results show our method brings significant and
consistent improvements in FID scores on different datasets and sampling
methods. For example, integrating Time-Shift Sampler to F-PNDM yields a
FID=3.88, achieving 44.49\% improvements as compared to F-PNDM, on CIFAR-10
with 10 sampling steps, which is more performant than the vanilla DDIM with 100
sampling steps. Our code is available at https://github.com/Mingxiao-Li/TS-DPM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at International Conference on Learning Representations
  (ICLR2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ To use or not to use proprietary street view images in (health and
  place) research? That is the question 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11504v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11504v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Helbich, Matthew Danish, SM Labib, Britta Ricker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer vision-based analysis of street view imagery has transformative
impacts on environmental assessments. Interactive web services, particularly
Google Street View, play an ever-important role in making imagery data
ubiquitous. Despite the technical ease of harnessing millions of Google Street
View images, this article questions the current practices in using this
proprietary data source from a European viewpoint. Our concern lies with
Google's terms of service, which restrict bulk image downloads and the
generation of street view image-based indices. To reconcile the challenge of
advancing society through groundbreaking research while maintaining data
license agreements and legal integrity, we believe it is crucial to 1) include
an author's statement on using proprietary street view data and the directives
it entails, 2) negotiate academic-specific license to democratize Google Street
View data access, and 3) adhere to open data principles and utilize open image
sources for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Video Domain Adaptation with Masked Pre-Training and
  Collaborative Self-Training <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02914v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02914v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arun Reddy, William Paul, Corban Rivera, Ketul Shah, Celso M. de Melo, Rama Chellappa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we tackle the problem of unsupervised domain adaptation (UDA)
for video action recognition. Our approach, which we call UNITE, uses an image
teacher model to adapt a video student model to the target domain. UNITE first
employs self-supervised pre-training to promote discriminative feature learning
on target domain videos using a teacher-guided masked distillation objective.
We then perform self-training on masked target data, using the video student
model and image teacher model together to generate improved pseudolabels for
unlabeled target videos. Our self-training process successfully leverages the
strengths of both models to achieve strong transfer performance across domains.
We evaluate our approach on multiple video domain adaptation benchmarks and
observe significant improvements upon previously reported results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024. 13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI-KD: Adversarial learning and Implicit regularization for
  self-Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10938v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10938v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyungmin Kim, Sungho Suh, Sunghyun Baek, Daehwan Kim, Daun Jeong, Hansang Cho, Junmo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel adversarial penalized self-knowledge distillation method,
named adversarial learning and implicit regularization for self-knowledge
distillation (AI-KD), which regularizes the training procedure by adversarial
learning and implicit distillations. Our model not only distills the
deterministic and progressive knowledge which are from the pre-trained and
previous epoch predictive probabilities but also transfers the knowledge of the
deterministic predictive distributions using adversarial learning. The
motivation is that the self-knowledge distillation methods regularize the
predictive probabilities with soft targets, but the exact distributions may be
hard to predict. Our method deploys a discriminator to distinguish the
distributions between the pre-trained and student models while the student
model is trained to fool the discriminator in the trained procedure. Thus, the
student model not only can learn the pre-trained model's predictive
probabilities but also align the distributions between the pre-trained and
student models. We demonstrate the effectiveness of the proposed method with
network architectures on multiple datasets and show the proposed method
achieves better performance than state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to KBS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Consistency Enhancement-Based Deep Multiview Clustering via Contrastive
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12648v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12648v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yang, Hua Mao, Wai Lok Woo, Jie Chen, Xi Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiview clustering (MVC) segregates data samples into meaningful clusters
by synthesizing information across multiple views. Moreover, deep
learning-based methods have demonstrated their strong feature learning
capabilities in MVC scenarios. However, effectively generalizing feature
representations while maintaining consistency is still an intractable problem.
In addition, most existing deep clustering methods based on contrastive
learning overlook the consistency of the clustering representations during the
clustering process. In this paper, we show how the above problems can be
overcome and propose a consistent enhancement-based deep MVC method via
contrastive learning (CCEC). Specifically, semantic connection blocks are
incorporated into a feature representation to preserve the consistent
information among multiple views. Furthermore, the representation process for
clustering is enhanced through spectral clustering, and the consistency across
multiple views is improved. Experiments conducted on five datasets demonstrate
the effectiveness and superiority of our method in comparison with the
state-of-the-art (SOTA) methods. The code for this method can be accessed at
https://anonymous.4open.science/r/CCEC-E84E/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>There are multiple errors that need to be corrected, including some
  formulas and concept descriptions. We will re upload the paper after the
  modifications are completed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LaserHuman: Language-guided Scene-aware Human Motion Generation in Free
  Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13307v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13307v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peishan Cong, Ziyi Wang, Zhiyang Dou, Yiming Ren, Wei Yin, Kai Cheng, Yujing Sun, Xiaoxiao Long, Xinge Zhu, Yuexin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language-guided scene-aware human motion generation has great significance
for entertainment and robotics. In response to the limitations of existing
datasets, we introduce LaserHuman, a pioneering dataset engineered to
revolutionize Scene-Text-to-Motion research. LaserHuman stands out with its
inclusion of genuine human motions within 3D environments, unbounded free-form
natural language descriptions, a blend of indoor and outdoor scenarios, and
dynamic, ever-changing scenes. Diverse modalities of capture data and rich
annotations present great opportunities for the research of conditional motion
generation, and can also facilitate the development of real-life applications.
Moreover, to generate semantically consistent and physically plausible human
motions, we propose a multi-conditional diffusion model, which is simple but
effective, achieving state-of-the-art performance on existing datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM4SGG: Large Language Model for Weakly Supervised Scene Graph
  Generation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10404v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10404v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kibum Kim, Kanghoon Yoon, Jaehyeong Jeon, Yeonjun In, Jinyoung Moon, Donghyun Kim, Chanyoung Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly-Supervised Scene Graph Generation (WSSGG) research has recently
emerged as an alternative to the fully-supervised approach that heavily relies
on costly annotations. In this regard, studies on WSSGG have utilized image
captions to obtain unlocalized triplets while primarily focusing on grounding
the unlocalized triplets over image regions. However, they have overlooked the
two issues involved in the triplet formation process from the captions: 1)
Semantic over-simplification issue arises when extracting triplets from
captions, where fine-grained predicates in captions are undesirably converted
into coarse-grained predicates, resulting in a long-tailed predicate
distribution, and 2) Low-density scene graph issue arises when aligning the
triplets in the caption with entity/predicate classes of interest, where many
triplets are discarded and not used in training, leading to insufficient
supervision. To tackle the two issues, we propose a new approach, i.e., Large
Language Model for weakly-supervised SGG (LLM4SGG), where we mitigate the two
issues by leveraging the LLM's in-depth understanding of language and reasoning
ability during the extraction of triplets from captions and alignment of
entity/predicate classes with target data. To further engage the LLM in these
processes, we adopt the idea of Chain-of-Thought and the in-context few-shot
learning strategy. To validate the effectiveness of LLM4SGG, we conduct
extensive experiments on Visual Genome and GQA datasets, showing significant
improvements in both Recall@K and mean Recall@K compared to the
state-of-the-art WSSGG methods. A further appeal is that LLM4SGG is
data-efficient, enabling effective model training with a small amount of
training images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages; CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intrinsic Image Diffusion for Indoor Single-view Material Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Kocsis, Vincent Sitzmann, Matthias Nießner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Intrinsic Image Diffusion, a generative model for appearance
decomposition of indoor scenes. Given a single input view, we sample multiple
possible material explanations represented as albedo, roughness, and metallic
maps. Appearance decomposition poses a considerable challenge in computer
vision due to the inherent ambiguity between lighting and material properties
and the lack of real datasets. To address this issue, we advocate for a
probabilistic formulation, where instead of attempting to directly predict the
true material properties, we employ a conditional generative model to sample
from the solution space. Furthermore, we show that utilizing the strong learned
prior of recent diffusion models trained on large-scale real-world images can
be adapted to material estimation and highly improves the generalization to
real images. Our method produces significantly sharper, more consistent, and
more detailed materials, outperforming state-of-the-art methods by $1.5dB$ on
PSNR and by $45\%$ better FID score on albedo prediction. We demonstrate the
effectiveness of our approach through experiments on both synthetic and
real-world datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://peter-kocsis.github.io/IntrinsicImageDiffusion/
  Video: https://youtu.be/lz0meJlj5cA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Point2RBox: Combine Knowledge from Synthetic Visual Patterns for
  End-to-end Oriented Object Detection with Single Point Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14758v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14758v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Yu, Xue Yang, Qingyun Li, Feipeng Da, Jifeng Dai, Yu Qiao, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapidly increasing demand for oriented object detection (OOD),
recent research involving weakly-supervised detectors for learning rotated box
(RBox) from the horizontal box (HBox) has attracted more and more attention. In
this paper, we explore a more challenging yet label-efficient setting, namely
single point-supervised OOD, and present our approach called Point2RBox.
Specifically, we propose to leverage two principles: 1) Synthetic pattern
knowledge combination: By sampling around each labeled point on the image, we
spread the object feature to synthetic visual patterns with known boxes to
provide the knowledge for box regression. 2) Transform self-supervision: With a
transformed input image (e.g. scaled/rotated), the output RBoxes are trained to
follow the same transformation so that the network can perceive the relative
size/rotation between objects. The detector is further enhanced by a few
devised techniques to cope with peripheral issues, e.g. the anchor/layer
assignment as the size of the object is not available in our point supervision
setting. To our best knowledge, Point2RBox is the first end-to-end solution for
point-supervised OOD. In particular, our method uses a lightweight paradigm,
yet it achieves a competitive performance among point-supervised alternatives,
41.05%/27.62%/80.01% on DOTA/DIOR/HRSC datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures, 5 tables, code:
  https://github.com/yuyi1005/point2rbox-mmrotate</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Driving Animatronic Robot Facial Expression From Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boren Li, Hang Li, Hangxin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Animatronic robots aim to enable natural human-robot interaction through
lifelike facial expressions. However, generating realistic, speech-synchronized
robot expressions is challenging due to the complexities of facial biomechanics
and responsive motion synthesis. This paper presents a principled,
skinning-centric approach to drive animatronic robot facial expressions from
speech. The proposed approach employs linear blend skinning (LBS) as the core
representation to guide tightly integrated innovations in embodiment design and
motion synthesis. LBS informs the actuation topology, enables human expression
retargeting, and allows speech-driven facial motion generation. The proposed
approach is capable of generating highly realistic, real-time facial
expressions from speech on an animatronic face, significantly advancing robots'
ability to replicate nuanced human expressions for natural interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review. For associated project page, see
  https://library87.github.io/animatronic-face-iros24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mpox-AISM: AI-Mediated Super Monitoring for Mpox and Like-Mpox 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09780v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09780v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubiao Yue, Minghua Jiang, Xinyue Zhang, Jialong Xu, Huacong Ye, Fan Zhang, Zhenzhang Li, Yang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The key to preventing the spread of mpox (monkeypox) lies in timely,
convenient, and accurate diagnosis for earlier-stage infected individuals.
Unfortunately, the resemblances between common skin diseases and mpox and the
need for professional diagnosis inevitably deteriorated the diagnosis of
earlier-stage patients with Mpox and contributed to its widespread outbreak in
crowded areas. Here, we proposed a real-time visualization strategy called
"Super Monitoring" using artificial intelligence and Internet technology,
thereby performing a low-cost, convenient, timely, and unspecialized diagnosis
for earlier-stage mpox. Specifically, such AI-mediated "super monitoring"
(Mpox-AISM) invokes a framework assembled by deep learning models, data
augmentation, self-supervised learning, and cloud services. Verified by
publicly available datasets, the Precision, Recall, Specificity, and F1-score
of Mpox-AISM in diagnosing mpox achieved 99.3%, 94.1%, 99.9%, and 96.6%,
respectively. Furthermore, Mpox-AISM's overall accuracy reaches 94.51% in
diagnosing mpox, six like-mpox skin diseases, and normal skin. We also employed
gradient-weighted class activation mapping to explain the decision-making
process of Mpox-AISM, thus handily understanding the specific characteristics
that may indicate the mpox's onset and improving its reliability. With the help
of the Internet and communication terminal, Mpox-AISM can perform a real-time,
low-cost, and convenient diagnosis for earlier-stage mpox in various real-world
settings, thereby effectively curbing the spread of mpox virus.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open-Vocabulary Camouflaged Object Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11241v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11241v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youwei Pang, Xiaoqi Zhao, Jiaming Zuo, Lihe Zhang, Huchuan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the emergence of the large-scale vision-language model (VLM), such
as CLIP, has opened the way towards open-world object perception. Many works
have explored the utilization of pre-trained VLM for the challenging
open-vocabulary dense prediction task that requires perceiving diverse objects
with novel classes at inference time. Existing methods construct experiments
based on the public datasets of related tasks, which are not tailored for open
vocabulary and rarely involve imperceptible objects camouflaged in complex
scenes due to data collection bias and annotation costs. To fill in the gaps,
we introduce a new task, open-vocabulary camouflaged object segmentation
(OVCOS), and construct a large-scale complex scene dataset (\textbf{OVCamo})
containing 11,483 hand-selected images with fine annotations and corresponding
object classes. Further, we build a strong single-stage open-vocabulary
\underline{c}amouflaged \underline{o}bject \underline{s}egmentation
transform\underline{er} baseline \textbf{OVCoser} attached to the
parameter-fixed CLIP with iterative semantic guidance and structure
enhancement. By integrating the guidance of class semantic knowledge and the
supplement of visual structure cues from the edge and depth information, the
proposed method can efficiently capture camouflaged objects. Moreover, this
effective framework also surpasses previous state-of-the-arts of
open-vocabulary semantic image segmentation by a large margin on our OVCamo
dataset. With the proposed dataset and baseline, we hope that this new task
with more practical value can further expand the research on open-vocabulary
dense prediction tasks. The code and data will be available in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update the style and add details</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantics Meets Temporal Correspondence: Self-supervised Object-centric
  Learning in Videos <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.09951v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.09951v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Qian, Shuangrui Ding, Xian Liu, Dahua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised methods have shown remarkable progress in learning high-level
semantics and low-level temporal correspondence. Building on these results, we
take one step further and explore the possibility of integrating these two
features to enhance object-centric representations. Our preliminary experiments
indicate that query slot attention can extract different semantic components
from the RGB feature map, while random sampling based slot attention can
exploit temporal correspondence cues between frames to assist instance
identification. Motivated by this, we propose a novel semantic-aware masked
slot attention on top of the fused semantic features and correspondence maps.
It comprises two slot attention stages with a set of shared learnable Gaussian
distributions. In the first stage, we use the mean vectors as slot
initialization to decompose potential semantics and generate semantic
segmentation masks through iterative attention. In the second stage, for each
semantics, we randomly sample slots from the corresponding Gaussian
distribution and perform masked feature aggregation within the semantic area to
exploit temporal correspondence patterns for instance identification. We adopt
semantic- and instance-level temporal consistency as self-supervision to
encourage temporally coherent object-centric representations. Our model
effectively identifies multiple object instances with semantic structure,
reaching promising results on unsupervised video object discovery. Furthermore,
we achieve state-of-the-art performance on dense label propagation tasks,
demonstrating the potential for object-centric analysis. The code is released
at https://github.com/shvdiwnkozbw/SMTC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ICP-Flow: <span class="highlight-title">LiDAR</span> Scene Flow Estimation with ICP <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yancong Lin, Holger Caesar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene flow characterizes the 3D motion between two LiDAR scans captured by an
autonomous vehicle at nearby timesteps. Prevalent methods consider scene flow
as point-wise unconstrained flow vectors that can be learned by either
large-scale training beforehand or time-consuming optimization at inference.
However, these methods do not take into account that objects in autonomous
driving often move rigidly. We incorporate this rigid-motion assumption into
our design, where the goal is to associate objects over scans and then estimate
the locally rigid transformations. We propose ICP-Flow, a learning-free flow
estimator. The core of our design is the conventional Iterative Closest Point
(ICP) algorithm, which aligns the objects over time and outputs the
corresponding rigid transformations. Crucially, to aid ICP, we propose a
histogram-based initialization that discovers the most likely translation, thus
providing a good starting point for ICP. The complete scene flow is then
recovered from the rigid transformations. We outperform state-of-the-art
baselines, including supervised models, on the Waymo dataset and perform
competitively on Argoverse-v2 and nuScenes. Further, we train a feedforward
neural network, supervised by the pseudo labels from our model, and achieve top
performance among all models capable of real-time inference. We validate the
advantage of our model on scene flow estimation with longer temporal gaps, up
to 0.4 seconds where other models fail to deliver meaningful results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024, camera-ready. Code: https://github.com/yanconglin/ICP-Flow</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GSVA: Generalized Segmentation via Multimodal Large Language Models <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10103v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10103v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuofan Xia, Dongchen Han, Yizeng Han, Xuran Pan, Shiji Song, Gao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalized Referring Expression Segmentation (GRES) extends the scope of
classic RES to refer to multiple objects in one expression or identify the
empty targets absent in the image. GRES poses challenges in modeling the
complex spatial relationships of the instances in the image and identifying
non-existing referents. Multimodal Large Language Models (MLLMs) have recently
shown tremendous progress in these complicated vision-language tasks.
Connecting Large Language Models (LLMs) and vision models, MLLMs are proficient
in understanding contexts with visual inputs. Among them, LISA, as a
representative, adopts a special [SEG] token to prompt a segmentation mask
decoder, e.g., SAM, to enable MLLMs in the RES task. However, existing
solutions to GRES remain unsatisfactory since current segmentation MLLMs cannot
correctly handle the cases where users might reference multiple subjects in a
singular prompt or provide descriptions incongruent with any image target. In
this paper, we propose Generalized Segmentation Vision Assistant (GSVA) to
address this gap. Specifically, GSVA reuses the [SEG] token to prompt the
segmentation model towards supporting multiple mask references simultaneously
and innovatively learns to generate a [REJ] token to reject the null targets
explicitly. Experiments validate GSVA's efficacy in resolving the GRES issue,
marking a notable enhancement and setting a new record on the GRES benchmark
gRefCOCO dataset. GSVA also proves effective across various classic referring
segmentation and comprehension tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024 (19 pages, 9 figures, 11 tables)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RTFS-Net: Recurrent Time-Frequency Modelling for Efficient Audio-Visual
  Speech Separation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17189v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17189v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Pegg, Kai Li, Xiaolin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual speech separation methods aim to integrate different modalities
to generate high-quality separated speech, thereby enhancing the performance of
downstream tasks such as speech recognition. Most existing state-of-the-art
(SOTA) models operate in the time domain. However, their overly simplistic
approach to modeling acoustic features often necessitates larger and more
computationally intensive models in order to achieve SOTA performance. In this
paper, we present a novel time-frequency domain audio-visual speech separation
method: Recurrent Time-Frequency Separation Network (RTFS-Net), which applies
its algorithms on the complex time-frequency bins yielded by the Short-Time
Fourier Transform. We model and capture the time and frequency dimensions of
the audio independently using a multi-layered RNN along each dimension.
Furthermore, we introduce a unique attention-based fusion technique for the
efficient integration of audio and visual information, and a new mask
separation approach that takes advantage of the intrinsic spectral nature of
the acoustic features for a clearer separation. RTFS-Net outperforms the prior
SOTA method in both inference speed and separation quality while reducing the
number of parameters by 90% and MACs by 83%. This is the first time-frequency
domain audio-visual speech separation method to outperform all contemporary
time-domain counterparts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The Twelfth International Conference on Learning
  Representations (ICLR) 2024, see https://openreview.net/forum?id=PEuDO2EiDr</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling Parts Beyond Objects:Towards Finer-Granularity Referring
  Expression Segmentation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08007v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08007v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wang, Tongtian Yue, Yisi Zhang, Longteng Guo, Xingjian He, Xinlong Wang, Jing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring expression segmentation (RES) aims at segmenting the foreground
masks of the entities that match the descriptive natural language expression.
Previous datasets and methods for classic RES task heavily rely on the prior
assumption that one expression must refer to object-level targets. In this
paper, we take a step further to finer-grained part-level RES task. To promote
the object-level RES task towards finer-grained vision-language understanding,
we put forward a new multi-granularity referring expression segmentation (MRES)
task and construct an evaluation benchmark called RefCOCOm by manual
annotations. By employing our automatic model-assisted data engine, we build
the largest visual grounding dataset namely MRES-32M, which comprises over
32.2M high-quality masks and captions on the provided 1M images. Besides, a
simple yet strong model named UniRES is designed to accomplish the unified
object-level and part-level grounding task. Extensive experiments on our
RefCOCOm for MRES and three datasets (i.e., RefCOCO(+/g) for classic RES task
demonstrate the superiority of our method over previous state-of-the-art
methods. To foster future research into fine-grained visual grounding, our
benchmark RefCOCOm, the MRES-32M dataset and model UniRES will be publicly
available at https://github.com/Rubics-Xuan/MRES
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work is accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13507v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13507v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinmin Li, Kuofeng Gao, Yang Bai, Jingyun Zhang, Shu-tao Xia, Yisen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable performance of video-based large language models
(LLMs), their adversarial threat remains unexplored. To fill this gap, we
propose the first adversarial attack tailored for video-based LLMs by crafting
flow-based multi-modal adversarial perturbations on a small fraction of frames
within a video, dubbed FMM-Attack. Extensive experiments show that our attack
can effectively induce video-based LLMs to generate incorrect answers when
videos are added with imperceptible adversarial perturbations. Intriguingly,
our FMM-Attack can also induce garbling in the model output, prompting
video-based LLMs to hallucinate. Overall, our observations inspire a further
understanding of multi-modal robustness and safety-related feature alignment
across different modalities, which is of great importance for various large
multi-modal models. Our code is available at
https://github.com/THU-Kingmin/FMM-Attack.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CBNet: A Plug-and-Play Network for Segmentation-Based Scene Text
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02340v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02340v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Zhao, Wei Feng, Zheng Zhang, Jingjing Lv, Xin Zhu, Zhangang Lin, Jinghe Hu, Jingping Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, segmentation-based methods are quite popular in scene text
detection, which mainly contain two steps: text kernel segmentation and
expansion. However, the segmentation process only considers each pixel
independently, and the expansion process is difficult to achieve a favorable
accuracy-speed trade-off. In this paper, we propose a Context-aware and
Boundary-guided Network (CBN) to tackle these problems. In CBN, a basic text
detector is firstly used to predict initial segmentation results. Then, we
propose a context-aware module to enhance text kernel feature representations,
which considers both global and local contexts. Finally, we introduce a
boundary-guided module to expand enhanced text kernels adaptively with only the
pixels on the contours, which not only obtains accurate text boundaries but
also keeps high speed, especially on high-resolution output maps. In
particular, with a lightweight backbone, the basic detector equipped with our
proposed CBN achieves state-of-the-art results on several popular benchmarks,
and our proposed CBN can be plugged into several segmentation-based methods.
Code is available at https://github.com/XiiZhao/cbn.pytorch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJCV 2024. Code is available at this https URL:
  https://github.com/XiiZhao/cbn.pytorch</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conditional Tuning Network for Few-Shot Adaptation of Segmentation
  Anything Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03631v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03631v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aoran Xiao, Weihao Xuan, Heli Qi, Yun Xing, Ruijie Ren, Xiaoqin Zhang, Ling Shao, Shijian Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent Segment Anything Model (SAM) has demonstrated remarkable zero-shot
capability and flexible geometric prompting in general image segmentation.
However, SAM often struggles when handling various unconventional images, such
as aerial, medical, and non-RGB images. This paper presents CAT-SAM, a
ConditionAl Tuning network that adapts SAM toward various unconventional target
tasks with just few-shot target samples. CAT-SAM freezes the entire SAM and
adapts its mask decoder and image encoder simultaneously with a small number of
learnable parameters. The core design is a prompt bridge structure that enables
decoder-conditioned joint tuning of the heavyweight image encoder and the
lightweight mask decoder. The bridging maps the prompt token of the mask
decoder to the image encoder, fostering synergic adaptation of the encoder and
the decoder with mutual benefits. We develop two representative tuning
strategies for the image encoder which leads to two CAT-SAM variants: one
injecting learnable prompt tokens in the input space and the other inserting
lightweight adapter networks. Extensive experiments over 11 unconventional
tasks show that both CAT-SAM variants achieve superior target segmentation
performance consistently even under the very challenging one-shot adaptation
setup. Project page: https://xiaoaoran.github.io/projects/CAT-SAM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://xiaoaoran.github.io/projects/CAT-SAM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ShaDocFormer: A Shadow-Attentive Threshold Detector With Cascaded Fusion
  Refiner for Document Shadow Removal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06670v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06670v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiwen Chen, Yingtie Lei, Shenghong Luo, Ziyang Zhou, Mingxian Li, Chi-Man Pun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document shadow is a common issue that arises when capturing documents using
mobile devices, which significantly impacts readability. Current methods
encounter various challenges, including inaccurate detection of shadow masks
and estimation of illumination. In this paper, we propose ShaDocFormer, a
Transformer-based architecture that integrates traditional methodologies and
deep learning techniques to tackle the problem of document shadow removal. The
ShaDocFormer architecture comprises two components: the Shadow-attentive
Threshold Detector (STD) and the Cascaded Fusion Refiner (CFR). The STD module
employs a traditional thresholding technique and leverages the attention
mechanism of the Transformer to gather global information, thereby enabling
precise detection of shadow masks. The cascaded and aggregative structure of
the CFR module facilitates a coarse-to-fine restoration process for the entire
image. As a result, ShaDocFormer excels in accurately detecting and capturing
variations in both shadow and illumination, thereby enabling effective removal
of shadows. Extensive experiments demonstrate that ShaDocFormer outperforms
current state-of-the-art methods in both qualitative and quantitative
measurements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJCNN 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MinD-3D: Reconstruct High-quality 3D objects in Human Brain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07485v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07485v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianxiong Gao, Yuqian Fu, Yun Wang, Xuelin Qian, Jianfeng Feng, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Recon3DMind, an innovative task aimed at
reconstructing 3D visuals from Functional Magnetic Resonance Imaging (fMRI)
signals, marking a significant advancement in the fields of cognitive
neuroscience and computer vision. To support this pioneering task, we present
the fMRI-Shape dataset, which includes data from 14 participants and features
360-degree videos of 3D objects to enable comprehensive fMRI signal capture
across various settings, thereby laying a foundation for future research.
Furthermore, we propose MinD-3D, a novel and effective three-stage framework
specifically designed to decode the brain's 3D visual information from fMRI
signals, demonstrating the feasibility of this challenging task. The framework
begins by extracting and aggregating features from fMRI frames through a
neuro-fusion encoder, subsequently employs a feature bridge diffusion model to
generate visual features, and ultimately recovers the 3D object via a
generative transformer decoder. We assess the performance of MinD-3D using a
suite of semantic and structural metrics and analyze the correlation between
the features extracted by our model and the visual regions of interest (ROIs)
in fMRI signals. Our findings indicate that MinD-3D not only reconstructs 3D
objects with high semantic relevance and spatial similarity but also
significantly enhances our understanding of the human brain's capabilities in
processing 3D visual information. Project page at:
https://jianxgao.github.io/MinD-3D.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Markov Random Field for Stereo Matching <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11193v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11193v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongfan Guan, Chen Wang, Yun-Hui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stereo matching is a core task for many computer vision and robotics
applications. Despite their dominance in traditional stereo methods, the
hand-crafted Markov Random Field (MRF) models lack sufficient modeling accuracy
compared to end-to-end deep models. While deep learning representations have
greatly improved the unary terms of the MRF models, the overall accuracy is
still severely limited by the hand-crafted pairwise terms and message passing.
To address these issues, we propose a neural MRF model, where both potential
functions and message passing are designed using data-driven neural networks.
Our fully data-driven model is built on the foundation of variational inference
theory, to prevent convergence issues and retain stereo MRF's graph inductive
bias. To make the inference tractable and scale well to high-resolution images,
we also propose a Disparity Proposal Network (DPN) to adaptively prune the
search space of disparity. The proposed approach ranks $1^{st}$ on both KITTI
2012 and 2015 leaderboards among all published methods while running faster
than 100 ms. This approach significantly outperforms prior global methods,
e.g., lowering D1 metric by more than 50% on KITTI 2015. In addition, our
method exhibits strong cross-domain generalization and can recover sharp edges.
The codes at https://github.com/aeolusguan/NMRF
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Prompt Learning in Vision Language Models <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11178v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11178v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihwan Bang, Sumyeong Ahn, Jae-Gil Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained Vision Language Models (VLMs) have demonstrated notable progress
in various zero-shot tasks, such as classification and retrieval. Despite their
performance, because improving performance on new tasks requires task-specific
knowledge, their adaptation is essential. While labels are needed for the
adaptation, acquiring them is typically expensive. To overcome this challenge,
active learning, a method of achieving a high performance by obtaining labels
for a small number of samples from experts, has been studied. Active learning
primarily focuses on selecting unlabeled samples for labeling and leveraging
them to train models. In this study, we pose the question, "how can the
pre-trained VLMs be adapted under the active learning framework?" In response
to this inquiry, we observe that (1) simply applying a conventional active
learning framework to pre-trained VLMs even may degrade performance compared to
random selection because of the class imbalance in labeling candidates, and (2)
the knowledge of VLMs can provide hints for achieving the balance before
labeling. Based on these observations, we devise a novel active learning
framework for VLMs, denoted as PCB. To assess the effectiveness of our
approach, we conduct experiments on seven different real-world datasets, and
the results demonstrate that PCB surpasses conventional active learning and
random sampling methods. Code will be available in
https://github.com/kaist-dmlab/pcb .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LMM-Assisted Breast Cancer Treatment Target Segmentation with
  Consistency Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15876v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15876v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kwanyoung Kim, Yujin Oh, Sangjoon Park, Hwa Kyung Byun, Jin Sung Kim, Yong Bae Kim, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Artificial Intelligence (AI) have profoundly
influenced medical fields, by providing tools to reduce clinical workloads.
However, most AI models are constrained to execute unimodal tasks, in stark
contrast to the comprehensive approaches utilized by medical professionals. To
address this, here we present RO-LMM, a multi-purpose large multimodal model
(LMM) tailored for the field of radiation oncology. This model covers series of
tasks within clinical workflow, adept at clinical report summarization,
radiation treatment plan suggestion, and plan-guided target volume
segmentation. In particular, to perform consecutive clinical tasks, we further
present a novel Consistency Embedding Fine-Tuning (CEFTune) technique, which
boosts LMM's robustness to noisy inputs while preserving the capability of
handling clean inputs, and transform this concept into LMM-driven segmentation
framework as Consistency Embedding Segmentation~(CESEG). Experimental results
on multi-centre cohorts demonstrate our RO-LMM's promising performance for
multiple clinical tasks with generalization capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 16 table, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NocPlace: Nocturnal Visual Place Recognition via Generative and
  Inherited Knowledge Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17159v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17159v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingxi Liu, Yiqun Wang, Huaqi Tao, Tingjun Huang, Fulin Tang, Yihong Wu, Jinqiang Cui, Hong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Place Recognition (VPR) is crucial in computer vision, aiming to
retrieve database images similar to a query image from an extensive collection
of known images. However, like many vision tasks, VPR always degrades at night
due to the scarcity of nighttime images. Moreover, VPR needs to address the
cross-domain problem of night-to-day rather than just the issue of a single
nighttime domain. In response to these issues, we present NocPlace, which
leverages generative and inherited knowledge transfer to embed resilience
against dazzling lights and extreme darkness in the global descriptor. First,
we establish a day-night urban scene dataset called NightCities, capturing
diverse lighting variations and dark scenarios across 60 cities globally. Then,
an image generation network is trained on this dataset and processes a
large-scale VPR dataset, obtaining its nighttime version. Finally, VPR models
are fine-tuned using descriptors inherited from themselves and night-style
images, which builds explicit cross-domain contrastive relationships.
Comprehensive experiments on various datasets demonstrate our contributions and
the superiority of NocPlace. Without adding any real-time computing resources,
NocPlace improves the performance of Eigenplaces by 7.6% on Tokyo 24/7 Night
and 16.8% on SVOX Night.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages,9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TD-MPC2: Scalable, Robust World Models for Continuous Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16828v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16828v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicklas Hansen, Hao Su, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  TD-MPC is a model-based reinforcement learning (RL) algorithm that performs
local trajectory optimization in the latent space of a learned implicit
(decoder-free) world model. In this work, we present TD-MPC2: a series of
improvements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves
significantly over baselines across 104 online RL tasks spanning 4 diverse task
domains, achieving consistently strong results with a single set of
hyperparameters. We further show that agent capabilities increase with model
and data size, and successfully train a single 317M parameter agent to perform
80 tasks across multiple task domains, embodiments, and action spaces. We
conclude with an account of lessons, opportunities, and risks associated with
large TD-MPC2 agents. Explore videos, models, data, code, and more at
https://tdmpc2.com
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024. Explore videos, models, data, code, and more at
  https://tdmpc2.com</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2009.09213v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2009.09213v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Huang, Felix Juefei-Xu, Qing Guo, Yang Liu, Geguang Pu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current high-fidelity generation and high-precision detection of DeepFake
images are at an arms race. We believe that producing DeepFakes that are highly
realistic and 'detection evasive' can serve the ultimate goal of improving
future generation DeepFake detection capabilities. In this paper, we propose a
simple yet powerful pipeline to reduce the artifact patterns of fake images
without hurting image quality by performing implicit spatial-domain notch
filtering. We first demonstrate that frequency-domain notch filtering, although
famously shown to be effective in removing periodic noise in the spatial
domain, is infeasible for our task at hand due to the manual designs required
for the notch filters. We, therefore, resort to a learning-based approach to
reproduce the notch filtering effects, but solely in the spatial domain. We
adopt a combination of adding overwhelming spatial noise for breaking the
periodic noise pattern and deep image filtering to reconstruct the noise-free
fake images, and we name our method DeepNotch. Deep image filtering provides a
specialized filter for each pixel in the noisy image, producing filtered images
with high fidelity compared to their DeepFake counterparts. Moreover, we also
use the semantic information of the image to generate an adversarial guidance
map to add noise intelligently. Our large-scale evaluation on 3 representative
state-of-the-art DeepFake detection methods (tested on 16 types of DeepFakes)
has demonstrated that our technique significantly reduces the accuracy of these
3 fake image detection methods, 36.79% on average and up to 97.02% in the best
case.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-20T00:00:00Z">2024-03-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">71</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural Language as Polices: Reasoning for Coordinate-Level Embodied
  Control with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Mikami, Andrew Melnik, Jun Miura, Ville Hautamäki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate experimental results with LLMs that address robotics action
planning problems. Recently, LLMs have been applied in robotics action
planning, particularly using a code generation approach that converts complex
high-level instructions into mid-level policy codes. In contrast, our approach
acquires text descriptions of the task and scene objects, then formulates
action planning through natural language reasoning, and outputs coordinate
level control commands, thus reducing the necessity for intermediate
representation code as policies. Our approach is evaluated on a multi-modal
prompt simulation benchmark, demonstrating that our prompt engineering
experiments with natural language reasoning significantly enhance success rates
compared to its absence. Furthermore, our approach illustrates the potential
for natural language descriptions to transfer robotics skills from known tasks
to previously unseen tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Convex Formulation of Frictional Contact for the Material Point Method
  and Rigid Bodies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeshun Zong, Chenfanfu Jiang, Xuchen Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel convex formulation that seamlessly
integrates the Material Point Method (MPM) with articulated rigid body dynamics
in frictional contact scenarios. We extend the linear corotational hyperelastic
model into the realm of elastoplasticity and include an efficient return
mapping algorithm. This approach is particularly effective for MPM simulations
involving significant deformation and topology changes, while preserving the
convexity of the optimization problem. Our method ensures global convergence,
enabling the use of large simulation time steps without compromising
robustness. We have validated our approach through rigorous testing and
performance evaluations, highlighting its superior capabilities in managing
complex simulations relevant to robotics. Compared to previous MPM based
robotic simulators, our method significantly improves the stability of contact
resolution -- a critical factor in robot manipulation tasks. We make our method
available in the open-source robotics toolkit, Drake.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Certified Human Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadhossein Bahari, Saeed Saadatnejad, Amirhossein Asgari Farsangi, Seyed-Mohsen Moosavi-Dezfooli, Alexandre Alahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory prediction plays an essential role in autonomous vehicles. While
numerous strategies have been developed to enhance the robustness of trajectory
prediction models, these methods are predominantly heuristic and do not offer
guaranteed robustness against adversarial attacks and noisy observations. In
this work, we propose a certification approach tailored for the task of
trajectory prediction. To this end, we address the inherent challenges
associated with trajectory prediction, including unbounded outputs, and
mutli-modality, resulting in a model that provides guaranteed robustness.
Furthermore, we integrate a denoiser into our method to further improve the
performance. Through comprehensive evaluations, we demonstrate the
effectiveness of the proposed technique across various baselines and using
standard trajectory prediction datasets. The code will be made available
online: https://s-attack.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embedding Pose Graph, Enabling 3D Foundation Model Capabilities with a
  Compact Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugues Thomas, Jian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the Embedding Pose Graph (EPG), an innovative method that
combines the strengths of foundation models with a simple 3D representation
suitable for robotics applications. Addressing the need for efficient spatial
understanding in robotics, EPG provides a compact yet powerful approach by
attaching foundation model features to the nodes of a pose graph. Unlike
traditional methods that rely on bulky data formats like voxel grids or point
clouds, EPG is lightweight and scalable. It facilitates a range of robotic
tasks, including open-vocabulary querying, disambiguation, image-based
querying, language-directed navigation, and re-localization in 3D environments.
We showcase the effectiveness of EPG in handling these tasks, demonstrating its
capacity to improve how robots interact with and navigate through complex
spaces. Through both qualitative and quantitative assessments, we illustrate
EPG's strong performance and its ability to outperform existing methods in
re-localization. Our work introduces a crucial step forward in enabling robots
to efficiently understand and operate within large-scale 3D spaces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Projection-free computation of robust controllable sets with constrained
  zonotopes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abraham P. Vinod, Avishai Weiss, Stefano Di Cairano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of computing robust controllable sets for discrete-time
linear systems with additive uncertainty. We propose a tractable and scalable
approach to inner- and outer-approximate robust controllable sets using
constrained zonotopes, when the additive uncertainty set is a symmetric,
convex, and compact set. Our least-squares-based approach uses novel
closed-form approximations of the Pontryagin difference between a constrained
zonotopic minuend and a symmetric, convex, and compact subtrahend. Unlike
existing approaches, our approach does not rely on convex optimization solvers,
and is projection-free for ellipsoidal and zonotopic uncertainty sets. We also
propose a least-squares-based approach to compute a convex, polyhedral
outer-approximation to constrained zonotopes, and characterize sufficient
conditions under which all these approximations are exact. We demonstrate the
computational efficiency and scalability of our approach in several case
studies, including the design of abort-safe rendezvous trajectories for a
spacecraft in near-rectilinear halo orbit under uncertainty. Our approach can
inner-approximate a 20-step robust controllable set for a 100-dimensional
linear system in under 15 seconds on a standard computer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning for Online Testing of Autonomous Driving Systems:
  a Replication and Extension Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Giamattei, Matteo Biagiola, Roberto Pietrantuono, Stefano Russo, Paolo Tonella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a recent study, Reinforcement Learning (RL) used in combination with
many-objective search, has been shown to outperform alternative techniques
(random search and many-objective search) for online testing of Deep Neural
Network-enabled systems. The empirical evaluation of these techniques was
conducted on a state-of-the-art Autonomous Driving System (ADS). This work is a
replication and extension of that empirical study. Our replication shows that
RL does not outperform pure random test generation in a comparison conducted
under the same settings of the original study, but with no confounding factor
coming from the way collisions are measured. Our extension aims at eliminating
some of the possible reasons for the poor performance of RL observed in our
replication: (1) the presence of reward components providing contrasting or
useless feedback to the RL agent; (2) the usage of an RL algorithm (Q-learning)
which requires discretization of an intrinsically continuous state space.
Results show that our new RL agent is able to converge to an effective policy
that outperforms random testing. Results also highlight other possible
improvements, which open to further investigations on how to best leverage RL
for online ADS testing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DBA-Fusion: Tightly Integrating Deep Dense Visual Bundle Adjustment with
  Multiple Sensors for Large-Scale Localization and <span class="highlight-title">Mapping</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhou, Xingxing Li, Shengyu Li, Xuanbin Wang, Shaoquan Feng, Yuxuan Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual simultaneous localization and mapping (VSLAM) has broad applications,
with state-of-the-art methods leveraging deep neural networks for better
robustness and applicability. However, there is a lack of research in fusing
these learning-based methods with multi-sensor information, which could be
indispensable to push related applications to large-scale and complex
scenarios. In this paper, we tightly integrate the trainable deep dense bundle
adjustment (DBA) with multi-sensor information through a factor graph. In the
framework, recurrent optical flow and DBA are performed among sequential
images. The Hessian information derived from DBA is fed into a generic factor
graph for multi-sensor fusion, which employs a sliding window and supports
probabilistic marginalization. A pipeline for visual-inertial integration is
firstly developed, which provides the minimum ability of metric-scale
localization and mapping. Furthermore, other sensors (e.g., global navigation
satellite system) are integrated for driftless and geo-referencing
functionality. Extensive tests are conducted on both public datasets and
self-collected datasets. The results validate the superior localization
performance of our approach, which enables real-time dense mapping in
large-scale environments. The code has been made open-source
(https://github.com/GREAT-WHU/DBA-Fusion).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Matters for Active Texture Recognition With Vision-Based Tactile
  Sensors <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alina Böhm, Tim Schneider, Boris Belousov, Alap Kshirsagar, Lisa Lin, Katja Doerschner, Knut Drewing, Constantin A. Rothkopf, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores active sensing strategies that employ vision-based
tactile sensors for robotic perception and classification of fabric textures.
We formalize the active sampling problem in the context of tactile fabric
recognition and provide an implementation of information-theoretic exploration
strategies based on minimizing predictive entropy and variance of probabilistic
models. Through ablation studies and human experiments, we investigate which
components are crucial for quick and reliable texture recognition. Along with
the active sampling strategies, we evaluate neural network architectures,
representations of uncertainty, influence of data augmentation, and dataset
variability. By evaluating our method on a previously published Active Clothing
Perception Dataset and on a real robotic system, we establish that the choice
of the active exploration strategy has only a minor influence on the
recognition accuracy, whereas data augmentation and dropout rate play a
significantly larger role. In a comparison study, while humans achieve 66.9%
recognition accuracy, our best approach reaches 90.0% in under 5 touches,
highlighting that vision-based tactile sensors are highly effective for fabric
texture recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 9 figures, accepted at 2024 IEEE International Conference on
  Robotics and Automation (ICRA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Loss Regularizing Robotic Terrain Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shakti Deo Kumar, Sudhanshu Tripathi, Krishna Ujjwal, Sarvada Sakshi Jha, Suddhasil De
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Locomotion mechanics of legged robots are suitable when pacing through
difficult terrains. Recognising terrains for such robots are important to fully
yoke the versatility of their movements. Consequently, robotic terrain
classification becomes significant to classify terrains in real time with high
accuracy. The conventional classifiers suffer from overfitting problem, low
accuracy problem, high variance problem, and not suitable for live dataset. On
the other hand, classifying a growing dataset is difficult for convolution
based terrain classification. Supervised recurrent models are also not
practical for this classification. Further, the existing recurrent
architectures are still evolving to improve accuracy of terrain classification
based on live variable-length sensory data collected from legged robots. This
paper proposes a new semi-supervised method for terrain classification of
legged robots, avoiding preprocessing of long variable-length dataset. The
proposed method has a stacked Long Short-Term Memory architecture, including a
new loss regularization. The proposed method solves the existing problems and
improves accuracy. Comparison with the existing architectures show the
improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preliminary draft of the work published in IEEE conference 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DVMNet: Computing Relative Pose for Unseen Objects Beyond Hypotheses <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Zhao, Tong Zhang, Zheng Dang, Mathieu Salzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Determining the relative pose of an object between two images is pivotal to
the success of generalizable object pose estimation. Existing approaches
typically approximate the continuous pose representation with a large number of
discrete pose hypotheses, which incurs a computationally expensive process of
scoring each hypothesis at test time. By contrast, we present a Deep Voxel
Matching Network (DVMNet) that eliminates the need for pose hypotheses and
computes the relative object pose in a single pass. To this end, we map the two
input RGB images, reference and query, to their respective voxelized 3D
representations. We then pass the resulting voxels through a pose estimation
module, where the voxels are aligned and the pose is computed in an end-to-end
fashion by solving a least-squares problem. To enhance robustness, we introduce
a weighted closest voxel algorithm capable of mitigating the impact of noisy
voxels. We conduct extensive experiments on the CO3D, LINEMOD, and Objaverse
datasets, demonstrating that our method delivers more accurate relative pose
estimates for novel objects at a lower computational cost compared to
state-of-the-art methods. Our code is released at:
https://github.com/sailor-z/DVMNet/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reward-Driven Automated Curriculum Learning for Interaction-Aware
  Self-Driving at Unsignalized Intersections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zengqi Peng, Xiao Zhou, Lei Zheng, Yubin Wang, Jun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a reward-driven automated curriculum reinforcement
learning approach for interaction-aware self-driving at unsignalized
intersections, taking into account the uncertainties associated with
surrounding vehicles (SVs). These uncertainties encompass the uncertainty of
SVs' driving intention and also the quantity of SVs. To deal with this problem,
the curriculum set is specifically designed to accommodate a progressively
increasing number of SVs. By implementing an automated curriculum selection
mechanism, the importance weights are rationally allocated across various
curricula, thereby facilitating improved sample efficiency and training
outcomes. Furthermore, the reward function is meticulously designed to guide
the agent towards effective policy exploration. Thus the proposed framework
could proactively address the above uncertainties at unsignalized intersections
by employing the automated curriculum learning technique that progressively
increases task difficulty, and this ensures safe self-driving through effective
interaction with SVs. Comparative experiments are conducted in $Highway\_Env$,
and the results indicate that our approach achieves the highest task success
rate, attains strong robustness to initialization parameters of the curriculum
selection module, and exhibits superior adaptability to diverse situational
configurations at unsignalized intersections. Furthermore, the effectiveness of
the proposed method is validated using the high-fidelity CARLA simulator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LaCE-LHMP: Airflow Modelling-Inspired Long-Term Human Motion Prediction
  By Enhancing Laminar Characteristics in Human Flow <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufei Zhu, Han Fan, Andrey Rudenko, Martin Magnusson, Erik Schaffernicht, Achim J. Lilienthal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-term human motion prediction (LHMP) is essential for safely operating
autonomous robots and vehicles in populated environments. It is fundamental for
various applications, including motion planning, tracking, human-robot
interaction and safety monitoring. However, accurate prediction of human
trajectories is challenging due to complex factors, including, for example,
social norms and environmental conditions. The influence of such factors can be
captured through Maps of Dynamics (MoDs), which encode spatial motion patterns
learned from (possibly scattered and partial) past observations of motion in
the environment and which can be used for data-efficient, interpretable motion
prediction (MoD-LHMP). To address the limitations of prior work, especially
regarding accuracy and sensitivity to anomalies in long-term prediction, we
propose the Laminar Component Enhanced LHMP approach (LaCE-LHMP). Our approach
is inspired by data-driven airflow modelling, which estimates laminar and
turbulent flow components and uses predominantly the laminar components to make
flow predictions. Based on the hypothesis that human trajectory patterns also
manifest laminar flow (that represents predictable motion) and turbulent flow
components (that reflect more unpredictable and arbitrary motion), LaCE-LHMP
extracts the laminar patterns in human dynamics and uses them for human motion
prediction. We demonstrate the superior prediction performance of LaCE-LHMP
through benchmark comparisons with state-of-the-art LHMP methods, offering an
unconventional perspective and a more intuitive understanding of human movement
patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 2024 IEEE International Conference on Robotics and
  Automation (ICRA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From One to Many: How Active Robot Swarm Sizes Influence Human Cognitive
  Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Kaduk, Müge Cavdan, Knut Drewing, Heiko Hamann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In robotics, understanding human interaction with autonomous systems is
crucial for enhancing collaborative technologies. We focus on human-swarm
interaction (HSI), exploring how differently sized groups of active robots
affect operators' cognitive and perceptual reactions over different durations.
We analyze the impact of different numbers of active robots within a 15-robot
swarm on operators' time perception, emotional state, flow experience, and task
difficulty perception. Our findings indicate that managing multiple active
robots when compared to one active robot significantly alters time perception
and flow experience, leading to a faster passage of time and increased flow.
More active robots and extended durations cause increased emotional arousal and
perceived task difficulty, highlighting the interaction between robot the
number of active robots and human cognitive processes. These insights inform
the creation of intuitive human-swarm interfaces and aid in developing swarm
robotic systems aligned with human cognitive structures, enhancing human-robot
collaboration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Motion Generation from Fine-grained Textual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunhang Li, Yansong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of text2motion is to generate motion sequences from given textual
descriptions, where a model should explore the interactions between natural
language instructions and human body movements. While most existing works are
confined to coarse-grained motion descriptions (e.g., "A man squats."),
fine-grained ones specifying movements of relevant body parts are barely
explored. Models trained with coarse texts may not be able to learn mappings
from fine-grained motion-related words to motion primitives, resulting in the
failure in generating motions from unseen descriptions. In this paper, we build
a large-scale language-motion dataset with fine-grained textual descriptions,
FineHumanML3D, by feeding GPT-3.5-turbo with delicate prompts. Accordingly, we
design a new text2motion model, FineMotionDiffuse, which makes full use of
fine-grained textual information. Our experiments show that FineMotionDiffuse
trained on FineHumanML3D acquires good results in quantitative evaluation. We
also find this model can better generate spatially/chronologically composite
motions by learning the implicit mappings from simple descriptions to the
corresponding basic motions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Iterative Active-Inactive Obstacle Classification for Time-Optimal
  Collision Avoidance <span class="chip">IROS24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehmetcan Kaymaz, Nazim Kemal Ure
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-optimal obstacle avoidance is a prevalent problem encountered in various
fields, including robotics and autonomous vehicles, where the task involves
determining a path for a moving vehicle to reach its goal while navigating
around obstacles within its environment. This problem becomes increasingly
challenging as the number of obstacles in the environment rises. We propose an
iterative active-inactive obstacle approach, which involves identifying a
subset of the obstacles as "active", that considers solely the effect of the
"active" obstacles on the path of the moving vehicle. The remaining obstacles
are considered "inactive" and are not considered in the path planning process.
The obstacles are classified as 'active' on the basis of previous findings
derived from prior iterations. This approach allows for a more efficient
calculation of the optimal path by reducing the number of obstacles that need
to be considered. The effectiveness of the proposed method is demonstrated with
two different dynamic models using the various number of obstacles. The results
show that the proposed method is able to find the optimal path in a timely
manner, while also being able to handle a large number of obstacles in the
environment and the constraints on the motion of the object.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is under review in IROS24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLIPSwarm: Generating Drone Shows from Text Prompts with Vision-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Pueyo, Eduardo Montijano, Ana C. Murillo, Mac Schwager
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces CLIPSwarm, a new algorithm designed to automate the
modeling of swarm drone formations based on natural language. The algorithm
begins by enriching a provided word, to compose a text prompt that serves as
input to an iterative approach to find the formation that best matches the
provided word. The algorithm iteratively refines formations of robots to align
with the textual description, employing different steps for "exploration" and
"exploitation". Our framework is currently evaluated on simple formation
targets, limited to contour shapes. A formation is visually represented through
alpha-shape contours and the most representative color is automatically found
for the input word. To measure the similarity between the description and the
visual representation of the formation, we use CLIP [1], encoding text and
images into vectors and assessing their similarity. Subsequently, the algorithm
rearranges the formation to visually represent the word more effectively,
within the given constraints of available drones. Control actions are then
assigned to the drones, ensuring robotic behavior and collision-free movement.
Experimental results demonstrate the system's efficacy in accurately modeling
robot formations from natural language descriptions. The algorithm's
versatility is showcased through the execution of drone shows in photorealistic
simulation with varying shapes. We refer the reader to the supplementary video
for a visual reference of the results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FACT: Fast and Active Coordinate Initialization for Vision-based Drone
  Swarms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Li, Anke Zhao, Yingjian Wang, Ziyi Xu, Xin Zhou, Jinni Zhou, Chao Xu, Fei Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Swarm robots have sparked remarkable developments across a range of fields.
While it is necessary for various applications in swarm robots, a fast and
robust coordinate initialization in vision-based drone swarms remains elusive.
To this end, our paper proposes a complete system to recover a swarm's initial
relative pose on platforms with size, weight, and power (SWaP) constraints. To
overcome limited coverage of field-of-view (FoV), the drones rotate in place to
obtain observations. To tackle the anonymous measurements, we formulate a
non-convex rotation estimation problem and transform it into a semi-definite
programming (SDP) problem, which can steadily obtain global optimal values.
Then we utilize the Hungarian algorithm to recover relative translation and
correspondences between observations and drone identities. To safely acquire
complete observations, we actively search for positions and generate feasible
trajectories to avoid collisions. To validate the practicability of our system,
we conduct experiments on a vision-based drone swarm with only stereo cameras
and inertial measurement units (IMUs) as sensors. The results demonstrate that
the system can robustly get accurate relative poses in real time with limited
onboard computation resources. The source code is released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mobile Robot Localization: a Modular, <span class="highlight-title">Odometry</span>-Improving Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Mozzarelli, Luca Cattaneo, Matteo Corno, Sergio Matteo Savaresi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the number of works published in recent years, vehicle localization
remains an open, challenging problem. While map-based localization and SLAM
algorithms are getting better and better, they remain a single point of failure
in typical localization pipelines. This paper proposes a modular localization
architecture that fuses sensor measurements with the outputs of off-the-shelf
localization algorithms. The fusion filter estimates model uncertainties to
improve odometry in case absolute pose measurements are lost entirely. The
architecture is validated experimentally on a real robot navigating
autonomously proving a reduction of the position error of more than 90% with
respect to the odometrical estimate without uncertainty estimation in a
two-minute navigation period without position measurements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE European Control Conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast-Poly: A Fast Polyhedral Framework For 3D Multi-Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Li, Dedong Liu, Lijun Zhao, Yitao Wu, Xian Wu, Jinghan Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Multi-Object Tracking (MOT) captures stable and comprehensive motion
states of surrounding obstacles, essential for robotic perception. However,
current 3D trackers face issues with accuracy and latency consistency. In this
paper, we propose Fast-Poly, a fast and effective filter-based method for 3D
MOT. Building upon our previous work Poly-MOT, Fast-Poly addresses object
rotational anisotropy in 3D space, enhances local computation densification,
and leverages parallelization technique, improving inference speed and
precision. Fast-Poly is extensively tested on two large-scale tracking
benchmarks with Python implementation. On the nuScenes dataset, Fast-Poly
achieves new state-of-the-art performance with 75.8% AMOTA among all methods
and can run at 34.2 FPS on a personal CPU. On the Waymo dataset, Fast-Poly
exhibits competitive accuracy with 63.6% MOTA and impressive inference speed
(35.5 FPS). The source code is publicly available at
https://github.com/lixiaoyu2000/FastPoly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>1st on the NuScenes Tracking benchmark with 75.8 AMOTA and 34.2 FPS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Navigation Map Generation for Mobile Robots in Urban
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Mozzarelli, Simone Specchia, Matteo Corno, Sergio Matteo Savaresi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental prerequisite for safe and efficient navigation of mobile robots
is the availability of reliable navigation maps upon which trajectories can be
planned. With the increasing industrial interest in mobile robotics, especially
in urban environments, the process of generating navigation maps has become of
particular interest, being a labor intensive step of the deployment process.
Automating this step is challenging and becomes even more arduous when the
perception capabilities are limited by cost considerations. This paper proposes
an algorithm to automatically generate navigation maps using a typical
navigation-oriented sensor setup: a single top-mounted 3D LiDAR sensor. The
proposed method is designed and validated with the urban environment as the
main use case: it is shown to be able to produce accurate maps featuring
different terrain types, positive obstacles of different heights as well as
negative obstacles. The algorithm is applied to data collected in a typical
urban environment with a wheeled inverted pendulum robot, showing its
robustness against localization, perception and dynamic uncertainties. The
generated map is validated against a human-made map.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Caching-Augmented Lifelong Multi-Agent Path Finding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yimin Tang, Zhenghong Yu, Yi Zheng, T. K. Satish Kumar, Jiaoyang Li, Sven Koenig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Agent Path Finding (MAPF), which involves finding collision-free paths
for multiple robots, is crucial in various applications. Lifelong MAPF, where
targets are reassigned to agents as soon as they complete their initial
objectives, offers a more accurate approximation of real-world warehouse
planning. In this paper, we present a novel mechanism named Caching-Augmented
Lifelong MAPF (CAL-MAPF), designed to improve the performance of Lifelong MAPF.
We have developed a new map grid type called cache for temporary item storage
and replacement and designed a lock mechanism for it to improve the stability
of the planning solution. This cache mechanism was evaluated using various
cache replacement policies and a spectrum of input task distributions. We
identified three main factors significantly impacting CAL-MAPF performance
through experimentation: suitable input task distribution, high cache hit rate,
and smooth traffic. Overall, CAL-MAPF has demonstrated potential for
performance improvements in certain task distributions, maps and agent
configurations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unifying Local and Global Multimodal Features for Place Recognition in
  Aliased and Low-Texture Environments <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto García-Hernández, Riccardo Giubilato, Klaus H. Strobl, Javier Civera, Rudolph Triebel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perceptual aliasing and weak textures pose significant challenges to the task
of place recognition, hindering the performance of Simultaneous Localization
and Mapping (SLAM) systems. This paper presents a novel model, called UMF
(standing for Unifying Local and Global Multimodal Features) that 1) leverages
multi-modality by cross-attention blocks between vision and LiDAR features, and
2) includes a re-ranking stage that re-orders based on local feature matching
the top-k candidates retrieved using a global representation. Our experiments,
particularly on sequences captured on a planetary-analogous environment, show
that UMF outperforms significantly previous baselines in those challenging
aliased environments. Since our work aims to enhance the reliability of SLAM in
all situations, we also explore its performance on the widely used RobotCar
dataset, for broader applicability. Code and models are available at
https://github.com/DLR-RM/UMF
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted submission to International Conference on Robotics and
  Automation (ICRA), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Centroidal State Estimation based on the Koopman Embedding for Dynamic
  Legged Locomotion <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahram Khorshidi, Murad Dawood, Maren Bennewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel approach to centroidal state estimation,
which plays a crucial role in predictive model-based control strategies for
dynamic legged locomotion. Our approach uses the Koopman operator theory to
transform the robot's complex nonlinear dynamics into a linear system, by
employing dynamic mode decomposition and deep learning for model construction.
We evaluate both models on their linearization accuracy and capability to
capture both fast and slow dynamic system responses. We then select the most
suitable model for estimation purposes, and integrate it within a moving
horizon estimator. This estimator is formulated as a convex quadratic program,
to facilitate robust, real-time centroidal state estimation. Through extensive
simulation experiments on a quadruped robot executing various dynamic gaits,
our data-driven framework outperforms conventional filtering techniques based
on nonlinear dynamics. Our estimator addresses challenges posed by force/torque
measurement noise in highly dynamic motions and accurately recovers the
centroidal states, demonstrating the adaptability and effectiveness of the
Koopman-based linear representation for complex locomotive behaviors.
Importantly, our model based on dynamic mode decomposition, trained with two
locomotion patterns (trot and jump), successfully estimates the centroidal
states for a different motion (bound) without retraining.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ManiPose: A Comprehensive Benchmark for Pose-aware Object Manipulation
  in Robotics <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaojun Yu, Ce Hao, Junbo Wang, Wenhai Liu, Liu Liu, Yao Mu, Yang You, Hengxu Yan, Cewu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic manipulation in everyday scenarios, especially in unstructured
environments, requires skills in pose-aware object manipulation (POM), which
adapts robots' grasping and handling according to an object's 6D pose.
Recognizing an object's position and orientation is crucial for effective
manipulation. For example, if a mug is lying on its side, it's more effective
to grasp it by the rim rather than the handle. Despite its importance, research
in POM skills remains limited, because learning manipulation skills requires
pose-varying simulation environments and datasets. This paper introduces
ManiPose, a pioneering benchmark designed to advance the study of pose-varying
manipulation tasks. ManiPose encompasses: 1) Simulation environments for POM
feature tasks ranging from 6D pose-specific pick-and-place of single objects to
cluttered scenes, further including interactions with articulated objects. 2) A
comprehensive dataset featuring geometrically consistent and
manipulation-oriented 6D pose labels for 2936 real-world scanned rigid objects
and 100 articulated objects across 59 categories. 3) A baseline for POM,
leveraging the inferencing abilities of LLM (e.g., ChatGPT) to analyze the
relationship between 6D pose and task-specific requirements, offers enhanced
pose-aware grasp prediction and motion planning capabilities. Our benchmark
demonstrates notable advancements in pose estimation, pose-aware manipulation,
and real-robot skill transfer, setting new standards for POM research. We will
open-source the ManiPose benchmark with the final version paper, inviting the
community to engage with our resources, available at our
website:https://sites.google.com/view/manipose.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, submitted to 2024 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeRM: A Generalist Robotic Model with Mixture-of-experts for Quadruped
  Robot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Song, Han Zhao, Pengxiang Ding, Can Cui, Shangke Lyu, Yaning Fan, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task robot learning holds significant importance in tackling diverse
and complex scenarios. However, current approaches are hindered by performance
issues and difficulties in collecting training datasets. In this paper, we
propose GeRM (Generalist Robotic Model). We utilize offline reinforcement
learning to optimize data utilization strategies to learn from both
demonstrations and sub-optimal data, thus surpassing the limitations of human
demonstrations. Thereafter, we employ a transformer-based VLA network to
process multi-modal inputs and output actions. By introducing the
Mixture-of-Experts structure, GeRM allows faster inference speed with higher
whole model capacity, and thus resolves the issue of limited RL parameters,
enhancing model performance in multi-task learning while controlling
computational costs. Through a series of experiments, we demonstrate that GeRM
outperforms other methods across all tasks, while also validating its
efficiency in both training and inference processes. Additionally, we uncover
its potential to acquire emergent skills. Additionally, we contribute the
QUARD-Auto dataset, collected automatically to support our training approach
and foster advancements in multi-task quadruped robot learning. This work
presents a new paradigm for reducing the cost of collecting robot data and
driving progress in the multi-task learning community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MULAN-WC: Multi-Robot Localization Uncertainty-aware Active NeRF with
  Wireless Coordination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiying Wang, Victor Cai, Stephanie Gil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents MULAN-WC, a novel multi-robot 3D reconstruction framework
that leverages wireless signal-based coordination between robots and Neural
Radiance Fields (NeRF). Our approach addresses key challenges in multi-robot 3D
reconstruction, including inter-robot pose estimation, localization uncertainty
quantification, and active best-next-view selection. We introduce a method for
using wireless Angle-of-Arrival (AoA) and ranging measurements to estimate
relative poses between robots, as well as quantifying and incorporating the
uncertainty embedded in the wireless localization of these pose estimates into
the NeRF training loss to mitigate the impact of inaccurate camera poses.
Furthermore, we propose an active view selection approach that accounts for
robot pose uncertainty when determining the next-best views to improve the 3D
reconstruction, enabling faster convergence through intelligent view selection.
Extensive experiments on both synthetic and real-world datasets demonstrate the
effectiveness of our framework in theory and in practice. Leveraging wireless
coordination and localization uncertainty-aware training, MULAN-WC can achieve
high-quality 3d reconstruction which is close to applying the ground truth
camera poses. Furthermore, the quantification of the information gain from a
novel view enables consistent rendering quality improvement with incrementally
captured images by commending the robot the novel view position. Our hardware
experiments showcase the practicality of deploying MULAN-WC to real robotic
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discretizing SO(2)-Equivariant Features for Robotic Kitting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiadong Zhou, Yadan Zeng, Huixu Dong, I-Ming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic kitting has attracted considerable attention in logistics and
industrial settings. However, existing kitting methods encounter challenges
such as low precision and poor efficiency, limiting their widespread
applications. To address these issues, we present a novel kitting framework
that improves both the precision and computational efficiency of complex
kitting tasks. Firstly, our approach introduces a fine-grained orientation
estimation technique in the picking module, significantly enhancing orientation
precision while effectively decoupling computational load from orientation
granularity. This approach combines an SO(2)-equivariant network with a group
discretization operation to preciously predict discrete orientation
distributions. Secondly, we develop the Hand-tool Kitting Dataset (HKD) to
evaluate the performance of different solutions in handling
orientation-sensitive kitting tasks. This dataset comprises a diverse
collection of hand tools and synthetically created kits, which reflects the
complexities encountered in real-world kitting scenarios. Finally, a series of
experiments are conducted to evaluate the performance of the proposed method.
The results demonstrate that our approach offers remarkable precision and
enhanced computational efficiency in robotic kitting tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AMP: Autoregressive Motion Prediction Revisited with Next Token
  Prediction for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaosong Jia, Shaoshuai Shi, Zijun Chen, Li Jiang, Wenlong Liao, Tao He, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As an essential task in autonomous driving (AD), motion prediction aims to
predict the future states of surround objects for navigation. One natural
solution is to estimate the position of other agents in a step-by-step manner
where each predicted time-step is conditioned on both observed time-steps and
previously predicted time-steps, i.e., autoregressive prediction. Pioneering
works like SocialLSTM and MFP design their decoders based on this intuition.
However, almost all state-of-the-art works assume that all predicted time-steps
are independent conditioned on observed time-steps, where they use a single
linear layer to generate positions of all time-steps simultaneously. They
dominate most motion prediction leaderboards due to the simplicity of training
MLPs compared to autoregressive networks.
  In this paper, we introduce the GPT style next token prediction into motion
forecasting. In this way, the input and output could be represented in a
unified space and thus the autoregressive prediction becomes more feasible.
However, different from language data which is composed of homogeneous units
-words, the elements in the driving scene could have complex spatial-temporal
and semantic relations. To this end, we propose to adopt three factorized
attention modules with different neighbors for information aggregation and
different position encoding styles to capture their relations, e.g., encoding
the transformation between coordinate systems for spatial relativity while
adopting RoPE for temporal relativity. Empirically, by equipping with the
aforementioned tailored designs, the proposed method achieves state-of-the-art
performance in the Waymo Open Motion and Waymo Interaction datasets. Notably,
AMP outperforms other recent autoregressive motion prediction methods: MotionLM
and StateTransformer, which demonstrates the effectiveness of the proposed
designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robotics meets Fluid Dynamics: A Characterization of the Induced Airflow
  around a Quadrotor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonard Bauersfeld, Koen Muller, Dominic Ziegler, Filippo Coletti, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread adoption of quadrotors for diverse applications, from
agriculture to public safety, necessitates an understanding of the aerodynamic
disturbances they create. This paper introduces a computationally lightweight
model for estimating the time-averaged magnitude of the induced flow below
quadrotors in hover. Unlike related approaches that rely on expensive
computational fluid dynamics (CFD) simulations or time-consuming empirical
measurements, our method leverages classical theory from turbulent flows. By
analyzing over 9 hours of flight data from drones of varying sizes within a
large motion capture system, we show that the combined flow from all propellers
of the drone is well-approximated by a turbulent jet. Through the use of a
novel normalization and scaling, we have developed and experimentally validated
a unified model that describes the mean velocity field of the induced flow for
different drone sizes. The model accurately describes the far-field airflow in
a very large volume below the drone which is difficult to simulate in CFD. Our
model, which requires only the drone's mass, propeller size, and drone size for
calculations, offers a practical tool for dynamic planning in multi-agent
scenarios, ensuring safer operations near humans and optimizing sensor
placements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7+1 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Workload Estimation for Unknown Tasks: A <span class="highlight-title">Survey</span> of Machine Learning
  Under Distribution Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josh Bhagat Smith, Julie A. Adams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-robot teams involve humans and robots collaborating to achieve tasks
under various environmental conditions. Successful teaming will require robots
to adapt autonomously to a human teammate's internal state. An important
element of such adaptation is the ability to estimate the human teammates'
workload in unknown situations. Existing workload models use machine learning
to model the relationships between physiological metrics and workload; however,
these methods are susceptible to individual differences and are heavily
influenced by other factors. These methods cannot generalize to unknown tasks,
as they rely on standard machine learning approaches that assume data consists
of independent and identically distributed (IID) samples. This assumption does
not necessarily hold for estimating workload for new tasks. A survey of non-IID
machine learning techniques is presented, where commonly used techniques are
evaluated using three criteria: portability, model complexity, and
adaptability. These criteria are used to argue which techniques are most
applicable for estimating workload for unknown tasks in dynamic, real-time
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Robot Connected Fermat Spiral Coverage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingtao Tang, Hang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Multi-Robot Connected Fermat Spiral (MCFS), a novel
algorithmic framework for Multi-Robot Coverage Path Planning (MCPP) that adapts
Connected Fermat Spiral (CFS) from the computer graphics community to
multi-robot coordination for the first time. MCFS uniquely enables the
orchestration of multiple robots to generate coverage paths that contour around
arbitrarily shaped obstacles, a feature that is notably lacking in traditional
methods. Our framework not only enhances area coverage and optimizes task
performance, particularly in terms of makespan, for workspaces rich in
irregular obstacles but also addresses the challenges of path continuity and
curvature critical for non-holonomic robots by generating smooth paths without
decomposing the workspace. MCFS solves MCPP by constructing a graph of isolines
and transforming MCPP into a combinatorial optimization problem, aiming to
minimize the makespan while covering all vertices. Our contributions include
developing a unified CFS version for scalable and adaptable MCPP, extending it
to MCPP with novel optimization techniques for cost reduction and path
continuity and smoothness, and demonstrating through extensive experiments that
MCFS outperforms existing MCPP methods in makespan, path curvature, coverage
ratio, and overlapping ratio. Our research marks a significant step in MCPP,
showcasing the fusion of computer graphics and automated planning principles to
advance the capabilities of multi-robot systems in complex environments. Our
code is available at https://github.com/reso1/MCFS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to ICAPS24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ POLICEd RL: Learning Closed-Loop Robot Control Policies with Provable
  Satisfaction of Hard Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean-Baptiste Bouvier, Kartik Nagpal, Negar Mehr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we seek to learn a robot policy guaranteed to satisfy state
constraints. To encourage constraint satisfaction, existing RL algorithms
typically rely on Constrained Markov Decision Processes and discourage
constraint violations through reward shaping. However, such soft constraints
cannot offer verifiable safety guarantees. To address this gap, we propose
POLICEd RL, a novel RL algorithm explicitly designed to enforce affine hard
constraints in closed-loop with a black-box environment. Our key insight is to
force the learned policy to be affine around the unsafe set and use this affine
region as a repulsive buffer to prevent trajectories from violating the
constraint. We prove that such policies exist and guarantee constraint
satisfaction. Our proposed framework is applicable to both systems with
continuous and discrete state and action spaces and is agnostic to the choice
of the RL training algorithm. Our results demonstrate the capacity of POLICEd
RL to enforce hard constraints in robotic tasks while significantly
outperforming existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Map-Aware Human Pose Prediction for Robot Follow-Ahead 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyuan Jiang, Burak Susam, Jun-Jee Chao, Volkan Isler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the robot follow-ahead task, a mobile robot is tasked to maintain its
relative position in front of a moving human actor while keeping the actor in
sight. To accomplish this task, it is important that the robot understand the
full 3D pose of the human (since the head orientation can be different than the
torso) and predict future human poses so as to plan accordingly. This
prediction task is especially tricky in a complex environment with junctions
and multiple corridors. In this work, we address the problem of forecasting the
full 3D trajectory of a human in such environments. Our main insight is to show
that one can first predict the 2D trajectory and then estimate the full 3D
trajectory by conditioning the estimator on the predicted 2D trajectory. With
this approach, we achieve results comparable or better than the
state-of-the-art methods three times faster. As part of our contribution, we
present a new dataset where, in contrast to existing datasets, the human motion
is in a much larger area than a single room. We also present a complete robot
system that integrates our human pose forecasting network on the mobile robot
to enable real-time robot follow-ahead and present results from real-world
experiments in multiple buildings on campus. Our project page, including
supplementary material and videos, can be found at:
https://qingyuan-jiang.github.io/iros2024_poseForecasting/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Look Before You Leap: Socially Acceptable High-Speed Ground Robot
  Navigation in Crowded Hallways <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lakshay Sharma, Jonathan P. How
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To operate safely and efficiently, autonomous warehouse/delivery robots must
be able to accomplish tasks while navigating in dynamic environments and
handling the large uncertainties associated with the motions/behaviors of other
robots and/or humans. A key scenario in such environments is the hallway
problem, where robots must operate in the same narrow corridor as human traffic
going in one or both directions. Traditionally, robot planners have tended to
focus on socially acceptable behavior in the hallway scenario at the expense of
performance. This paper proposes a planner that aims to address the consequent
"robot freezing problem" in hallways by allowing for "peek-and-pass" maneuvers.
We then go on to demonstrate in simulation how this planner improves robot time
to goal without violating social norms. Finally, we show initial hardware
demonstrations of this planner in the real world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Waypoint-Based Reinforcement Learning for Robot Manipulation Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaunak A. Mehta, Soheil Habibian, Dylan P. Losey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot arms should be able to learn new tasks. One framework here is
reinforcement learning, where the robot is given a reward function that encodes
the task, and the robot autonomously learns actions to maximize its reward.
Existing approaches to reinforcement learning often frame this problem as a
Markov decision process, and learn a policy (or a hierarchy of policies) to
complete the task. These policies reason over hundreds of fine-grained actions
that the robot arm needs to take: e.g., moving slightly to the right or
rotating the end-effector a few degrees. But the manipulation tasks that we
want robots to perform can often be broken down into a small number of
high-level motions: e.g., reaching an object or turning a handle. In this paper
we therefore propose a waypoint-based approach for model-free reinforcement
learning. Instead of learning a low-level policy, the robot now learns a
trajectory of waypoints, and then interpolates between those waypoints using
existing controllers. Our key novelty is framing this waypoint-based setting as
a sequence of multi-armed bandits: each bandit problem corresponds to one
waypoint along the robot's motion. We theoretically show that an ideal solution
to this reformulation has lower regret bounds than standard frameworks. We also
introduce an approximate posterior sampling solution that builds the robot's
motion one waypoint at a time. Results across benchmark simulations and two
real-world experiments suggest that this proposed approach learns new tasks
more quickly than state-of-the-art baselines. See videos here:
https://youtu.be/MMEd-lYfq4Y
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UNO Push: Unified Nonprehensile Object Pushing via Non-Parametric
  Estimation and Model Predictive Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaotian Wang, Kejia Ren, Kaiyu Hang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonprehensile manipulation through precise pushing is an essential skill that
has been commonly challenged by perception and physical uncertainties, such as
those associated with contacts, object geometries, and physical properties. For
this, we propose a unified framework that jointly addresses system modeling,
action generation, and control. While most existing approaches either heavily
rely on a priori system information for analytic modeling, or leverage a large
dataset to learn dynamic models, our framework approximates a system transition
function via non-parametric learning only using a small number of exploratory
actions (ca. 10). The approximated function is then integrated with model
predictive control to provide precise pushing manipulation. Furthermore, we
show that the approximated system transition functions can be robustly
transferred across novel objects while being online updated to continuously
improve the manipulation accuracy. Through extensive experiments on a real
robot platform with a set of novel objects and comparing against a
state-of-the-art baseline, we show that the proposed unified framework is a
light-weight and highly effective approach to enable precise pushing
manipulation all by itself. Our evaluation results illustrate that the system
can robustly ensure millimeter-level precision and can straightforwardly work
on any novel object.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Security in Multi-Robot Systems through Co-Observation
  Planning, Reachability Analysis, and Network Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Yang, Roberto Tron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses security challenges in multi-robot systems (MRS) where
adversaries may compromise robot control, risking unauthorized access to
forbidden areas. We propose a novel multi-robot optimal planning algorithm that
integrates mutual observations and introduces reachability constraints for
enhanced security. This ensures that, even with adversarial movements,
compromised robots cannot breach forbidden regions without missing scheduled
co-observations. The reachability constraint uses ellipsoidal
over-approximation for efficient intersection checking and gradient
computation. To enhance system resilience and tackle feasibility challenges, we
also introduce sub-teams. These cohesive units replace individual robot
assignments along each route, enabling redundant robots to deviate for
co-observations across different trajectories, securing multiple sub-teams
without requiring modifications. We formulate the cross-trajectory
co-observation plan by solving a network flow coverage problem on the
checkpoint graph generated from the original unsecured MRS trajectories,
providing the same security guarantees against plan-deviation attacks. We
demonstrate the effectiveness and robustness of our proposed algorithm, which
significantly strengthens the security of multi-robot systems in the face of
adversarial threats.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures, submitted to IEEE Transactions on Control of
  Network Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Rule-Compliance Path Planner for Lane-Merge Scenarios Based on
  Responsibility-Sensitive Safety <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Lin, Ehsan Javanmardi, Yuze Jiang, Manabu Tsukada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lane merging is one of the critical tasks for self-driving cars, and how to
perform lane-merge maneuvers effectively and safely has become one of the
important standards in measuring the capability of autonomous driving systems.
However, due to the ambiguity in driving intentions and right-of-way issues,
the lane merging process in autonomous driving remains deficient in terms of
maintaining or ceding the right-of-way and attributing liability, which could
result in protracted durations for merging and problems such as trajectory
oscillation. Hence, we present a rule-compliance path planner (RCPP) for
lane-merge scenarios, which initially employs the extended
responsibility-sensitive safety (RSS) to elucidate the right-of-way, followed
by the potential field-based sigmoid planner for path generation. In the
simulation, we have validated the efficacy of the proposed algorithm. The
algorithm demonstrated superior performance over previous approaches in aspects
such as merging time (Saved 72.3%), path length (reduced 53.4%), and
eliminating the trajectory oscillation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated reinforcement learning for robot motion planning with
  zero-shot generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyuan Yuan, Siyuan Xu, Minghui Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers the problem of learning a control policy for robot
motion planning with zero-shot generalization, i.e., no data collection and
policy adaptation is needed when the learned policy is deployed in new
environments. We develop a federated reinforcement learning framework that
enables collaborative learning of multiple learners and a central server, i.e.,
the Cloud, without sharing their raw data. In each iteration, each learner
uploads its local control policy and the corresponding estimated normalized
arrival time to the Cloud, which then computes the global optimum among the
learners and broadcasts the optimal policy to the learners. Each learner then
selects between its local control policy and that from the Cloud for next
iteration. The proposed framework leverages on the derived zero-shot
generalization guarantees on arrival time and safety. Theoretical guarantees on
almost-sure convergence, almost consensus, Pareto improvement and optimality
gap are also provided. Monte Carlo simulation is conducted to evaluate the
proposed framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AMCO: Adaptive Multimodal Coupling of Vision and Proprioception for
  Quadruped Robot Navigation in Outdoor Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Elnoor, Kasun Weerakoon, Adarsh Jagan Sathyamoorthy, Tianrui Guan, Vignesh Rajagopal, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present AMCO, a novel navigation method for quadruped robots that
adaptively combines vision-based and proprioception-based perception
capabilities. Our approach uses three cost maps: general knowledge map;
traversability history map; and current proprioception map; which are derived
from a robot's vision and proprioception data, and couples them to obtain a
coupled traversability cost map for navigation. The general knowledge map
encodes terrains semantically segmented from visual sensing, and represents a
terrain's typically expected traversability. The traversability history map
encodes the robot's recent proprioceptive measurements on a terrain and its
semantic segmentation as a cost map. Further, the robot's present
proprioceptive measurement is encoded as a cost map in the current
proprioception map. As the general knowledge map and traversability history map
rely on semantic segmentation, we evaluate the reliability of the visual
sensory data by estimating the brightness and motion blur of input RGB images
and accordingly combine the three cost maps to obtain the coupled
traversability cost map used for navigation. Leveraging this adaptive coupling,
the robot can depend on the most reliable input modality available. Finally, we
present a novel planner that selects appropriate gaits and velocities for
traversing challenging outdoor environments using the coupled traversability
cost map. We demonstrate AMCO's navigation performance in different real-world
outdoor environments and observe 10.8%-34.9% reduction w.r.t. two stability
metrics, and up to 50% improvement in terms of success rate compared to current
navigation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Contact Model based on Denoising Diffusion to Learn Variable Impedance
  Control for Contact-rich Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masashi Okada, Mayumi Komatsu, Tadahiro Taniguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a novel approach is proposed for learning robot control in
contact-rich tasks such as wiping, by developing Diffusion Contact Model (DCM).
Previous methods of learning such tasks relied on impedance control with
time-varying stiffness tuning by performing Bayesian optimization by
trial-and-error with robots. The proposed approach aims to reduce the cost of
robot operation by predicting the robot contact trajectories from the variable
stiffness inputs and using neural models. However, contact dynamics are
inherently highly nonlinear, and their simulation requires iterative
computations such as convex optimization. Moreover, approximating such
computations by using finite-layer neural models is difficult. To overcome
these limitations, the proposed DCM used the denoising diffusion models that
could simulate the complex dynamics via iterative computations of multi-step
denoising, thus improving the prediction accuracy. Stiffness tuning experiments
conducted in simulated and real environments showed that the DCM achieved
comparable performance to a conventional robot-based optimization method while
reducing the number of robot trials.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "It's Not a Replacement:" Enabling Parent-Robot Collaboration to Support
  In-Home Learning Experiences of Young Children 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui-Ru Ho, Edward Hubbard, Bilge Mutlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning companion robots for young children are increasingly adopted in
informal learning environments. Although parents play a pivotal role in their
children's learning, very little is known about how parents prefer to
incorporate robots into their children's learning activities. We developed
prototype capabilities for a learning companion robot to deliver educational
prompts and responses to parent-child pairs during reading sessions and
conducted in-home user studies involving 10 families with children aged 3-5.
Our data indicates that parents want to work with robots as collaborators to
augment parental activities to foster children's learning, introducing the
notion of parent-robot collaboration. Our findings offer an empirical
understanding of the needs and challenges of parent-child interaction in
informal learning scenarios and design opportunities for integrating a
companion robot into these interactions. We offer insights into how robots
might be designed to facilitate parent-robot collaboration, including parenting
policies, collaboration patterns, and interaction paradigms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quadcopter Team Configurable Motion Guided by a Quadruped 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14029v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14029v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Ghufran, Sourish Tetakayala, Jack Hughes, Aron Wilson, Hossein Rastgoftar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper focuses on modeling and experimental evaluation of a quadcopter
team configurable coordination guided by a single quadruped robot. We consider
the quadcopter team as particles of a two-dimensional deformable body and
propose a two-dimensional affine transformation model for safe and
collision-free configurable coordination of this heterogeneous robotic system.
The proposed affine transformation is decomposed into translation, that is
specified by the quadruped global position, and configurable motion of the
quadcopters, which is determined by a nonsingular Jacobian matrix so that the
quadcopter team can safely navigate a constrained environment while avoiding
collision. We propose two methods to experimentally evaluate the proposed
heterogeneous robot coordination model. The first method measures real
positions of quadcopters, quadruped, and environmental objects all with respect
to the global coordinate system. On the other hand, the second method measures
position with respect to the local coordinate system fixed on the dog robot
which in turn enables safe planning the Jacobian matrix of the quadcopter team
while the world is virtually approached the robotic system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HRI Curriculum for a Liberal Arts Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason R. Wilson, Emily Jensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we discuss the opportunities and challenges of teaching a
human-robot interaction course at an undergraduate liberal arts college. We
provide a sample syllabus adapted from a previous version of a course.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the Designing an Intro to HRI Course Workshop at HRI
  2024 (arXiv:2403.05588)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Crowdsourcing Task Traces for Service Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Porfirio, Allison Sauppé, Maya Cakmak, Aws Albarghouthi, Bilge Mutlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Demonstration is an effective end-user development paradigm for teaching
robots how to perform new tasks. In this paper, we posit that demonstration is
useful not only as a teaching tool, but also as a way to understand and assist
end-user developers in thinking about a task at hand. As a first step toward
gaining this understanding, we constructed a lightweight web interface to
crowdsource step-by-step instructions of common household tasks, leveraging the
imaginations and past experiences of potential end-user developers. As evidence
of the utility of our interface, we deployed the interface on Amazon Mechanical
Turk and collected 207 task traces that span 18 different task categories. We
describe our vision for how these task traces can be operationalized as task
models within end-user development tools and provide a roadmap for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the companion proceedings of the 2023 ACM/IEEE
  International Conference on Human-Robot Interaction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Imitation Learning of Task-Oriented Object Grasping and
  Rearrangement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14000v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14000v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen Cai, Jianfeng Gao, Christoph Pohl, Tamim Asfour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task-oriented object grasping and rearrangement are critical skills for
robots to accomplish different real-world manipulation tasks. However, they
remain challenging due to partial observations of the objects and shape
variations in categorical objects. In this paper, we propose the Multi-feature
Implicit Model (MIMO), a novel object representation that encodes multiple
spatial features between a point and an object in an implicit neural field.
Training such a model on multiple features ensures that it embeds the object
shapes consistently in different aspects, thus improving its performance in
object shape reconstruction from partial observation, shape similarity measure,
and modeling spatial relations between objects. Based on MIMO, we propose a
framework to learn task-oriented object grasping and rearrangement from single
or multiple human demonstration videos. The evaluations in simulation show that
our approach outperforms the state-of-the-art methods for multi- and
single-view observations. Real-world experiments demonstrate the efficacy of
our approach in one- and few-shot imitation learning of manipulation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Goal-Oriented End-User Programming of Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Porfirio, Mark Roberts, Laura M. Hiatt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-user programming (EUP) tools must balance user control with the robot's
ability to plan and act autonomously. Many existing task-oriented EUP tools
enforce a specific level of control, e.g., by requiring that users hand-craft
detailed sequences of actions, rather than offering users the flexibility to
choose the level of task detail they wish to express. We thereby created a
novel EUP system, Polaris, that in contrast to most existing EUP tools, uses
goal predicates as the fundamental building block of programs. Users can
thereby express high-level robot objectives or lower-level checkpoints at their
choosing, while an off-the-shelf task planner fills in any remaining program
detail. To ensure that goal-specified programs adhere to user expectations of
robot behavior, Polaris is equipped with a Plan Visualizer that exposes the
planner's output to the user before runtime. In what follows, we describe our
design of Polaris and its evaluation with 32 human participants. Our results
support the Plan Visualizer's ability to help users craft higher-quality
programs. Furthermore, there are strong associations between user perception of
the robot and Plan Visualizer usage, and evidence that robot familiarity has a
key role in shaping user experience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the proceedings of the 2024 ACM/IEEE International
  Conference on Human-Robot Interaction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open Access NAO (OAN): a ROS2-based software framework for HRI
  applications with the NAO robot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Bono, Kenji Brameld, Luigi D'Alfonso, Giuseppe Fedele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new software framework for HRI experimentation with the
sixth version of the common NAO robot produced by the United Robotics Group.
Embracing the common demand of researchers for better performance and new
features for NAO, the authors took advantage of the ability to run ROS2 onboard
on the NAO to develop a framework independent of the APIs provided by the
manufacturer. Such a system provides NAO with not only the basic skills of a
humanoid robot such as walking and reproducing movements of interest but also
features often used in HRI such as: speech recognition/synthesis, face and
object detention, and the use of Generative Pre-trained Transformer (GPT)
models for conversation. The developed code is therefore configured as a
ready-to-use but also highly expandable and improvable tool thanks to the
possibilities provided by the ROS community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sensory Glove-Based Surgical Robot User Interface <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Borgioli, Ki-Hwan Oh, Alberto Mangano, Alvaro Ducas, Luciano Ambrosini, Federico Pinto, Paula A Lopez, Jessica Cassiani, Milos Zefran, Liaohai Chen, Pier Cristoforo Giulianotti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic surgery has reached a high level of maturity and has become an
integral part of standard surgical care. However, existing surgeon consoles are
bulky and take up valuable space in the operating room, present challenges for
surgical team coordination, and their proprietary nature makes it difficult to
take advantage of recent technological advances, especially in virtual and
augmented reality. One potential area for further improvement is the
integration of modern sensory gloves into robotic platforms, allowing surgeons
to control robotic arms directly with their hand movements intuitively. We
propose one such system that combines an HTC Vive tracker, a Manus Meta Prime 3
XR sensory glove, and God Vision wireless smart glasses. The system controls
one arm of a da Vinci surgical robot. In addition to moving the arm, the
surgeon can use fingers to control the end-effector of the surgical instrument.
Hand gestures are used to implement clutching and similar functions. In
particular, we introduce clutching of the instrument orientation, a
functionality not available in the da Vinci system. The vibrotactile elements
of the glove are used to provide feedback to the user when gesture commands are
invoked. A preliminary evaluation of the system shows that it has excellent
tracking accuracy and allows surgeons to efficiently perform common surgical
training tasks with minimal practice with the new interface; this suggests that
the interface is highly intuitive. The proposed system is inexpensive, allows
rapid prototyping, and opens opportunities for further innovations in the
design of surgical robot interfaces.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, 7 tables, submitted to International Conference
  on Intelligent Robots and Systems (IROS)2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safety-Aware Perception for Autonomous Collision Avoidance in Dynamic
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan M. Bena, Chongbo Zhao, Quan Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous collision avoidance requires accurate environmental perception;
however, flight systems often possess limited sensing capabilities with
field-of-view (FOV) restrictions. To navigate this challenge, we present a
safety-aware approach for online determination of the optimal sensor-pointing
direction $\psi_\text{d}$ which utilizes control barrier functions (CBFs).
First, we generate a spatial density function $\Phi$ which leverages CBF
constraints to map the collision risk of all local coordinates. Then, we
convolve $\Phi$ with an attitude-dependent sensor FOV quality function to
produce the objective function $\Gamma$ which quantifies the total observed
risk for a given pointing direction. Finally, by finding the global optimizer
for $\Gamma$, we identify the value of $\psi_\text{d}$ which maximizes the
perception of risk within the FOV. We incorporate $\psi_\text{d}$ into a
safety-critical flight architecture and conduct a numerical analysis using
multiple simulated mission profiles. Our algorithm achieves a success rate of
$88-96\%$, constituting a $16-29\%$ improvement compared to the best heuristic
methods. We demonstrate the functionality of our approach via a flight
demonstration using the Crazyflie 2.1 micro-quadrotor. Without a priori
obstacle knowledge, the quadrotor follows a dynamic flight path while
simultaneously calculating and tracking $\psi_\text{d}$ to perceive and avoid
two static obstacles with an average computation time of 371 $\mu$s.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmented Reality Demonstrations for Scalable Robot Imitation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Yang, Bryce Ikeda, Gedas Bertasius, Daniel Szafir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot Imitation Learning (IL) is a widely used method for training robots to
perform manipulation tasks that involve mimicking human demonstrations to
acquire skills. However, its practicality has been limited due to its
requirement that users be trained in operating real robot arms to provide
demonstrations. This paper presents an innovative solution: an Augmented
Reality (AR)-assisted framework for demonstration collection, empowering
non-roboticist users to produce demonstrations for robot IL using devices like
the HoloLens 2. Our framework facilitates scalable and diverse demonstration
collection for real-world tasks. We validate our approach with experiments on
three classical robotics tasks: reach, push, and pick-and-place. The real robot
performs each task successfully while replaying demonstrations collected via
AR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ada-NAV: Adaptive Trajectory Length-Based Sample Efficient Policy
  Learning for Robotic Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06192v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06192v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhrij Patel, Kasun Weerakoon, Wesley A. Suttle, Alec Koppel, Brian M. Sadler, Tianyi Zhou, Amrit Singh Bedi, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory length stands as a crucial hyperparameter within reinforcement
learning (RL) algorithms, significantly contributing to the sample inefficiency
in robotics applications. Motivated by the pivotal role trajectory length plays
in the training process, we introduce Ada-NAV, a novel adaptive trajectory
length scheme designed to enhance the training sample efficiency of RL
algorithms in robotic navigation tasks. Unlike traditional approaches that
treat trajectory length as a fixed hyperparameter, we propose to dynamically
adjust it based on the entropy of the underlying navigation policy.
Interestingly, Ada-NAV can be applied to both existing on-policy and off-policy
RL methods, which we demonstrate by empirically validating its efficacy on
three popular RL methods: REINFORCE, Proximal Policy Optimization (PPO), and
Soft Actor-Critic (SAC). We demonstrate through simulated and real-world
robotic experiments that Ada-NAV outperforms conventional methods that employ
constant or randomly sampled trajectory lengths. Specifically, for a fixed
sample budget, Ada-NAV achieves an 18\% increase in navigation success rate, a
20-38\% reduction in navigation path length, and a 9.32\% decrease in elevation
costs. Furthermore, we showcase the versatility of Ada-NAV by integrating it
with the Clearpath Husky robot, illustrating its applicability in complex
outdoor environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ uPLAM: Robust Panoptic Localization and <span class="highlight-title">Mapping</span> Leveraging Perception
  Uncertainties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05840v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05840v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kshitij Sirohi, Daniel Büscher, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The availability of a robust map-based localization system is essential for
the operation of many autonomously navigating vehicles. Since uncertainty is an
inevitable part of perception, it is beneficial for the robustness of the robot
to consider it in typical downstream tasks of navigation stacks. In particular
localization and mapping methods, which in modern systems often employ
convolutional neural networks (CNNs) for perception tasks, require proper
uncertainty estimates. In this work, we present uncertainty-aware Panoptic
Localization and Mapping (uPLAM), which employs pixel-wise uncertainty
estimates for panoptic CNNs as a bridge to fuse modern perception with
classical probabilistic localization and mapping approaches. Beyond the
perception, we introduce an uncertainty-based map aggregation technique to
create accurate panoptic maps, containing surface semantics and landmark
instances. Moreover, we provide cell-wise map uncertainties, and present a
particle filter-based localization method that employs perception
uncertainties. Extensive evaluations show that our proposed incorporation of
uncertainties leads to more accurate maps with reliable uncertainty estimates
and improved localization accuracy. Additionally, we present the Freiburg
Panoptic Driving dataset for evaluating panoptic mapping and localization
methods. We make our code and dataset available at:
\url{http://uplam.cs.uni-freiburg.de}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exposing the Unseen: Exposure Time Emulation for Offline Benchmarking of
  Vision Algorithms <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13139v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13139v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivier Gamache, Jean-Michel Fortin, Matěj Boxan, Maxime Vaidis, François Pomerleau, Philippe Giguère
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Odometry (VO) is one of the fundamental tasks in computer vision for
robotics. However, its performance is deeply affected by High Dynamic Range
(HDR) scenes, omnipresent outdoor. While new Automatic-Exposure (AE) approaches
to mitigate this have appeared, their comparison in a reproducible manner is
problematic. This stems from the fact that the behavior of AE depends on the
environment, and it affects the image acquisition process. Consequently, AE has
traditionally only been benchmarked in an online manner, making the experiments
non-reproducible. To solve this, we propose a new methodology based on an
emulator that can generate images at any exposure time. It leverages BorealHDR,
a unique multi-exposure stereo dataset collected over 10 km, on 55 trajectories
with challenging illumination conditions. Moreover, it includes
lidar-inertial-based global maps with pose estimation for each image frame as
well as Global Navigation Satellite System (GNSS) data, for comparison. We show
that using these images acquired at different exposure times, we can emulate
realistic images, keeping a Root-Mean-Square Error (RMSE) below 1.78 % compared
to ground truth images. To demonstrate the practicality of our approach for
offline benchmarking, we compared three state-of-the-art AE algorithms on key
elements of Visual Simultaneous Localization And Mapping (VSLAM) pipeline,
against four baselines. Consequently, reproducible evaluation of AE is now
possible, speeding up the development of future approaches. Our code and
dataset are available online at this link:
https://github.com/norlab-ulaval/BorealHDR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, submitted to 2024 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigation of Enhanced Inertial Navigation Algorithms by Functional
  Iteration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.05855v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.05855v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyan Jiang, Maoran Zhu, Yuanxin Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The defects of the traditional strapdown inertial navigation algorithms
become well acknowledged and the corresponding enhanced algorithms have been
quite recently proposed trying to mitigate both theoretical and algorithmic
defects. In this paper, the analytical accuracy evaluation of both the
traditional algorithms and the enhanced algorithms is investigated, against the
true reference for the first time enabled by the functional iteration approach
having provable convergence. The analyses by the help of MATLAB Symbolic
Toolbox show that the resultant error orders of all algorithms under
investigation are consistent with those in the existing literatures, and the
enhanced attitude algorithm notably reduces error orders of the traditional
counterpart, while the impact of the enhanced velocity algorithm on error order
reduction is insignificant. Simulation results agree with analyses that the
superiority of the enhanced algorithm over the traditional one in the
body-frame attitude computation scenario diminishes significantly in the entire
inertial navigation computation scenario, while the functional iteration
approach possesses significant accuracy superiority even under sustained lowly
dynamic conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intention-Aware Planner for Robust and Safe Aerial Tracking <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08854v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08854v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuyu Ren, Huan Yu, Jiajun Dai, Zhi Zheng, Jun Meng, Li Xu, Chao Xu, Fei Gao, Yanjun Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous target tracking with quadrotors has wide applications in many
scenarios, such as cinematographic follow-up shooting or suspect chasing.
Target motion prediction is necessary when designing the tracking planner.
However, the widely used constant velocity or constant rotation assumption can
not fully capture the dynamics of the target. The tracker may fail when the
target happens to move aggressively, such as sudden turn or deceleration. In
this paper, we propose an intention-aware planner by additionally considering
the intention of the target to enhance safety and robustness in aerial tracking
applications. Firstly, a designated intention prediction method is proposed,
which combines a user-defined potential assessment function and a state
observation function. A reachable region is generated to specifically evaluate
the turning intentions. Then we design an intention-driven hybrid A* method to
predict the future possible positions for the target. Finally, an
intention-aware optimization approach is designed to generate a
spatial-temporal optimal trajectory, allowing the tracker to perceive
unexpected situations from the target. Benchmark comparisons and real-world
experiments are conducted to validate the performance of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 10 figures, submitted to 2024 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Surfer: Progressive Reasoning with World Models for Robotic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11335v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11335v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengzhen Ren, Kaidong Zhang, Hetao Zheng, Zixuan Li, Yuhang Wen, Fengda Zhu, Mas Ma, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Considering how to make the model accurately understand and follow natural
language instructions and perform actions consistent with world knowledge is a
key challenge in robot manipulation. This mainly includes human fuzzy
instruction reasoning and the following of physical knowledge. Therefore, the
embodied intelligence agent must have the ability to model world knowledge from
training data. However, most existing vision and language robot manipulation
methods mainly operate in less realistic simulator and language settings and
lack explicit modeling of world knowledge. To bridge this gap, we introduce a
novel and simple robot manipulation framework, called Surfer. It is based on
the world model, treats robot manipulation as a state transfer of the visual
scene, and decouples it into two parts: action and scene. Then, the
generalization ability of the model on new instructions and new scenes is
enhanced by explicit modeling of the action and scene prediction in multi-modal
information. In addition to the framework, we also built a robot manipulation
simulator that supports full physics execution based on the MuJoCo physics
engine. It can automatically generate demonstration training data and test
data, effectively reducing labor costs. To conduct a comprehensive and
systematic evaluation of the robot manipulation model in terms of language
understanding and physical execution, we also created a robotic manipulation
benchmark with progressive reasoning tasks, called SeaWave. It contains 4
levels of progressive reasoning tasks and can provide a standardized testing
platform for embedded AI agents in multi-modal environments. On average, Surfer
achieved a success rate of 54.74% on the defined four levels of manipulation
tasks, exceeding the best baseline performance of 47.64%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM3:Large Language Model-based Task and Motion Planning with Motion
  Failure Reasoning <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11552v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11552v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Wang, Muzhi Han, Ziyuan Jiao, Zeyu Zhang, Ying Nian Wu, Song-Chun Zhu, Hangxin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional Task and Motion Planning (TAMP) approaches rely on manually
crafted interfaces connecting symbolic task planning with continuous motion
generation. These domain-specific and labor-intensive modules are limited in
addressing emerging tasks in real-world settings. Here, we present LLM^3, a
novel Large Language Model (LLM)-based TAMP framework featuring a
domain-independent interface. Specifically, we leverage the powerful reasoning
and planning capabilities of pre-trained LLMs to propose symbolic action
sequences and select continuous action parameters for motion planning.
Crucially, LLM^3 incorporates motion planning feedback through prompting,
allowing the LLM to iteratively refine its proposals by reasoning about motion
failure. Consequently, LLM^3 interfaces between task planning and motion
planning, alleviating the intricate design process of handling domain-specific
messages between them. Through a series of simulations in a box-packing domain,
we quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP
problems and the efficiency in selecting action parameters. Ablation studies
underscore the significant contribution of motion failure reasoning to the
success of LLM^3. Furthermore, we conduct qualitative experiments on a physical
manipulator, demonstrating the practical applicability of our approach in
real-world settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2024. Codes available:
  https://github.com/AssassinWS/LLM-TAMP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pseudo-rigid body networks: learning interpretable deformable object
  dynamics from partial observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.07975v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.07975v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shamil Mamedov, A. René Geist, Jan Swevers, Sebastian Trimpe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate prediction of deformable linear object (DLO) dynamics is challenging
if the task at hand requires a human-interpretable yet computationally fast
model. In this work, we draw inspiration from the pseudo-rigid body method
(PRB) and model a DLO as a serial chain of rigid bodies whose internal state is
unrolled through time by a dynamics network. This dynamics network is trained
jointly with a physics-informed encoder which maps observed motion variables to
the DLO's hidden state. To encourage that the state acquires a physically
meaningful representation, we leverage the forward kinematics of the PRB model
as decoder. We demonstrate in robot experiments that the proposed DLO dynamics
model provides physically interpretable predictions from partial observations
while being on par with black-box models regarding prediction accuracy. The
project code is available at: http://tinyurl.com/prb-networks
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Camera Height Doesn't Change: Unsupervised Training for Metric Monocular
  Road-Scene Depth Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04530v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04530v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genki Kinoshita, Ko Nishino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel training method for making any monocular
depth network learn absolute scale and estimate metric road-scene depth just
from regular training data, i.e., driving videos. We refer to this training
framework as StableCamH. The key idea is to leverage cars found on the road as
sources of scale supervision but to incorporate them in the training robustly.
StableCamH detects and estimates the sizes of cars in the frame and aggregates
scale information extracted from them into a camera height estimate whose
consistency across the entire video sequence is enforced as scale supervision.
This realizes robust unsupervised training of any, otherwise scale-oblivious,
monocular depth network to become not only scale-aware but also metric-accurate
without the need for auxiliary sensors and extra supervision. Extensive
experiments on the KITTI and Cityscapes datasets show the effectiveness of
StableCamH and its state-of-the-art accuracy compared with related methods. We
also show that StableCamH enables training on mixed datasets of different
camera heights, which leads to larger-scale training and thus higher
generalization. Metric depth reconstruction is essential in any road-scene
visual modeling, and StableCamH democratizes its deployment by establishing the
means to train any model as a metric depth estimator.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NDT-Map-Code: A 3D global descriptor for real-time loop closure
  detection in <span class="highlight-title">lidar</span> <span class="highlight-title">SLAM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.08221v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.08221v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lizhou Liao, Wenlei Yan, Li Sun, Xinhui Bai, Zhenxing You, Hongyuan Yuan, Chunyun Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Loop-closure detection, also known as place recognition, aiming to identify
previously visited locations, is an essential component of a SLAM system.
Existing research on lidar-based loop closure heavily relies on dense point
cloud and 360 FOV lidars. This paper proposes an out-of-the-box NDT (Normal
Distribution Transform) based global descriptor, NDT-Map-Code, designed for
both on-road driving and underground valet parking scenarios. NDT-Map-Code can
be directly extracted from the NDT map without the need for a dense point
cloud, resulting in excellent scalability and low maintenance cost. The NDT
representation is leveraged to identify representative patterns, which are
further encoded according to their spatial location (bearing, range, and
height). Experimental results on the NIO underground parking lot dataset and
the KITTI dataset demonstrate that our method achieves significantly better
performance compared to the state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-State Fusion: Improving Deep Neural Networks for Autonomous
  Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.06112v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.06112v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elia Cereda, Stefano Bonato, Mirko Nava, Alessandro Giusti, Daniele Palossi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-based deep learning perception fulfills a paramount role in robotics,
facilitating solutions to many challenging scenarios, such as acrobatic
maneuvers of autonomous unmanned aerial vehicles (UAVs) and robot-assisted
high-precision surgery. Control-oriented end-to-end perception approaches,
which directly output control variables for the robot, commonly take advantage
of the robot's state estimation as an auxiliary input. When intermediate
outputs are estimated and fed to a lower-level controller, i.e. mediated
approaches, the robot's state is commonly used as an input only for egocentric
tasks, which estimate physical properties of the robot itself. In this work, we
propose to apply a similar approach for the first time -- to the best of our
knowledge -- to non-egocentric mediated tasks, where the estimated outputs
refer to an external subject. We prove how our general methodology improves the
regression performance of deep convolutional neural networks (CNNs) on a broad
class of non-egocentric 3D pose estimation problems, with minimal computational
cost. By analyzing three highly-different use cases, spanning from grasping
with a robotic arm to following a human subject with a pocket-sized UAV, our
results consistently improve the R\textsuperscript{2} regression metric, up to
+0.51, compared to their stateless baselines. Finally, we validate the in-field
performance of a closed-loop autonomous cm-scale UAV on the human pose
estimation task. Our results show a significant reduction, i.e., 24\% on
average, on the mean absolute error of our stateful CNN, compared to a
State-of-the-Art stateless counterpart.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication in the Journal of
  Intelligent & Robotic Systems. \copyright 2024 Springer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed Pose-graph Optimization with Multi-level Partitioning for
  Collaborative <span class="highlight-title">SLAM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01657v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01657v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cunhao Li, Peng Yi, Guanghui Guo, Yiguang Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The back-end module of Distributed Collaborative Simultaneous Localization
and Mapping (DCSLAM) requires solving a nonlinear Pose Graph Optimization (PGO)
under a distributed setting, also known as SE(d)-synchronization. Most existing
distributed graph optimization algorithms employ a simple sequential
partitioning scheme, which may result in unbalanced subgraph dimensions due to
the different geographic locations of each robot, and hence imposes extra
communication load. Moreover, the performance of current Riemannian
optimization algorithms can be further accelerated. In this letter, we propose
a novel distributed pose graph optimization algorithm combining multi-level
partitioning with an accelerated Riemannian optimization method. Firstly, we
employ the multi-level graph partitioning algorithm to preprocess the naive
pose graph to formulate a balanced optimization problem. In addition, inspired
by the accelerated coordinate descent method, we devise an Improved Riemannian
Block Coordinate Descent (IRBCD) algorithm and the critical point obtained is
globally optimal. Finally, we evaluate the effects of four common graph
partitioning approaches on the correlation of the inter-subgraphs, and discover
that the Highest scheme has the best partitioning performance. Also, we
implement simulations to quantitatively demonstrate that our proposed algorithm
outperforms the state-of-the-art distributed pose graph optimization protocols.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Results and Lessons Learned from Autonomous Driving Transportation
  Services in Airfield, Crowded Indoor, and Urban Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Doosan Baek, Sanghyun Kim, Seung-Woo Seo, Sang-Hyun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous vehicles have been actively investigated over the past few
decades. Several recent works show the potential of autonomous vehicles in
urban environments with impressive experimental results. However, these works
note that autonomous vehicles are still occasionally inferior to expert drivers
in complex scenarios. Furthermore, they do not focus on the possibilities of
autonomous driving transportation services in other areas beyond urban
environments. This paper presents the research results and lessons learned from
autonomous driving transportation services in airfield, crowded indoor, and
urban environments. We discuss how we address several unique challenges in
these diverse environments. We also offer an overview of remaining challenges
that have not received much attention but must be addressed. This paper aims to
share our unique experience to support researchers who are interested in
exploring autonomous driving transportation services in various real-world
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Out-of-Distribution Generalization of Learned Dynamics by
  Learning Pseudometrics and Constraint Manifolds <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12245v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12245v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yating Lin, Glen Chou, Dmitry Berenson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method for improving the prediction accuracy of learned robot
dynamics models on out-of-distribution (OOD) states. We achieve this by
leveraging two key sources of structure often present in robot dynamics: 1)
sparsity, i.e., some components of the state may not affect the dynamics, and
2) physical limits on the set of possible motions, in the form of nonholonomic
constraints. Crucially, we do not assume this structure is known a priori, and
instead learn it from data. We use contrastive learning to obtain a distance
pseudometric that uncovers the sparsity pattern in the dynamics, and use it to
reduce the input space when learning the dynamics. We then learn the unknown
constraint manifold by approximating the normal space of possible motions from
the data, which we use to train a Gaussian process (GP) representation of the
constraint manifold. We evaluate our approach on a physical differential-drive
robot and a simulated quadrotor, showing improved prediction accuracy on OOD
data relative to baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to ICRA 2024, 6 pages + references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LingoQA: Video Question Answering for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ana-Maria Marcu, Long Chen, Jan Hünermann, Alice Karnsund, Benoit Hanotte, Prajwal Chidananda, Saurabh Nair, Vijay Badrinarayanan, Alex Kendall, Jamie Shotton, Elahe Arani, Oleg Sinavski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving has long faced a challenge with public acceptance due to
the lack of explainability in the decision-making process. Video
question-answering (QA) in natural language provides the opportunity for
bridging this gap. Nonetheless, evaluating the performance of Video QA models
has proved particularly tough due to the absence of comprehensive benchmarks.
To fill this gap, we introduce LingoQA, a benchmark specifically for autonomous
driving Video QA. The LingoQA trainable metric demonstrates a 0.95 Spearman
correlation coefficient with human evaluations. We introduce a Video QA dataset
of central London consisting of 419k samples that we release with the paper. We
establish a baseline vision-language model and run extensive ablation studies
to understand its performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Benchmark and dataset are available at
  https://github.com/wayveai/LingoQA/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Designing Library of Skill-Agents for Hardware-Level Reusability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02316v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02316v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Takamatsu, Daichi Saito, Katsushi Ikeuchi, Atsushi Kanehira, Kazuhiro Sasabuchi, Naoki Wake
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To use new robot hardware in a new environment, it is necessary to develop a
control program tailored to that specific robot in that environment.
Considering the reusability of software among robots is crucial to minimize the
effort involved in this process and maximize software reuse across different
robots in different environments. This paper proposes a method to remedy this
process by considering hardware-level reusability, using
Learning-from-observation (LfO) paradigm with a pre-designed skill-agent
library. The LfO framework represents the required actions in
hardware-independent representations, referred to as task models, from
observing human demonstrations, capturing the necessary parameters for the
interaction between the environment and the robot. When executing the desired
actions from the task models, a set of skill agents is employed to convert the
representations into robot commands. This paper focuses on the latter part of
the LfO framework, utilizing the set to generate robot actions from the task
models, and explores a hardware-independent design approach for these skill
agents. These skill agents are described in a hardware-independent manner,
considering the relative relationship between the robot's hand position and the
environment. As a result, it is possible to execute these actions on robots
with different hardware configurations by simply swapping the inverse
kinematics solver. This paper, first, defines a necessary and sufficient
skill-agent set corresponding to cover all possible actions, and considers the
design principles for these skill agents in the library. We provide concrete
examples of such skill agents and demonstrate the practicality of using these
skill agents by showing that the same representations can be executed on two
different robots, Nextage and Fetch, using the proposed skill-agents set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Working Backwards: Learning to Place by Picking <span class="chip">IROS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02352v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02352v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Limoyo, Abhisek Konar, Trevor Ablett, Jonathan Kelly, Francois R. Hogan, Gregory Dudek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present placing via picking (PvP), a method to autonomously collect
real-world demonstrations for a family of placing tasks in which objects must
be manipulated to specific contact-constrained locations. With PvP, we approach
the collection of robotic object placement demonstrations by reversing the
grasping process and exploiting the inherent symmetry of the pick and place
problems. Specifically, we obtain placing demonstrations from a set of grasp
sequences of objects initially located at their target placement locations. Our
system can collect hundreds of demonstrations in contact-constrained
environments without human intervention by combining two modules: tactile
regrasping and compliant control for grasps. We train a policy directly from
visual observations through behavioral cloning, using the
autonomously-collected demonstrations. By doing so, the policy can generalize
to object placement scenarios outside of the training environment without
privileged information (e.g., placing a plate picked up from a table). We
validate our approach in home robotic scenarios that include dishwasher loading
and table setting. Our approach yields robotic placing policies that outperform
policies trained with kinesthetic teaching, both in terms of performance and
data efficiency, while requiring no human supervision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the IEEE/RSJ International Conference on Intelligent
  Robotics and Systems (IROS'24), Abu Dhabi, UAE, Oct 14-18, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PhotoBot: Reference-Guided Interactive Photography via Natural Language <span class="chip">IROS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11061v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11061v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Limoyo, Jimmy Li, Dmitriy Rivkin, Jonathan Kelly, Gregory Dudek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce PhotoBot, a framework for fully automated photo acquisition
based on an interplay between high-level human language guidance and a robot
photographer. We propose to communicate photography suggestions to the user via
reference images that are selected from a curated gallery. We leverage a visual
language model (VLM) and an object detector to characterize the reference
images via textual descriptions and then use a large language model (LLM) to
retrieve relevant reference images based on a user's language query through
text-based reasoning. To correspond the reference image and the observed scene,
we exploit pre-trained features from a vision transformer capable of capturing
semantic similarity across marked appearance variations. Using these features,
we compute pose adjustments for an RGB-D camera by solving a
perspective-n-point (PnP) problem. We demonstrate our approach using a
manipulator equipped with a wrist camera. Our user studies show that photos
taken by PhotoBot are often more aesthetically pleasing than those taken by
users themselves, as measured by human feedback. We also show that PhotoBot can
generalize to other reference sources such as paintings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the IEEE/RSJ International Conference on Intelligent
  Robotics and Systems (IROS'24), Abu Dhabi, UAE, Oct 14-18, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Social Robots for Sleep Health: A Scoping <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04169v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04169v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Nikhil Antony, Mengchi Li, Shu-Han Lin, Junxin Li, Chien-Ming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Poor sleep health is an increasingly concerning public healthcare crisis,
especially when coupled with a dwindling number of health professionals
qualified to combat it. However, there is a growing body of scientific
literature on the use of digital technologies in supporting and sustaining
individuals' healthy sleep habits. Social robots are a relatively recent
technology that has been used to facilitate health care interventions and may
have potential in improving sleep health outcomes, as well. Social robots'
unique characteristics -- such as anthropomorphic physical embodiment or
effective communication methods -- help to engage users and motivate them to
comply with specific interventions, thus improving the interventions' outcomes.
This scoping review aims to evaluate current scientific evidence for employing
social robots in sleep health interventions, identify critical research gaps,
and suggest future directions for developing and using social robots to improve
people's sleep health. Our analysis of the reviewed studies found them limited
due to a singular focus on the older adult population, use of small sample
sizes, limited intervention durations, and other compounding factors.
Nevertheless, the reviewed studies reported several positive outcomes,
highlighting the potential social robots hold in this field. Although our
review found limited clinical evidence for the efficacy of social robots as
purveyors of sleep health interventions, it did elucidate the potential for a
successful future in this domain if current limitations are addressed and more
research is conducted.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic
  Object Rearrangement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15821v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15821v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Chang, Kai Gao, Kowndinya Boyalakuntla, Alex Lee, Baichuan Huang, Harish Udhaya Kumar, Jinjin Yu, Abdeslam Boularias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel approach to the executable semantic object rearrangement
problem. In this challenge, a robot seeks to create an actionable plan that
rearranges objects within a scene according to a pattern dictated by a natural
language description. Unlike existing methods such as StructFormer and
StructDiffusion, which tackle the issue in two steps by first generating poses
and then leveraging a task planner for action plan formulation, our method
concurrently addresses pose generation and action planning. We achieve this
integration using a Language-Guided Monte-Carlo Tree Search (LGMCTS).
Quantitative evaluations are provided on two simulation datasets, and
complemented by qualitative tests with a real robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code and supplementary materials are accessible at
  https://github.com/changhaonan/LG-MCTS</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Pretraining Data Diversity for Self-Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hasan Abed Al Kader Hammoud, Tuhin Das, Fabio Pizzati, Philip Torr, Adel Bibi, Bernard Ghanem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the impact of training with more diverse datasets, characterized
by the number of unique samples, on the performance of self-supervised learning
(SSL) under a fixed computational budget. Our findings consistently demonstrate
that increasing pretraining data diversity enhances SSL performance, albeit
only when the distribution distance to the downstream data is minimal. Notably,
even with an exceptionally large pretraining data diversity achieved through
methods like web crawling or diffusion-generated data, among other ways, the
distribution shift remains a challenge. Our experiments are comprehensive with
seven SSL methods using large-scale datasets such as ImageNet and YFCC100M
amounting to over 200 GPU days. Code and trained models will be available at
https://github.com/hammoudhasan/DiversitySSL .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Editing Massive Concepts in Text-to-Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianwei Xiong, Yue Wu, Enze Xie, Yue Wu, Zhenguo Li, Xihui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models suffer from the risk of generating outdated,
copyrighted, incorrect, and biased content. While previous methods have
mitigated the issues on a small scale, it is essential to handle them
simultaneously in larger-scale real-world scenarios. We propose a two-stage
method, Editing Massive Concepts In Diffusion Models (EMCID). The first stage
performs memory optimization for each individual concept with dual
self-distillation from text alignment loss and diffusion noise prediction loss.
The second stage conducts massive concept editing with multi-layer, closed form
model editing. We further propose a comprehensive benchmark, named ImageNet
Concept Editing Benchmark (ICEB), for evaluating massive concept editing for
T2I models with two subtasks, free-form prompts, massive concept categories,
and extensive evaluation metrics. Extensive experiments conducted on our
proposed benchmark and previous benchmarks demonstrate the superior scalability
of EMCID for editing up to 1,000 concepts, providing a practical approach for
fast adjustment and re-deployment of T2I diffusion models in real-world
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://silentview.github.io/EMCID/ . Code:
  https://github.com/SilentView/EMCID</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Liu, Zeyi Sun, Yuhang Zang, Wei Li, Pan Zhang, Xiaoyi Dong, Yuanjun Xiong, Dahua Lin, Jiaqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CLIP (Contrastive Language-Image Pre-training) uses contrastive learning from
noise image-text pairs to excel at recognizing a wide array of candidates, yet
its focus on broad associations hinders the precision in distinguishing subtle
differences among fine-grained items. Conversely, Multimodal Large Language
Models (MLLMs) excel at classifying fine-grained categories, thanks to their
substantial knowledge from pre-training on web-level corpora. However, the
performance of MLLMs declines with an increase in category numbers, primarily
due to growing complexity and constraints of limited context window size. To
synergize the strengths of both approaches and enhance the few-shot/zero-shot
recognition abilities for datasets characterized by extensive and fine-grained
vocabularies, this paper introduces RAR, a Retrieving And Ranking augmented
method for MLLMs. We initially establish a multi-modal retriever based on CLIP
to create and store explicit memory for different categories beyond the
immediate context window. During inference, RAR retrieves the top-k similar
results from the memory and uses MLLMs to rank and make the final predictions.
Our proposed approach not only addresses the inherent limitations in
fine-grained recognition but also preserves the model's comprehensive knowledge
base, significantly boosting accuracy across a range of vision-language
recognition tasks. Notably, our approach demonstrates a significant improvement
in performance on 5 fine-grained visual recognition benchmarks, 11 few-shot
image recognition datasets, and the 2 object detection datasets under the
zero-shot recognition setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project: https://github.com/Liuziyu77/RAR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RadSplat: Radiance Field-Informed Gaussian Splatting for Robust
  Real-Time Rendering with 900+ FPS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Niemeyer, Fabian Manhardt, Marie-Julie Rakotosaona, Michael Oechsle, Daniel Duckworth, Rama Gosula, Keisuke Tateno, John Bates, Dominik Kaeser, Federico Tombari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in view synthesis and real-time rendering have achieved
photorealistic quality at impressive rendering speeds. While Radiance
Field-based methods achieve state-of-the-art quality in challenging scenarios
such as in-the-wild captures and large-scale scenes, they often suffer from
excessively high compute requirements linked to volumetric rendering. Gaussian
Splatting-based methods, on the other hand, rely on rasterization and naturally
achieve real-time rendering but suffer from brittle optimization heuristics
that underperform on more challenging scenes. In this work, we present
RadSplat, a lightweight method for robust real-time rendering of complex
scenes. Our main contributions are threefold. First, we use radiance fields as
a prior and supervision signal for optimizing point-based scene
representations, leading to improved quality and more robust optimization.
Next, we develop a novel pruning technique reducing the overall point count
while maintaining high quality, leading to smaller and more compact scene
representations with faster inference speeds. Finally, we propose a novel
test-time filtering approach that further accelerates rendering and allows to
scale to larger, house-sized scenes. We find that our method enables
state-of-the-art synthesis of complex captures at 900+ FPS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page at https://m-niemeyer.github.io/radsplat/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning from Models and Data for Visual Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruozhen He, Paola Cascante-Bonilla, Ziyan Yang, Alexander C. Berg, Vicente Ordonez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SynGround, a novel framework that combines data-driven learning
and knowledge transfer from various large-scale pretrained models to enhance
the visual grounding capabilities of a pretrained vision-and-language model.
The knowledge transfer from the models initiates the generation of image
descriptions through an image description generator. These descriptions serve
dual purposes: they act as prompts for synthesizing images through a
text-to-image generator, and as queries for synthesizing text, from which
phrases are extracted using a large language model. Finally, we leverage an
open-vocabulary object detector to generate synthetic bounding boxes for the
synthetic images and texts. We finetune a pretrained vision-and-language model
on this dataset by optimizing a mask-attention consistency objective that
aligns region annotations with gradient-based model explanations. The resulting
model improves the grounding capabilities of an off-the-shelf
vision-and-language model. Particularly, SynGround improves the pointing game
accuracy of ALBEF on the Flickr30k dataset from 79.38% to 87.26%, and on
RefCOCO+ Test A from 69.35% to 79.06% and on RefCOCO+ Test B from 53.77% to
63.67%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://catherine-r-he.github.io/SynGround/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bounding Box Stability against Feature Dropout Reflects Detector
  Generalization across Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yang, Wenhai Wang, Zhe Chen, Jifeng Dai, Liang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bounding boxes uniquely characterize object detection, where a good detector
gives accurate bounding boxes of categories of interest. However, in the
real-world where test ground truths are not provided, it is non-trivial to find
out whether bounding boxes are accurate, thus preventing us from assessing the
detector generalization ability. In this work, we find under feature map
dropout, good detectors tend to output bounding boxes whose locations do not
change much, while bounding boxes of poor detectors will undergo noticeable
position changes. We compute the box stability score (BoS score) to reflect
this stability. Specifically, given an image, we compute a normal set of
bounding boxes and a second set after feature map dropout. To obtain BoS score,
we use bipartite matching to find the corresponding boxes between the two sets
and compute the average Intersection over Union (IoU) across the entire test
set. We contribute to finding that BoS score has a strong, positive correlation
with detection accuracy measured by mean average precision (mAP) under various
test environments. This relationship allows us to predict the accuracy of
detectors on various real-world test sets without accessing test ground truths,
verified on canonical detection tasks such as vehicle detection and pedestrian
detection. Code and data are available at https://github.com/YangYangGirl/BoS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZigMa: Zigzag Mamba Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, Bjorn Ommer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The diffusion model has long been plagued by scalability and quadratic
complexity issues, especially within transformer-based structures. In this
study, we aim to leverage the long sequence modeling capability of a
State-Space Model called Mamba to extend its applicability to visual data
generation. Firstly, we identify a critical oversight in most current
Mamba-based vision methods, namely the lack of consideration for spatial
continuity in the scan scheme of Mamba. Secondly, building upon this insight,
we introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba,
which outperforms Mamba-based baselines and demonstrates improved speed and
memory utilization compared to transformer-based baselines. Lastly, we
integrate Zigzag Mamba with the Stochastic Interpolant framework to investigate
the scalability of the model on large-resolution visual datasets, such as
FacesHQ $1024\times 1024$ and UCF101, MultiModal-CelebA-HQ, and MS COCO
$256\times 256$. Code will be released at https://taohu.me/zigma/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://taohu.me/zigma/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TimeRewind: Rewinding Time with Image-and-Events Video Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingxi Chen, Brandon Y. Feng, Haoming Cai, Mingyang Xie, Christopher Metzler, Cornelia Fermuller, Yiannis Aloimonos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the novel challenge of ``rewinding'' time from a single
captured image to recover the fleeting moments missed just before the shutter
button is pressed. This problem poses a significant challenge in computer
vision and computational photography, as it requires predicting plausible
pre-capture motion from a single static frame, an inherently ill-posed task due
to the high degree of freedom in potential pixel movements. We overcome this
challenge by leveraging the emerging technology of neuromorphic event cameras,
which capture motion information with high temporal resolution, and integrating
this data with advanced image-to-video diffusion models. Our proposed framework
introduces an event motion adaptor conditioned on event camera data, guiding
the diffusion model to generate videos that are visually coherent and
physically grounded in the captured events. Through extensive experimentation,
we demonstrate the capability of our approach to synthesize high-quality videos
that effectively ``rewind'' time, showcasing the potential of combining event
camera technology with generative models. Our work opens new avenues for
research at the intersection of computer vision, computational photography, and
generative modeling, offering a forward-thinking solution to capturing missed
moments and enhancing future consumer cameras and smartphones. Please see the
project page at https://timerewind.github.io/ for video results and code
release.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical NeuroSymbolic Approach for Action Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lauren Okamoto, Paritosh Parmar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Action quality assessment (AQA) applies computer vision to quantitatively
assess the performance or execution of a human action. Current AQA approaches
are end-to-end neural models, which lack transparency and tend to be biased
because they are trained on subjective human judgements as ground-truth. To
address these issues, we introduce a neuro-symbolic paradigm for AQA, which
uses neural networks to abstract interpretable symbols from video data and
makes quality assessments by applying rules to those symbols. We take diving as
the case study. We found that domain experts prefer our system and find it more
informative than purely neural approaches to AQA in diving. Our system also
achieves state-of-the-art action recognition and temporal segmentation, and
automatically generates a detailed report that breaks the dive down into its
elements and provides objective scoring with visual evidence. As verified by a
group of domain experts, this report may be used to assist judges in scoring,
help train judges, and provide feedback to divers. We will open-source all of
our annotated training data and code for ease of reproducibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridge the Modality and Capacity Gaps in Vision-Language Model Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Yi, De-Chuan Zhan, Han-Jia Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) excel in zero-shot image classification by
pairing images with textual category names. The expanding variety of
Pre-Trained VLMs enhances the likelihood of identifying a suitable VLM for
specific tasks. Thus, a promising zero-shot image classification strategy is
selecting the most appropriate Pre-Trained VLM from the VLM Zoo, relying solely
on the text data of the target dataset without access to the dataset's images.
In this paper, we analyze two inherent challenges in assessing the ability of a
VLM in this Language-Only VLM selection: the "Modality Gap" -- the disparity in
VLM's embeddings across two different modalities, making text a less reliable
substitute for images; and the "Capability Gap" -- the discrepancy between the
VLM's overall ranking and its ranking for target dataset, hindering direct
prediction of a model's dataset-specific performance from its general
performance. We propose VLM Selection With gAp Bridging (SWAB) to mitigate the
negative impact of these two gaps. SWAB first adopts optimal transport to
capture the relevance between open-source datasets and target dataset with a
transportation matrix. It then uses this matrix to transfer useful statistics
of VLMs from open-source datasets to the target dataset for bridging those two
gaps and enhancing the VLM's capacity estimation for VLM selection. Experiments
across various VLMs and image classification datasets validate SWAB's
effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DepthFM: Fast Monocular Depth Estimation with Flow Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Gui, Johannes S. Fischer, Ulrich Prestel, Pingchuan Ma, Dmytro Kotovenko, Olga Grebenkova, Stefan Andreas Baumann, Vincent Tao Hu, Björn Ommer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular depth estimation is crucial for numerous downstream vision tasks
and applications. Current discriminative approaches to this problem are limited
due to blurry artifacts, while state-of-the-art generative methods suffer from
slow sampling due to their SDE nature. Rather than starting from noise, we seek
a direct mapping from input image to depth map. We observe that this can be
effectively framed using flow matching, since its straight trajectories through
solution space offer efficiency and high quality. Our study demonstrates that a
pre-trained image diffusion model can serve as an adequate prior for a flow
matching depth model, allowing efficient training on only synthetic data to
generalize to real images. We find that an auxiliary surface normals loss
further improves the depth estimates. Due to the generative nature of our
approach, our model reliably predicts the confidence of its depth estimates. On
standard benchmarks of complex natural scenes, our lightweight approach
exhibits state-of-the-art performance at favorable low computational cost
despite only being trained on little synthetic data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Certified Human Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadhossein Bahari, Saeed Saadatnejad, Amirhossein Asgari Farsangi, Seyed-Mohsen Moosavi-Dezfooli, Alexandre Alahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory prediction plays an essential role in autonomous vehicles. While
numerous strategies have been developed to enhance the robustness of trajectory
prediction models, these methods are predominantly heuristic and do not offer
guaranteed robustness against adversarial attacks and noisy observations. In
this work, we propose a certification approach tailored for the task of
trajectory prediction. To this end, we address the inherent challenges
associated with trajectory prediction, including unbounded outputs, and
mutli-modality, resulting in a model that provides guaranteed robustness.
Furthermore, we integrate a denoiser into our method to further improve the
performance. Through comprehensive evaluations, we demonstrate the
effectiveness of the proposed technique across various baselines and using
standard trajectory prediction datasets. The code will be made available
online: https://s-attack.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Describe-and-Dissect: Interpreting Neurons in Vision Networks with
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Bai, Rahul A. Iyer, Tuomas Oikarinen, Tsui-Wei Weng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose Describe-and-Dissect (DnD), a novel method to
describe the roles of hidden neurons in vision networks. DnD utilizes recent
advancements in multimodal deep learning to produce complex natural language
descriptions, without the need for labeled training data or a predefined set of
concepts to choose from. Additionally, DnD is training-free, meaning we don't
train any new models and can easily leverage more capable general purpose
models in the future. We have conducted extensive qualitative and quantitative
analysis to show that DnD outperforms prior work by providing higher quality
neuron descriptions. Specifically, our method on average provides the highest
quality labels and is more than 2 times as likely to be selected as the best
explanation for a neuron than the best baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Principled Representation Learning from Videos for Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dipendra Misra, Akanksha Saran, Tengyang Xie, Alex Lamb, John Langford
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study pre-training representations for decision-making using video data,
which is abundantly available for tasks such as game agents and software
testing. Even though significant empirical advances have been made on this
problem, a theoretical understanding remains absent. We initiate the
theoretical investigation into principled approaches for representation
learning and focus on learning the latent state representations of the
underlying MDP using video data. We study two types of settings: one where
there is iid noise in the observation, and a more challenging setting where
there is also the presence of exogenous noise, which is non-iid noise that is
temporally correlated, such as the motion of people or cars in the background.
We study three commonly used approaches: autoencoding, temporal contrastive
learning, and forward modeling. We prove upper bounds for temporal contrastive
learning and forward modeling in the presence of only iid noise. We show that
these approaches can learn the latent state and use it to do efficient
downstream RL with polynomial sample complexity. When exogenous noise is also
present, we establish a lower bound result showing that the sample complexity
of learning from video data can be exponentially worse than learning from
action-labeled trajectory data. This partially explains why reinforcement
learning with video pre-training is hard. We evaluate these representational
learning methods in two visual domains, yielding results that are consistent
with our theoretical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 Spotlight Conference Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Practical End-to-End Optical Music Recognition for Pianoform Music 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiří Mayer, Milan Straka, Jan Hajič jr., Pavel Pecina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The majority of recent progress in Optical Music Recognition (OMR) has been
achieved with Deep Learning methods, especially models following the end-to-end
paradigm, reading input images and producing a linear sequence of tokens.
Unfortunately, many music scores, especially piano music, cannot be easily
converted to a linear sequence. This has led OMR researchers to use custom
linearized encodings, instead of broadly accepted structured formats for music
notation. Their diversity makes it difficult to compare the performance of OMR
systems directly. To bring recent OMR model progress closer to useful results:
(a) We define a sequential format called Linearized MusicXML, allowing to train
an end-to-end model directly and maintaining close cohesion and compatibility
with the industry-standard MusicXML format. (b) We create a dev and test set
for benchmarking typeset OMR with MusicXML ground truth based on the OpenScore
Lieder corpus. They contain 1,438 and 1,493 pianoform systems, each with an
image from IMSLP. (c) We train and fine-tune an end-to-end model to serve as a
baseline on the dataset and employ the TEDn metric to evaluate the model. We
also test our model against the recently published synthetic pianoform dataset
GrandStaff and surpass the state-of-the-art results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15+4 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HierCode: A Lightweight Hierarchical Codebook for Zero-shot Chinese Text
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyi Zhang, Yuanzhi Zhu, Dezhi Peng, Peirong Zhang, Zhenhua Yang, Zhibo Yang, Cong Yao, Lianwen Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text recognition, especially for complex scripts like Chinese, faces unique
challenges due to its intricate character structures and vast vocabulary.
Traditional one-hot encoding methods struggle with the representation of
hierarchical radicals, recognition of Out-Of-Vocabulary (OOV) characters, and
on-device deployment due to their computational intensity. To address these
challenges, we propose HierCode, a novel and lightweight codebook that exploits
the innate hierarchical nature of Chinese characters. HierCode employs a
multi-hot encoding strategy, leveraging hierarchical binary tree encoding and
prototype learning to create distinctive, informative representations for each
character. This approach not only facilitates zero-shot recognition of OOV
characters by utilizing shared radicals and structures but also excels in
line-level recognition tasks by computing similarity with visual features, a
notable advantage over existing methods. Extensive experiments across diverse
benchmarks, including handwritten, scene, document, web, and ancient text, have
showcased HierCode's superiority for both conventional and zero-shot Chinese
character or text recognition, exhibiting state-of-the-art performance with
significantly fewer parameters and fast inference speed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Cars meet Drones: Hyperbolic Federated Learning for Source-Free
  Domain Adaptation in Adverse Weather 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giulia Rizzoli, Matteo Caligiuri, Donald Shenaj, Francesco Barbato, Pietro Zanuttigh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Federated Learning (FL), multiple clients collaboratively train a global
model without sharing private data. In semantic segmentation, the Federated
source Free Domain Adaptation (FFreeDA) setting is of particular interest,
where clients undergo unsupervised training after supervised pretraining at the
server side. While few recent works address FL for autonomous vehicles,
intrinsic real-world challenges such as the presence of adverse weather
conditions and the existence of different autonomous agents are still
unexplored. To bridge this gap, we address both problems and introduce a new
federated semantic segmentation setting where both car and drone clients
co-exist and collaborate. Specifically, we propose a novel approach for this
setting which exploits a batch-norm weather-aware strategy to dynamically adapt
the model to the different weather conditions, while hyperbolic space
prototypes are used to align the heterogeneous client representations. Finally,
we introduce FLYAWARE, the first semantic segmentation dataset with adverse
weather data for aerial vehicles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Gait Video Analysis in Neurodegenerative Diseases by Knowledge
  Augmentation in Vision Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diwei Wang, Kun Yuan, Candice Muller, Frédéric Blanc, Nicolas Padoy, Hyewon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a knowledge augmentation strategy for assessing the diagnostic
groups and gait impairment from monocular gait videos. Based on a large-scale
pre-trained Vision Language Model (VLM), our model learns and improves visual,
textual, and numerical representations of patient gait videos, through a
collective learning across three distinct modalities: gait videos,
class-specific descriptions, and numerical gait parameters. Our specific
contributions are two-fold: First, we adopt a knowledge-aware prompt tuning
strategy to utilize the class-specific medical description in guiding the text
prompt learning. Second, we integrate the paired gait parameters in the form of
numerical texts to enhance the numeracy of the textual representation. Results
demonstrate that our model not only significantly outperforms state-of-the-art
(SOTA) in video-based classification tasks but also adeptly decodes the learned
class-specific text features into natural language descriptions using the
vocabulary of quantitative gait parameters. The code and the model will be made
available at our project page.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging High-Resolution Features for Improved Deep Hashing-based
  Image Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aymene Berriche, Mehdi Adjal Zakaria, Riyadh Baghdadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep hashing techniques have emerged as the predominant approach for
efficient image retrieval. Traditionally, these methods utilize pre-trained
convolutional neural networks (CNNs) such as AlexNet and VGG-16 as feature
extractors. However, the increasing complexity of datasets poses challenges for
these backbone architectures in capturing meaningful features essential for
effective image retrieval. In this study, we explore the efficacy of employing
high-resolution features learned through state-of-the-art techniques for image
retrieval tasks. Specifically, we propose a novel methodology that utilizes
High-Resolution Networks (HRNets) as the backbone for the deep hashing task,
termed High-Resolution Hashing Network (HHNet). Our approach demonstrates
superior performance compared to existing methods across all tested benchmark
datasets, including CIFAR-10, NUS-WIDE, MS COCO, and ImageNet. This performance
improvement is more pronounced for complex datasets, which highlights the need
to learn high-resolution features for intricate image retrieval tasks.
Furthermore, we conduct a comprehensive analysis of different HRNet
configurations and provide insights into the optimal architecture for the deep
hashing task
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fu-Yun Wang, Xiaoshi Wu, Zhaoyang Huang, Xiaoyu Shi, Dazhong Shen, Guanglu Song, Yu Liu, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video outpainting is a challenging task, aiming at generating video content
outside the viewport of the input video while maintaining inter-frame and
intra-frame consistency. Existing methods fall short in either generation
quality or flexibility. We introduce MOTIA Mastering Video Outpainting Through
Input-Specific Adaptation, a diffusion-based pipeline that leverages both the
intrinsic data-specific patterns of the source video and the image/video
generative prior for effective outpainting. MOTIA comprises two main phases:
input-specific adaptation and pattern-aware outpainting. The input-specific
adaptation phase involves conducting efficient and effective pseudo outpainting
learning on the single-shot source video. This process encourages the model to
identify and learn patterns within the source video, as well as bridging the
gap between standard generative processes and outpainting. The subsequent
phase, pattern-aware outpainting, is dedicated to the generalization of these
learned patterns to generate outpainting outcomes. Additional strategies
including spatial-aware insertion and noise travel are proposed to better
leverage the diffusion model's generative prior and the acquired video patterns
from source videos. Extensive evaluations underscore MOTIA's superiority,
outperforming existing state-of-the-art methods in widely recognized
benchmarks. Notably, these advancements are achieved without necessitating
extensive, task-specific tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code will be available at https://github.com/G-U-N/Be-Your-Outpainter</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DBA-Fusion: Tightly Integrating Deep Dense Visual Bundle Adjustment with
  Multiple Sensors for Large-Scale Localization and <span class="highlight-title">Mapping</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhou, Xingxing Li, Shengyu Li, Xuanbin Wang, Shaoquan Feng, Yuxuan Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual simultaneous localization and mapping (VSLAM) has broad applications,
with state-of-the-art methods leveraging deep neural networks for better
robustness and applicability. However, there is a lack of research in fusing
these learning-based methods with multi-sensor information, which could be
indispensable to push related applications to large-scale and complex
scenarios. In this paper, we tightly integrate the trainable deep dense bundle
adjustment (DBA) with multi-sensor information through a factor graph. In the
framework, recurrent optical flow and DBA are performed among sequential
images. The Hessian information derived from DBA is fed into a generic factor
graph for multi-sensor fusion, which employs a sliding window and supports
probabilistic marginalization. A pipeline for visual-inertial integration is
firstly developed, which provides the minimum ability of metric-scale
localization and mapping. Furthermore, other sensors (e.g., global navigation
satellite system) are integrated for driftless and geo-referencing
functionality. Extensive tests are conducted on both public datasets and
self-collected datasets. The results validate the superior localization
performance of our approach, which enables real-time dense mapping in
large-scale environments. The code has been made open-source
(https://github.com/GREAT-WHU/DBA-Fusion).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fostc3net:A Lightweight YOLOv5 Based On the Network Structure
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danqing Ma, Shaojie Li, Bo Dang, Hengyi Zang, Xinqi Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transmission line detection technology is crucial for automatic monitoring
and ensuring the safety of electrical facilities. The YOLOv5 series is
currently one of the most advanced and widely used methods for object
detection. However, it faces inherent challenges, such as high computational
load on devices and insufficient detection accuracy. To address these concerns,
this paper presents an enhanced lightweight YOLOv5 technique customized for
mobile devices, specifically intended for identifying objects associated with
transmission lines. The C3Ghost module is integrated into the convolutional
network of YOLOv5 to reduce floating point operations per second (FLOPs) in the
feature channel fusion process and improve feature expression performance. In
addition, a FasterNet module is introduced to replace the c3 module in the
YOLOv5 Backbone. The FasterNet module uses Partial Convolutions to process only
a portion of the input channels, improving feature extraction efficiency and
reducing computational overhead. To address the imbalance between simple and
challenging samples in the dataset and the diversity of aspect ratios of
bounding boxes, the wIoU v3 LOSS is adopted as the loss function. To validate
the performance of the proposed approach, Experiments are conducted on a custom
dataset of transmission line poles. The results show that the proposed model
achieves a 1% increase in detection accuracy, a 13% reduction in FLOPs, and a
26% decrease in model parameters compared to the existing YOLOv5.In the
ablation experiment, it was also discovered that while the Fastnet module and
the CSghost module improved the precision of the original YOLOv5 baseline
model, they caused a decrease in the mAP@.5-.95 metric. However, the
improvement of the wIoUv3 loss function significantly mitigated the decline of
the mAP@.5-.95 metric.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Insight Into the Collocation of Multi-Source Satellite Imagery for
  Multi-Scale Vessel Detection <span class="chip">RSS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tran-Vu La, Minh-Tan Pham, Marco Chini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ship detection from satellite imagery using Deep Learning (DL) is an
indispensable solution for maritime surveillance. However, applying DL models
trained on one dataset to others having differences in spatial resolution and
radiometric features requires many adjustments. To overcome this issue, this
paper focused on the DL models trained on datasets that consist of different
optical images and a combination of radar and optical data. When dealing with a
limited number of training images, the performance of DL models via this
approach was satisfactory. They could improve 5-20% of average precision,
depending on the optical images tested. Likewise, DL models trained on the
combined optical and radar dataset could be applied to both optical and radar
images. Our experiments showed that the models trained on an optical dataset
could be used for radar images, while those trained on a radar dataset offered
very poor scores when applied to optical images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, accepted to IGARSS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MotorEase: Automated Detection of Motor Impairment Accessibility Issues
  in Mobile App UIs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arun Krishnavajjala, SM Hasan Mansur, Justin Jose, Kevin Moran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has begun to examine the potential of automatically finding
and fixing accessibility issues that manifest in software. However, while
recent work makes important progress, it has generally been skewed toward
identifying issues that affect users with certain disabilities, such as those
with visual or hearing impairments. However, there are other groups of users
with different types of disabilities that also need software tooling support to
improve their experience. As such, this paper aims to automatically identify
accessibility issues that affect users with motor-impairments.
  To move toward this goal, this paper introduces a novel approach, called
MotorEase, capable of identifying accessibility issues in mobile app UIs that
impact motor-impaired users. Motor-impaired users often have limited ability to
interact with touch-based devices, and instead may make use of a switch or
other assistive mechanism -- hence UIs must be designed to support both limited
touch gestures and the use of assistive devices. MotorEase adapts computer
vision and text processing techniques to enable a semantic understanding of app
UI screens, enabling the detection of violations related to four popular,
previously unexplored UI design guidelines that support motor-impaired users,
including: (i) visual touch target size, (ii) expanding sections, (iii)
persisting elements, and (iv) adjacent icon visual distance. We evaluate
MotorEase on a newly derived benchmark, called MotorCheck, that contains 555
manually annotated examples of violations to the above accessibility
guidelines, across 1599 screens collected from 70 applications via a mobile app
testing tool. Our experiments illustrate that MotorEase is able to identify
violations with an average accuracy of ~90%, and a false positive rate of less
than 9%, outperforming baseline techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICSE 2024 Research Track, 13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPTNet: An Efficient Alternative Framework for Generalized Category
  Discovery with Spatial Prompt Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjun Wang, Sagar Vaze, Kai Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalized Category Discovery (GCD) aims to classify unlabelled images from
both `seen' and `unseen' classes by transferring knowledge from a set of
labelled `seen' class images. A key theme in existing GCD approaches is
adapting large-scale pre-trained models for the GCD task. An alternate
perspective, however, is to adapt the data representation itself for better
alignment with the pre-trained model. As such, in this paper, we introduce a
two-stage adaptation approach termed SPTNet, which iteratively optimizes model
parameters (i.e., model-finetuning) and data parameters (i.e., prompt
learning). Furthermore, we propose a novel spatial prompt tuning method (SPT)
which considers the spatial property of image data, enabling the method to
better focus on object parts, which can transfer between seen and unseen
classes. We thoroughly evaluate our SPTNet on standard benchmarks and
demonstrate that our method outperforms existing GCD methods. Notably, we find
our method achieves an average accuracy of 61.4% on the SSB, surpassing prior
state-of-the-art methods by approximately 10%. The improvement is particularly
remarkable as our method yields extra parameters amounting to only 0.117% of
those in the backbone architecture. Project page:
https://visual-ai.github.io/sptnet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a conference paper at ICLR 2024; Project page:
  https://visual-ai.github.io/sptnet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DVMNet: Computing Relative Pose for Unseen Objects Beyond Hypotheses <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Zhao, Tong Zhang, Zheng Dang, Mathieu Salzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Determining the relative pose of an object between two images is pivotal to
the success of generalizable object pose estimation. Existing approaches
typically approximate the continuous pose representation with a large number of
discrete pose hypotheses, which incurs a computationally expensive process of
scoring each hypothesis at test time. By contrast, we present a Deep Voxel
Matching Network (DVMNet) that eliminates the need for pose hypotheses and
computes the relative object pose in a single pass. To this end, we map the two
input RGB images, reference and query, to their respective voxelized 3D
representations. We then pass the resulting voxels through a pose estimation
module, where the voxels are aligned and the pose is computed in an end-to-end
fashion by solving a least-squares problem. To enhance robustness, we introduce
a weighted closest voxel algorithm capable of mitigating the impact of noisy
voxels. We conduct extensive experiments on the CO3D, LINEMOD, and Objaverse
datasets, demonstrating that our method delivers more accurate relative pose
estimates for novel objects at a lower computational cost compared to
state-of-the-art methods. Our code is released at:
https://github.com/sailor-z/DVMNet/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Step-Calibrated Diffusion for Biomedical Optical Image Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Lyu, Sung Jik Cha, Cheng Jiang, Asadur Chowdury, Xinhai Hou, Edward Harake, Akhil Kondepudi, Christian Freudiger, Honglak Lee, Todd C. Hollon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality, high-resolution medical imaging is essential for clinical care.
Raman-based biomedical optical imaging uses non-ionizing infrared radiation to
evaluate human tissues in real time and is used for early cancer detection,
brain tumor diagnosis, and intraoperative tissue analysis. Unfortunately,
optical imaging is vulnerable to image degradation due to laser scattering and
absorption, which can result in diagnostic errors and misguided treatment.
Restoration of optical images is a challenging computer vision task because the
sources of image degradation are multi-factorial, stochastic, and
tissue-dependent, preventing a straightforward method to obtain paired
low-quality/high-quality data. Here, we present Restorative Step-Calibrated
Diffusion (RSCD), an unpaired image restoration method that views the image
restoration problem as completing the finishing steps of a diffusion-based
image generation task. RSCD uses a step calibrator model to dynamically
determine the severity of image degradation and the number of steps required to
complete the reverse diffusion process for image restoration. RSCD outperforms
other widely used unpaired image restoration methods on both image quality and
perceptual evaluation metrics for restoring optical images. Medical imaging
experts consistently prefer images restored using RSCD in blinded comparison
experiments and report minimal to no hallucinations. Finally, we show that RSCD
improves performance on downstream clinical imaging tasks, including automated
brain tumor diagnosis and deep tissue imaging. Our code is available at
https://github.com/MLNeurosurg/restorative_step-calibrated_diffusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AUD-TGN: Advancing Action Unit Detection with Temporal Convolution and
  GPT-2 in Wild Audiovisual Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Yu, Zerui Zhang, Zhihong Wei, Gongpeng Zhao, Zhongpeng Cai, Yongqi Wang, Guochen Xie, Jichao Zhu, Wangyuan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging the synergy of both audio data and visual data is essential for
understanding human emotions and behaviors, especially in in-the-wild setting.
Traditional methods for integrating such multimodal information often stumble,
leading to less-than-ideal outcomes in the task of facial action unit
detection. To overcome these shortcomings, we propose a novel approach
utilizing audio-visual multimodal data. This method enhances audio feature
extraction by leveraging Mel Frequency Cepstral Coefficients (MFCC) and Log-Mel
spectrogram features alongside a pre-trained VGGish network. Moreover, this
paper adaptively captures fusion features across modalities by modeling the
temporal relationships, and ultilizes a pre-trained GPT-2 model for
sophisticated context-aware fusion of multimodal information. Our method
notably improves the accuracy of AU detection by understanding the temporal and
contextual nuances of the data, showcasing significant advancements in the
comprehension of intricate scenarios. These findings underscore the potential
of integrating temporal dynamics and contextual interpretation, paving the way
for future research endeavors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retina Vision Transformer (RetinaViT): Introducing Scaled Patches into
  Vision Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Shu, Michael E. Bain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans see low and high spatial frequency components at the same time, and
combine the information from both to form a visual scene. Drawing on this
neuroscientific inspiration, we propose an altered Vision Transformer
architecture where patches from scaled down versions of the input image are
added to the input of the first Transformer Encoder layer. We name this model
Retina Vision Transformer (RetinaViT) due to its inspiration from the human
visual system. Our experiments show that when trained on the ImageNet-1K
dataset with a moderate configuration, RetinaViT achieves a 3.3% performance
improvement over the original ViT. We hypothesize that this improvement can be
attributed to the inclusion of low spatial frequency components in the input,
which improves the ability to capture structural features, and to select and
forward important features to deeper layers. RetinaViT thereby opens doors to
further investigations into vertical pathways and attention patterns.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DanceCamera3D: 3D Camera Movement Synthesis with Music and Dance <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Wang, Jia Jia, Shikun Sun, Haozhe Wu, Rong Han, Zhenyu Li, Di Tang, Jiaqing Zhou, Jiebo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Choreographers determine what the dances look like, while cameramen determine
the final presentation of dances. Recently, various methods and datasets have
showcased the feasibility of dance synthesis. However, camera movement
synthesis with music and dance remains an unsolved challenging problem due to
the scarcity of paired data. Thus, we present DCM, a new multi-modal 3D
dataset, which for the first time combines camera movement with dance motion
and music audio. This dataset encompasses 108 dance sequences (3.2 hours) of
paired dance-camera-music data from the anime community, covering 4 music
genres. With this dataset, we uncover that dance camera movement is
multifaceted and human-centric, and possesses multiple influencing factors,
making dance camera synthesis a more challenging task compared to camera or
dance synthesis alone. To overcome these difficulties, we propose
DanceCamera3D, a transformer-based diffusion model that incorporates a novel
body attention loss and a condition separation strategy. For evaluation, we
devise new metrics measuring camera movement quality, diversity, and dancer
fidelity. Utilizing these metrics, we conduct extensive experiments on our DCM
dataset, providing both quantitative and qualitative evidence showcasing the
effectiveness of our DanceCamera3D model. Code and video demos are available at
https://github.com/Carmenw1203/DanceCamera3D-Official.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ T-Pixel2Mesh: Combining Global and Local Transformer for 3D Mesh
  Generation from a Single Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Zhang, Boyan Jiang, Keke He, Junwei Zhu, Ying Tai, Chengjie Wang, Yinda Zhang, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pixel2Mesh (P2M) is a classical approach for reconstructing 3D shapes from a
single color image through coarse-to-fine mesh deformation. Although P2M is
capable of generating plausible global shapes, its Graph Convolution Network
(GCN) often produces overly smooth results, causing the loss of fine-grained
geometry details. Moreover, P2M generates non-credible features for occluded
regions and struggles with the domain gap from synthetic data to real-world
images, which is a common challenge for single-view 3D reconstruction methods.
To address these challenges, we propose a novel Transformer-boosted
architecture, named T-Pixel2Mesh, inspired by the coarse-to-fine approach of
P2M. Specifically, we use a global Transformer to control the holistic shape
and a local Transformer to progressively refine the local geometry details with
graph-based point upsampling. To enhance real-world reconstruction, we present
the simple yet effective Linear Scale Search (LSS), which serves as prompt
tuning during the input preprocessing. Our experiments on ShapeNet demonstrate
state-of-the-art performance, while results on real-world data show the
generalization capability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Received by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProMamba: Prompt-Mamba for polyp segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhao Xie, Ruofan Liao, Ziang Zhang, Sida Yi, Yuesheng Zhu, Guibo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting polyps through colonoscopy is an important task in medical image
segmentation, which provides significant assistance and reference value for
clinical surgery. However, accurate segmentation of polyps is a challenging
task due to two main reasons. Firstly, polyps exhibit various shapes and
colors. Secondly, the boundaries between polyps and their normal surroundings
are often unclear. Additionally, significant differences between different
datasets lead to limited generalization capabilities of existing methods. To
address these issues, we propose a segmentation model based on Prompt-Mamba,
which incorporates the latest Vision-Mamba and prompt technologies. Compared to
previous models trained on the same dataset, our model not only maintains high
segmentation accuracy on the validation part of the same dataset but also
demonstrates superior accuracy on unseen datasets, exhibiting excellent
generalization capabilities. Notably, we are the first to apply the
Vision-Mamba architecture to polyp segmentation and the first to utilize prompt
technology in a polyp segmentation model. Our model efficiently accomplishes
segmentation tasks, surpassing previous state-of-the-art methods by an average
of 5% across six datasets. Furthermore, we have developed multiple versions of
our model with scaled parameter counts, achieving better performance than
previous models even with fewer parameters. Our code and trained weights will
be released soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures,3 tabels</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recursive Cross-Modal Attention for Multimodal Fusion in Dimensional
  Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        R. Gnana Praveen, Jahangir Alam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal emotion recognition has recently gained a lot of attention since
it can leverage diverse and complementary relationships over multiple
modalities, such as audio, visual, and text. Most state-of-the-art methods for
multimodal fusion rely on recurrent networks or conventional attention
mechanisms that do not effectively leverage the complementary nature of the
modalities. In this paper, we focus on dimensional emotion recognition based on
the fusion of facial, vocal, and text modalities extracted from videos.
Specifically, we propose a recursive cross-modal attention (RCMA) to
effectively capture the complementary relationships across the modalities in a
recursive fashion. The proposed model is able to effectively capture the
inter-modal relationships by computing the cross-attention weights across the
individual modalities and the joint representation of the other two modalities.
To further improve the inter-modal relationships, the obtained attended
features of the individual modalities are again fed as input to the cross-modal
attention to refine the feature representations of the individual modalities.
In addition to that, we have used Temporal convolution networks (TCNs) to
capture the temporal modeling (intra-modal relationships) of the individual
modalities. By deploying the TCNs as well cross-modal attention in a recursive
fashion, we are able to effectively capture both intra- and inter-modal
relationships across the audio, visual, and text modalities. Experimental
results on validation-set videos from the AffWild2 dataset indicate that our
proposed fusion model is able to achieve significant improvement over the
baseline for the sixth challenge of Affective Behavior Analysis in-the-Wild
2024 (ABAW6) competition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2209.09068;
  text overlap with arXiv:2203.14779 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Variational Autoencoder for Low-cost Cardiac Hemodynamics
  Instability Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammod N. I. Suvon, Prasun C. Tripathi, Wenrui Fan, Shuo Zhou, Xianyuan Liu, Samer Alabed, Venet Osmani, Andrew J. Swift, Chen Chen, Haiping Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in non-invasive detection of cardiac hemodynamic
instability (CHDI) primarily focus on applying machine learning techniques to a
single data modality, e.g. cardiac magnetic resonance imaging (MRI). Despite
their potential, these approaches often fall short especially when the size of
labeled patient data is limited, a common challenge in the medical domain.
Furthermore, only a few studies have explored multimodal methods to study CHDI,
which mostly rely on costly modalities such as cardiac MRI and echocardiogram.
In response to these limitations, we propose a novel multimodal variational
autoencoder ($\text{CardioVAE}_\text{X,G}$) to integrate low-cost chest X-ray
(CXR) and electrocardiogram (ECG) modalities with pre-training on a large
unlabeled dataset. Specifically, $\text{CardioVAE}_\text{X,G}$ introduces a
novel tri-stream pre-training strategy to learn both shared and
modality-specific features, thus enabling fine-tuning with both unimodal and
multimodal datasets. We pre-train $\text{CardioVAE}_\text{X,G}$ on a large,
unlabeled dataset of $50,982$ subjects from a subset of MIMIC database and then
fine-tune the pre-trained model on a labeled dataset of $795$ subjects from the
ASPIRE registry. Comprehensive evaluations against existing methods show that
$\text{CardioVAE}_\text{X,G}$ offers promising performance (AUROC $=0.79$ and
Accuracy $=0.77$), representing a significant step forward in non-invasive
prediction of CHDI. Our model also excels in producing fine interpretations of
predictions directly associated with clinical features, thereby supporting
clinical decision-making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning User Embeddings from Human Gaze for Personalised Saliency
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Strohm, Mihai Bâce, Andreas Bulling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reusable embeddings of user behaviour have shown significant performance
improvements for the personalised saliency prediction task. However, prior
works require explicit user characteristics and preferences as input, which are
often difficult to obtain. We present a novel method to extract user embeddings
from pairs of natural images and corresponding saliency maps generated from a
small amount of user-specific eye tracking data. At the core of our method is a
Siamese convolutional neural encoder that learns the user embeddings by
contrasting the image and personal saliency map pairs of different users.
Evaluations on two public saliency datasets show that the generated embeddings
have high discriminative power, are effective at refining universal saliency
maps to the individual users, and generalise well across users and images.
Finally, based on our model's ability to encode individual user
characteristics, our work points towards other applications that can benefit
from reusable embeddings of gaze behaviour.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZoDi: Zero-Shot Domain Adaptation with Diffusion-Based Image Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiroki Azuma, Yusuke Matsui, Atsuto Maki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models achieve high accuracy in segmentation tasks among
others, yet domain shift often degrades the models' performance, which can be
critical in real-world scenarios where no target images are available. This
paper proposes a zero-shot domain adaptation method based on diffusion models,
called ZoDi, which is two-fold by the design: zero-shot image transfer and
model adaptation. First, we utilize an off-the-shelf diffusion model to
synthesize target-like images by transferring the domain of source images to
the target domain. In this we specifically try to maintain the layout and
content by utilising layout-to-image diffusion models with stochastic
inversion. Secondly, we train the model using both source images and
synthesized images with the original segmentation maps while maximizing the
feature similarity of images from the two domains to learn domain-robust
representations. Through experiments we show benefits of ZoDi in the task of
image segmentation over state-of-the-art methods. It is also more applicable
than existing CLIP-based methods because it assumes no specific backbone or
models, and it enables to estimate the model's performance without target
images by inspecting generated images. Our implementation will be publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-Point Learning and Refining for Category-Agnostic Pose Estimation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Chen, Jiebin Yan, Yuming Fang, Li Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Category-agnostic pose estimation (CAPE) aims to predict keypoints for
arbitrary classes given a few support images annotated with keypoints. Existing
methods only rely on the features extracted at support keypoints to predict or
refine the keypoints on query image, but a few support feature vectors are
local and inadequate for CAPE. Considering that human can quickly perceive
potential keypoints of arbitrary objects, we propose a novel framework for CAPE
based on such potential keypoints (named as meta-points). Specifically, we
maintain learnable embeddings to capture inherent information of various
keypoints, which interact with image feature maps to produce meta-points
without any support. The produced meta-points could serve as meaningful
potential keypoints for CAPE. Due to the inevitable gap between inherency and
annotation, we finally utilize the identities and details offered by support
keypoints to assign and refine meta-points to desired keypoints in query image.
In addition, we propose a progressive deformable point decoder and a slacked
regression loss for better prediction and supervision. Our novel framework not
only reveals the inherency of keypoints but also outperforms existing methods
of CAPE. Comprehensive experiments and in-depth studies on large-scale MP-100
dataset demonstrate the effectiveness of our framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ H-vmunet: High-order Vision Mamba UNet for Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renkai Wu, Yinghao Liu, Pengchen Liang, Qing Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of medical image segmentation, variant models based on
Convolutional Neural Networks (CNNs) and Visual Transformers (ViTs) as the base
modules have been very widely developed and applied. However, CNNs are often
limited in their ability to deal with long sequences of information, while the
low sensitivity of ViTs to local feature information and the problem of
secondary computational complexity limit their development. Recently, the
emergence of state-space models (SSMs), especially 2D-selective-scan (SS2D),
has had an impact on the longtime dominance of traditional CNNs and ViTs as the
foundational modules of visual neural networks. In this paper, we extend the
adaptability of SS2D by proposing a High-order Vision Mamba UNet (H-vmunet) for
medical image segmentation. Among them, the proposed High-order
2D-selective-scan (H-SS2D) progressively reduces the introduction of redundant
information during SS2D operations through higher-order interactions. In
addition, the proposed Local-SS2D module improves the learning ability of local
features of SS2D at each order of interaction. We conducted comparison and
ablation experiments on three publicly available medical image datasets
(ISIC2017, Spleen, and CVC-ClinicDB), and the results all demonstrate the
strong competitiveness of H-vmunet in medical image segmentation tasks. The
code is available from https://github.com/wurenkai/H-vmunet .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VL-Mamba: Exploring State Space Models for Multimodal Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanyuan Qiao, Zheng Yu, Longteng Guo, Sihan Chen, Zijia Zhao, Mingzhen Sun, Qi Wu, Jing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have attracted widespread interest
and have rich applications. However, the inherent attention mechanism in its
Transformer structure requires quadratic complexity and results in expensive
computational overhead. Therefore, in this work, we propose VL-Mamba, a
multimodal large language model based on state space models, which have been
shown to have great potential for long-sequence modeling with fast inference
and linear scaling in sequence length. Specifically, we first replace the
transformer-based backbone language model such as LLama or Vicuna with the
pre-trained Mamba language model. Then, we empirically explore how to
effectively apply the 2D vision selective scan mechanism for multimodal
learning and the combinations of different vision encoders and variants of
pretrained Mamba language models. The extensive experiments on diverse
multimodal benchmarks with competitive performance show the effectiveness of
our proposed VL-Mamba and demonstrate the great potential of applying state
space models for multimodal learning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReGround: Improving Textual and Spatial Grounding at No Cost 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuseung Lee, Minhyuk Sung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When an image generation process is guided by both a text prompt and spatial
cues, such as a set of bounding boxes, do these elements work in harmony, or
does one dominate the other? Our analysis of a pretrained image diffusion model
that integrates gated self-attention into the U-Net reveals that spatial
grounding often outweighs textual grounding due to the sequential flow from
gated self-attention to cross-attention. We demonstrate that such bias can be
significantly mitigated without sacrificing accuracy in either grounding by
simply rewiring the network architecture, changing from sequential to parallel
for gated self-attention and cross-attention. This surprisingly simple yet
effective solution does not require any fine-tuning of the network but
significantly reduces the trade-off between the two groundings. Our experiments
demonstrate significant improvements from the original GLIGEN to the rewired
version in the trade-off between textual grounding and spatial grounding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://re-ground.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging feature communication in federated learning for remote
  sensing image classification <span class="chip">RSS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anh-Kiet Duong, Hoàng-Ân Lê, Minh-Tan Pham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of Federated Learning (FL) applied to remote sensing image
classification, this study introduces and assesses several innovative
communication strategies. Our exploration includes feature-centric
communication, pseudo-weight amalgamation, and a combined method utilizing both
weights and features. Experiments conducted on two public scene classification
datasets unveil the effectiveness of these strategies, showcasing accelerated
convergence, heightened privacy, and reduced network information exchange. This
research provides valuable insights into the implications of feature-centric
communication in FL, offering potential applications tailored for remote
sensing scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, to appear in IGARSS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Portrait4D-v2: Pseudo Multi-View Data Creates Better 4D Head Synthesizer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Deng, Duomin Wang, Baoyuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel learning approach for feed-forward one-shot
4D head avatar synthesis. Different from existing methods that often learn from
reconstructing monocular videos guided by 3DMM, we employ pseudo multi-view
videos to learn a 4D head synthesizer in a data-driven manner, avoiding
reliance on inaccurate 3DMM reconstruction that could be detrimental to the
synthesis performance. The key idea is to first learn a 3D head synthesizer
using synthetic multi-view images to convert monocular real videos into
multi-view ones, and then utilize the pseudo multi-view videos to learn a 4D
head synthesizer via cross-view self-reenactment. By leveraging a simple vision
transformer backbone with motion-aware cross-attentions, our method exhibits
superior performance compared to previous methods in terms of reconstruction
fidelity, geometry consistency, and motion control accuracy. We hope our method
offers novel insights into integrating 3D priors with 2D supervisions for
improved 4D head avatar creation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://yudeng.github.io/Portrait4D-v2/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Find n' Propagate: Open-Vocabulary 3D Object Detection in Urban
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Djamahl Etchegaray, Zi Huang, Tatsuya Harada, Yadan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we tackle the limitations of current LiDAR-based 3D object
detection systems, which are hindered by a restricted class vocabulary and the
high costs associated with annotating new object classes. Our exploration of
open-vocabulary (OV) learning in urban environments aims to capture novel
instances using pre-trained vision-language models (VLMs) with multi-sensor
data. We design and benchmark a set of four potential solutions as baselines,
categorizing them into either top-down or bottom-up approaches based on their
input data strategies. While effective, these methods exhibit certain
limitations, such as missing novel objects in 3D box estimation or applying
rigorous priors, leading to biases towards objects near the camera or of
rectangular geometries. To overcome these limitations, we introduce a universal
\textsc{Find n' Propagate} approach for 3D OV tasks, aimed at maximizing the
recall of novel objects and propagating this detection capability to more
distant areas thereby progressively capturing more. In particular, we utilize a
greedy box seeker to search against 3D novel boxes of varying orientations and
depth in each generated frustum and ensure the reliability of newly identified
boxes by cross alignment and density ranker. Additionally, the inherent bias
towards camera-proximal objects is alleviated by the proposed remote simulator,
which randomly diversifies pseudo-labeled novel instances in the self-training
process, combined with the fusion of base samples in the memory bank. Extensive
experiments demonstrate a 53% improvement in novel recall across diverse OV
settings, VLMs, and 3D detectors. Notably, we achieve up to a 3.97-fold
increase in Average Precision (AP) for novel object classes. The source code is
made available in the supplementary material.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ground-A-Score: Scaling Up the Score Distillation for Multi-Attribute
  Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hangeol Chang, Jinho Chang, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advancements in text-to-image diffusion models facilitating
various image editing techniques, complex text prompts often lead to an
oversight of some requests due to a bottleneck in processing text information.
To tackle this challenge, we present Ground-A-Score, a simple yet powerful
model-agnostic image editing method by incorporating grounding during score
distillation. This approach ensures a precise reflection of intricate prompt
requirements in the editing outcomes, taking into account the prior knowledge
of the object locations within the image. Moreover, the selective application
with a new penalty coefficient and contrastive loss helps to precisely target
editing areas while preserving the integrity of the objects in the source
image. Both qualitative assessments and quantitative analyses confirm that
Ground-A-Score successfully adheres to the intricate details of extended and
multifaceted prompts, ensuring high-quality outcomes that respect the original
image attributes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diversity-aware Channel Pruning for StyleGAN Compression <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiwoo Chung, Sangeek Hyun, Sang-Heon Shim, Jae-Pil Heo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  StyleGAN has shown remarkable performance in unconditional image generation.
However, its high computational cost poses a significant challenge for
practical applications. Although recent efforts have been made to compress
StyleGAN while preserving its performance, existing compressed models still lag
behind the original model, particularly in terms of sample diversity. To
overcome this, we propose a novel channel pruning method that leverages varying
sensitivities of channels to latent vectors, which is a key factor in sample
diversity. Specifically, by assessing channel importance based on their
sensitivities to latent vector perturbations, our method enhances the diversity
of samples in the compressed model. Since our method solely focuses on the
channel pruning stage, it has complementary benefits with prior training
schemes without additional training cost. Extensive experiments demonstrate
that our method significantly enhances sample diversity across various
datasets. Moreover, in terms of FID scores, our method not only surpasses
state-of-the-art by a large margin but also achieves comparable scores with
only half training iterations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024. Project page:
  https://jiwoogit.github.io/DCP-GAN_site</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Next day fire prediction via semantic segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantinos Alexis, Stella Girtsou, Alexis Apostolakis, Giorgos Giannopoulos, Charalampos Kontoes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present a deep learning pipeline for next day fire
prediction. The next day fire prediction task consists in learning models that
receive as input the available information for an area up until a certain day,
in order to predict the occurrence of fire for the next day. Starting from our
previous problem formulation as a binary classification task on instances
(daily snapshots of each area) represented by tabular feature vectors, we
reformulate the problem as a semantic segmentation task on images; there, each
pixel corresponds to a daily snapshot of an area, while its channels represent
the formerly tabular training features. We demonstrate that this problem
formulation, built within a thorough pipeline achieves state of the art
results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in MACLEAN@ECML/PKDD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What explains the success of cross-modal fine-tuning with ORCA? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paloma García-de-Herreros, Vagrant Gautam, Philipp Slusallek, Dietrich Klakow, Marius Mosbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ORCA (Shen et al., 2023) is a recent technique for cross-modal fine-tuning,
i.e., applying pre-trained transformer models to modalities beyond their
training data. The technique consists primarily of training an embedder and
fine-tuning the embedder and model. Despite its high performance on a variety
of downstream tasks, we do not understand precisely how each of these
components contribute to ORCA's success. Therefore, we run a series of
ablations and find that embedder training does not help 2D tasks at all,
contrary to what the original paper posits. In 1D tasks, some amount of
embedder training is necessary but more is not better. In 4 out of 6 datasets
we experiment with, it is model fine-tuning that makes the biggest difference.
Through our ablations and baselines, we contribute a better understanding of
the individual components of ORCA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IDAdapter: Learning Mixed Features for Tuning-Free Personalization of
  Text-to-Image Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siying Cui, Jiankang Deng, Jia Guo, Xiang An, Yongle Zhao, Xinyu Wei, Ziyong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging Stable Diffusion for the generation of personalized portraits has
emerged as a powerful and noteworthy tool, enabling users to create
high-fidelity, custom character avatars based on their specific prompts.
However, existing personalization methods face challenges, including test-time
fine-tuning, the requirement of multiple input images, low preservation of
identity, and limited diversity in generated outcomes. To overcome these
challenges, we introduce IDAdapter, a tuning-free approach that enhances the
diversity and identity preservation in personalized image generation from a
single face image. IDAdapter integrates a personalized concept into the
generation process through a combination of textual and visual injections and a
face identity loss. During the training phase, we incorporate mixed features
from multiple reference images of a specific identity to enrich
identity-related content details, guiding the model to generate images with
more diverse styles, expressions, and angles compared to previous works.
Extensive evaluations demonstrate the effectiveness of our method, achieving
both diversity and identity fidelity in generated images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compress3D: a Compressed Latent Space for 3D Generation from a Single
  Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Zhang, Tianyu Yang, Yu Li, Lei Zhang, Xi Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D generation has witnessed significant advancements, yet efficiently
producing high-quality 3D assets from a single image remains challenging. In
this paper, we present a triplane autoencoder, which encodes 3D models into a
compact triplane latent space to effectively compress both the 3D geometry and
texture information. Within the autoencoder framework, we introduce a 3D-aware
cross-attention mechanism, which utilizes low-resolution latent representations
to query features from a high-resolution 3D feature volume, thereby enhancing
the representation capacity of the latent space. Subsequently, we train a
diffusion model on this refined latent space. In contrast to solely relying on
image embedding for 3D generation, our proposed method advocates for the
simultaneous utilization of both image embedding and shape embedding as
conditions. Specifically, the shape embedding is estimated via a diffusion
prior model conditioned on the image embedding. Through comprehensive
experiments, we demonstrate that our method outperforms state-of-the-art
algorithms, achieving superior performance while requiring less training data
and time. Our approach enables the generation of high-quality 3D assets in
merely 7 seconds on a single A100 GPU.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REAL: Representation Enhanced Analytic Learning for Exemplar-free
  Class-incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Run He, Huiping Zhuang, Di Fang, Yizhu Chen, Kai Tong, Cen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exemplar-free class-incremental learning (EFCIL) aims to mitigate
catastrophic forgetting in class-incremental learning without available
historical data. Compared with its counterpart (replay-based CIL) that stores
historical samples, the EFCIL suffers more from forgetting issues under the
exemplar-free constraint. In this paper, inspired by the recently developed
analytic learning (AL) based CIL, we propose a representation enhanced analytic
learning (REAL) for EFCIL. The REAL constructs a dual-stream base pretraining
(DS-BPT) and a representation enhancing distillation (RED) process to enhance
the representation of the extractor. The DS-BPT pretrains model in streams of
both supervised learning and self-supervised contrastive learning (SSCL) for
base knowledge extraction. The RED process distills the supervised knowledge to
the SSCL pretrained backbone and facilitates a subsequent AL-basd CIL that
converts the CIL to a recursive least-square problem. Our method addresses the
issue of insufficient discriminability in representations of unseen data caused
by a frozen backbone in the existing AL-based CIL. Empirical results on various
datasets including CIFAR-100, ImageNet-100 and ImageNet-1k, demonstrate that
our REAL outperforms the state-of-the-arts in EFCIL, and achieves comparable or
even more superior performance compared with the replay-based methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Motion Generation from Fine-grained Textual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunhang Li, Yansong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of text2motion is to generate motion sequences from given textual
descriptions, where a model should explore the interactions between natural
language instructions and human body movements. While most existing works are
confined to coarse-grained motion descriptions (e.g., "A man squats."),
fine-grained ones specifying movements of relevant body parts are barely
explored. Models trained with coarse texts may not be able to learn mappings
from fine-grained motion-related words to motion primitives, resulting in the
failure in generating motions from unseen descriptions. In this paper, we build
a large-scale language-motion dataset with fine-grained textual descriptions,
FineHumanML3D, by feeding GPT-3.5-turbo with delicate prompts. Accordingly, we
design a new text2motion model, FineMotionDiffuse, which makes full use of
fine-grained textual information. Our experiments show that FineMotionDiffuse
trained on FineHumanML3D acquires good results in quantitative evaluation. We
also find this model can better generate spatially/chronologically composite
motions by learning the implicit mappings from simple descriptions to the
corresponding basic motions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What if...?: Counterfactual Inception to Mitigate Hallucination Effects
  in Large Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junho Kim, Yeon Ju Kim, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a way of enhancing the reliability of Large Multimodal
Models (LMMs) in addressing hallucination effects, where models generate
incorrect or unrelated responses. Without additional instruction tuning
paradigm, we introduce Counterfactual Inception, a novel method that implants
counterfactual thoughts into LMMs using carefully chosen, misaligned
counterfactual keywords. This method is grounded in the concept of
counterfactual thinking, a cognitive process where humans consider alternative
realities and outcomes. By applying this human-like reasoning mechanism to
LMMs, we aim to reduce hallucination effects and improve the models'
trustworthiness. We also propose Dual-modality Verification Process (DVP), a
rigorous framework for selecting optimal counterfactual keywords to trigger
counterfactual thinking into LMMs, concurrently considering visual and
linguistic context. Our extensive experiments across various LMMs, including
both open-source and proprietary models, corroborate that our method
significantly mitigates hallucination phenomena across different datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review, code available:
  https://github.com/IVY-LVLM/Counterfactual-Inception</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scale Decoupled Distillation <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shicai Wei Chunbo Luo Yang Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Logit knowledge distillation attracts increasing attention due to its
practicality in recent studies. However, it often suffers inferior performance
compared to the feature knowledge distillation. In this paper, we argue that
existing logit-based methods may be sub-optimal since they only leverage the
global logit output that couples multiple semantic knowledge. This may transfer
ambiguous knowledge to the student and mislead its learning. To this end, we
propose a simple but effective method, i.e., Scale Decoupled Distillation
(SDD), for logit knowledge distillation. SDD decouples the global logit output
into multiple local logit outputs and establishes distillation pipelines for
them. This helps the student to mine and inherit fine-grained and unambiguous
logit knowledge. Moreover, the decoupled knowledge can be further divided into
consistent and complementary logit knowledge that transfers the semantic
information and sample ambiguity, respectively. By increasing the weight of
complementary parts, SDD can guide the student to focus more on ambiguous
samples, improving its discrimination ability. Extensive experiments on several
benchmark datasets demonstrate the effectiveness of SDD for wide
teacher-student pairs, especially in the fine-grained classification task. Code
is available at: https://github.com/shicaiwei123/SDD-CVPR2024
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2024 10 pages 6figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-confidence pseudo-labels for domain adaptation in COVID-19
  detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Turnbull, Simon Mutch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper outlines our submission for the 4th COV19D competition as part of
the `Domain adaptation, Explainability, Fairness in AI for Medical Image
Analysis' (DEF-AI-MIA) workshop at the Computer Vision and Pattern Recognition
Conference (CVPR). The competition consists of two challenges. The first is to
train a classifier to detect the presence of COVID-19 from over one thousand CT
scans from the COV19-CT-DB database. The second challenge is to perform domain
adaptation by taking the dataset from Challenge 1 and adding a small number of
scans (some annotated and other not) for a different distribution. We
preprocessed the CT scans to segment the lungs, and output volumes with the
lungs individually and together. We then trained 3D ResNet and Swin Transformer
models on these inputs. We annotated the unlabeled CT scans using an ensemble
of these models and chose the high-confidence predictions as pseudo-labels for
fine-tuning. This resulted in a best cross-validation mean F1 score of 93.39\%
for Challenge 1 and a mean F1 score of 92.15 for Challenge 2.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinmin Li, Kuofeng Gao, Yang Bai, Jingyun Zhang, Shu-tao Xia, Yisen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable performance of video-based large language models
(LLMs), their adversarial threat remains unexplored. To fill this gap, we
propose the first adversarial attack tailored for video-based LLMs by crafting
flow-based multi-modal adversarial perturbations on a small fraction of frames
within a video, dubbed FMM-Attack. Extensive experiments show that our attack
can effectively induce video-based LLMs to generate incorrect answers when
videos are added with imperceptible adversarial perturbations. Intriguingly,
our FMM-Attack can also induce garbling in the model output, prompting
video-based LLMs to hallucinate. Overall, our observations inspire a further
understanding of multi-modal robustness and safety-related feature alignment
across different modalities, which is of great importance for various large
multi-modal models. Our code is available at
https://github.com/THU-Kingmin/FMM-Attack.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yumeng Li, William Beluch, Margret Keuper, Dan Zhang, Anna Khoreva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite tremendous progress in the field of text-to-video (T2V) synthesis,
open-sourced T2V diffusion models struggle to generate longer videos with
dynamically varying and evolving content. They tend to synthesize quasi-static
videos, ignoring the necessary visual change-over-time implied in the text
prompt. At the same time, scaling these models to enable longer, more dynamic
video synthesis often remains computationally intractable. To address this
challenge, we introduce the concept of Generative Temporal Nursing (GTN), where
we aim to alter the generative process on the fly during inference to improve
control over the temporal dynamics and enable generation of longer videos. We
propose a method for GTN, dubbed VSTAR, which consists of two key ingredients:
1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis
based on the original single prompt leveraging LLMs, which gives accurate
textual guidance to different visual states of longer videos, and 2) Temporal
Attention Regularization (TAR) - a regularization technique to refine the
temporal attention units of the pre-trained T2V diffusion models, which enables
control over the video dynamics. We experimentally showcase the superiority of
the proposed approach in generating longer, visually appealing videos over
existing open-sourced T2V models. We additionally analyze the temporal
attention maps realized with and without VSTAR, demonstrating the importance of
applying our method to mitigate neglect of the desired visual change over time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://yumengli007.github.io/VSTAR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Baselines for Data-efficient Perceptual Augmentation of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Théophane Vallaeys, Mustafa Shukor, Matthieu Cord, Jakob Verbeek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The abilities of large language models (LLMs) have recently progressed to
unprecedented levels, paving the way to novel applications in a wide variety of
areas. In computer vision, LLMs can be used to prime vision-language tasks such
image captioning and visual question answering when coupled with pre-trained
vision backbones. While different approaches have been explored to interface
LLMs with ``perceptual backbones'' that process, e.g., visual or audio data,
they are often explored for different tasks, different datasets, and using
different perceptual backbones and language models, hindering direct comparison
of the interfacing mechanisms. To remedy this lack of comparability between
methods, we present an extensive experimental evaluation of different
interfacing mechanisms, across multiple tasks (including image, video, and
audio captioning as well as visual question answering), datasets and backbones,
paying special attention to low-data settings. We find improved performance
using existing mechanisms over state-of-the-art results, and identify a new
interfacing mechanism that yields (near) optimal results across different
tasks, while obtaining a 4x reduction in training time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Optimal Transport Framework for Cross-Modal Retrieval with
  Noisy Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochen Han, Minnan Luo, Huan Liu, Fang Nan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-modal retrieval (CMR) aims to establish interaction between different
modalities, among which supervised CMR is emerging due to its flexibility in
learning semantic category discrimination. Despite the remarkable performance
of previous supervised CMR methods, much of their success can be attributed to
the well-annotated data. However, even for unimodal data, precise annotation is
expensive and time-consuming, and it becomes more challenging with the
multimodal scenario. In practice, massive multimodal data are collected from
the Internet with coarse annotation, which inevitably introduces noisy labels.
Training with such misleading labels would bring two key challenges --
enforcing the multimodal samples to \emph{align incorrect semantics} and
\emph{widen the heterogeneous gap}, resulting in poor retrieval performance. To
tackle these challenges, this work proposes UOT-RCL, a Unified framework based
on Optimal Transport (OT) for Robust Cross-modal Retrieval. First, we propose a
semantic alignment based on partial OT to progressively correct the noisy
labels, where a novel cross-modal consistent cost function is designed to blend
different modalities and provide precise transport cost. Second, to narrow the
discrepancy in multi-modal data, an OT-based relation alignment is proposed to
infer the semantic-level cross-modal matching. Both of these two components
leverage the inherent correlation among multi-modal data to facilitate
effective cost function. The experiments on three widely-used cross-modal
retrieval datasets demonstrate that our UOT-RCL surpasses the state-of-the-art
approaches and significantly improves the robustness against noisy labels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deepfake Detection without Deepfakes: Generalization via Synthetic
  Frequency Patterns Injection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Alessandro Coccomini, Roberto Caldelli, Claudio Gennaro, Giuseppe Fiameni, Giuseppe Amato, Fabrizio Falchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deepfake detectors are typically trained on large sets of pristine and
generated images, resulting in limited generalization capacity; they excel at
identifying deepfakes created through methods encountered during training but
struggle with those generated by unknown techniques. This paper introduces a
learning approach aimed at significantly enhancing the generalization
capabilities of deepfake detectors. Our method takes inspiration from the
unique "fingerprints" that image generation processes consistently introduce
into the frequency domain. These fingerprints manifest as structured and
distinctly recognizable frequency patterns. We propose to train detectors using
only pristine images injecting in part of them crafted frequency patterns,
simulating the effects of various deepfake generation techniques without being
specific to any. These synthetic patterns are based on generic shapes, grids,
or auras. We evaluated our approach using diverse architectures across 25
different generation methods. The models trained with our approach were able to
perform state-of-the-art deepfake detection, demonstrating also superior
generalization capabilities in comparison with previous methods. Indeed, they
are untied to any specific generation technique and can effectively identify
deepfakes regardless of how they were made.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Diffusion Models to Real-World 3D <span class="highlight-title">LiDAR</span> Scene Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Nunes, Rodrigo Marcuzzi, Benedikt Mersch, Jens Behley, Cyrill Stachniss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer vision techniques play a central role in the perception stack of
autonomous vehicles. Such methods are employed to perceive the vehicle
surroundings given sensor data. 3D LiDAR sensors are commonly used to collect
sparse 3D point clouds from the scene. However, compared to human perception,
such systems struggle to deduce the unseen parts of the scene given those
sparse point clouds. In this matter, the scene completion task aims at
predicting the gaps in the LiDAR measurements to achieve a more complete scene
representation. Given the promising results of recent diffusion models as
generative models for images, we propose extending them to achieve scene
completion from a single 3D LiDAR scan. Previous works used diffusion models
over range images extracted from LiDAR data, directly applying image-based
diffusion methods. Distinctly, we propose to directly operate on the points,
reformulating the noising and denoising diffusion process such that it can
efficiently work at scene scale. Together with our approach, we propose a
regularization loss to stabilize the noise predicted during the denoising
process. Our experimental evaluation shows that our method can complete the
scene given a single LiDAR scan as input, producing a scene with more details
compared to state-of-the-art scene completion methods. We believe that our
proposed diffusion process formulation can support further research in
diffusion models applied to scene-scale point cloud data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive trajectory matching for medical <span class="highlight-title">dataset</span> distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Yu, Yang Liu, Qingchao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is essential but challenging to share medical image datasets due to
privacy issues, which prohibit building foundation models and knowledge
transfer. In this paper, we propose a novel dataset distillation method to
condense the original medical image datasets into a synthetic one that
preserves useful information for building an analysis model without accessing
the original datasets. Existing methods tackle only natural images by randomly
matching parts of the training trajectories of the model parameters trained by
the whole real datasets. However, through extensive experiments on medical
image datasets, the training process is extremely unstable and achieves
inferior distillation results. To solve these barriers, we propose to design a
novel progressive trajectory matching strategy to improve the training
stability for medical image dataset distillation. Additionally, it is observed
that improved stability prevents the synthetic dataset diversity and final
performance improvements. Therefore, we propose a dynamic overlap mitigation
module that improves the synthetic dataset diversity by dynamically eliminating
the overlap across different images and retraining parts of the synthetic
images for better convergence. Finally, we propose a new medical image dataset
distillation benchmark of various modalities and configurations to promote fair
evaluations. It is validated that our proposed method achieves 8.33%
improvement over previous state-of-the-art methods on average, and 11.7%
improvement when ipc=2 (i.e., image per class is 2). Codes and benchmarks will
be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLIPSwarm: Generating Drone Shows from Text Prompts with Vision-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Pueyo, Eduardo Montijano, Ana C. Murillo, Mac Schwager
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces CLIPSwarm, a new algorithm designed to automate the
modeling of swarm drone formations based on natural language. The algorithm
begins by enriching a provided word, to compose a text prompt that serves as
input to an iterative approach to find the formation that best matches the
provided word. The algorithm iteratively refines formations of robots to align
with the textual description, employing different steps for "exploration" and
"exploitation". Our framework is currently evaluated on simple formation
targets, limited to contour shapes. A formation is visually represented through
alpha-shape contours and the most representative color is automatically found
for the input word. To measure the similarity between the description and the
visual representation of the formation, we use CLIP [1], encoding text and
images into vectors and assessing their similarity. Subsequently, the algorithm
rearranges the formation to visually represent the word more effectively,
within the given constraints of available drones. Control actions are then
assigned to the drones, ensuring robotic behavior and collision-free movement.
Experimental results demonstrate the system's efficacy in accurately modeling
robot formations from natural language descriptions. The algorithm's
versatility is showcased through the execution of drone shows in photorealistic
simulation with varying shapes. We refer the reader to the supplementary video
for a visual reference of the results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An AI-Assisted Skincare Routine Recommendation System in XR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gowravi Malalur Rajegowda, Yannis Spyridis, Barbara Villarini, Vasileios Argyriou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been an increasing interest in the use of
artificial intelligence (AI) and extended reality (XR) in the beauty industry.
In this paper, we present an AI-assisted skin care recommendation system
integrated into an XR platform. The system uses a convolutional neural network
(CNN) to analyse an individual's skin type and recommend personalised skin care
products in an immersive and interactive manner. Our methodology involves
collecting data from individuals through a questionnaire and conducting skin
analysis using a provided facial image in an immersive environment. This data
is then used to train the CNN model, which recognises the skin type and
existing issues and allows the recommendation engine to suggest personalised
skin care products. We evaluate our system in terms of the accuracy of the CNN
model, which achieves an average score of 93% in correctly classifying existing
skin issues. Being integrated into an XR system, this approach has the
potential to significantly enhance the beauty industry by providing immersive
and engaging experiences to users, leading to more efficient and consistent
skincare routines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqiao Zhang, Tianwei Lin, Jiang Liu, Fangxun Shu, Haoyuan Li, Lei Zhang, He Wanggui, Hao Zhou, Zheqi Lv, Hao Jiang, Juncheng Li, Siliang Tang, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements indicate that scaling up Multimodal Large Language Models
(MLLMs) effectively enhances performance on downstream multimodal tasks. The
prevailing MLLM paradigm, \emph{e.g.}, LLaVA, transforms visual features into
text-like tokens using a \emph{static} vision-language mapper, thereby enabling
\emph{static} LLMs to develop the capability to comprehend visual information
through visual instruction tuning. Although promising, the \emph{static} tuning
strategy~\footnote{The static tuning refers to the trained model with static
parameters.} that shares the same parameters may constrain performance across
different downstream multimodal tasks. In light of this, we introduce
HyperLLaVA, which involves adaptive tuning of the projector and LLM parameters,
in conjunction with a dynamic visual expert and language expert, respectively.
These experts are derived from HyperNetworks, which generates adaptive
parameter shifts through visual and language guidance, enabling dynamic
projector and LLM modeling in two-stage training.
  Our experiments demonstrate that our solution significantly surpasses LLaVA
on existing MLLM benchmarks, including MME, MMBench, SEED-Bench, and
LLaVA-Bench. ~\footnote{Our project is available on the link
https://github.com/DCDmllm/HyperLLaVA}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedCycle: Unpaired Medical Report Generation via Cycle-Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elad Hirsch, Gefen Dawidowicz, Ayellet Tal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating medical reports for X-ray images presents a significant challenge,
particularly in unpaired scenarios where access to paired image-report data for
training is unavailable. Previous works have typically learned a joint
embedding space for images and reports, necessitating a specific labeling
schema for both. We introduce an innovative approach that eliminates the need
for consistent labeling schemas, thereby enhancing data accessibility and
enabling the use of incompatible datasets. This approach is based on
cycle-consistent mapping functions that transform image embeddings into report
embeddings, coupled with report auto-encoding for medical report generation.
Our model and objectives consider intricate local details and the overarching
semantic context within images and reports. This approach facilitates the
learning of effective mapping functions, resulting in the generation of
coherent reports. It outperforms state-of-the-art results in unpaired chest
X-ray report generation, demonstrating improvements in both language and
clinical metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast-Poly: A Fast Polyhedral Framework For 3D Multi-Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Li, Dedong Liu, Lijun Zhao, Yitao Wu, Xian Wu, Jinghan Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Multi-Object Tracking (MOT) captures stable and comprehensive motion
states of surrounding obstacles, essential for robotic perception. However,
current 3D trackers face issues with accuracy and latency consistency. In this
paper, we propose Fast-Poly, a fast and effective filter-based method for 3D
MOT. Building upon our previous work Poly-MOT, Fast-Poly addresses object
rotational anisotropy in 3D space, enhances local computation densification,
and leverages parallelization technique, improving inference speed and
precision. Fast-Poly is extensively tested on two large-scale tracking
benchmarks with Python implementation. On the nuScenes dataset, Fast-Poly
achieves new state-of-the-art performance with 75.8% AMOTA among all methods
and can run at 34.2 FPS on a personal CPU. On the Waymo dataset, Fast-Poly
exhibits competitive accuracy with 63.6% MOTA and impressive inference speed
(35.5 FPS). The source code is publicly available at
https://github.com/lixiaoyu2000/FastPoly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>1st on the NuScenes Tracking benchmark with 75.8 AMOTA and 34.2 FPS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic Geometry Models for Texture Synthesis of Machined Metallic
  Surfaces: Sandblasting and Milling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natascha Jeziorski, Claudia Redenbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training defect detection algorithms for visual surface inspection systems
requires a large and representative set of training data. Often there is not
enough real data available which additionally cannot cover the variety of
possible defects. Synthetic data generated by a synthetic visual surface
inspection environment can overcome this problem. Therefore, a digital twin of
the object is needed, whose micro-scale surface topography is modeled by
texture synthesis models. We develop stochastic texture models for sandblasted
and milled surfaces based on topography measurements of such surfaces. As the
surface patterns differ significantly, we use separate modeling approaches for
the two cases. Sandblasted surfaces are modeled by a combination of data-based
texture synthesis methods that rely entirely on the measurements. In contrast,
the model for milled surfaces is procedural and includes all process-related
parameters known from the machine settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing 6D Pose Estimation in Augmented Reality -- Overcoming
  Projection Ambiguity with Uncontrolled Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mayura Manawadu, Sieun Park, Soon-Yong Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the challenge of accurate 6D pose estimation in
Augmented Reality (AR), a critical component for seamlessly integrating virtual
objects into real-world environments. Our research primarily addresses the
difficulty of estimating 6D poses from uncontrolled RGB images, a common
scenario in AR applications, which lacks metadata such as focal length. We
propose a novel approach that strategically decomposes the estimation of z-axis
translation and focal length, leveraging the neural-render and compare strategy
inherent in the FocalPose architecture. This methodology not only streamlines
the 6D pose estimation process but also significantly enhances the accuracy of
3D object overlaying in AR settings. Our experimental results demonstrate a
marked improvement in 6D pose estimation accuracy, with promising applications
in manufacturing and robotics. Here, the precise overlay of AR visualizations
and the advancement of robotic vision systems stand to benefit substantially
from our findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MTP: Advancing Remote Sensing Foundation Model via Multi-Task
  Pretraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Wang, Jing Zhang, Minqiang Xu, Lin Liu, Dongsheng Wang, Erzhong Gao, Chengxi Han, Haonan Guo, Bo Du, Dacheng Tao, Liangpei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models have reshaped the landscape of Remote Sensing (RS) by
enhancing various image interpretation tasks. Pretraining is an active research
topic, encompassing supervised and self-supervised learning methods to
initialize model weights effectively. However, transferring the pretrained
models to downstream tasks may encounter task discrepancy due to their
formulation of pretraining as image classification or object discrimination
tasks. In this study, we explore the Multi-Task Pretraining (MTP) paradigm for
RS foundation models to address this issue. Using a shared encoder and
task-specific decoder architecture, we conduct multi-task supervised
pretraining on the SAMRS dataset, encompassing semantic segmentation, instance
segmentation, and rotated object detection. MTP supports both convolutional
neural networks and vision transformer foundation models with over 300 million
parameters. The pretrained models are finetuned on various RS downstream tasks,
such as scene classification, horizontal and rotated object detection, semantic
segmentation, and change detection. Extensive experiments across 14 datasets
demonstrate the superiority of our models over existing ones of similar size
and their competitive performance compared to larger state-of-the-art models,
thus validating the effectiveness of MTP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The codes and pretrained models will be released at
  https://github.com/ViTAE-Transformer/MTP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diversified and Personalized Multi-rater Medical Image Segmentation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yicheng Wu, Xiangde Luo, Zhe Xu, Xiaoqing Guo, Lie Ju, Zongyuan Ge, Wenjun Liao, Jianfei Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Annotation ambiguity due to inherent data uncertainties such as blurred
boundaries in medical scans and different observer expertise and preferences
has become a major obstacle for training deep-learning based medical image
segmentation models. To address it, the common practice is to gather multiple
annotations from different experts, leading to the setting of multi-rater
medical image segmentation. Existing works aim to either merge different
annotations into the "groundtruth" that is often unattainable in numerous
medical contexts, or generate diverse results, or produce personalized results
corresponding to individual expert raters. Here, we bring up a more ambitious
goal for multi-rater medical image segmentation, i.e., obtaining both
diversified and personalized results. Specifically, we propose a two-stage
framework named D-Persona (first Diversification and then Personalization). In
Stage I, we exploit multiple given annotations to train a Probabilistic U-Net
model, with a bound-constrained loss to improve the prediction diversity. In
this way, a common latent space is constructed in Stage I, where different
latent codes denote diversified expert opinions. Then, in Stage II, we design
multiple attention-based projection heads to adaptively query the corresponding
expert prompts from the shared latent space, and then perform the personalized
medical image segmentation. We evaluated the proposed model on our in-house
Nasopharyngeal Carcinoma dataset and the public lung nodule dataset (i.e.,
LIDC-IDRI). Extensive experiments demonstrated our D-Persona can provide
diversified and personalized results at the same time, achieving new SOTA
performance for multi-rater medical image segmentation. Our code will be
released at https://github.com/ycwu1997/D-Persona.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cell Tracking in C. elegans with Cell Position Heatmap-Based Alignment
  and Pairwise Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaito Shiku, Hiromitsu Shirai, Takeshi Ishihara, Ryoma Bise
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D cell tracking in a living organism has a crucial role in live cell image
analysis. Cell tracking in C. elegans has two difficulties. First, cell
migration in a consecutive frame is large since they move their head during
scanning. Second, cell detection is often inconsistent in consecutive frames
due to touching cells and low-contrast images, and these inconsistent
detections affect the tracking performance worse. In this paper, we propose a
cell tracking method to address these issues, which has two main contributions.
First, we introduce cell position heatmap-based non-rigid alignment with
test-time fine-tuning, which can warp the detected points to near the positions
at the next frame. Second, we propose a pairwise detection method, which uses
the information of detection results at the previous frame for detecting cells
at the current frame. The experimental results demonstrate the effectiveness of
each module, and the proposed method achieved the best performance in
comparison.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 5 figures, Accepted in EMBC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ S2DM: Sector-Shaped Diffusion Models for Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Lang, Yuxuan Ge, Zheng Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have achieved great success in image generation. However,
when leveraging this idea for video generation, we face significant challenges
in maintaining the consistency and continuity across video frames. This is
mainly caused by the lack of an effective framework to align frames of videos
with desired temporal features while preserving consistent semantic and
stochastic features. In this work, we propose a novel Sector-Shaped Diffusion
Model (S2DM) whose sector-shaped diffusion region is formed by a set of
ray-shaped reverse diffusion processes starting at the same noise point. S2DM
can generate a group of intrinsically related data sharing the same semantic
and stochastic features while varying on temporal features with appropriate
guided conditions. We apply S2DM to video generation tasks, and explore the use
of optical flow as temporal conditions. Our experimental results show that S2DM
outperforms many existing methods in the task of video generation without any
temporal-feature modelling modules. For text-to-video generation tasks where
temporal conditions are not explicitly given, we propose a two-stage generation
strategy which can decouple the generation of temporal features from
semantic-content features. We show that, without additional training, our model
integrated with another temporal conditions generative model can still achieve
comparable performance with existing works. Our results can be viewd at
https://s2dm.github.io/S2DM/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DOR3D-Net: Dense Ordinal Regression Network for 3D Hand Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yamin Mao, Zhihua Liu, Weiming Li, SoonYong Cho, Qiang Wang, Xiaoshuai Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth-based 3D hand pose estimation is an important but challenging research
task in human-machine interaction community. Recently, dense regression methods
have attracted increasing attention in 3D hand pose estimation task, which
provide a low computational burden and high accuracy regression way by densely
regressing hand joint offset maps. However, large-scale regression offset
values are often affected by noise and outliers, leading to a significant drop
in accuracy. To tackle this, we re-formulate 3D hand pose estimation as a dense
ordinal regression problem and propose a novel Dense Ordinal Regression 3D Pose
Network (DOR3D-Net). Specifically, we first decompose offset value regression
into sub-tasks of binary classifications with ordinal constraints. Then, each
binary classifier can predict the probability of a binary spatial relationship
relative to joint, which is easier to train and yield much lower level of
noise. The estimated hand joint positions are inferred by aggregating the
ordinal regression results at local positions with a weighted sum. Furthermore,
both joint regression loss and ordinal regression loss are used to train our
DOR3D-Net in an end-to-end manner. Extensive experiments on public datasets
(ICVL, MSRA, NYU and HANDS2017) show that our design provides significant
improvements over SOTA methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unifying Local and Global Multimodal Features for Place Recognition in
  Aliased and Low-Texture Environments <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto García-Hernández, Riccardo Giubilato, Klaus H. Strobl, Javier Civera, Rudolph Triebel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perceptual aliasing and weak textures pose significant challenges to the task
of place recognition, hindering the performance of Simultaneous Localization
and Mapping (SLAM) systems. This paper presents a novel model, called UMF
(standing for Unifying Local and Global Multimodal Features) that 1) leverages
multi-modality by cross-attention blocks between vision and LiDAR features, and
2) includes a re-ranking stage that re-orders based on local feature matching
the top-k candidates retrieved using a global representation. Our experiments,
particularly on sequences captured on a planetary-analogous environment, show
that UMF outperforms significantly previous baselines in those challenging
aliased environments. Since our work aims to enhance the reliability of SLAM in
all situations, we also explore its performance on the widely used RobotCar
dataset, for broader applicability. Code and models are available at
https://github.com/DLR-RM/UMF
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted submission to International Conference on Robotics and
  Automation (ICRA), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust image segmentation model based on binary level set 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to improve the robustness of traditional image segmentation models
to noise, this paper models the illumination term in intensity inhomogeneity
images. Additionally, to enhance the model's robustness to noisy images, we
incorporate the binary level set model into the proposed model. Compared to the
traditional level set, the binary level set eliminates the need for continuous
reinitialization. Moreover, by introducing the variational operator GL, our
model demonstrates better capability in segmenting noisy images. Finally, we
employ the three-step splitting operator method for solving, and the
effectiveness of the proposed model is demonstrated on various images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SCI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IIDM: Image-to-Image Diffusion Model for Semantic Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Liu,  Xiaobin-Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic image synthesis aims to generate high-quality images given semantic
conditions, i.e. segmentation masks and style reference images. Existing
methods widely adopt generative adversarial networks (GANs). GANs take all
conditional inputs and directly synthesize images in a single forward step. In
this paper, semantic image synthesis is treated as an image denoising task and
is handled with a novel image-to-image diffusion model (IIDM). Specifically,
the style reference is first contaminated with random noise and then
progressively denoised by IIDM, guided by segmentation masks. Moreover, three
techniques, refinement, color-transfer and model ensembles, are proposed to
further boost the generation quality. They are plug-in inference modules and do
not require additional training. Extensive experiments show that our IIDM
outperforms existing state-of-the-art methods by clear margins. Further
analysis is provided via detailed demonstrations. We have implemented IIDM
based on the Jittor framework; code is available at
https://github.com/ader47/jittor-jieke-semantic_images_synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures, accetped by CVMJ 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Correlation Clustering of Organoid Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jannik Presberger, Rashmiparvathi Keshara, David Stein, Yung Hae Kim, Anne Grapin-Botton, Bjoern Andres
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In biological and medical research, scientists now routinely acquire
microscopy images of hundreds of morphologically heterogeneous organoids and
are then faced with the task of finding patterns in the image collection, i.e.,
subsets of organoids that appear similar and potentially represent the same
morphological class. We adopt models and algorithms for correlating organoid
images, i.e., for quantifying the similarity in appearance and geometry of the
organoids they depict, and for clustering organoid images by consolidating
conflicting correlations. For correlating organoid images, we adopt and compare
two alternatives, a partial quadratic assignment problem and a twin network.
For clustering organoid images, we employ the correlation clustering problem.
Empirically, we learn the parameters of these models, infer a clustering of
organoid images, and quantify the accuracy of the inferred clusters, with
respect to a training set and a test set we contribute of state-of-the-art
light microscopy images of organoids clustered manually by biologists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-shot Oriented Object Detection with Memorable Contrastive Learning
  in Remote Sensing Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Zhou, Wuzhou Li, Yi Cao, Hongtao Cai, Xiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot object detection (FSOD) has garnered significant research attention
in the field of remote sensing due to its ability to reduce the dependency on
large amounts of annotated data. However, two challenges persist in this area:
(1) axis-aligned proposals, which can result in misalignment for arbitrarily
oriented objects, and (2) the scarcity of annotated data still limits the
performance for unseen object categories. To address these issues, we propose a
novel FSOD method for remote sensing images called Few-shot Oriented object
detection with Memorable Contrastive learning (FOMC). Specifically, we employ
oriented bounding boxes instead of traditional horizontal bounding boxes to
learn a better feature representation for arbitrary-oriented aerial objects,
leading to enhanced detection performance. To the best of our knowledge, we are
the first to address oriented object detection in the few-shot setting for
remote sensing images. To address the challenging issue of object
misclassification, we introduce a supervised contrastive learning module with a
dynamically updated memory bank. This module enables the use of large batches
of negative samples and enhances the model's capability to learn discriminative
features for unseen classes. We conduct comprehensive experiments on the DOTA
and HRSC2016 datasets, and our model achieves state-of-the-art performance on
the few-shot oriented object detection task. Code and pretrained models will be
released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 tables, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counting Network for Learning from Majority Label 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaito Shiku, Shinnosuke Matsuo, Daiki Suehiro, Ryoma Bise
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper proposes a novel problem in multi-class Multiple-Instance Learning
(MIL) called Learning from the Majority Label (LML). In LML, the majority class
of instances in a bag is assigned as the bag's label. LML aims to classify
instances using bag-level majority classes. This problem is valuable in various
applications. Existing MIL methods are unsuitable for LML due to aggregating
confidences, which may lead to inconsistency between the bag-level label and
the label obtained by counting the number of instances for each class. This may
lead to incorrect instance-level classification. We propose a counting network
trained to produce the bag-level majority labels estimated by counting the
number of instances for each class. This led to the consistency of the majority
class between the network outputs and one obtained by counting the number of
instances. Experimental results show that our counting network outperforms
conventional MIL methods on four datasets The code is publicly available at
https://github.com/Shiku-Kaito/Counting-Network-for-Learning-from-Majority-Label.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, Accepted in ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ManiPose: A Comprehensive Benchmark for Pose-aware Object Manipulation
  in Robotics <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaojun Yu, Ce Hao, Junbo Wang, Wenhai Liu, Liu Liu, Yao Mu, Yang You, Hengxu Yan, Cewu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic manipulation in everyday scenarios, especially in unstructured
environments, requires skills in pose-aware object manipulation (POM), which
adapts robots' grasping and handling according to an object's 6D pose.
Recognizing an object's position and orientation is crucial for effective
manipulation. For example, if a mug is lying on its side, it's more effective
to grasp it by the rim rather than the handle. Despite its importance, research
in POM skills remains limited, because learning manipulation skills requires
pose-varying simulation environments and datasets. This paper introduces
ManiPose, a pioneering benchmark designed to advance the study of pose-varying
manipulation tasks. ManiPose encompasses: 1) Simulation environments for POM
feature tasks ranging from 6D pose-specific pick-and-place of single objects to
cluttered scenes, further including interactions with articulated objects. 2) A
comprehensive dataset featuring geometrically consistent and
manipulation-oriented 6D pose labels for 2936 real-world scanned rigid objects
and 100 articulated objects across 59 categories. 3) A baseline for POM,
leveraging the inferencing abilities of LLM (e.g., ChatGPT) to analyze the
relationship between 6D pose and task-specific requirements, offers enhanced
pose-aware grasp prediction and motion planning capabilities. Our benchmark
demonstrates notable advancements in pose estimation, pose-aware manipulation,
and real-robot skill transfer, setting new standards for POM research. We will
open-source the ManiPose benchmark with the final version paper, inviting the
community to engage with our resources, available at our
website:https://sites.google.com/view/manipose.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, submitted to 2024 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AGFSync: Leveraging AI-Generated Feedback for Preference Optimization in
  Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingkun An, Yinghao Zhu, Zongjian Li, Haoran Feng, Bohua Chen, Yemin Shi, Chengwei Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-Image (T2I) diffusion models have achieved remarkable success in
image generation. Despite their progress, challenges remain in both
prompt-following ability, image quality and lack of high-quality datasets,
which are essential for refining these models. As acquiring labeled data is
costly, we introduce AGFSync, a framework that enhances T2I diffusion models
through Direct Preference Optimization (DPO) in a fully AI-driven approach.
AGFSync utilizes Vision-Language Models (VLM) to assess image quality across
style, coherence, and aesthetics, generating feedback data within an AI-driven
loop. By applying AGFSync to leading T2I models such as SD v1.4, v1.5, and
SDXL, our extensive experiments on the TIFA dataset demonstrate notable
improvements in VQA scores, aesthetic evaluations, and performance on the HPSv2
benchmark, consistently outperforming the base models. AGFSync's method of
refining T2I diffusion models paves the way for scalable alignment techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OrthCaps: An Orthogonal CapsNet with Sparse Attention Routing and
  Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Geng, Jiaming Wang, Jiawei Gong, Yuerong Xue, Jun Xu, Fanglin Chen, Xiaolin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Redundancy is a persistent challenge in Capsule Networks (CapsNet),leading to
high computational costs and parameter counts. Although previous works have
introduced pruning after the initial capsule layer, dynamic routing's fully
connected nature and non-orthogonal weight matrices reintroduce redundancy in
deeper layers. Besides, dynamic routing requires iterating to converge, further
increasing computational demands. In this paper, we propose an Orthogonal
Capsule Network (OrthCaps) to reduce redundancy, improve routing performance
and decrease parameter counts. Firstly, an efficient pruned capsule layer is
introduced to discard redundant capsules. Secondly, dynamic routing is replaced
with orthogonal sparse attention routing, eliminating the need for iterations
and fully connected structures. Lastly, weight matrices during routing are
orthogonalized to sustain low capsule similarity, which is the first approach
to introduce orthogonality into CapsNet as far as we know. Our experiments on
baseline datasets affirm the efficiency and robustness of OrthCaps in
classification tasks, in which ablation studies validate the criticality of
each component. Remarkably, OrthCaps-Shallow outperforms other Capsule Network
benchmarks on four datasets, utilizing only 110k parameters, which is a mere
1.25% of a standard Capsule Network's total. To the best of our knowledge, it
achieves the smallest parameter count among existing Capsule Networks.
Similarly, OrthCaps-Deep demonstrates competitive performance across four
datasets, utilizing only 1.2% of the parameters required by its counterparts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Gaussian Mixture Normalizing Flow Modeling for Unified
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xincheng Yao, Ruoqi Li, Zefeng Qian, Lu Wang, Chongyang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unified anomaly detection (AD) is one of the most challenges for anomaly
detection, where one unified model is trained with normal samples from multiple
classes with the objective to detect anomalies in these classes. For such a
challenging task, popular normalizing flow (NF) based AD methods may fall into
a "homogeneous mapping" issue,where the NF-based AD models are biased to
generate similar latent representations for both normal and abnormal features,
and thereby lead to a high missing rate of anomalies. In this paper, we propose
a novel Hierarchical Gaussian mixture normalizing flow modeling method for
accomplishing unified Anomaly Detection, which we call HGAD. Our HGAD consists
of two key components: inter-class Gaussian mixture modeling and intra-class
mixed class centers learning. Compared to the previous NF-based AD methods, the
hierarchical Gaussian mixture modeling approach can bring stronger
representation capability to the latent space of normalizing flows, so that
even complex multi-class distribution can be well represented and learned in
the latent space. In this way, we can avoid mapping different class
distributions into the same single Gaussian prior, thus effectively avoiding or
mitigating the "homogeneous mapping" issue. We further indicate that the more
distinguishable different class centers, the more conducive to avoiding the
bias issue. Thus, we further propose a mutual information maximization loss for
better structuring the latent feature space. We evaluate our method on four
real-world AD benchmarks, where we can significantly improve the previous
NF-based AD methods and also outperform the SOTA unified AD methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ vid-TLDR: Training Free Token merging for Light-weight Video Transformer <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joonmyung Choi, Sanghyeok Lee, Jaewon Chu, Minhyuk Choi, Hyunwoo J. Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Transformers have become the prevalent solution for various video
downstream tasks with superior expressive power and flexibility. However, these
video transformers suffer from heavy computational costs induced by the massive
number of tokens across the entire video frames, which has been the major
barrier to training the model. Further, the patches irrelevant to the main
contents, e.g., backgrounds, degrade the generalization performance of models.
To tackle these issues, we propose training free token merging for lightweight
video Transformer (vid-TLDR) that aims to enhance the efficiency of video
Transformers by merging the background tokens without additional training. For
vid-TLDR, we introduce a novel approach to capture the salient regions in
videos only with the attention map. Further, we introduce the saliency-aware
token merging strategy by dropping the background tokens and sharpening the
object scores. Our experiments show that vid-TLDR significantly mitigates the
computational complexity of video Transformers while achieving competitive
performance compared to the base model without vid-TLDR. Code is available at
https://github.com/mlvlab/vid-TLDR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference on Computer Vision and Pattern Recognition (CVPR), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TiBiX: Leveraging Temporal Information for Bidirectional X-ray and
  Report Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Santosh Sanjeev, Fadillah Adamsyah Maani, Arsen Abzhanov, Vijay Ram Papineni, Ibrahim Almakky, Bartłomiej W. Papież, Mohammad Yaqub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of vision language models in the medical imaging domain,
numerous studies have focused on two dominant research activities: (1) report
generation from Chest X-rays (CXR), and (2) synthetic scan generation from text
or reports. Despite some research incorporating multi-view CXRs into the
generative process, prior patient scans and reports have been generally
disregarded. This can inadvertently lead to the leaving out of important
medical information, thus affecting generation quality. To address this, we
propose TiBiX: Leveraging Temporal information for Bidirectional X-ray and
Report Generation. Considering previous scans, our approach facilitates
bidirectional generation, primarily addressing two challenging problems: (1)
generating the current image from the previous image and current report and (2)
generating the current report based on both the previous and current images.
Moreover, we extract and release a curated temporal benchmark dataset derived
from the MIMIC-CXR dataset, which focuses on temporal data. Our comprehensive
experiments and ablation studies explore the merits of incorporating prior CXRs
and achieve state-of-the-art (SOTA) results on the report generation task.
Furthermore, we attain on-par performance with SOTA image generation efforts,
thus serving as a new baseline in longitudinal bidirectional CXR-to-report
generation. The code is available at https://github.com/BioMedIA-MBZUAI/TiBiX.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FissionFusion: Fast Geometric Generation and Hierarchical Souping for
  Medical Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Santosh Sanjeev, Nuren Zhaksylyk, Ibrahim Almakky, Anees Ur Rehman Hashmi, Mohammad Areeb Qazi, Mohammad Yaqub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scarcity of well-annotated medical datasets requires leveraging transfer
learning from broader datasets like ImageNet or pre-trained models like CLIP.
Model soups averages multiple fine-tuned models aiming to improve performance
on In-Domain (ID) tasks and enhance robustness against Out-of-Distribution
(OOD) datasets. However, applying these methods to the medical imaging domain
faces challenges and results in suboptimal performance. This is primarily due
to differences in error surface characteristics that stem from data
complexities such as heterogeneity, domain shift, class imbalance, and
distributional shifts between training and testing phases. To address this
issue, we propose a hierarchical merging approach that involves local and
global aggregation of models at various levels based on models' hyperparameter
configurations. Furthermore, to alleviate the need for training a large number
of models in the hyperparameter search, we introduce a computationally
efficient method using a cyclical learning rate scheduler to produce multiple
models for aggregation in the weight space. Our method demonstrates significant
improvements over the model souping approach across multiple datasets (around
6% gain in HAM10000 and CheXpert datasets) while maintaining low computational
costs for model generation and selection. Moreover, we achieve better results
on OOD datasets than model soups. The code is available at
https://github.com/BioMedIA-MBZUAI/FissionFusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Critical Subgraph Mining for Cognitive Impairment Conversion
  Prediction with T1-MRI-based Brain Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilin Leng, Wenju Cui, Bai Chen, Xi Jiang, Shuangqing Chen, Jian Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prediction the conversion to early-stage dementia is critical for mitigating
its progression but remains challenging due to subtle cognitive impairments and
structural brain changes. Traditional T1-weighted magnetic resonance imaging
(T1-MRI) research focus on identifying brain atrophy regions but often fails to
address the intricate connectivity between them. This limitation underscores
the necessity of focuing on inter-regional connectivity for a comprehensive
understand of the brain's complex network. Moreover, there is a pressing demand
for methods that adaptively preserve and extract critical information,
particularly specialized subgraph mining techniques for brain networks. These
are essential for developing high-quality feature representations that reveal
critical spatial impacts of structural brain changes and its topology. In this
paper, we propose Brain-SubGNN, a novel graph representation network to mine
and enhance critical subgraphs based on T1-MRI. This network provides a
subgraph-level interpretation, enhancing interpretability and insights for
graph analysis. The process begins by extracting node features and a
correlation matrix between nodes to construct a task-oriented brain network.
Brain-SubGNN then adaptively identifies and enhances critical subgraphs,
capturing both loop and neighbor subgraphs. This method reflects the loop
topology and local changes, indicative of long-range connections, and maintains
local and global brain attributes. Extensive experiments validate the
effectiveness and advantages of Brain-SubGNN, demonstrating its potential as a
powerful tool for understanding and diagnosing early-stage dementia. Source
code is available at https://github.com/Leng-10/Brain-SubGNN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Novel View Synthesis from Heterogeneous Low-light Captures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Zheng, Hao Sun, Huiyao Xu, Fanjiang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural radiance field has achieved fundamental success in novel view
synthesis from input views with the same brightness level captured under fixed
normal lighting. Unfortunately, synthesizing novel views remains to be a
challenge for input views with heterogeneous brightness level captured under
low-light condition. The condition is pretty common in the real world. It
causes low-contrast images where details are concealed in the darkness and
camera sensor noise significantly degrades the image quality. To tackle this
problem, we propose to learn to decompose illumination, reflectance, and noise
from input views according to that reflectance remains invariant across
heterogeneous views. To cope with heterogeneous brightness and noise levels
across multi-views, we learn an illumination embedding and optimize a noise map
individually for each view. To allow intuitive editing of the illumination, we
design an illumination adjustment module to enable either brightening or
darkening of the illumination component. Comprehensive experiments demonstrate
that this approach enables effective intrinsic decomposition for low-light
multi-view noisy images and achieves superior visual quality and numerical
performance for synthesizing novel views compared to state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AMP: Autoregressive Motion Prediction Revisited with Next Token
  Prediction for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaosong Jia, Shaoshuai Shi, Zijun Chen, Li Jiang, Wenlong Liao, Tao He, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As an essential task in autonomous driving (AD), motion prediction aims to
predict the future states of surround objects for navigation. One natural
solution is to estimate the position of other agents in a step-by-step manner
where each predicted time-step is conditioned on both observed time-steps and
previously predicted time-steps, i.e., autoregressive prediction. Pioneering
works like SocialLSTM and MFP design their decoders based on this intuition.
However, almost all state-of-the-art works assume that all predicted time-steps
are independent conditioned on observed time-steps, where they use a single
linear layer to generate positions of all time-steps simultaneously. They
dominate most motion prediction leaderboards due to the simplicity of training
MLPs compared to autoregressive networks.
  In this paper, we introduce the GPT style next token prediction into motion
forecasting. In this way, the input and output could be represented in a
unified space and thus the autoregressive prediction becomes more feasible.
However, different from language data which is composed of homogeneous units
-words, the elements in the driving scene could have complex spatial-temporal
and semantic relations. To this end, we propose to adopt three factorized
attention modules with different neighbors for information aggregation and
different position encoding styles to capture their relations, e.g., encoding
the transformation between coordinate systems for spatial relativity while
adopting RoPE for temporal relativity. Empirically, by equipping with the
aforementioned tailored designs, the proposed method achieves state-of-the-art
performance in the Waymo Open Motion and Waymo Interaction datasets. Notably,
AMP outperforms other recent autoregressive motion prediction methods: MotionLM
and StateTransformer, which demonstrates the effectiveness of the proposed
designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient scene text image super-resolution with semantic guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        LeoWu TomyEnrique, Xiangcheng Du, Kangliang Liu, Han Yuan, Zhao Zhou, Cheng Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene text image super-resolution has significantly improved the accuracy of
scene text recognition. However, many existing methods emphasize performance
over efficiency and ignore the practical need for lightweight solutions in
deployment scenarios. Faced with the issues, our work proposes an efficient
framework called SGENet to facilitate deployment on resource-limited platforms.
SGENet contains two branches: super-resolution branch and semantic guidance
branch. We apply a lightweight pre-trained recognizer as a semantic extractor
to enhance the understanding of text information. Meanwhile, we design the
visual-semantic alignment module to achieve bidirectional alignment between
image features and semantics, resulting in the generation of highquality prior
guidance. We conduct extensive experiments on benchmark dataset, and the
proposed SGENet achieves excellent performance with fewer computational costs.
Code is available at https://github.com/SijieLiu518/SGENet
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gaussian Splatting on the Move: Blur and Rolling Shutter Compensation
  for Natural Camera Motion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Otto Seiskari, Jerry Ylilammi, Valtteri Kaatrasalo, Pekka Rantalankila, Matias Turkulainen, Juho Kannala, Esa Rahtu, Arno Solin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality scene reconstruction and novel view synthesis based on Gaussian
Splatting (3DGS) typically require steady, high-quality photographs, often
impractical to capture with handheld cameras. We present a method that adapts
to camera motion and allows high-quality scene reconstruction with handheld
video data suffering from motion blur and rolling shutter distortion. Our
approach is based on detailed modelling of the physical image formation process
and utilizes velocities estimated using visual-inertial odometry (VIO). Camera
poses are considered non-static during the exposure time of a single image
frame and camera poses are further optimized in the reconstruction process. We
formulate a differentiable rendering pipeline that leverages screen space
approximation to efficiently incorporate rolling-shutter and motion blur
effects into the 3DGS framework. Our results with both synthetic and real data
demonstrate superior performance in mitigating camera motion over existing
methods, thereby advancing 3DGS in naturalistic settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Source code available at https://github.com/SpectacularAI/3dgs-deblur</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Out-of-Distribution Detection Using Peer-Class Generated by Large
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        K Huang, G Song, Hanwen Su, Jiyan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection is a critical task to ensure the
reliability and security of machine learning models deployed in real-world
applications. Conventional methods for OOD detection that rely on single-modal
information, often struggle to capture the rich variety of OOD instances. The
primary difficulty in OOD detection arises when an input image has numerous
similarities to a particular class in the in-distribution (ID) dataset, e.g.,
wolf to dog, causing the model to misclassify it. Nevertheless, it may be easy
to distinguish these classes in the semantic domain. To this end, in this
paper, a novel method called ODPC is proposed, in which specific prompts to
generate OOD peer classes of ID semantics are designed by a large language
model as an auxiliary modality to facilitate detection. Moreover, a contrastive
loss based on OOD peer classes is devised to learn compact representations of
ID classes and improve the clarity of boundaries between different classes. The
extensive experiments on five benchmark datasets show that the method we
propose can yield state-of-the-art results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> DD-RobustBench: An Adversarial Robustness Benchmark for <span class="highlight-title">Dataset</span>
  Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Wu, Jiawei Du, Ping Liu, Yuewei Lin, Wenqing Cheng, <span class="highlight-author">Wei Xu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset distillation is an advanced technique aimed at compressing datasets
into significantly smaller counterparts, while preserving formidable training
performance. Significant efforts have been devoted to promote evaluation
accuracy under limited compression ratio while overlooked the robustness of
distilled dataset. In this work, we introduce a comprehensive benchmark that,
to the best of our knowledge, is the most extensive to date for evaluating the
adversarial robustness of distilled datasets in a unified way. Our benchmark
significantly expands upon prior efforts by incorporating a wider range of
dataset distillation methods, including the latest advancements such as TESLA
and SRe2L, a diverse array of adversarial attack methods, and evaluations
across a broader and more extensive collection of datasets such as ImageNet-1K.
Moreover, we assessed the robustness of these distilled datasets against
representative adversarial attack algorithms like PGD and AutoAttack, while
exploring their resilience from a frequency perspective. We also discovered
that incorporating distilled data into the training batches of the original
dataset can yield to improvement of robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyperFusion: A Hypernetwork Approach to Multimodal Integration of
  Tabular and Medical Imaging Data for Predictive Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Duenias, Brennan Nichyporuk, Tal Arbel, Tammy Riklin Raviv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of diverse clinical modalities such as medical imaging and
the tabular data obtained by the patients' Electronic Health Records (EHRs) is
a crucial aspect of modern healthcare. The integrative analysis of multiple
sources can provide a comprehensive understanding of a patient's condition and
can enhance diagnoses and treatment decisions. Deep Neural Networks (DNNs)
consistently showcase outstanding performance in a wide range of multimodal
tasks in the medical domain. However, the complex endeavor of effectively
merging medical imaging with clinical, demographic and genetic information
represented as numerical tabular data remains a highly active and ongoing
research pursuit.
  We present a novel framework based on hypernetworks to fuse clinical imaging
and tabular data by conditioning the image processing on the EHR's values and
measurements. This approach aims to leverage the complementary information
present in these modalities to enhance the accuracy of various medical
applications. We demonstrate the strength and the generality of our method on
two different brain Magnetic Resonance Imaging (MRI) analysis tasks, namely,
brain age prediction conditioned by subject's sex, and multiclass Alzheimer's
Disease (AD) classification conditioned by tabular data. We show that our
framework outperforms both single-modality models and state-of-the-art
MRI-tabular data fusion methods. The code, enclosed to this manuscript will be
made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models
  with Abstract Visual Patterns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yew Ken Chia, Vernon Toh Yan Han, Deepanway Ghosal, Lidong Bing, Soujanya Poria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large multimodal models extend the impressive capabilities of large language
models by integrating multimodal understanding abilities. However, it is not
clear how they can emulate the general intelligence and reasoning ability of
humans. As recognizing patterns and abstracting concepts are key to general
intelligence, we introduce PuzzleVQA, a collection of puzzles based on abstract
patterns. With this dataset, we evaluate large multimodal models with abstract
patterns based on fundamental concepts, including colors, numbers, sizes, and
shapes. Through our experiments on state-of-the-art large multimodal models, we
find that they are not able to generalize well to simple abstract patterns.
Notably, even GPT-4V cannot solve more than half of the puzzles. To diagnose
the reasoning challenges in large multimodal models, we progressively guide the
models with our ground truth reasoning explanations for visual perception,
inductive reasoning, and deductive reasoning. Our systematic analysis finds
that the main bottlenecks of GPT-4V are weaker visual perception and inductive
reasoning abilities. Through this work, we hope to shed light on the
limitations of large multimodal models and how they can better emulate human
cognitive processes in the future (Our data and code will be released publicly
at https://github.com/declare-lab/LLM-PuzzleTest).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LaserHuman: Language-guided Scene-aware Human Motion Generation in Free
  Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peishan Cong, Ziyi WangZhiyang Dou, Yiming Ren, Wei Yin, Kai Cheng, Yujing Sun, Xiaoxiao Long, Xinge Zhu, Yuexin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language-guided scene-aware human motion generation has great significance
for entertainment and robotics. In response to the limitations of existing
datasets, we introduce LaserHuman, a pioneering dataset engineered to
revolutionize Scene-Text-to-Motion research. LaserHuman stands out with its
inclusion of genuine human motions within 3D environments, unbounded free-form
natural language descriptions, a blend of indoor and outdoor scenarios, and
dynamic, ever-changing scenes. Diverse modalities of capture data and rich
annotations present great opportunities for the research of conditional motion
generation, and can also facilitate the development of real-life applications.
Moreover, to generate semantically consistent and physically plausible human
motions, we propose a multi-conditional diffusion model, which is simple but
effective, achieving state-of-the-art performance on existing datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DetDiffusion: Synergizing Generative and Perceptive Models for Enhanced
  Data Generation and Perception <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibo Wang, Ruiyuan Gao, Kai Chen, Kaiqiang Zhou, Yingjie Cai, Lanqing Hong, Zhenguo Li, Lihui Jiang, Dit-Yan Yeung, Qiang Xu, Kai Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current perceptive models heavily depend on resource-intensive datasets,
prompting the need for innovative solutions. Leveraging recent advances in
diffusion models, synthetic data, by constructing image inputs from various
annotations, proves beneficial for downstream tasks. While prior methods have
separately addressed generative and perceptive models, DetDiffusion, for the
first time, harmonizes both, tackling the challenges in generating effective
data for perceptive models. To enhance image generation with perceptive models,
we introduce perception-aware loss (P.A. loss) through segmentation, improving
both quality and controllability. To boost the performance of specific
perceptive models, our method customizes data augmentation by extracting and
utilizing perception-aware attribute (P.A. Attr) during generation.
Experimental results from the object detection task highlight DetDiffusion's
superior performance, establishing a new state-of-the-art in layout-guided
generation. Furthermore, image syntheses from DetDiffusion can effectively
augment training data, significantly enhancing downstream detection
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rotary Position Embedding for Vision Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byeongho Heo, Song Park, Dongyoon Han, Sangdoo Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rotary Position Embedding (RoPE) performs remarkably on language models,
especially for length extrapolation of Transformers. However, the impacts of
RoPE on computer vision domains have been underexplored, even though RoPE
appears capable of enhancing Vision Transformer (ViT) performance in a way
similar to the language domain. This study provides a comprehensive analysis of
RoPE when applied to ViTs, utilizing practical implementations of RoPE for 2D
vision data. The analysis reveals that RoPE demonstrates impressive
extrapolation performance, i.e., maintaining precision while increasing image
resolution at inference. It eventually leads to performance improvement for
ImageNet-1k, COCO detection, and ADE-20k segmentation. We believe this study
provides thorough guidelines to apply RoPE into ViT, promising improved
backbone performance with minimal extra computational overhead. Our code and
pre-trained models are available at https://github.com/naver-ai/rope-vit
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnyHome: Open-Vocabulary Generation of Structured and Textured 3D Homes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06644v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06644v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rao Fu, Zehao Wen, Zichen Liu, Srinath Sridhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by cognitive theories, we introduce AnyHome, a framework that
translates any text into well-structured and textured indoor scenes at a
house-scale. By prompting Large Language Models (LLMs) with designed templates,
our approach converts provided textual narratives into amodal structured
representations. These representations guarantee consistent and realistic
spatial layouts by directing the synthesis of a geometry mesh within defined
constraints. A Score Distillation Sampling process is then employed to refine
the geometry, followed by an egocentric inpainting process that adds lifelike
textures to it. AnyHome stands out with its editability, customizability,
diversity, and realism. The structured representations for scenes allow for
extensive editing at varying levels of granularity. Capable of interpreting
texts ranging from simple labels to detailed narratives, AnyHome generates
detailed geometries and textures that outperform existing methods in both
quantitative and qualitative measures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Magic-Me: Identity-Specific Video Customized Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui Yang, Zhen Dong, Kurt Keutzer, Jiashi Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating content with specified identities (ID) has attracted significant
interest in the field of generative models. In the field of text-to-image
generation (T2I), subject-driven creation has achieved great progress with the
identity controlled via reference images. However, its extension to video
generation is not well explored. In this work, we propose a simple yet
effective subject identity controllable video generation framework, termed
Video Custom Diffusion (VCD). With a specified identity defined by a few
images, VCD reinforces the identity characteristics and injects frame-wise
correlation at the initialization stage for stable video outputs. To achieve
this, we propose three novel components that are essential for high-quality
identity preservation and stable video generation: 1) a noise initialization
method with 3D Gaussian Noise Prior for better inter-frame stability; 2) an ID
module based on extended Textual Inversion trained with the cropped identity to
disentangle the ID information from the background 3) Face VCD and Tiled VCD
modules to reinforce faces and upscale the video to higher resolution while
preserving the identity's features. We conducted extensive experiments to
verify that VCD is able to generate stable videos with better ID over the
baselines. Besides, with the transferability of the encoded identity in the ID
module, VCD is also working well with personalized text-to-image models
available publicly. The codes are available at
https://github.com/Zhen-Dong/Magic-Me.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page at https://magic-me-webpage.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11085v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11085v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixian Ma, Weikai Huang, Jieyu Zhang, Tanmay Gupta, Ranjay Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world multi-modal problems are rarely solved by a single machine
learning model, and often require multi-step computational plans that involve
stitching several models. Tool-augmented LLMs hold tremendous promise for
automating the generation of such computational plans. However, the lack of
standardized benchmarks for evaluating LLMs as planners for multi-step
multi-modal tasks has prevented a systematic study of planner design decisions.
Should LLMs generate a full plan in a single shot or step-by-step? Should they
invoke tools directly with Python code or through structured data formats like
JSON? Does feedback improve planning? To answer these questions and more, we
introduce m&m's: a benchmark containing 4K+ multi-step multi-modal tasks
involving 33 tools that include multi-modal models, (free) public APIs, and
image processing modules. For each of these task queries, we provide
automatically generated plans using this realistic toolset. We further provide
a high-quality subset of 1,565 task plans that are human-verified and correctly
executable. With m&m's, we evaluate 6 popular LLMs with 2 planning strategies
(multi-step vs. step-by-step planning), 2 plan formats (JSON vs. code), and 3
types of feedback (parsing/verification/execution). Finally, we summarize
takeaways from our extensive experiments. Our dataset and code are available on
HuggingFace (https://huggingface.co/datasets/zixianma/mnms) and Github
(https://github.com/RAIVNLab/mnms).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TrackDiffusion: Tracklet-Conditioned Video Generation via Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00651v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00651v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengxiang Li, Kai Chen, Zhili Liu, Ruiyuan Gao, Lanqing Hong, Guo Zhou, Hua Yao, Dit-Yan Yeung, Huchuan Lu, Xu Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite remarkable achievements in video synthesis, achieving granular
control over complex dynamics, such as nuanced movement among multiple
interacting objects, still presents a significant hurdle for dynamic world
modeling, compounded by the necessity to manage appearance and disappearance,
drastic scale changes, and ensure consistency for instances across frames.
These challenges hinder the development of video generation that can faithfully
mimic real-world complexity, limiting utility for applications requiring
high-level realism and controllability, including advanced scene simulation and
training of perception systems. To address that, we propose TrackDiffusion, a
novel video generation framework affording fine-grained trajectory-conditioned
motion control via diffusion models, which facilitates the precise manipulation
of the object trajectories and interactions, overcoming the prevalent
limitation of scale and continuity disruptions. A pivotal component of
TrackDiffusion is the instance enhancer, which explicitly ensures inter-frame
consistency of multiple objects, a critical factor overlooked in the current
literature. Moreover, we demonstrate that generated video sequences by our
TrackDiffusion can be used as training data for visual perception models. To
the best of our knowledge, this is the first work to apply video diffusion
models with tracklet conditions and demonstrate that generated frames can be
beneficial for improving the performance of object trackers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding
  and Reasoning in Pathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16355v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16355v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Sun, Hao Wu, Chenglu Zhu, Sunyi Zheng, Qizi Chen, Kai Zhang, Yunlong Zhang, Dan Wan, Xiaoxiao Lan, Mengyue Zheng, Jingxiong Li, Xinheng Lyu, Tao Lin, Lin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of large multimodal models has unlocked remarkable potential in
AI, particularly in pathology. However, the lack of specialized, high-quality
benchmark impeded their development and precise evaluation. To address this, we
introduce PathMMU, the largest and highest-quality expert-validated pathology
benchmark for Large Multimodal Models (LMMs). It comprises 33,428 multimodal
multi-choice questions and 24,067 images from various sources, each accompanied
by an explanation for the correct answer. The construction of PathMMU harnesses
GPT-4V's advanced capabilities, utilizing over 30,000 image-caption pairs to
enrich captions and generate corresponding Q&As in a cascading process.
Significantly, to maximize PathMMU's authority, we invite seven pathologists to
scrutinize each question under strict standards in PathMMU's validation and
test sets, while simultaneously setting an expert-level performance benchmark
for PathMMU. We conduct extensive evaluations, including zero-shot assessments
of 14 open-sourced and 4 closed-sourced LMMs and their robustness to image
corruption. We also fine-tune representative LMMs to assess their adaptability
to PathMMU. The empirical findings indicate that advanced LMMs struggle with
the challenging PathMMU benchmark, with the top-performing LMM, GPT-4V,
achieving only a 49.8% zero-shot performance, significantly lower than the
71.8% demonstrated by human pathologists. After fine-tuning, significantly
smaller open-sourced LMMs can outperform GPT-4V but still fall short of the
expertise shown by pathologists. We hope that the PathMMU will offer valuable
insights and foster the development of more specialized, next-generation LMMs
for pathology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.05666v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.05666v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifu Wang, Xuefei Ning, Matthew B. Blaschko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intersection over Union (IoU) losses are surrogates that directly optimize
the Jaccard index. Leveraging IoU losses as part of the loss function have
demonstrated superior performance in semantic segmentation tasks compared to
optimizing pixel-wise losses such as the cross-entropy loss alone. However, we
identify a lack of flexibility in these losses to support vital training
techniques like label smoothing, knowledge distillation, and semi-supervised
learning, mainly due to their inability to process soft labels. To address
this, we introduce Jaccard Metric Losses (JMLs), which are identical to the
soft Jaccard loss in standard settings with hard labels but are fully
compatible with soft labels. We apply JMLs to three prominent use cases of soft
labels: label smoothing, knowledge distillation and semi-supervised learning,
and demonstrate their potential to enhance model accuracy and calibration. Our
experiments show consistent improvements over the cross-entropy loss across 4
semantic segmentation datasets (Cityscapes, PASCAL VOC, ADE20K, DeepGlobe Land)
and 13 architectures, including classic CNNs and recent vision transformers.
Remarkably, our straightforward approach significantly outperforms
state-of-the-art knowledge distillation and semi-supervised learning methods.
The code is available at
\href{https://github.com/zifuwanggg/JDTLosses}{https://github.com/zifuwanggg/JDTLosses}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Periodic Vibration Gaussian: Dynamic Urban Scene Reconstruction and
  Real-time Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yurui Chen, Chun Gu, Junzhe Jiang, Xiatian Zhu, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling dynamic, large-scale urban scenes is challenging due to their highly
intricate geometric structures and unconstrained dynamics in both space and
time. Prior methods often employ high-level architectural priors, separating
static and dynamic elements, resulting in suboptimal capture of their
synergistic interactions. To address this challenge, we present a unified
representation model, called Periodic Vibration Gaussian (PVG). PVG builds upon
the efficient 3D Gaussian splatting technique, originally designed for static
scene representation, by introducing periodic vibration-based temporal
dynamics. This innovation enables PVG to elegantly and uniformly represent the
characteristics of various objects and elements in dynamic urban scenes. To
enhance temporally coherent and large scene representation learning with sparse
training data, we introduce a novel temporal smoothing mechanism and a
position-aware adaptive control strategy respectively. Extensive experiments on
Waymo Open Dataset and KITTI benchmarks demonstrate that PVG surpasses
state-of-the-art alternatives in both reconstruction and novel view synthesis
for both dynamic and static scenes. Notably, PVG achieves this without relying
on manually labeled object bounding boxes or expensive optical flow estimation.
Moreover, PVG exhibits 900-fold acceleration in rendering over the best
alternative.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://fudan-zvg.github.io/PVG/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Normalizing flow-based deep variational Bayesian network for seismic
  multi-hazards and impacts estimation from InSAR imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13805v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13805v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuechun Li, Paula M. Burgi, Wei Ma, Hae Young Noh, David J. Wald, Susu Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Onsite disasters like earthquakes can trigger cascading hazards and impacts,
such as landslides and infrastructure damage, leading to catastrophic losses;
thus, rapid and accurate estimates are crucial for timely and effective
post-disaster responses. Interferometric Synthetic aperture radar (InSAR) data
is important in providing high-resolution onsite information for rapid hazard
estimation. Most recent methods using InSAR imagery signals predict a single
type of hazard and thus often suffer low accuracy due to noisy and complex
signals induced by co-located hazards, impacts, and irrelevant environmental
changes (e.g., vegetation changes, human activities). We introduce a novel
stochastic variational inference with normalizing flows derived to jointly
approximate posteriors of multiple unobserved hazards and impacts from noisy
InSAR imagery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper needs to be reviewed by the USGS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty-Aware Source-Free Adaptive Image Super-Resolution with
  Wavelet Augmentation Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17783v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17783v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuang Ai, Xiaoqiang Zhou, Huaibo Huang, Lei Zhang, Ran He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised Domain Adaptation (UDA) can effectively address domain gap
issues in real-world image Super-Resolution (SR) by accessing both the source
and target data. Considering privacy policies or transmission restrictions of
source data in practical scenarios, we propose a SOurce-free Domain Adaptation
framework for image SR (SODA-SR) to address this issue, i.e., adapt a
source-trained model to a target domain with only unlabeled target data.
SODA-SR leverages the source-trained model to generate refined pseudo-labels
for teacher-student learning. To better utilize pseudo-labels, we propose a
novel wavelet-based augmentation method, named Wavelet Augmentation Transformer
(WAT), which can be flexibly incorporated with existing networks, to implicitly
produce useful augmented data. WAT learns low-frequency information of varying
levels across diverse samples, which is aggregated efficiently via deformable
attention. Furthermore, an uncertainty-aware self-training mechanism is
proposed to improve the accuracy of pseudo-labels, with inaccurate predictions
being rectified by uncertainty estimation. To acquire better SR results and
avoid overfitting pseudo-labels, several regularization losses are proposed to
constrain target LR and SR images in the frequency domain. Experiments show
that without accessing source data, SODA-SR outperforms state-of-the-art UDA
methods in both synthetic$\rightarrow$real and real$\rightarrow$real adaptation
settings, and is not constrained by specific network architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Architecture-Agnostic Untrained Network Priors for Image
  Reconstruction with Frequency Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09988v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09988v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilin Liu, Yunkui Pang, Jiang Li, Yong Chen, Pew-Thian Yap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Untrained networks inspired by deep image prior have shown promising
capabilities in recovering a high-quality image from noisy or partial
measurements, without requiring training data. Their success has been widely
attributed to the spectral bias acting as an implicit regularization induced by
suitable network architectures. However, applications of such network-based
priors often entail superfluous architectural decisions, overfitting risks, and
slow optimization, all of which hinder their practicality. In this work, we
propose efficient, architecture-agnostic methods for a more direct frequency
control over the network priors: 1) constraining the bandwidth of the
white-noise input, 2) controlling the bandwidth of the interpolation-based
upsamplers, and 3) regularizing the Lipschitz constants of the layers. We show
that even with just one extra line of code, the overfitting issues in
underperforming architectures can be alleviated such that their performance
gaps with the high-performing counterparts can be largely closed despite their
distinct configurations, mitigating the need for architecture tuning. This then
makes it possible to employ a more compact model to achieve similar or superior
performance to larger models with greater efficiency. Our regularized network
priors compare favorably with current supervised and self-supervised methods on
MRI reconstruction and image inpainting tasks, serving as a stronger zero-shot
baseline reconstructor. Our code will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple Semantic-Aided Few-Shot Learning <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18649v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18649v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai Zhang, Junzhe Xu, Shanlin Jiang, Zhenan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning from a limited amount of data, namely Few-Shot Learning, stands out
as a challenging computer vision task. Several works exploit semantics and
design complicated semantic fusion mechanisms to compensate for rare
representative features within restricted data. However, relying on naive
semantics such as class names introduces biases due to their brevity, while
acquiring extensive semantics from external knowledge takes a huge time and
effort. This limitation severely constrains the potential of semantics in
few-shot learning. In this paper, we design an automatic way called Semantic
Evolution to generate high-quality semantics. The incorporation of high-quality
semantics alleviates the need for complex network structures and learning
algorithms used in previous works. Hence, we employ a simple two-layer network
termed Semantic Alignment Network to transform semantics and visual features
into robust class prototypes with rich discriminative features for few-shot
classification. The experimental results show our framework outperforms all
previous methods on six benchmarks, demonstrating a simple network with
high-quality semantics can beat intricate multi-modal modules on few-shot
classification tasks. Code is available at
https://github.com/zhangdoudou123/SemFew.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMICL: Empowering Vision-language Model with Multi-Modal In-Context
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07915v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07915v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, Baobao Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the resurgence of deep learning, vision-language models (VLMs) enhanced
by large language models (LLMs) have grown exponentially in popularity.
However, while LLMs can utilize extensive background knowledge and task
information with in-context learning, most VLMs still struggle with
understanding complex multi-modal prompts with multiple images, making VLMs
less effective in downstream vision-language tasks. In this paper, we address
the limitation above by 1) introducing vision-language Model with Multi-Modal
In-Context Learning(MMICL), a new approach to allow the VLM to deal with
multi-modal inputs efficiently; 2) proposing a novel context scheme to augment
the in-context learning ability of the VLM; 3) constructing the Multi-modal
In-Context Learning (MIC) dataset, designed to enhance the VLM's ability to
understand complex multi-modal prompts. Our experiments confirm that MMICL
achieves new state-of-the-art zero-shot performance on a wide range of general
vision-language tasks, especially for complex benchmarks, including MME and
MMBench. Our analysis demonstrates that MMICL effectively tackles the challenge
of complex multi-modal prompt understanding and emerges the impressive ICL
ability. Furthermore, we observe that MMICL successfully alleviates language
bias in VLMs, a common issue for VLMs that often leads to hallucination when
faced with extensive textual context. Our code, dataset, dataset tool, and
model are available at https://github.com/PKUnlp-icler/MIC
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Prompt Perceiver: Empower Adaptiveness, Generalizability and
  Fidelity for All-in-One Image Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02918v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02918v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuang Ai, Huaibo Huang, Xiaoqiang Zhou, Jiexiang Wang, Ran He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite substantial progress, all-in-one image restoration (IR) grapples with
persistent challenges in handling intricate real-world degradations. This paper
introduces MPerceiver: a novel multimodal prompt learning approach that
harnesses Stable Diffusion (SD) priors to enhance adaptiveness,
generalizability and fidelity for all-in-one image restoration. Specifically,
we develop a dual-branch module to master two types of SD prompts: textual for
holistic representation and visual for multiscale detail representation. Both
prompts are dynamically adjusted by degradation predictions from the CLIP image
encoder, enabling adaptive responses to diverse unknown degradations. Moreover,
a plug-in detail refinement module improves restoration fidelity via direct
encoder-to-decoder information transformation. To assess our method, MPerceiver
is trained on 9 tasks for all-in-one IR and outperforms state-of-the-art
task-specific methods across most tasks. Post multitask pre-training,
MPerceiver attains a generalized representation in low-level vision, exhibiting
remarkable zero-shot and few-shot capabilities in unseen tasks. Extensive
experiments on 16 IR tasks underscore the superiority of MPerceiver in terms of
adaptiveness, generalizability and fidelity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Auto-Vocabulary Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04539v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04539v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Osman Ülger, Maksymilian Kulicki, Yuki Asano, Martin R. Oswald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-ended image understanding tasks gained significant attention from the
research community, particularly with the emergence of Vision-Language Models.
Open-Vocabulary Segmentation (OVS) methods are capable of performing semantic
segmentation without relying on a fixed vocabulary, and in some cases, they
operate without the need for training or fine-tuning. However, OVS methods
typically require users to specify the vocabulary based on the task or dataset
at hand. In this paper, we introduce \textit{Auto-Vocabulary Semantic
Segmentation (AVS)}, advancing open-ended image understanding by eliminating
the necessity to predefine object categories for segmentation. Our approach,
\ours, presents a framework that autonomously identifies relevant class names
using enhanced BLIP embeddings, which are utilized for segmentation afterwards.
Given that open-ended object category predictions cannot be directly compared
with a fixed ground truth, we develop a Large Language Model-based
Auto-Vocabulary Evaluator (LAVE) to efficiently evaluate the automatically
generated class names and their corresponding segments. Our method sets new
benchmarks on datasets such as PASCAL VOC and Context, ADE20K, and Cityscapes
for AVS and showcases competitive performance to OVS methods that require
specified class names.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoNeS: Conditional neural fields with shift modulation for
  multi-sequence MRI translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.03320v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.03320v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunjie Chen, Marius Staring, Olaf M. Neve, Stephan R. Romeijn, Erik F. Hensen, Berit M. Verbist, Jelmer M. Wolterink, Qian Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-sequence magnetic resonance imaging (MRI) has found wide applications
in both modern clinical studies and deep learning research. However, in
clinical practice, it frequently occurs that one or more of the MRI sequences
are missing due to different image acquisition protocols or contrast agent
contraindications of patients, limiting the utilization of deep learning models
trained on multi-sequence data. One promising approach is to leverage
generative models to synthesize the missing sequences, which can serve as a
surrogate acquisition. State-of-the-art methods tackling this problem are based
on convolutional neural networks (CNN) which usually suffer from spectral
biases, resulting in poor reconstruction of high-frequency fine details. In
this paper, we propose Conditional Neural fields with Shift modulation (CoNeS),
a model that takes voxel coordinates as input and learns a representation of
the target images for multi-sequence MRI translation. The proposed model uses a
multi-layer perceptron (MLP) instead of a CNN as the decoder for pixel-to-pixel
mapping. Hence, each target image is represented as a neural field that is
conditioned on the source image via shift modulation with a learned latent
code. Experiments on BraTS 2018 and an in-house clinical dataset of vestibular
schwannoma patients showed that the proposed method outperformed
state-of-the-art methods for multi-sequence MRI translation both visually and
quantitatively. Moreover, we conducted spectral analysis, showing that CoNeS
was able to overcome the spectral bias issue common in conventional CNN models.
To further evaluate the usage of synthesized images in clinical downstream
tasks, we tested a segmentation network using the synthesized images at
inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org/2024:004</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.16296v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.16296v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifu Wang, Teodora Popordanoska, Jeroen Bertels, Robin Lemmens, Matthew B. Blaschko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The soft Dice loss (SDL) has taken a pivotal role in numerous automated
segmentation pipelines in the medical imaging community. Over the last years,
some reasons behind its superior functioning have been uncovered and further
optimizations have been explored. However, there is currently no implementation
that supports its direct utilization in scenarios involving soft labels. Hence,
a synergy between the use of SDL and research leveraging the use of soft
labels, also in the context of model calibration, is still missing. In this
work, we introduce Dice semimetric losses (DMLs), which (i) are by design
identical to SDL in a standard setting with hard labels, but (ii) can be
employed in settings with soft labels. Our experiments on the public QUBIQ,
LiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels
(e.g. averaging, label smoothing, and knowledge distillation) over hard labels
(e.g. majority voting and random selection). As a result, we obtain superior
Dice scores and model calibration, which supports the wider adoption of DMLs in
practice. The code is available at https://github.com/zifuwanggg/JDTLosses
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Poly Kernel Inception Network for Remote Sensing Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinhao Cai, Qiuxia Lai, Yuwei Wang, Wenguan Wang, Zeren Sun, Yazhou Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection in remote sensing images (RSIs) often suffers from several
increasing challenges, including the large variation in object scales and the
diverse-ranging context. Prior methods tried to address these challenges by
expanding the spatial receptive field of the backbone, either through
large-kernel convolution or dilated convolution. However, the former typically
introduces considerable background noise, while the latter risks generating
overly sparse feature representations. In this paper, we introduce the Poly
Kernel Inception Network (PKINet) to handle the above challenges. PKINet
employs multi-scale convolution kernels without dilation to extract object
features of varying scales and capture local context. In addition, a Context
Anchor Attention (CAA) module is introduced in parallel to capture long-range
contextual information. These two components work jointly to advance the
performance of PKINet on four challenging remote sensing detection benchmarks,
namely DOTA-v1.0, DOTA-v1.5, HRSC2016, and DIOR-R.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by IEEE Conference on Computer Vision and Pattern
  Recognition, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weakly supervised segmentation of intracranial aneurysms using a novel
  3D focal modulation UNet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03001v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03001v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhossein Rasoulian, Arash Harirpoush, Soorena Salari, Yiming Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate identification and quantification of unruptured intracranial
aneurysms (UIAs) is crucial for the risk assessment and treatment of this
cerebrovascular disorder. Current 2D manual assessment on 3D magnetic resonance
angiography (MRA) is suboptimal and time-consuming. In addition, one major
issue in medical image segmentation is the need for large well-annotated data,
which can be expensive to obtain. Techniques that mitigate this requirement,
such as weakly supervised learning with coarse labels are highly desirable. In
the paper, we propose FocalSegNet, a novel 3D focal modulation UNet, to detect
an aneurysm and offer an initial, coarse segmentation of it from time-of-flight
MRA image patches, which is further refined with a dense conditional random
field (CRF) post-processing layer to produce a final segmentation map. We
trained and evaluated our model on a public dataset, and in terms of UIA
detection, our model showed a low false-positive rate of 0.21 and a high
sensitivity of 0.80. For voxel-wise aneurysm segmentation, we achieved a Dice
score of 0.68 and a 95% Hausdorff distance of ~0.95 mm, demonstrating its
strong performance. We evaluated our algorithms against the state-of-the-art 3D
Residual-UNet and Swin-UNETR, and illustrated the superior performance of our
proposed FocalSegNet, highlighting the advantages of employing focal modulation
for this task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ View-Consistent 3D Editing with Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Wang, Xuanyu Yi, Zike Wu, Na Zhao, Long Chen, Hanwang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of 3D Gaussian Splatting (3DGS) has revolutionized 3D editing,
offering efficient, high-fidelity rendering and enabling precise local
manipulations. Currently, diffusion-based 2D editing models are harnessed to
modify multi-view rendered images, which then guide the editing of 3DGS models.
However, this approach faces a critical issue of multi-view inconsistency,
where the guidance images exhibit significant discrepancies across views,
leading to mode collapse and visual artifacts of 3DGS. To this end, we
introduce View-consistent Editing (VcEdit), a novel framework that seamlessly
incorporates 3DGS into image editing processes, ensuring multi-view consistency
in edited guidance images and effectively mitigating mode collapse issues.
VcEdit employs two innovative consistency modules: the Cross-attention
Consistency Module and the Editing Consistency Module, both designed to reduce
inconsistencies in edited images. By incorporating these consistency modules
into an iterative pattern, VcEdit proficiently resolves the issue of multi-view
inconsistency, facilitating high-quality 3DGS editing across a diverse range of
scenes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with
  Non-linear Prediction <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiyi Lv, Yuhang Huang, Ning Zhang, Ruei-Sung Lin, Mei Han, Dan Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Multiple Object Tracking, objects often exhibit non-linear motion of
acceleration and deceleration, with irregular direction changes.
Tacking-by-detection (TBD) trackers with Kalman Filter motion prediction work
well in pedestrian-dominant scenarios but fall short in complex situations when
multiple objects perform non-linear and diverse motion simultaneously. To
tackle the complex non-linear motion, we propose a real-time diffusion-based
MOT approach named DiffMOT. Specifically, for the motion predictor component,
we propose a novel Decoupled Diffusion-based Motion Predictor (D$^2$MP). It
models the entire distribution of various motion presented by the data as a
whole. It also predicts an individual object's motion conditioning on an
individual's historical motion information. Furthermore, it optimizes the
diffusion process with much fewer sampling steps. As a MOT tracker, the DiffMOT
is real-time at 22.7FPS, and also outperforms the state-of-the-art on
DanceTrack and SportsMOT datasets with $62.3\%$ and $76.2\%$ in HOTA metrics,
respectively. To the best of our knowledge, DiffMOT is the first to introduce a
diffusion probabilistic model into the MOT to tackle non-linear motion
prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OSCaR: Object State Captioning and State Change Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17128v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17128v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nguyen Nguyen, Jing Bi, Ali Vosoughi, Yapeng Tian, Pooyan Fazli, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capability of intelligent models to extrapolate and comprehend changes in
object states is a crucial yet demanding aspect of AI research, particularly
through the lens of human interaction in real-world settings. This task
involves describing complex visual environments, identifying active objects,
and interpreting their changes as conveyed through language. Traditional
methods, which isolate object captioning and state change detection, offer a
limited view of dynamic environments. Moreover, relying on a small set of
symbolic words to represent changes has restricted the expressiveness of the
language. To address these challenges, in this paper, we introduce the Object
State Captioning and State Change Representation (OSCaR) dataset and benchmark.
OSCaR consists of 14,084 annotated video segments with nearly 1,000 unique
objects from various egocentric video collections. It sets a new testbed for
evaluating multimodal large language models (MLLMs). Our experiments
demonstrate that while MLLMs show some skill, they lack a full understanding of
object state changes. The benchmark includes a fine-tuned model that, despite
initial capabilities, requires significant improvements in accuracy and
generalization ability for effective understanding of these changes. Our code
and dataset are available at https://github.com/nguyennm1024/OSCaR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Privacy Effect of Data Enhancement via the Lens of Memorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.08270v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.08270v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Li, Qiongxiu Li, Zhanhao Hu, Xiaolin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning poses severe privacy concerns as it has been shown that the
learned models can reveal sensitive information about their training data. Many
works have investigated the effect of widely adopted data augmentation and
adversarial training techniques, termed data enhancement in the paper, on the
privacy leakage of machine learning models. Such privacy effects are often
measured by membership inference attacks (MIAs), which aim to identify whether
a particular example belongs to the training set or not. We propose to
investigate privacy from a new perspective called memorization. Through the
lens of memorization, we find that previously deployed MIAs produce misleading
results as they are less likely to identify samples with higher privacy risks
as members compared to samples with low privacy risks. To solve this problem,
we deploy a recent attack that can capture individual samples' memorization
degrees for evaluation. Through extensive experiments, we unveil several
findings about the connections between three essential properties of machine
learning models, including privacy, generalization gap, and adversarial
robustness. We demonstrate that the generalization gap and privacy leakage are
less correlated than those of the previous results. Moreover, there is not
necessarily a trade-off between adversarial robustness and privacy as stronger
adversarial robustness does not make the model more susceptible to privacy
attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TIFS, 17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Fusion Method with Spatiotemporal Sequences and Relationship
  Learning for Valence-Arousal Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12425v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12425v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Yu, Gongpeng Zhao, Yongqi Wang, Zhihong Wei, Yang Zheng, Zerui Zhang, Zhongpeng Cai, Guochen Xie, Jichao Zhu, Wangyuan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents our approach for the VA (Valence-Arousal) estimation task
in the ABAW6 competition. We devised a comprehensive model by preprocessing
video frames and audio segments to extract visual and audio features. Through
the utilization of Temporal Convolutional Network (TCN) modules, we effectively
captured the temporal and spatial correlations between these features.
Subsequently, we employed a Transformer encoder structure to learn long-range
dependencies, thereby enhancing the model's performance and generalization
ability. Our method leverages a multimodal data fusion approach, integrating
pre-trained audio and video backbones for feature extraction, followed by
TCN-based spatiotemporal encoding and Transformer-based temporal information
capture. Experimental results demonstrate the effectiveness of our approach,
achieving competitive performance in VA estimation on the AffWild2 dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages,3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Surfer: Progressive Reasoning with World Models for Robotic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11335v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11335v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengzhen Ren, Kaidong Zhang, Hetao Zheng, Zixuan Li, Yuhang Wen, Fengda Zhu, Mas Ma, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Considering how to make the model accurately understand and follow natural
language instructions and perform actions consistent with world knowledge is a
key challenge in robot manipulation. This mainly includes human fuzzy
instruction reasoning and the following of physical knowledge. Therefore, the
embodied intelligence agent must have the ability to model world knowledge from
training data. However, most existing vision and language robot manipulation
methods mainly operate in less realistic simulator and language settings and
lack explicit modeling of world knowledge. To bridge this gap, we introduce a
novel and simple robot manipulation framework, called Surfer. It is based on
the world model, treats robot manipulation as a state transfer of the visual
scene, and decouples it into two parts: action and scene. Then, the
generalization ability of the model on new instructions and new scenes is
enhanced by explicit modeling of the action and scene prediction in multi-modal
information. In addition to the framework, we also built a robot manipulation
simulator that supports full physics execution based on the MuJoCo physics
engine. It can automatically generate demonstration training data and test
data, effectively reducing labor costs. To conduct a comprehensive and
systematic evaluation of the robot manipulation model in terms of language
understanding and physical execution, we also created a robotic manipulation
benchmark with progressive reasoning tasks, called SeaWave. It contains 4
levels of progressive reasoning tasks and can provide a standardized testing
platform for embedded AI agents in multi-modal environments. On average, Surfer
achieved a success rate of 54.74% on the defined four levels of manipulation
tasks, exceeding the best baseline performance of 47.64%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Spatiotemporal Inconsistency via Thumbnail Layout for Face
  Deepfake Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10261v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10261v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuting Xu, Jian Liang, Lijun Sheng, Xiao-Yu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deepfake threats to society and cybersecurity have provoked significant
public apprehension, driving intensified efforts within the realm of deepfake
video detection. Current video-level methods are mostly based on {3D CNNs}
resulting in high computational demands, although have achieved good
performance. This paper introduces an elegantly simple yet effective strategy
named Thumbnail Layout (TALL), which transforms a video clip into a pre-defined
layout to realize the preservation of spatial and temporal dependencies. This
transformation process involves sequentially masking frames at the same
positions within each frame. These frames are then resized into sub-frames and
reorganized into the predetermined layout, forming thumbnails. TALL is
model-agnostic and has remarkable simplicity, necessitating only minimal code
modifications. Furthermore, we introduce a graph reasoning block (GRB) and
semantic consistency (SC) loss to strengthen TALL, culminating in TALL++. GRB
enhances interactions between different semantic regions to capture
semantic-level inconsistency clues. The semantic consistency loss imposes
consistency constraints on semantic features to improve model generalization
ability. Extensive experiments on intra-dataset, cross-dataset,
diffusion-generated image detection, and deepfake generation method recognition
show that TALL++ achieves results surpassing or comparable to the
state-of-the-art methods, demonstrating the effectiveness of our approaches for
various deepfake detection problems. The code is available at
https://github.com/rainy-xu/TALL4Deepfake.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJCV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vulnerability analysis of captcha using Deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09389v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09389v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaskaran Singh Walia, Aryan Odugoudar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several websites improve their security and avoid dangerous Internet attacks
by implementing CAPTCHAs (Completely Automated Public Turing test to tell
Computers and Humans Apart), a type of verification to identify whether the
end-user is human or a robot. The most prevalent type of CAPTCHA is text-based,
designed to be easily recognized by humans while being unsolvable towards
machines or robots. However, as deep learning technology progresses,
development of convolutional neural network (CNN) models that predict
text-based CAPTCHAs becomes easier. The purpose of this research is to
investigate the flaws and vulnerabilities in the CAPTCHA generating systems in
order to design more resilient CAPTCHAs. To achieve this, we created CapNet, a
Convolutional Neural Network. The proposed platform can evaluate both numerical
and alphanumerical CAPTCHAs
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analyzing and Improving the Training Dynamics of Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02696v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02696v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, Samuli Laine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models currently dominate the field of data-driven image synthesis
with their unparalleled scaling to large datasets. In this paper, we identify
and rectify several causes for uneven and ineffective training in the popular
ADM diffusion model architecture, without altering its high-level structure.
Observing uncontrolled magnitude changes and imbalances in both the network
activations and weights over the course of training, we redesign the network
layers to preserve activation, weight, and update magnitudes on expectation. We
find that systematic application of this philosophy eliminates the observed
drifts and imbalances, resulting in considerably better networks at equal
computational complexity. Our modifications improve the previous record FID of
2.41 in ImageNet-512 synthesis to 1.81, achieved using fast deterministic
sampling.
  As an independent contribution, we present a method for setting the
exponential moving average (EMA) parameters post-hoc, i.e., after completing
the training run. This allows precise tuning of EMA length without the cost of
performing several training runs, and reveals its surprising interactions with
network architecture, training time, and guidance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Style Injection in Diffusion: A Training-free Approach for Adapting
  Large-scale Diffusion Models for Style Transfer <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09008v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09008v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiwoo Chung, Sangeek Hyun, Jae-Pil Heo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the impressive generative capabilities of diffusion models, existing
diffusion model-based style transfer methods require inference-stage
optimization (e.g. fine-tuning or textual inversion of style) which is
time-consuming, or fails to leverage the generative ability of large-scale
diffusion models. To address these issues, we introduce a novel artistic style
transfer method based on a pre-trained large-scale diffusion model without any
optimization. Specifically, we manipulate the features of self-attention layers
as the way the cross-attention mechanism works; in the generation process,
substituting the key and value of content with those of style image. This
approach provides several desirable characteristics for style transfer
including 1) preservation of content by transferring similar styles into
similar image patches and 2) transfer of style based on similarity of local
texture (e.g. edge) between content and style images. Furthermore, we introduce
query preservation and attention temperature scaling to mitigate the issue of
disruption of original content, and initial latent Adaptive Instance
Normalization (AdaIN) to deal with the disharmonious color (failure to transfer
the colors of style). Our experimental results demonstrate that our proposed
method surpasses state-of-the-art methods in both conventional and
diffusion-based style transfer baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024. Project page:
  https://jiwoogit.github.io/StyleID_site</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Person Identity, Gender and Age Estimation from Hand Images using
  Deep Multi-Task Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.15263v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.15263v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathanael L. Baisa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a multi-task representation learning framework to
jointly estimate the identity, gender and age of individuals from their hand
images for the purpose of criminal investigations since the hand images are
often the only available information in cases of serious crime such as sexual
abuse. We investigate different up-to-date deep learning architectures and
compare their performance for joint estimation of identity, gender and age from
hand images of perpetrators of serious crime. To simplify the age prediction,
we create age groups for the age estimation. We make extensive evaluations and
comparisons of both convolution-based and transformer-based deep learning
architectures on a publicly available 11k hands dataset. Our experimental
analysis shows that it is possible to efficiently estimate not only identity
but also other attributes such as gender and age of suspects jointly from hand
images for criminal investigations, which is crucial in assisting international
police forces in the court to identify and convict abusers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2209.04821</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ iComMa: Inverting 3D Gaussian Splatting for Camera Pose Estimation via
  Comparing and Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Sun, Xuan Wang, Yunfan Zhang, Jie Zhang, Caigui Jiang, Yu Guo, Fei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method named iComMa to address the 6D camera pose estimation
problem in computer vision. Conventional pose estimation methods typically rely
on the target's CAD model or necessitate specific network training tailored to
particular object classes. Some existing methods have achieved promising
results in mesh-free object and scene pose estimation by inverting the Neural
Radiance Fields (NeRF). However, they still struggle with adverse
initializations such as large rotations and translations. To address this
issue, we propose an efficient method for accurate camera pose estimation by
inverting 3D Gaussian Splatting (3DGS). Specifically, a gradient-based
differentiable framework optimizes camera pose by minimizing the residual
between the query image and the rendered image, requiring no training. An
end-to-end matching module is designed to enhance the model's robustness
against adverse initializations, while minimizing pixel-level comparing loss
aids in precise pose estimation. Experimental results on synthetic and complex
real-world data demonstrate the effectiveness of the proposed approach in
challenging conditions and the accuracy of camera pose estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IVAC-P2L: Leveraging Irregular Repetition Priors for Improving Video
  Action Counting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11959v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11959v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Wang, Zhi-Qi Cheng, Youtian Du, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Action Counting (VAC) is crucial in analyzing sports, fitness, and
everyday activities by quantifying repetitive actions in videos. However,
traditional VAC methods have overlooked the complexity of action repetitions,
such as interruptions and the variability in cycle duration. Our research
addresses the shortfall by introducing a novel approach to VAC, called
Irregular Video Action Counting (IVAC). IVAC prioritizes modeling irregular
repetition patterns in videos, which we define through two primary aspects:
Inter-cycle Consistency and Cycle-interval Inconsistency. Inter-cycle
Consistency ensures homogeneity in the spatial-temporal representations of
cycle segments, signifying action uniformity within cycles. Cycle-interval
inconsistency highlights the importance of distinguishing between cycle
segments and intervals based on their inherent content differences. To
encapsulate these principles, we propose a new methodology that includes
consistency and inconsistency modules, supported by a unique pull-push loss
(P2L) mechanism. The IVAC-P2L model applies a pull loss to promote coherence
among cycle segment features and a push loss to clearly distinguish features of
cycle segments from interval segments. Empirical evaluations conducted on the
RepCount dataset demonstrate that the IVAC-P2L model sets a new benchmark in
VAC task performance. Furthermore, the model demonstrates exceptional
adaptability and generalization across various video contents, outperforming
existing models on two additional datasets, UCFRep and Countix, without the
need for dataset-specific optimization. These results confirm the efficacy of
our approach in addressing irregular repetitions in videos and pave the way for
further advancements in video analysis and understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Source code: https://github.com/hwang-cs-ime/IVAC-P2L</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Face Authentication With Separate Loss Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.11427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.11427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anh-Kiet Duong, Hoang-Lan Nguyen, Toan-Thinh Truong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The overall objective of the main project is to propose and develop a system
of facial authentication in unlocking phones or applications in phones using
facial recognition. The system will include four separate architectures: face
detection, face recognition, face spoofing, and classification of closed eyes.
In which, we consider the problem of face recognition to be the most important,
determining the true identity of the person standing in front of the screen
with absolute accuracy is what facial recognition systems need to achieve.
Along with the development of the face recognition problem, the problem of the
anti-fake face is also gradually becoming popular and equally important. Our
goal is to propose and develop two loss functions: LMCot and Double Loss. Then
apply them to the face authentication process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in Vietnamese language</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Impact of Synthetic Images on Morphing Attack Detection Using a Siamese
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09380v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09380v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Tapia, Christoph Busch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper evaluated the impact of synthetic images on Morphing Attack
Detection (MAD) using a Siamese network with a semi-hard-loss function. Intra
and cross-dataset evaluations were performed to measure synthetic image
generalisation capabilities using a cross-dataset for evaluation. Three
different pre-trained networks were used as feature extractors from traditional
MobileNetV2, MobileNetV3 and EfficientNetB0. Our results show that MAD trained
on EfficientNetB0 from FERET, FRGCv2, and FRLL can reach a lower error rate in
comparison with SOTA. Conversely, worse performances were reached when the
system was trained only with synthetic images. A mixed approach (synthetic +
digital) database may help to improve MAD and reduce the error rate. This fact
shows that we still need to keep going with our efforts to include synthetic
images in the training process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Arxiv version of CIARP2023 - fixed typo errors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Immunohistochemistry guided segmentation of benign epithelial cells, in
  situ lesions, and invasive epithelial cells in breast cancer slides 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13261v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13261v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maren Høibø, André Pedersen, Vibeke Grotnes Dale, Sissel Marie Berget, Borgny Ytterhus, Cecilia Lindskog, Elisabeth Wik, Lars A. Akslen, Ingerid Reinertsen, Erik Smistad, Marit Valla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital pathology enables automatic analysis of histopathological sections
using artificial intelligence (AI). Automatic evaluation could improve
diagnostic efficiency and help find associations between morphological features
and clinical outcome. For development of such prediction models, identifying
invasive epithelial cells, and separating these from benign epithelial cells
and in situ lesions would be the first step. In this study, we aimed to develop
an AI model for segmentation of epithelial cells in sections from breast
cancer. We generated epithelial ground truth masks by restaining hematoxylin
and eosin (HE) sections with cytokeratin (CK) AE1/AE3, and by pathologists'
annotations. HE/CK image pairs were used to train a convolutional neural
network, and data augmentation was used to make the model more robust. Tissue
microarrays (TMAs) from 839 patients, and whole slide images from two patients
were used for training and evaluation of the models. The sections were derived
from four cohorts of breast cancer patients. TMAs from 21 patients from a fifth
cohort was used as a second test set. In quantitative evaluation, a mean Dice
score of 0.70, 0.79, and 0.75 for invasive epithelial cells, benign epithelial
cells, and in situ lesions, respectively, were achieved. In qualitative scoring
(0-5) by pathologists, results were best for all epithelium and invasive
epithelium, with scores of 4.7 and 4.4. Scores for benign epithelium and in
situ lesions were 3.7 and 2.0. The proposed model segmented epithelial cells in
HE stained breast cancer slides well, but further work is needed for accurate
division between the classes. Immunohistochemistry, together with pathologists'
annotations, enabled the creation of accurate ground truths. The model is made
freely available in FastPathology and the code is available at
https://github.com/AICAN-Research/breast-epithelium-segmentation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures. Submitted to a scientific journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoST: Motion Style Transformer between Diverse Action Contents <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06225v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06225v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boeun Kim, Jungho Kim, Hyung Jin Chang, Jin Young Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While existing motion style transfer methods are effective between two
motions with identical content, their performance significantly diminishes when
transferring style between motions with different contents. This challenge lies
in the lack of clear separation between content and style of a motion. To
tackle this challenge, we propose a novel motion style transformer that
effectively disentangles style from content and generates a plausible motion
with transferred style from a source motion. Our distinctive approach to
achieving the goal of disentanglement is twofold: (1) a new architecture for
motion style transformer with `part-attentive style modulator across body
parts' and `Siamese encoders that encode style and content features
separately'; (2) style disentanglement loss. Our method outperforms existing
methods and demonstrates exceptionally high quality, particularly in motion
pairs with different contents, without the need for heuristic post-processing.
Codes are available at https://github.com/Boeun-Kim/MoST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AttriCLIP: A Non-Incremental Learner for Incremental Knowledge Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11488v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11488v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runqi Wang, Xiaoyue Duan, Guoliang Kang, Jianzhuang Liu, Shaohui Lin, Songcen Xu, Jinhu Lv, Baochang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning aims to enable a model to incrementally learn knowledge
from sequentially arrived data. Previous works adopt the conventional
classification architecture, which consists of a feature extractor and a
classifier. The feature extractor is shared across sequentially arrived tasks
or classes, but one specific group of weights of the classifier corresponding
to one new class should be incrementally expanded. Consequently, the parameters
of a continual learner gradually increase. Moreover, as the classifier contains
all historical arrived classes, a certain size of the memory is usually
required to store rehearsal data to mitigate classifier bias and catastrophic
forgetting. In this paper, we propose a non-incremental learner, named
AttriCLIP, to incrementally extract knowledge of new classes or tasks.
Specifically, AttriCLIP is built upon the pre-trained visual-language model
CLIP. Its image encoder and text encoder are fixed to extract features from
both images and text. Text consists of a category name and a fixed number of
learnable parameters which are selected from our designed attribute word bank
and serve as attributes. As we compute the visual and textual similarity for
classification, AttriCLIP is a non-incremental learner. The attribute prompts,
which encode the common knowledge useful for classification, can effectively
mitigate the catastrophic forgetting and avoid constructing a replay memory. We
evaluate our AttriCLIP and compare it with CLIP-based and previous
state-of-the-art continual learning methods in realistic settings with
domain-shift and long-sequence learning. The results show that our method
performs favorably against previous state-of-the-arts. The implementation code
can be available at https://github.com/bhrqw/AttriCLIP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Camera Height Doesn't Change: Unsupervised Training for Metric Monocular
  Road-Scene Depth Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04530v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04530v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genki Kinoshita, Ko Nishino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel training method for making any monocular
depth network learn absolute scale and estimate metric road-scene depth just
from regular training data, i.e., driving videos. We refer to this training
framework as StableCamH. The key idea is to leverage cars found on the road as
sources of scale supervision but to incorporate them in the training robustly.
StableCamH detects and estimates the sizes of cars in the frame and aggregates
scale information extracted from them into a camera height estimate whose
consistency across the entire video sequence is enforced as scale supervision.
This realizes robust unsupervised training of any, otherwise scale-oblivious,
monocular depth network to become not only scale-aware but also metric-accurate
without the need for auxiliary sensors and extra supervision. Extensive
experiments on the KITTI and Cityscapes datasets show the effectiveness of
StableCamH and its state-of-the-art accuracy compared with related methods. We
also show that StableCamH enables training on mixed datasets of different
camera heights, which leads to larger-scale training and thus higher
generalization. Metric depth reconstruction is essential in any road-scene
visual modeling, and StableCamH democratizes its deployment by establishing the
means to train any model as a metric depth estimator.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ End-to-end Learned Visual <span class="highlight-title">Odometry</span> with Events and Frames 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09947v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09947v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roberto Pellerito, Marco Cannici, Daniel Gehrig, Joris Belhadj, Olivier Dubois-Matra, Massimo Casasco, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Odometry (VO) is crucial for autonomous robotic navigation, especially
in GPS-denied environments like planetary terrains. To improve robustness,
recent model-based VO systems have begun combining standard and event-based
cameras. Event cameras excel in low-light and high-speed motion, while standard
cameras provide dense and easier-to-track features, even in low-textured areas.
However, the field of image- and event-based VO still predominantly relies on
model-based methods and is yet to fully integrate recent image-only
advancements leveraging end-to-end learning-based architectures. Seamlessly
integrating the two modalities remains challenging due to their different
nature, one asynchronous, the other not, limiting the potential for a more
effective image- and event-based VO. We introduce RAMP-VO, the first end-to-end
learned image- and event-based VO system. It leverages novel Recurrent,
Asynchronous, and Massively Parallel (RAMP) encoders capable of fusing
asynchronous events with image data, providing 8x faster inference and 33% more
accurate predictions than existing solutions. Despite being trained only in
simulation, RAMP-VO outperforms image- and event-based methods by 46% and 60%,
respectively, on traditional, real-world benchmarks as well as newly introduced
Apollo and Malapert landing sequences, paving the way for robust and
asynchronous VO in space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QUASAR: QUality and Aesthetics Scoring with Advanced Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06866v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06866v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergey Kastryulin, Denis Prokopenko, Artem Babenko, Dmitry V. Dylov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a new data-driven, non-parametric method for image
quality and aesthetics assessment, surpassing existing approaches and requiring
no prompt engineering or fine-tuning. We eliminate the need for expressive
textual embeddings by proposing efficient image anchors in the data. Through
extensive evaluations of 7 state-of-the-art self-supervised models, our method
demonstrates superior performance and robustness across various datasets and
benchmarks. Notably, it achieves high agreement with human assessments even
with limited data and shows high robustness to the nature of data and their
pre-processing pipeline. Our contributions offer a streamlined solution for
assessment of images while providing insights into the perception of visual
information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Hybrid Transformer-Sequencer approach for Age and Gender
  classification from in-wild facial images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12483v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12483v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aakash Singh, Vivek Kumar Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancements in computer vision and image processing techniques have led
to emergence of new application in the domain of visual surveillance, targeted
advertisement, content-based searching, and human-computer interaction etc. Out
of the various techniques in computer vision, face analysis, in particular, has
gained much attention. Several previous studies have tried to explore different
applications of facial feature processing for a variety of tasks, including age
and gender classification. However, despite several previous studies having
explored the problem, the age and gender classification of in-wild human faces
is still far from the achieving the desired levels of accuracy required for
real-world applications. This paper, therefore, attempts to bridge this gap by
proposing a hybrid model that combines self-attention and BiLSTM approaches for
age and gender classification problems. The proposed models performance is
compared with several state-of-the-art model proposed so far. An improvement of
approximately 10percent and 6percent over the state-of-the-art implementations
for age and gender classification, respectively, are noted for the proposed
model. The proposed model is thus found to achieve superior performance and is
found to provide a more generalized learning. The model can, therefore, be
applied as a core classification component in various image processing and
computer vision problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic
  Autonomous Driving Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07920v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07920v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Zhou, Zhiwei Lin, Xiaojun Shan, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DrivingGaussian, an efficient and effective framework for
surrounding dynamic autonomous driving scenes. For complex scenes with moving
objects, we first sequentially and progressively model the static background of
the entire scene with incremental static 3D Gaussians. We then leverage a
composite dynamic Gaussian graph to handle multiple moving objects,
individually reconstructing each object and restoring their accurate positions
and occlusion relationships within the scene. We further use a LiDAR prior for
Gaussian Splatting to reconstruct scenes with greater details and maintain
panoramic consistency. DrivingGaussian outperforms existing methods in dynamic
driving scene reconstruction and enables photorealistic surround-view synthesis
with high-fidelity and multi-camera consistency. Our project page is at:
https://github.com/VDIGPKU/DrivingGaussian.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of
  Diffusion Probabilistic Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.10711v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.10711v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiachun Pan, Jun Hao Liew, Vincent Y. F. Tan, Jiashi Feng, Hanshu Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing customization methods require access to multiple reference examples
to align pre-trained diffusion probabilistic models (DPMs) with user-provided
concepts. This paper aims to address the challenge of DPM customization when
the only available supervision is a differentiable metric defined on the
generated contents. Since the sampling procedure of DPMs involves recursive
calls to the denoising UNet, na\"ive gradient backpropagation requires storing
the intermediate states of all iterations, resulting in extremely high memory
consumption. To overcome this issue, we propose a novel method AdjointDPM,
which first generates new samples from diffusion models by solving the
corresponding probability-flow ODEs. It then uses the adjoint sensitivity
method to backpropagate the gradients of the loss to the models' parameters
(including conditioning signals, network weights, and initial noises) by
solving another augmented ODE. To reduce numerical errors in both the forward
generation and gradient backpropagation processes, we further reparameterize
the probability-flow ODE and augmented ODE as simple non-stiff ODEs using
exponential integration. Finally, we demonstrate the effectiveness of
AdjointDPM on three interesting tasks: converting visual effects into
identification text embeddings, finetuning DPMs for specific types of
stylization, and optimizing initial noise to generate adversarial samples for
security auditing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KP-RED: Exploiting Semantic Keypoints for Joint 3D Shape Retrieval and
  Deformation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruida Zhang, Chenyangguang Zhang, Yan Di, Fabian Manhardt, Xingyu Liu, Federico Tombari, Xiangyang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present KP-RED, a unified KeyPoint-driven REtrieval and
Deformation framework that takes object scans as input and jointly retrieves
and deforms the most geometrically similar CAD models from a pre-processed
database to tightly match the target. Unlike existing dense matching based
methods that typically struggle with noisy partial scans, we propose to
leverage category-consistent sparse keypoints to naturally handle both full and
partial object scans. Specifically, we first employ a lightweight retrieval
module to establish a keypoint-based embedding space, measuring the similarity
among objects by dynamically aggregating deformation-aware local-global
features around extracted keypoints. Objects that are close in the embedding
space are considered similar in geometry. Then we introduce the neural
cage-based deformation module that estimates the influence vector of each
keypoint upon cage vertices inside its local support region to control the
deformation of the retrieved shape. Extensive experiments on the synthetic
dataset PartNet and the real-world dataset Scan2CAD demonstrate that KP-RED
surpasses existing state-of-the-art approaches by a large margin. Codes and
trained models will be released in https://github.com/lolrudy/KP-RED.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Produce Semi-dense Correspondences for Visual Localization <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08359v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08359v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khang Truong Giang, Soohwan Song, Sungho Jo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the challenge of performing visual localization in
demanding conditions such as night-time scenarios, adverse weather, and
seasonal changes. While many prior studies have focused on improving
image-matching performance to facilitate reliable dense keypoint matching
between images, existing methods often heavily rely on predefined feature
points on a reconstructed 3D model. Consequently, they tend to overlook
unobserved keypoints during the matching process. Therefore, dense keypoint
matches are not fully exploited, leading to a notable reduction in accuracy,
particularly in noisy scenes. To tackle this issue, we propose a novel
localization method that extracts reliable semi-dense 2D-3D matching points
based on dense keypoint matches. This approach involves regressing semi-dense
2D keypoints into 3D scene coordinates using a point inference network. The
network utilizes both geometric and visual cues to effectively infer 3D
coordinates for unobserved keypoints from the observed ones. The abundance of
matching information significantly enhances the accuracy of camera pose
estimation, even in scenarios involving noisy or sparse 3D models.
Comprehensive evaluations demonstrate that the proposed method outperforms
other methods in challenging scenes and achieves competitive results in
large-scale visual localization benchmarks. The code will be available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Genixer: Empowering Multimodal Large Language Models as a Powerful Data
  Generator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06731v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06731v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henry Hengyuan Zhao, Pan Zhou, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning data is essential for training the Multimodal Large
Language Models (MLLMs). However, the creation of high-quality instruction
tuning data presents significant challenges. Prior methods that depended on
GPT-4 for data generation were not only costly but also lacked satisfactory
performance in complex tasks (i.e., grounding-based reasoning tasks). To
address these issues, we developed an innovative data generation pipeline,
Genixer, to generate various high-quality instruction tuning data, including
nine representative tasks, e.g., Common VQA, REC, REG, and PointQ.
Specifically, Genixer provides a unified solution with four key steps for
alleviating the difficulty of data generation: (i) instruction data collection,
(ii) instruction template design, (iii) empowering MLLM, and (iv) data
generation and filtering. Subsequently, the superior qualitative results of our
Genixer demonstrate that current MLLMs have a strong potential to evolve into
powerful data generators. Additionally, to validate the efficacy of generated
data quantitatively, we add the instruction tuning data produced by Genixer
into the training of two representative MLLMs and observe the consistent
improvements on various VQA tasks and multimodal benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modality-missing RGBT Tracking: Invertible Prompt Learning and
  High-quality Benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16244v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16244v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andong Lu, Jiacong Zhao, Chenglong Li, Jin Tang, Bin Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current RGBT tracking research relies on the complete multi-modal input, but
modal information might miss due to some factors such as thermal sensor
self-calibration and data transmission error, called modality-missing challenge
in this work. To address this challenge, we propose a novel invertible prompt
learning approach, which integrates the content-preserving prompts into a
well-trained tracking model to adapt to various modality-missing scenarios, for
robust RGBT tracking. Given one modality-missing scenario, we propose to
utilize the available modality to generate the prompt of the missing modality
to adapt to RGBT tracking model. However, the cross-modality gap between
available and missing modalities usually causes semantic distortion and
information loss in prompt generation. To handle this issue, we design the
invertible prompter by incorporating the full reconstruction of the input
available modality from the generated prompt. To provide a comprehensive
evaluation platform, we construct several high-quality benchmark datasets, in
which various modality-missing scenarios are considered to simulate real-world
challenges. Extensive experiments on three modality-missing benchmark datasets
show that our method achieves significant performance improvements compared
with state-of-the-art methods. We have released the code and simulation
datasets at:
\href{https://github.com/Alexadlu/Modality-missing-RGBT-Tracking.git}{https://github.com/Alexadlu/Modality-missing-RGBT-Tracking.git}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EAGLE: Eigen Aggregation Learning for Object-Centric Unsupervised
  Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanyoung Kim, Woojung Han, Dayun Ju, Seong Jae Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation has innately relied on extensive pixel-level annotated
data, leading to the emergence of unsupervised methodologies. Among them,
leveraging self-supervised Vision Transformers for unsupervised semantic
segmentation (USS) has been making steady progress with expressive deep
features. Yet, for semantically segmenting images with complex objects, a
predominant challenge remains: the lack of explicit object-level semantic
encoding in patch-level features. This technical limitation often leads to
inadequate segmentation of complex objects with diverse structures. To address
this gap, we present a novel approach, EAGLE, which emphasizes object-centric
representation learning for unsupervised semantic segmentation. Specifically,
we introduce EiCue, a spectral technique providing semantic and structural cues
through an eigenbasis derived from the semantic similarity matrix of deep image
features and color affinity from an image. Further, by incorporating our
object-centric contrastive loss with EiCue, we guide our model to learn
object-level representations with intra- and inter-image object-feature
consistency, thereby enhancing semantic accuracy. Extensive experiments on
COCO-Stuff, Cityscapes, and Potsdam-3 datasets demonstrate the state-of-the-art
USS results of EAGLE with accurate and consistent semantic segmentation across
complex scenes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Effective Multiple-in-One Image Restoration: A Sequential and
  Prompt Learning Strategy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03379v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03379v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangtao Kong, Chao Dong, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While single task image restoration (IR) has achieved significant successes,
it remains a challenging issue to train a single model which can tackle
multiple IR tasks. In this work, we investigate in-depth the multiple-in-one
(MiO) IR problem, which comprises seven popular IR tasks. We point out that MiO
IR faces two pivotal challenges: the optimization of diverse objectives and the
adaptation to multiple tasks. To tackle these challenges, we present two simple
yet effective strategies. The first strategy, referred to as sequential
learning, attempts to address how to optimize the diverse objectives, which
guides the network to incrementally learn individual IR tasks in a sequential
manner rather than mixing them together. The second strategy, i.e., prompt
learning, attempts to address how to adapt to the different IR tasks, which
assists the network to understand the specific task and improves the
generalization ability. By evaluating on 19 test sets, we demonstrate that the
sequential and prompt learning strategies can significantly enhance the MiO
performance of commonly used CNN and Transformer backbones. Our experiments
also reveal that the two strategies can supplement each other to learn better
degradation representations and enhance the model robustness. It is expected
that our proposed MiO IR formulation and strategies could facilitate the
research on how to train IR models with higher generalization capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Posterior Distillation Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13831v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13831v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juil Koo, Chanho Park, Minhyuk Sung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Posterior Distillation Sampling (PDS), a novel optimization
method for parametric image editing based on diffusion models. Existing
optimization-based methods, which leverage the powerful 2D prior of diffusion
models to handle various parametric images, have mainly focused on generation.
Unlike generation, editing requires a balance between conforming to the target
attribute and preserving the identity of the source content. Recent 2D image
editing methods have achieved this balance by leveraging the stochastic latent
encoded in the generative process of diffusion models. To extend the editing
capabilities of diffusion models shown in pixel space to parameter space, we
reformulate the 2D image editing method into an optimization form named PDS.
PDS matches the stochastic latents of the source and the target, enabling the
sampling of targets in diverse parameter spaces that align with a desired
attribute while maintaining the source's identity. We demonstrate that this
optimization resembles running a generative process with the target attribute,
but aligning this process with the trajectory of the source's generative
process. Extensive editing results in Neural Radiance Fields and Scalable
Vector Graphics representations demonstrate that PDS is capable of sampling
targets to fulfill the aforementioned balance across various parameter spaces.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://posterior-distillation-sampling.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StyleHumanCLIP: Text-guided Garment Manipulation for StyleGAN-Human 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16759v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16759v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takato Yoshikawa, Yuki Endo, Yoshihiro Kanamori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper tackles text-guided control of StyleGAN for editing garments in
full-body human images. Existing StyleGAN-based methods suffer from handling
the rich diversity of garments and body shapes and poses. We propose a
framework for text-guided full-body human image synthesis via an
attention-based latent code mapper, which enables more disentangled control of
StyleGAN than existing mappers. Our latent code mapper adopts an attention
mechanism that adaptively manipulates individual latent codes on different
StyleGAN layers under text guidance. In addition, we introduce feature-space
masking at inference time to avoid unwanted changes caused by text inputs. Our
quantitative and qualitative evaluations reveal that our method can control
generated images more faithfully to given texts than existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>VISIAPP 2024, project page:
  https://www.cgg.cs.tsukuba.ac.jp/~yoshikawa/pub/style_human_clip/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MEDBind: Unifying Language and Multimodal Medical Data Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12894v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12894v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Gao, Sangwook Kim, David E Austin, Chris McIntosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical vision-language pretraining models (VLPM) have achieved remarkable
progress in fusing chest X-rays (CXR) with clinical texts, introducing
image-text data binding approaches that enable zero-shot learning and
downstream clinical tasks. However, the current landscape lacks the holistic
integration of additional medical modalities, such as electrocardiograms (ECG).
We present MEDBind (Medical Electronic patient recorD), which learns joint
embeddings across CXR, ECG, and medical text. Using text data as the central
anchor, MEDBind features tri-modality binding, delivering competitive
performance in top-K retrieval, zero-shot, and few-shot benchmarks against
established VLPM, and the ability for CXR-to-ECG zero-shot classification and
retrieval. This seamless integration is achieved through combination of
contrastive loss on modality-text pairs with our proposed contrastive loss
function, Edge-Modality Contrastive Loss, fostering a cohesive embedding space
for CXR, ECG, and text. Finally, we demonstrate that MEDBind can improve
downstream tasks by directly integrating CXR and ECG embeddings into a
large-language model for multimodal prompt tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SALAD: Part-Level Latent Diffusion for 3D Shape Generation and
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12236v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12236v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juil Koo, Seungwoo Yoo, Minh Hieu Nguyen, Minhyuk Sung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a cascaded diffusion model based on a part-level implicit 3D
representation. Our model achieves state-of-the-art generation quality and also
enables part-level shape editing and manipulation without any additional
training in conditional setup. Diffusion models have demonstrated impressive
capabilities in data generation as well as zero-shot completion and editing via
a guided reverse process. Recent research on 3D diffusion models has focused on
improving their generation capabilities with various data representations,
while the absence of structural information has limited their capability in
completion and editing tasks. We thus propose our novel diffusion model using a
part-level implicit representation. To effectively learn diffusion with
high-dimensional embedding vectors of parts, we propose a cascaded framework,
learning diffusion first on a low-dimensional subspace encoding extrinsic
parameters of parts and then on the other high-dimensional subspace encoding
intrinsic attributes. In the experiments, we demonstrate the outperformance of
our method compared with the previous ones both in generation and part-level
completion and manipulation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://salad3d.github.io</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-03-28T05:24:29.553247313Z">
            2024-03-28 05:24:29 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
